@article{pub.1000538141,
 abstract = {The strength of appearance-based mapping models for mobile robots lies in their ability to represent the environment through high-level image features and to provide human-readable information. However, developing a mapping and a localization method using these kinds of models is very challenging, especially if robots must deal with long-term mapping, localization, navigation, occlusions, and dynamic environments. In other words, the mobile robot has to deal with environmental appearance change, which modifies its representation of the environment. This paper proposes an indoor appearance-based mapping and a localization method for mobile robots based on the human memory model, which was used to build a Feature Stability Histogram (FSH) at each node in the robot topological map. This FSH registers local feature stability over time through a voting scheme, and the most stable features were considered for mapping, for Bayesian localization and for incrementally updating the current appearance reference view in the topological map. The experimental results are presented using an omnidirectional images dataset acquired over the long-term and considering: illumination changes (time of day, different seasons), occlusions, random removal of features, and perceptual aliasing. The results include a comparison with the approach proposed by Dayoub and Duckett (2008)Â [19] and the popular Bag-of-Words (Bazeille and Filliat, 2010)Â [35] approach. The obtained results confirm the viability of our method and indicate that it can adapt the internal map representation over time to localize the robot both globally and locally.},
 author = {Bacca, B. and Salvi, J. and CufÃ­, X.},
 doi = {10.1016/j.robot.2011.06.008},
 journal = {Robotics and Autonomous Systems},
 keywords = {},
 number = {10},
 pages = {840-857},
 title = {Appearance-based mapping and localization for mobile robots using a feature stability histogram},
 url = {https://app.dimensions.ai/details/publication/pub.1000538141},
 volume = {59},
 year = {2011}
}

@inbook{pub.1000747008,
 abstract = {Vision-based SLAM is mostly a solved problem providing clear, sharp images can be obtained. However, in outdoor environments a number of factors such as rough terrain, high speeds and hardware limitations can result in these conditions not being met. High speed transit on rough terrain can lead to image blur and under/over exposure, problems that cannot easily be dealt with using low cost hardware. Furthermore, recently there has been a growth in interest in lifelong autonomy for robots, which brings with it the challenge in outdoor environments of dealing with a moving sun and lack of constant artificial lighting. In this paper, we present a lightweight approach to visual localization and visual odometry that addresses the challenges posed by perceptual change and low cost cameras. The approach combines low resolution imagery with the SLAM algorithm, RatSLAM. We test the system using a cheap consumer camera mounted on a small vehicle in a mixed urban and vegetated environment, at times ranging from dawn to dusk and in conditions ranging from sunny weather to rain. We first show that the system is able to provide reliable mapping and recall over the course of the day and incrementally incorporate new visual scenes from different times into an existing map. We then restrict the system to only learning visual scenes at one time of day, and show that the system is still able to localize and map at other times of day. The results demonstrate the viability of the approach in situations where image quality is poor and environmental or hardware factors preclude the use of visual features.},
 author = {Milford, Michael and George, Ashley},
 booktitle = {Field and Service Robotics},
 doi = {10.1007/978-3-642-40686-7_38},
 keywords = {},
 note = {https://eprints.qut.edu.au/59841/24/fsr2012_camera_ready_%282%29.pdf},
 pages = {569-583},
 publisher = {},
 title = {Featureless Visual Processing for SLAM in Changing Outdoor Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1000747008},
 year = {2014}
}

@article{pub.1002410606,
 abstract = {Early treatment of acute hepatitis C virus (HCV) infection with interferon alfa monotherapy is highly effective but is associated with frequent unfavorable side effects. There is no fully published study yet exploring the safety, efficacy and required treatment duration of interferonâ€free treatment of acute hepatitis C virus monoinfection. Preliminary reports suggested that ledipasvir/sofosbuvir therapy is effective in acute hepatitis C but relapses were reported in HIVâ€coinfected patients after 6 weeks of treatment. The German HepNet Acute HCV IV Study was designed as a singleâ€arm, prospective multicenter pilot study to evaluate the efficacy and safety of treatment with sofosbuvir plus ledipasvir (SOF/LDV) for 6 weeks without ribavirin in patients with acute genotype 1 HCV monoinfection. We here report the final 24 weeksâ€™ postâ€treatment results. Twenty patients were included by 10 centers (60% male, mean age 46 Â± 12 years; 11 patients HCV genotype 1a, 9 patients genotype 1b). The main risk factors for HCV infection were sexual transmission (n = 11) and medical procedures/needle stick injuries (n = 5). Median alanine aminotransferase (ALT) and median bilirubin levels before start of antiviral treatment were 225 U/l (range 32â€“2716) and 13.6 Âµmol/l (range 5.13â€“111), respectively. ALT levels rapidly declined during therapy and values normalized already by treatment weeks 2 in nine patients and by week 4 in 17 patients. HCV RNA was undetectable by the Roche COBASÂ® AmpliPrep/COBASÂ® TaqManÂ® HCV Test v2.0 by weeks 2, 4 and 6 in eight, 13, and 20 patients, respectively. SVRâ€12 was 100% and 19 patients have completed FUâ€week 24 and all remained HCVâ€RNA negative. One patient was lost to followâ€up at week 24 post treatment. Treatment for 6 weeks with LDV/SOF was well tolerated and highly effective in HCV genotype 1 monoinfected patients with acute hepatitis C. Virological response was durable after therapy for at least 24 weeks. A rapid improvement in biochemical disease activity was observed during therapy. Shortâ€duration treatment of acute hepatitis C could prevent the spread of HCV in highâ€risk populations and may be costâ€saving as compared with treatment of chronic hepatitis C. Katja Deterding: Lecturer fees and travel grants from Gilead, MSD/Merck and AbbVie Direct antiviral agents (DAA) showed very good results in terms of efficacy and safety in clinical trials,1 but realâ€life data are still needed in order to confirm this profile.2 In Romania, through a governamental programme, approx. 5000 patients with virus C compensated liver cirrhosis will receive reimbursed DAA with Paritaprevir/Ombitasvir/ritonavir, Dasabuvir with Ribavirin for 12 weeks during 2015â€“2016. We analysed a national prospective cohort enrolling 654 patients who started the therapy in December 2015â€“January 2016. All patients had genotype 1b. The only key inclusion criteria was Childâ€Pugh score â©½6. Only serious adverse events leading to discontinuation of therapy were reported. Data were obtained from the National Health Insurance House. Efficacy was assessed by the percentage of patients achieving SVR (HCV RNA undetectable) 12 weeks postâ€treatment (SVR12). Ordinal and scale variables with nonâ€normal distribution were summarized as median (min, max), and compared by Mannâ€“Whitney U test, while categorical variables were summarized as number (%) and compared by Fisher exact test. Two patients were lost to followâ€up because of adverse events probably not related to therapy, eight stopped the treatment because of hepatic decompensation (1.2%), one patient stopped because of severe arrythmia (drugâ€“drug interaction involved). This cohort was 46% females, mean age 58.16 years (35â€“79), 67% preâ€treated with Pegâ€Interferon+ Ribavirin, 70% associated NASH, 72% with severe necroâ€inflammatory activity (severity score 3â€ Fibromax), 30% with coâ€morbidities, 11.8% with Child Pugh A 6 points, 10 with virus B coâ€infection (all of them with HBVâ€DNA viral load below 20 IU/ml). The mean MELD (model for end stage liver disease) score was 6.15 Â± 3.01 (0.5â€“20). SVR was reported in 641/644 (99.53%), 3/644 relapsed. Due to the small sample of patients it was not possible to identify predictive factors for viral response. Liver decompensation was statistically associated with higher BMI (body mass index) (p = 0.029), higher bilirubin (p = 0.001), higher MELD score (p = 0.001) and lower platelet count (p = 0.005). Paritaprevir/Ombitasvir/ritonavir, Dasabuvir with Ribavirin proved to be highly efficient in our population of cirrhotics with a 99.53% SVR. Serious adverse events related to therapy were reported in 9/641 (1.4%), most of them liver decompensation (1.2%), for which we identified the following risk factors: higher IMC, higher bilirubin, higher MELD score and lower platelet count. *These data are the property of the Romanian government. Single port laparoscopic surgery as an alternative to standard laparoscopy in patients undergoing cholecystectomy for benign disease has not yet been accepted as standard procedure. The aim of the MUltiâ€port versus SIngleâ€port Cholecystectomy (MUSIC) trial was to compare single access (SPC) with standard laparoscopy (MPC) in terms of morbidity. A nonâ€inferiority phase 3 trial was undertaken at 20 centres and hospitals in six countries. Patients requiring cholecystectomy were randomly assigned to either SPC or MPC, in each centre. Primary outcome was overall morbidity within 60 days from surgery. Analysis was by intention to treat. The study was registered with ClinicalTrials.gov, number NCT01104727. The study was undertaken between April 2011 and May 2015. Six hundred patients were randomly assigned to SPC (n = 297) and MPC (n = 303), and were eligible for analyses. Thirteen patients (4.7%) in the SPC group experienced complications within 60 days after surgery, compared with 16 (6.1%) in the MPC group (p = 0.468); however, single access procedures took longer (70 min [25â€“265] vs. 55 min [22â€“185]; p < 0.001). No difference in hospital stay and perception of pain was observed. Six patients experienced an incisional hernia at 1 year in the SPC group, compared with three in the MPC group (p = 0.331). While patients better appreciated their cosmetic results in the SPC group, plastic surgeons were in favour of the cosmetic results in the MPC group. No difference in Quality of Life score was assessed by GIQLI at 60 days was observed. In selected patients with benign gall bladder disease requiring cholecystectomy, single access technique is nonâ€inferior to standard laparoscopy in terms of safety, despite a longer operative time required. While concerns about possible increase of risk of incisional hernias following SPC are not justified, cosmetic results were perceived better in the single access technique. All authors have declared no conflicts of interest. The influence of comorbidity on haemorrhagic complications in cholecystectomy is insufficiently known. Furthermore, the need for cessation of prescription drugs in order to reduce haemorrhagic complications is still a matter of debate. All cholecystectomies registered in the Swedish populationâ€based Register for Gallstone Surgery and ERCP (GallRiks) were identified. The effect of comorbidity and prescribed drugs on bleeding was assessed by linking data in the GallRiks to the National Patient Register and the Prescribed Drug Register respectively. The risk for haemorrhage leading to intervention was determined by variable regression, and Kaplanâ€“Meier analysis assessed survival rate following perioperative haemorrhage. A total of 94,557 patients were included between 2005 and 2015, of which 799 (0.8%) and 1192 (1.3%) patients were registered as having perioperative and postoperative haemorrhage, respectively. An increased risk for haemorrhagic complications was seen in patients with kidney disease, previous myocardial infarction, heart failure, cerebrovascular disease and obesity (all p < 0.05). Prescription of tricyclic antidepressant or dipyridamole was associated with a significantly increased risk for perioperative haemorrhage (p < 0.05). However, this increase in risk did not remain significant following Bonferroni correction for mass significance. Perioperative haemorrhage increased the risk of death occurring within the first postoperative year [Hazard Ratio, (HR): 4.9, CI: 3.52â€“6.93] as well as bile duct injury (OR: 2.45, CI: 1.79â€“3.37). The increased risk for haemorrhage associated with comorbidity must be taken into account when assessing patients prior to cholecystectomy. Perioperative bleeding increases postoperative mortality and is associated with an increased risk for bile duct injury. The Mayo Clinic recently presented a new staging system that is applicable to all patients with perihilar cholangiocarcinoma (PHC) regardless of subsequent treatment.1 The staging system assigns patients to one of four stages, depending on the patients' performance status, serum CA19â€9 level, and radiological parameters including tumor size, suspected vascular involvement, and metastatic disease. We aimed to validate this staging system. All consecutive patients with PHC who were evaluated and treated in two tertiary centers between January 2002 and December 2014 were identified. Baseline characteristics required for the prognostic model were collected from medical records and imaging parameters were reassessed by experienced abdominal radiologists. Overall survival (OS) was analyzed using the Kaplanâ€“Meier method and comparison of staging groups was performed using the logâ€rank test and Cox proportional hazard regression analysis. Discriminative performance was quantified by the concordance (C)â€index. Subgroup analysis was performed for treatment subgroups. A total of 600 patients were staged according to the Mayo Clinic model, allocating 23, 80, 357 and 140 patients to stages I, II, III and IV, respectively. Median OS was 11.6 months. Median OS of stages I, II, III and IV was 33.2, 19.7, 12.1 and 6.0 months, with hazard ratios (95% confidence interval) of 1.0 (reference), 2.02 (1.14â€“3.58), 2.71 (1.59â€“4.64) and 4.00 (2.30â€“6.95), respectively (p < 0.001). The Câ€index (95% CI) was 0.59 (0.56â€“0.61) for the entire cohort. Statistically significant prognostic stratification was also observed in the laparotomy subgroup (p = 0.011). The Mayo Clinic staging system for patients with PHC demonstrated stratification in four stages that differed significantly in median survival. As the discriminative performance of the model was moderate, it may require improvement prior to clinical implementation. Despite the advancement of new surgical and chemotherapeutic treatments for patients with perihilar cholangiocarcinoma (CCA), the prognosis for patients is still poor. photochemical internalisation (PCI) is a novel therapeutic technology, which may improve the treatment options for this patient group. PCI can enhance the therapeutic effect of a variety of molecules, including several commonly used cytotoxic drugs. Fimaporfin (TPCS2a) is a proprietary photosensitiser that is used to induce PCI. It is designed to localise to endosomal membranes and induce endosomal disruption upon light activation. Many cancer drugs and other molecules of therapeutic interest may end up entrapped in endosomes. PCI can induce the release of such molecules from the endosomes, thereby enabling them to reach their therapeutic target in the cell cytosol or nucleus. Based on positive preclinical results for PCI and gemcitabine and promising results from the first in man study with PCI in patients with various solid tumours,1,2 it was decided to start a phase I/II study to investigate the effect of PCI with gemcitabine in patients with advanced inoperable perhilar CCA. The aim of this study was to establish the safety and tolerability of PCI with gemcitabine followed by standard systemic chemotherapy. The study was conducted at several centers in Germany, Norway and UK. Local radiological assessments were performed in phase I. A retrospective independent radiological evaluation was performed for the last two dose cohorts, to verify promising early signs of efficacy. A total of 16 patients were treated in the dose escalating Phase I part of this study. Four different dose cohorts were tested. The last patient completed the 6â€month followâ€up period in September 2016. Of the 16 patients treated, 11 completed the eight cycles of combination therapy, and five patients were early withdrawals. No patients expired while still on study. The treatment was well tolerated with no DLTs, and no serious safety concerns were raised. In the last two dose cohorts, independent radiological evaluation showed that seven out of eight patients had evaluable tumour, and four of these showed an objective tumour response at 6 months, including two complete responses. The patient number in this clinical study is small, but the results indicate high durable objective tumour response rates. The early signs of efficacy using PCI treatment with fimaporfin and gemcitabine are encouraging. Neonates born to hepatitis C virus (HCV)â€positive mothers are usually not screened for HCV. Unscreened children may act as active sources for social HCV transmission; additionally, infected mothers not be routinely investigated for HCV, and factors contributing to both vertical and horizontal HCV transmission still remain controversial, needing optimization. We aimed to investigate the factors contributing for HCV transmission worldwide, hoping for a safer world of HCV. A prospective cohort study was conducted among 3836 pregnant women in both urban and rural areas across Egypt for HCV screening in both mothers and neonates born to HCVâ€positive mothers. All pregnant women were screened during the third trimester or just before delivery; neonates born to HCVâ€positive mothers were evaluated during 24â€hours postâ€delivery to record vertical transmission cases. Data miningâ€driven computational analysis was used to quantify the findings. Among 3836 randomized pregnant women, prevalence of HCVâ€4 was identified in 80/3836 (2.08%). All HCVâ€infected women have experienced surgical intervention 18/80 (22.5%) or CS 62/80 (77.5%). HCV vertical transmission was identified in 10 neonates; 10/80 (12.5%). Some husbands showed positive HCV likely from sexual transmission; maternal viremia >3 Ã— 106 is the predicting factor for such vertical transmission. Cesarean section and surgical operations among the leading highest factors for HCV horizontal transmission. Screening women who had experienced surgical intervention or CS during the childâ€bearing period and before pregnancy might prevent HCV Motherâ€toâ€Child Transmission (MTCT). CS should be ethically justified and would decrease global HCV transmission. All authors have declared no conflicts of interest. Recurrent Clostridium difficile infection (CDI) remains a significant challenge. Microbiotaâ€based therapies appear highly effective in preventing CDI recurrences. However, there is a need for an offâ€theâ€shelf, standardised, safe and effective microbiota product. To evaluate the safety and efficacy of RBX2660, a microbiotaâ€based drug containing live humanâ€derived microbes targeted at recurrent CDI, in a randomised, doubleâ€blinded, placeboâ€controlled study. Patients were randomised to: 2 doses of RBX2660 (Group A); 2 doses of placebo (Group B); or 1 dose of RBX2660 and 1 dose of placebo (Group C) via enema, doses 7 days apart. CDI symptoms were controlled with antibiotics (ABX) prior to enrollment; ABX were discontinued 24â€“48 hours prior to the first dose of RBX2660. Success was defined as absence of Clostridium difficile â€associated diarrhoea (CDAD) at 8 weeks from the last dose. Failures were defined as: recurrence of CDAD <8 weeks after the last dose; a positive stool CDI test; CDI retreatment; and no other cause for diarrhoea. Failures received up to 2 doses of openâ€label RBX2660, 7 days apart. Safety was assessed via a patient diary; in clinic at 1, 4, and 8 weeks and via telephone at 2, 3 and 5â€“7 weeks and at 3, 6, 12 and 24 months. A total of 127 patients (Group A, n = 41; Group B, n = 44; Group C: n = 42); median age: 63.0 range: 18â€“92 years; sex: 62% female; median prior CDI episodes (4 range: 2â€“14) were included in the intentâ€toâ€treat analysis. Baseline characteristics were similar in the three groups. In the blinded phase, the success in Group C (1 dose) was superior to Group B (placebo): 66.7%, 28/42 vs. 45.5%, 20/44, p = 0.048. There was no difference between Group A: 2 doses (61%, 25/41) vs. Group C: 1 dose (66.7%), p = 0.589. The combined success in Groups A and C (1 or 2 doses) was superior to Group B: placebo, 63.9% vs. 45.5%, p = 0.046. Of patients in groups A and C who went on to receive openâ€label treatment 21/30 succeeded, giving a combined success of 89.2% (n = 74/83) for all patients who were randomised to receive at least 1 active treatment compared with a 45.5% (20/44) placebo response, p < 0.0001. Adverse events (AEs) at 8 weeks were primarily gastrointestinal; there were no unanticipated AEs. There was no significant difference in the proportion of adverse or serious AEs among the three groups. RBX2660 was more effective than placebo with a high overall success in a randomised, doubleâ€blinded, placeboâ€controlled study for recurrent CDI. The data indicate that RBX2660 administered via enema is an effective and safe treatment for the prevention of recurrent CDI. Increasing evidence suggests that an altered gut microbiota can contribute to colorectal carcinogenesis.1,2 Studies have reported associations between colorectal cancer (CRC) and clinical infections by specific bacteria such as Streptococcus bovis, 3 partly due to tissue damage at the neoplasia as an entry point for bacterial entry into blood stream. In this study, we aim to evaluate the incidence of CRC in patients with bacteremia from microbes enriched in the CRC microbiota. We retrieved cultureâ€confirmed bacteraemia cases due to CRC associated bacteria in all public hospitals in Hong Kong between 2006 to 2015. Ageâ€, genderâ€ and comorbidityâ€matched controls from the same period were retrieved. We defined the index date as the date of positive blood culture, and compared the incidence of biopsyâ€proven CRC by the Kaplanâ€“Meier estimator. The association of CRC with specific organisms was assessed by comparing cases and controls in a ratio of 1 to 5. A total of 13,096 bacteraemia cases from 11 bacterial genera and 42,939 matched controls were retrieved. Previously reported bacteria, including Bacteroides fragilis and Streptococcus bovis, were strongly associated with incidence of CRC (B. fragilis HR = 3.65, 95% CI = 2.20â€“6.06, p = 7.5 Ã— 10âˆ’8 ; S. bovis HR = 4.55, 95% CI = 2.45â€“8.43, p = 1.3 Ã— 10âˆ’7 ). Furthermore, we observed significant associations between CRC and other enriched bacteria, including Fusobacterium nucleatum (HR = 7.96, 95% CI = 2.26â€“28.02, p = 1.3 Ã— 10âˆ’4 ), Peptostreptococcus species (HR = 2.99, 95% CI = 1.34â€“6.66, p = 0.0049) and Gemella morbillorum (HR = 5.56, 95% CI = 1.12â€“27.61, p = 0.0180). These associations remained significant after false discovery rate (FDR) correction. This is the first study to date to systemically test for associations between bacteraemia from CRC microbiota and subsequent onset of the cancer. These results support the putative role of bacteria in colorectal carcinogenesis, and provide insight into how different bacteria may gain entry to the bloodstream from the dysbiotic mucosae. Given the large effect size, bacteraemia from these species should also alert clinicians to consider colonoscopy to look for neoplastic lesions. Fecal microbiota transfer (FMT) is a highly effective therapy for recurrent Clostridium difficile infection (CDI). The therapy with an undefined mixture of living bacteria, however, is associated with uncontrollabel risks for development of infectious, chronic metabolic or malignant diseases. Immunocompromised patients are at a particular risk. We hypothesized that intact microorganisms may be less relevant than the molecular constituents such as cell wall components, metabolites and/or mediators. We therefore investigated intestinal microenvironment transfer (IMEnT) with sterile stool filtrate instead of conventional FMT in order to reconstitute the â€œnormalâ€ intestinal environment as a therapeutic principle. Five patients with recurrent CDI were recruited at the Department of Internal Medicine I of the University Hospital Schleswigâ€Holstein (Kiel, Germany) and subjected to a single IMEnT administration via nasojejunal tube. IMEnT filtrates were prepared from five donors chosen by the patients, which were fully characterized according to current standards of FMT. Filtrates were tested for sterility by standard anaerobic and aerobic culture techniques. Microbiota, virus and proteome analyses of selected patient and donor stool samples as well as filtrate material were performed. In all five patients, IMEnT restored normal stool habits and permanently eliminated CDI symptoms for a minimum observation period of 6 months. Proteome analyses of selected IMEnT filtrates revealed no obvious bacterial or host protein candidates associated with therapeutic efficacy. 16 S rRNA gene sequencing detected diverse bacterial DNA in the sterile filtrates. Enrichment of virusâ€like particles from an exemplary filtrate revealed a complex signature of bacteriophages. The analysis of 16 Sâ€based bacterial phylogenies as well as the virome profiling suggest drastic longitudinal changes of microbial and viral community structures in recipients of IMEnT treatment. Our preliminary data suggests that IMEnT could be an alternative to FMT, particularly in immunocompromised patients. It appears plausible that bacterial components, metabolites or bacteriophages substantially contribute to the efficacy of FMT in recurrent CDI. Georg Waetzig: Employee of CONARIS Dirk Seegert: Employee of CONARIS Stefan Schreiber: Scientific advisor to CONARIS. Table 1: Mean normalized signal for probes sorted by significant difference between donors and IBS patients before FMT, and 1 week and 3 weeks after FMT strain Donors Patients P* Before FMT P** After 1 week P*** After 3 weeks Before FMT After 1 week After 3 weeks Firmicutes, Tenericutes, Bacteroidetes 244 Â± 29 128 Â± 29 143 Â± 32 179 Â± 50 0.014 0.052 0.31 Ruminococcus gnavus 4.6 Â± 1.1 116 Â± 68 29 Â± 16 26 Â± 18 0.003 0.097 0.32 Dialister invisus 193 Â± 46.3 37 Â± 19.3 114 Â± 38.1 130 Â± 60.9 0.014 0.2 0.35 Clostridia, Veillonella, Helicobacter 328 Â± 34 227 Â± 31 272 Â± 36 289 Â± 38 0.025 0.32 0.5 Lactobacillus, Pediococcus 13 Â± 10 3.5 Â± 0.2 7.2 Â± 3 2.7 Â± 0.06 0.02 0.07 0.35 Streptococcus 49.3 Â± 9.7 79 Â± 15.4 48.8 Â± 9.6 52 Â± 11.3 0.036 0.72 0.36 Streptococcus sanguinis and thermophilus 12.2 Â± 4.7 67 Â± 30.7 26.2 Â± 17 29.9 Â± 18 0.007 0.53 0.43 Anaerotruncus 61.5 Â± 0.5 63.2 Â± 0.5 62.7 Â± 0.3 61.5 Â± 0.5 0.043 0.045 0.98 Bacteroides 144 Â± 4.5 169 Â± 8.2 149 Â± 7.3 143 Â± 6 0.003 0.79 0.82 Bacteroides, Prevotella 483 Â± 51.4 634 Â± 28.5 599 Â± 25.8 551 Â± 63.9 0.04 0.24 0.59 Proteobacteria 12.4 Â± 1.6 26 Â± 6 221 Â± 91 16.5 Â± 3 0.04 0.004 0.43 Pseudomonas 6.98 Â± 0.3 7.99 Â± 0.3 7.6 Â± 0.2 7.3 Â± 0.2 0.017 0.03 0.14 Shigella, Escherichia 22 Â± 6.8 46 Â± 16 240 Â± 63 40 Â± 13 0.095 0.0003 0.1 Actinobacteria 159 Â± 36 25 Â± 4.8 47 Â± 10 111 Â± 33 0.0006 0.0095 0.35 Atopobium 4.8 Â± 0.1 4.49 Â± 0.1 4.47 Â± 0.1 4.59 Â± 0.1 0.12 0.02 0.08 Bifidobacterium 189 Â± 43 25 Â± 5.3 49 Â± 11 123 Â± 38 0.0004 0.008 0.28 Actinomycetales 11.4 Â± 1.0 8.7 Â± 0.9 11.2 Â± 1.9 11.1 Â± 2.1 0.03 0.32 0.42 Data are presented as the mean Â± SEM. Comparison: Mannâ€Whitney U test. *Donors vs. patients before FMT, **Donors vs. patients 1 week after FMT, *** Donors vs. patients 3 weeks after FMT. Data are presented as the mean Â± SEM. Comparison: Mannâ€Whitney U test. *Donors vs. patients before FMT, **Donors vs. patients 1 week after FMT, *** Donors vs. patients 3 weeks after FMT. Alterations in gut microbiota are suggested to play an important role in the development of irritable bowel syndrome (IBS). Through manipulating the gut microbiome of the new host, fecal microbiota transplantation (FMT) has been used to treat patients with treatmentâ€resistant, antibioticâ€associated Clostridium difficile colitis. The aim was to investigate the effect of FMT on the symptoms and on modifying the gut microbiota in patients with IBS. The study included 13 patients (four females and nine males, age range 20â€“44 years) with diarrheaâ€predominant IBS (IBSâ€D) according to Rome III criteria and 13 healthy asymptomatic donors. The patients received freshly donated feces from a relative and was administered in to the descending part of the duodenum via a gastroscope. Feces were collected from the donors and the patients before FMT and again from the patients after 1 week and 3 weeks. The samples were stored in freezers (â€“80Â°C) until analysis. Microbiota analysis was performed using the GAâ€map Dysbiosis test (Genetic Analysis AS, Oslo, Norway) by algorithmically assessing fecal bacterial abundance and profile (dysbiosis index, DI), and potential deviation in the microbiome from normobiosis.1 DI is based on 54 DNA probes targeting more than 300 bacterial strains based on their 16 S rRNA sequence in seven variable regions (V3â€V9). A DI above 2 shows a microbiota profile that differs from that of the normobiotic reference collection.1 In addition, the donors and patients completed the following questionnaires before FMT and again for the patients at 3 weeks after FMT: IBS symptom questionnaire (IBSâ€SQ), IBSâ€symptom severity scoring system (IBSâ€SSS), short form of Nepean Dyspepsia Index (SFâ€NDI) and Bristol stool scale form. The DI (mean Â± SEM) of the donors (1.8 Â± 0.23) differed significantly from the patients before FMT (2.7 Â± 0.37, p = 0.009) and at 1 week after FMT (2.7 Â± 0.38, p = 0.039) but not at 3 weeks after FMT (2.3 Â± 0.29, p = 0.1). The profile of a selection of the most important bacteria (Table 1) showed significant differences in several strains of the gut microbiota between the donors and IBS patients before receiving FMT, which became nonâ€significant after 3 weeks from receiving FMT. The scores of IBSâ€SQ were significantly reduced during the 3 weeks after receiving FMT; total (p < 0.0001), nausea (p = 0.001), bloating (p < 0.0001), abdominal pain (p = 0.0005), constipation (p = 0.01), diarrhea (p < 0.0001), but not for anorexia (p = 0.09). The total scores of IBSâ€SSS, SFâ€NDI and Bristol stool scale were significantly reduced after receiving FMT (p = 0.0004, 0.004 and 0.008, respectively). No adverse effects were reported after FMT. This is the first study to show the kinetics of microbial community composition in IBS patients following FMT. The results show that FMT helps in restoring alterations in the signals of several strains of the gut microbiota in IBS patients. This suggests that the microbiota profile between donors and patients following FMT has become similar and may have contributed in improving the symptoms and quality of life for these patients. FMT may be used as a treatment for IBS. Irritable Bowel Syndrome (IBS) is a brain gut disorder involving alterations in mucosal barrier function and central pain processing networks.1 Although it has been proposed that increased gut barrier permeability is related to increased visceral pain, no studies have thus far addressed if peripheral and central alterations are linked in IBS. The objectives of this study were to: (1) identify if changes in gut mucosal barrier function are correlated with changes in brain functional connectivity in the default mode network (DMN), a network associated with selfâ€related processing, and (2) test if this correlation differs in patients with IBS. Colonic biopsies were acquired on 32 women with IBS (mean age 32.6y) and 15 healthy women (HC; mean age 29.7y) and mounted in Ussing chambers to measure mucosal paracellular permability using the 384 Da inert probe 51 chromium (Cr)â€EDTA. Symptoms were measured with the IBS Severity Scoring System (IBSâ€SSS). Resting state fMRI scans were acquired on the same set of subjects using a 3T MRI scanner. The fMRI data were preprocessed using standard procedures and group independent component analysis was used to identify the DMN. Betweenâ€group covariate analysis was performed across the whole brain to identify differences in the correlation between peripheral measures of 51 Crâ€EDTA permeability and functional connectivity in the DMN. The 51 Crâ€EDTA colonic permeability was significantly higher in IBS (p < 0.05). The IBS patients' symptom severity was mostly moderate to severe (IBSâ€SSS mean 342, range 112â€“480). Significant betweenâ€group wholeâ€brain differences in the correlation of 51 Crâ€EDTA levels and DMN functional connectivity were observed (p < 0.05, Family Wise Error correction). While increased 51 Crâ€EDTA permeability in both groups was linked to greater connectivity with regions in both ascending and descending pain pathways, the specific regions differed between groups. In IBS, increased permeability correlated to increased connectivity of DMN with posterior insulae, paracentral lobule, periaqueductal gray and anterior cingulate. In HCs, these correlations involved thalamus, sensorimotor cortex, midâ€pons, and nucleus raphe magnus. Even though altered DMN connectivity has been reported in chronic pain,2 this is the first study to demonstrate that peripheral measures of gut barrier function are associated with changes in DMN functional connectivity, demonstrating a link between peripheral and central functions. In IBS, increased colonic mucosal permeability correlated with increased connectivity between distinct brain regions in both the ascending and descending pain pathways with those brain regions that comprise the DMN. The present results suggest that in IBS alterations in the DMN connectivity may be related to alterations in colonic mucosal permeability. Fecal microbiota transplantation (FMT) has been investigated in various studies as an alternative approach for the treatment of patients with ulcerative colitis (UC). Results of recent randomized studies are controversial and response rates to FMT vary considerably between studies. So far factors influencing the efficacy of FMT in UC are poorly understood. We investigated the microbiota of UC patients treated by repeated FMT and corresponding donors for a specific microbial signature correlating to clinical efficacy of FMT. Microbiota analysis was performed by 16S rRNA geneâ€based microbiota analysis. 17 patients were treated with repeated FMT (n = 5) in 14â€day intervals after an antibiotic pretreatment for 10 days. Fecal suspension were derived from 14 donors; the same donor had to serve for one patient throughout the study. Patients were divided into responder vs. non responder according to a reduction of the total Mayo score of >3 points and remission as a total Mayo score of <2 points at day 90. 59% (10/17) of patients showed a response and 24% (4/17) a remission to FMT. Donorsâ€™ microbiota structure associated with recipient response had a higher bacterial diversity than donorsâ€™ microbiota structure, who did not induce response. Furthermore, abundance of Akkermansia muciniphila, as well as unclassified Ruminococcaceae was significantly higher in the donor stools transferred to patients with remission compared with donors leading to nonâ€response. Interestingly, engraftment of the donor microbiota was not associated with treatment success, since all recipientsâ€™ microbiotas, regardless of response, shifted towards the respective donor microbiota. However, microbiota analysis prior to FMT revealed no specific differences in the microbiota structure of responder and nonâ€responder. Taxonomic composition of donor microbiota, especially high abundance of A. muciniphila and unclassified Ruminococcaceae is a major factor for efficacy of FMT in chronic active UC. Selective focus on composition of donor microbiota might increase efficacy of FMT in chronic active UC. Helicobacter pylori is a key factor in the pathogenesis of chronic gastritis, peptic ulcer disease, and gastric cancer. Eradication of H. pylori infection is an effective strategy in treatment. However, in recent years the efficacy of the treatment has decreased significantly because H. pylori has acquired resistance to many classes of antibiotics. Until now there is no comprehensive method to detect the antibiotic resistanceâ€associated variants from Formalinâ€Fixed Paraffin Embedded (FFPE) gastric biopsies. American Molecular Laboratories (AML) has developed a Next Generation Sequencing (NGS)â€based H. pylori antibiotic resistance panel to detect and report major gene variants associated with antibiotic resistance using the FFPE specimens. AML H. pylori antibiotic resistance panel is based on AMLâ€™s proprietary technology and processing pipeline. It includes unique methods to retrieve and enrich H. pylori DNA from FFPE tissue samples in combination with a proprietary library preparation followed by NGS on Illumina MiSeq to produce H. pylori genomic sequence data. The NGS data is analyzed using a software optimized to align the FFPE sample derived data to detect and report the antibiotic resistance conferring variants. For validation of the AML H. pylori antibiotic resistance panel, 70 FFPE gastric biopsies which were H. pylori infection position were tested. All of the detected antibiotic resistance gene variations were confirmed by Sanger sequencing. With this method, a total of 71 samples were tested. rdxA gene variants were detected in 24 samples, 21 samples showed variants in gyrA gene, 16 samples were with 23S rRNA gene variants and one with rpoB variant, that confer antibiotic resistance to metronidazole, fluoroquinolones, clarithromycin and rifabutin antibiotics, respectively. Interestingly, one sample harbored antibiotic conferring resistance variants in three genes and a second sample with variants in two genes, highlighting the importance of comprehensive screening for antibiotic resistance. In addition, variants in 24 samples were confirmed using Ion Torrent Ion S5 platform. The average sequencing depth is higher than 500 with >95% Q30 sequence data which provide a high testing sensitivity. The AML method is able to detect as low as 10 copies of H. pylori from the mixed FFPE biopsy with >99% specificity. The detected frequency of gene mutation is from 99.7% to 43.5%, indicative of mixed population of H pylori in the specimen that illustrates importance of emerging antibiotic resistance. American Molecular Laboratories (AML) successfully developed and launched the H. pylori antibiotic resistance test panel to detect and analyze the variants of all genes associated with antibiotic resistance in H. pylori from the FFPE gastric biopsy. Compared with other methods, the H. pylori antibiotic resistance panel based on NGS provides a comprehensive, accuracy, reliable and robust approach for detecting H. pylori antibiotic resistance. TNFâ€inhibitors (TNFi) have improved treatment of Crohn's disease (CD), ulcerative colitis (UC), spondyloarthritis (SpA), rheumatoid arthritis (RA), psoriatic arthritis (PsA) and chronic plaque psoriasis (Ps). The NORâ€SWITCH study was funded by the Norwegian government. The aim of the NORâ€SWITCH study was to examine switching from originator to biosimilar infliximab regarding efficacy, safety and immunogenicity. The study was designed as a 52â€week randomized, doubleâ€blind, nonâ€inferiority, phase IV trial. Adult patients with a diagnosis of CD, UC, SpA, RA, PsA or Ps on stable treatment with the originator infliximab (RemicadeÂ®, INX) for at least 6 months were eligible. Patients with informed consent were randomized 1:1 to either continued INX or switch to CTâ€P13 treatment (biosimilar infliximab, RemsimaÂ®), using unchanged dosing regimen. Data were collected at infusion visits. The primary endpoint was disease worsening during followâ€up according to worsening in diseaseâ€specific composite measures and/or a consensus between investigator and patient leading to major change in treatment. Exploratory subgroup analyses were performed to examine disease worsening within each of the six diagnoses. The nonâ€inferiority margin was set to 15% and power calculations indicated that 394 patients were required in the primary Per Protocol Set (PPS). The primary endpoint was analysed using logistic regression, adjusted for diagnosis and disease duration at baseline. Between 6 October 2014 and 8 July 2016, 481 patients (INX 241, CTâ€P13 240, Full Analysis Set, FAS) at 40 Norwegian study centres were randomized, received treatment and were followed for 52 weeks. The main demographic and baseline characteristics are shown in the table. Disease worsening occurred in 26.2% and 29.6% of patients in the INX and CTâ€P13 arms, respectively (PPS). The 95% confidence interval of the adjusted treatment difference (âˆ’4.4%) was âˆ’12.7 to 3.9, which was within the preâ€specified nonâ€inferiority margin. The frequency of disease worsening in each specific diagnosis is shown in the table (exploratory analyses). Changes in the generic disease variables and disease specific composite measures were similar in both arms (table). The incidence of antiâ€drug antibodies detected during the study was 17 (7.1%) and 19 (7.9%) in the INX and CTâ€P13 patients, respectively (FAS). The trough drug levels and the frequencies of reported adverse events including infusion reactions were also similar (data not shown). Data are n (%), mean (SD) or median (25â€“75 percentiles). 95% CI, 95% confidence interval of the adjusted treatment difference. BASDAI: Bath Ankylosing Spondylitis Disease Activity Index; ASDAS: Ankylosing Spondylitis Disease Activity Score; DAS28: Disease Activity Score in 28 joints, CDAI: Clinical Disease Activity Index; SDAI: Simplified Disease Activity Index; PASI: Psoriasis Area and Severity Index. Table: Demographic and baseline characteristics (FAS), percentage of patients with disease worsening and change in disease measures during 52 weeks followâ€up (PPS) INX CTâ€P13 Number of patients (FAS) 241 240 Demographics and baseline characteristics Age (years) 47.5 (14.8) 48.2 (14.9) Females 99 (41.1%) 87 (36.2%) Disease duration (years) 16.7 (10.9) 17.5 (10.5) Duration of ongoing infliximab treatment (years) 6.7 (3.6) 6.9 (3.8) Concomitant immunosuppressive comedication 113 (46.9%) 129 (53.8%) Diagnoses Crohnâ€™s disease 78 (32.4%) 77 (32.1%) Ulcerative colitis 47 (19.5%) 46 (19.2%) Spondyloarthritis 45 (18.7%) 46 (19.2%) Rheumatoid arthritis 39 (16.2%) 38 (15.8%) Psoriatic arthritis 14 (5.8%) 16 (6.7%) Psoriasis 18 (7.5%) 17 (7.1%) Harveyâ€Bradshaw Index 2.0 (1â€“4) 2.0 (0â€“4) Partial Mayo Score 0 (0â€“1) 0 (0â€“1) Fecal calprotectin (mg/kg) 56 (25â€“173) 53 (22â€“210) Câ€reactive protein (mg/L) 2.2 (1.0â€“5.0) 2.0 (1.0â€“5.0) Number of patients (PPS) 202 206 95% CI Disease worsening All 53 (26.2%) 61 (29.6%) âˆ’12.7 to 3.9% Crohn's disease 14 (21.2%) 23 (36.5%) âˆ’29.3 to 0.7% Ulcerative colitis 3 (9.1%) 5 (11.9%) âˆ’15.2 to 10.0% Spondyloarthritis 17 (39.5%) 14 (33.3%) âˆ’14.5 to 27.2% Psoriatic arthritis 7 (53.8%) 8 (61.5%) âˆ’45.4 to 28.1% Psoriasis 1 (5.9%) 2 (12.5%) âˆ’26.9 to 13.2% Change in disease measures from baseline Physicianâ€™s Global Assessment of Disease Activity (0â€“10) 0.09 (1.62) 0.11 (1.56) âˆ’0.39 to 0.09 Patientâ€™s Global Assessment of Disease Activity (0â€“10) 0.43 (1.87) 0.30 (2.20) âˆ’0.37 to 0.29 Log10 erythrocyte sedimentation rate (mm/h) 0.019 (0.254) 0.006 (0.308) âˆ’0.065 to 0.028 Log10 Câ€reactive protein (mg/L) 0.020 (0.345) 0.023 (0.419) âˆ’0.086 to 0.038 Log10 fecal calprotectin (mg/kg) 0.035 (0.506) 0.096 (0.477) âˆ’0.118 to 0.177 Harveyâ€Bradshaw Index (CD) 0.26 (2.35) 0.49 (3.15) âˆ’1.14 to 0.33 Partial Mayo Score (UC) 0.09 (1.28) âˆ’0.17 (1.68) âˆ’0.30 to 0.59 BASDAI (SpA) 0.25 (1.01) âˆ’0.15 (1.38) âˆ’0.50 to 0.47 ASDAS (SpA) 0.07 (0.59) âˆ’0.19 (0.67) âˆ’0.27 to 0.24 DAS28 (RA, PsA) 0.30 (0.98) 0.08 (0.93) âˆ’0.08 to 0.61 CDAI (RA, PsA) 1.51 (5.54) 0.67 (3.94) âˆ’0.35 to 2.94 SDAI (RA, PsA) 1.56 (5.67) 0.69 (4.41) âˆ’0.68 to 2.86 PASI (Ps) âˆ’0.50 (1.88) âˆ’0.44 (1.87) âˆ’1.10 to 0.55 The NORâ€SWITCH trial demonstrated that switch from INX to CTâ€P13 was not inferior to continued treatment with INX. *on behalf of the Norâ€Switch study Group All authors have declared no conflicts of interest. Mongersen (GEDâ€0301) is an antisense oligodeoxynucleotide that is complementary to the sequence of the messenger ribonucleic acid (mRNA) transcript of Smad7. It is formulated as a gastroâ€resistant, delayed release, pHâ€dependent tablet designed to deliver the active substance in the distal GI tract with negligible systemic exposure and is being evaluated for the treatment of patients with active Crohnâ€™s disease. Clinical efficacy has been demonstrated in a phase 2 study.1 The intent of this phase 1b study is to examine endoscopic improvement and clinical benefit with three different mongersen treatment regimens. Subjects with active CD (Crohnâ€™s Disease Activity Index [CDAI] score of 220â€“450, total simple endoscopic score for CD [SESâ€CD] of â‰¥7 or ileal disease SESâ€CD of â‰¥4) were randomised to 4, 8 or 12 weeks of mongersen 160 mg daily, followed by an observation period without study drug. Endoscopic and clinical assessments are reported through week 12. Endoscopic assessments at baseline (BL) and week 12 were centrally read. Daily electronic diary records were used to collect CD symptoms. including abdominal pain and soft/loose stool frequency; a clinical evaluation, including CDAI determination, occurred at monthly visits. 63 subjects were enrolled; at BL, mean age was 41.5 years, mean SESâ€CD was 11.2 and mean CDAI was 294. Mean CD duration was 11.6 years; with 46% having had prior exposure to TNFâ€Î± therapy; 46% of subjects had involvement distal to the mid transverse colon. 52 subjects had evaluable endoscopies at week 12. Of these subjects, 37% had an endoscopic response, defined as â‰¥25% reduction in SESâ€CD from BL to week 12, with no meaningful difference across treatment groups. Of the subjects with greater endoscopic disease activity at BL (SESâ€CD >12), 63% had a reduction â‰¥25% and 31% had a reduction â‰¥50%. Clinical improvement was seen by week 2, with rates of clinical response (CDAI decrease â‰¥100), and clinical remission (CDAI <150) maintained or increased in all three treatment groups over 12 weeks. The highest clinical response (67%) and clinical remission(48%) rates were seen in the 12â€week treatment group. Change from BL in CDAI was also higher in the 12â€week treatment group, with a mean reduction of 133 points. Further analysis of the clinical improvement criteria showed similar benefit among subjects who had greater BL disease severity based on CDAI (>300), longer duration of CD history (>5 years) or history of CD surgery. Adverse event (AE) rates and serious AE rates were low and similar across treatment groups. Mongersen was generally safe and well tolerated with no new safety signals emerging. Oral mongersen resulted in meaningful endoscopic improvement at an early time point (12 weeks) for subjects with active CD. Increasing clinical improvement (response and remission) was seen over time, with the mongersen 12â€week treatment group demonstrating greater efficacy. No new safety signals were identified. The ILâ€23 pathway is implicated in the pathogenesis of Crohn's disease (CD) both genetically and biologically. The efficacy and safety of risankizumab (formerly BI 655066), a humanised monoclonal antibody that selectively inhibits ILâ€23 through specific targeting of the ILâ€23 p19 subunit, is being assessed in a randomised Phase II study in patients (pts) with moderateâ€toâ€severe active CD (NCT02031276). The study has three treatment periods: a 12â€week (wk), doubleâ€blind, intravenous (IV) induction period (P1); a 14â€wk, openâ€label (OL), IV reâ€induction/washout period (P2); and a 26â€wk OL subcutaneous maintenance period. In P1, pts (N = 121) with clinically active CD (CD Activity Index [CDAI]>220) confirmed by endoscopy (CD Endoscopic Index of Severity [CDEIS]?7;?4 for pts with isolated ileitis) were randomised to either 200 or 600 mg risankizumab or placebo (PBO) at Wks 0, 4 and 8. The primary endpoint was clinical remission (CR; CDAI <150) at Wk 12. At Wk 12, pts entered P2; here, pts in deep remission (CR + endoscopic remission) underwent washout and all other pts underwent OL IV reâ€induction therapy (600 mg risankizumab at Wks 14, 18 and 22). Baseline demographics and disease characteristics were similar between study arms. Mean age was 38.1 yrs and median CDAI and CDEIS scores were 298 and 12; 94% of pts had previously received >1 TNF antagonists. At Wk 12, CR was achieved by 24.4% and 36.6% of pts with 200 and 600 mg risankizumab vs. 15.4% for PBO (p = 0.31 and p = 0.025) and deep remission was achieved by 2.4% and 12.2% of pts with 200 and 600 mg risankizumab vs. 0% for PBO (p = 0.31 and p = 0.016). In pts entering P2 without CR, OL reâ€induction of PBO pts induced a rate of CR similar to the 600 mg arm in the blinded P1, dose escalation from 200 to 600 mg induced a high CR rate, and reâ€induction treatment in the 600 mg arm further increased the CR rate in this group at Wk 26 (Table). No pts with deep remission relapsed during the risankizumab washout phase to Wk 26. AEs were similar between risankizumab and PBO with no doseâ€related increase in AEs in P1. Risankizumab was well tolerated in P2. Reâ€induction therapy with 600 mg risankizumab was effective in increasing clinical remission rates further at Wk 26. Overall, risankizumab was well tolerated. Clinical remission at Wk 26 by outcome of induction treatment (Wk 12) Placebo 200 mg Risankizumab 600 mg Risankizumab Number of evaluable patients in P2 30 34 34 Pts in clinical remission at Week 12, N 6 8 9 Pts in clinical remission at Week 26, n (%) Yes 6 (100) 6 (75) 9(100) No 0 2 (25) 0 Pts not in clinical remission at Week 12, N 24 26 25 Pts in clinical remission at Week 26, n (%) Yes 12 (50) 15 (58) 8 (32) No 12 (50) 11 (42) 17 (68) The proportion of patients in clinical remission at Wk 26 after OL IV reâ€induction therapy (600 mg risankizumab at Wks 14, 18 and 22) in P2 are shown by original P1 treatment designation and the proportion of patients in clinical remission at Wk 26 after OL IV reâ€induction therapy (600 mg risankizumab at Wks 14, 18 and 22) in P2 are shown by original P1 treatment designation and prior Wk 12 clinical remission status. All patients who were not in deep remission (clinical remission [CDAI <150] + endoscopic remission [CDEIS â‰¤4; for patients with initial isolated ileitis a CDEIS â‰¤2]) at Wk 12 underwent risankizumab reâ€induction therapy. The proportion of patients in clinical remission at Wk 26 after OL IV reâ€induction therapy (600 mg risankizumab at Wks 14, 18 and 22) in P2 are shown by original P1 treatment designation and the proportion of patients in clinical remission at Wk 26 after OL IV reâ€induction therapy (600 mg risankizumab at Wks 14, 18 and 22) in P2 are shown by original P1 treatment designation and prior Wk 12 clinical remission status. All patients who were not in deep remission (clinical remission [CDAI <150] + endoscopic remission [CDEIS â‰¤4; for patients with initial isolated ileitis a CDEIS â‰¤2]) at Wk 12 underwent risankizumab reâ€induction therapy. Brian Feagan: AbbVie, ActoGeniX, Akros, Albireo Pharma, Allergan, Amgen, AstraZeneca, Avaxia Biologics Inc., Avir Pharma, Axcan Pharma, Baxter Healthcare Corporation, Biogen Idec, Boehringerâ€Ingelheim, Bristolâ€Myers Squibb, Calypso Biotech, Celgene, Elan/Biogen, enGene, Ferring, Genentech/Roche, GiCare Pharma, Gilead, Given Imaging, GlaxoSmithKline, Ironwood, Janssen Biotech/Centocor, Johnson & Johnson/Janssen, Kyowa Hakko Kirin Co., Ltd., Lilly, Lycera Biotech, Merck, Mesoblast Pharma, Millennium, Nestles, Novo Nordisk, Novartis, Pfizer, Prometheus Therapeutics and Diagnostics, Protagonist, Receptos, Salix, Sanofi, Shire, Sigmoid Pharma, Synergy Pharma, Takeda, Teva Pharma, TiGenix, Tillotts, UCB, Vertex, VHsquared, Wyeth, Zealand, Zyngenia and Robarts Clinical Trials Inc. William J. Sandborn: Boehringer Ingelheim, AbbVie, Amgen, Janssen, Lilly and AstraZeneca/MedImmune Julian PanÃ©s: AbbVie, Arena, Boehringerâ€Ingelheim, Galapagos, Genentech/Roche, Janssen, MSD, Pfizer, Takeda, TiGenix and Topivert. Marc Ferrante: Boehringer Ingelheim, Takeda, AbbVie, Falk, Ferring, Janssen, MSD, Tillotts, Chiesi, Mitsubishi Tanabe and Zeria Pharmaceutical Co. Edouard Louis: AbbVie, MSD, Ferring, Takeda, Celltrion, Mundipharma, Hospira and Janssen. Geert D'Haens: AbbVie, Ablynx, Amakem, AM Pharma, Avaxia, Biogen, Bristolâ€Myers Squibb, Boehringer Ingelheim, Celgene, Celltrion, Cosmo, Covidien, enGene, Ferring, Dr FALK Pharma, Galapagos, Gilead, GlaxoSmithKline, Hospira, Immunic, Johnson & Johnson, Lycera, Medimetrics, Millennium/Takeda, Mitsubishi Pharma, Merck Sharp and Dohme, Mundipharma, Novo Nordisk, Norgine, Pfizer, Prometheus laboratories/NestlÃ©, Protagonist, Receptos, Robarts Clinical Trials, Salix, Sandoz, Setpoint, Shire, Teva, Tigenix, Tillotts, TopiVert, Versant and Vifor Denis Franchimont: AbbVie, MSD, Amgen, Ferring, Takeda, Mundipharma, Hospira and Pfizer. Arthur Kaser: Boehringer Ingelheim, Ferring, Genentech, GlaxoSmithKline, Hospira, Janssen, Kymab, Second Genome and VHsquared Ltd. Olivier Dewit: AbbVie, MSD, Ferring, Takeda, Mundipharma, Hospira Ursula Seidler: Boehringer Ingelheim, Pfizer, Janssen, Roche, Gilead, Salix, Mitsubishi, MSD and Takeda. Markus Neurath: AbbVie Deutschland GmbH & Co. KG, Bionorica SE, Boehringer Ingelheim GmbH & Co. KG, e.Bavarian Health GmbH, Falk Foundation, F. Hoffmann La Roche GmbH, Genentech Inc., Hexal AG, Index Pharmaceuticals AB, Janssenâ€Cilag GmbH, MSD Sharp and Dohme GmbH, Pentax Europe GmbH, PPM Services S. A., Takeda Pharma Vertrieb GmbH & Co. KG and Tillotts Pharma AG. IgG1 antiâ€TNF antibodies infliximab and adalimumab achieve complete mucosal healing in a considerable proportion of Crohn's patients. In contrast, the antiâ€TNF fab' fragment certolizumab shows a low healing rate. These observations suggest that besides TNF neutralization, other antibody characteristics contribute to the therapeutic potency. We have previously shown that the Fc region of antiâ€TNF induces CD206+ regulatory macrophages with woundâ€healing properties in IBD patients. Here we demonstrate the importance of Fcâ€receptor signaling for the therapeutic effect of antiâ€TNF in vivo and in vitro. Furthermore, increasing the Fc binding affinity resulted in improved therapeutic efficacy. RAGâ€/â€ mice lacking all activating FcÎ³ receptors were generated. We constructed defucosylated antiâ€murine TNF and defucosylated adalimumab. All in vivo studies were performed in the Tâ€cell transfer colitis model. For in vitro studies, Tâ€cell proliferation and CD206+ macrophages percentages were measured in mixed lymphocyte reactions containing human PBMC from healthy donors. Antiâ€TNF treatment achieved near complete healing of mouse intestines in the Tâ€cell transfer model. However, mice lacking all activating FcÎ³â€receptors were completely unresponsive to antiâ€TNF therapy. In line with our previous human data, colons of wildâ€type mice treated with antiâ€TNF contained increased amounts of CD206+ macrophages, but this effect was completely abrogated in FcÎ³â€receptor KO mice. Further studies in vitro revealed that blocking FcÎ³â€receptor III (CD16) impaired both induction and function of human CD206+ macrophages, suggesting this may be the most prominent receptor mediating antiâ€TNF effects in humans. Interestingly, affinity for FcÎ³â€receptor III can be increased by defucosylation of the Fc region of the antibody. Supporting the idea of a crucial role for Fc receptor binding in antiâ€TNF efficacy, defucosylation of antiâ€TNF enhanced the induction of CD206+ macrophages and improved inhibition of Tâ€cell proliferation in vitro. Finally, defucosylated antiâ€TNF significantly improved therapeutic efficacy in the Tâ€cell transfer model and increased the amounts of CD206+ macrophages in the colon. Activating Fcâ€gamma receptors are indispensable for the therapeutic effect of antiâ€TNF in IBD in vivo. Enhancing the IgG Fc binding affinity of antiâ€TNF by defucosylation, improved the effectiveness in vivo and in vitro. Since more than oneâ€third of IBD patients do not respond to induction therapy, enhancing the Fc binding affinity of antiâ€TNF by defucosylation could be an effective approach to improve clinical outcome. Geert D'Haens: reports having received consulting and/or lecture fees from AbbVie and more; however, this will exceed the maximum of 255 characters and is therefore not possible to submit. Brad Mcrae: employee at Abbvie Gijs R. van den Brink: has received consulting fees from Abbott laboratories and lecture fees from Abbott laboratories, Merck Sharp & Dohme and Ferring Pharmaceuticals. He has received research grants from Abbott laboratories, Crucell and Ferring Pharmaceuticals. Inflammatory bowel diseases (IBD), including Crohnâ€™s disease (CD) and ulcerative colitis (UC), are chronic, relapsing and destructive inflammatory disorders of the gastrointestinal tract. Interleukinâ€6 (Ilâ€6), with its contextâ€dependent proâ€ and antiâ€inflammatory properties, is known as a key modulator of the inflammatory response in IBD, as well as in other inflammatory diseases and cancer, and therefore regarded as a prominent target for clinical intervention.1 Sgp130Fc is the natural inhibitor of the soluble interleukinâ€6 receptor transsignaling responses and therefore inhibits the proâ€inflammatory effects of ILâ€6 while the antiâ€inflammatory impact of ILâ€6 via the membraneâ€bound receptor (soâ€called classical signaling) is left intact. We analyzed the effect of transgenically increased sgp130Fc serum levels in DSSâ€colitis and AOMâ€DSS induced tumorgenesis. For this purpose we used a mouse model with wildâ€type mice (all on a C57BL6/N background) (n = 28), mice with genetically increased sgp130Fc levels (n = 34), and wildâ€type mice that were injected recombinant sgp130Fc once a week (n = 17). The three groups of mice received 3% DSS in the drinking water for 5 days. To induce tumorgenesis in mice with genetically increased sgp130Fc levels and the corresponding wildâ€type mice, we injected AOM at two different time points and let the mice go through three consecutive cycles (each over 7 days) of 2% DSS in the drinking water. In the model of DSS colitis, significantly (p < 0.05) more weight loss but no difference in disease activity was seen in the transgenic mice compared with the wildâ€type mice. Mice that were injected with recombinant sgp130Fc had significantly (p < 0.05) lower disease activity compared with the wildâ€type mice. However, these clinical data could not be supported by histological changes in the tissue since no differences in mucosal disease activity were observed. FACS analysis of colonic lamina propria cells in DSSâ€colitis showed that the number of macrophages was not increased in the early phase of inflammation (day 4) in mice treated with recombinant sgp130Fc compared with wildâ€type mice. However, at a later time point (day 8) the level of macrophages increased in transgenic sgp130Fc mice compared with wildâ€type mice and mice injected with sgp130Fc but remained low in mice treated with recombinant sgp130Fc mice. In the model of AOMâ€DSSâ€induced colon cancer, no significant differences could be observed concerning tumor number, tumor load and tumor size in transgenic sgp130Fc mice compared with the wildâ€type animals. Although the injection of recombinant sgp130Fc limits the disease activity in DSSâ€colitis, these changes in clinical outcome are not accompanied by changes in mucosal disease activity. In contrast, the transgenic increase of sgp130Fc leads to an increase in disease activity in the DSSâ€colitis mouse model, while no significant differences in the histology could be detected. The effect of limited tumorgenesis via injected sgp130Fc could not be seen in the transgenic sgp130Fc mice. Finally, it needs to be pointed out that transgenically elevated serum levels of sgp130Fc have a strikingly different effect on colitis disease activity than temporarily increased sgp130Fc levels.. Of the 5 sphingosineâ€1â€phosphate (S1P) receptor subtypes, activation of 1, 4 and 5 may be involved with decreasing intestinal inflammation and subtypes 2 and 3 with cardiac, pulmonary and tumorâ€related side effects. Current S1P modulators lack receptor selectivity and are associated with cardiopulmonary side effects. Etrasimod is a nextâ€generation oral S1P modulator with an optimized S1P receptor activity profile currently in Phase 2 clinical development for treating ulcerative colitis (UC). The objective of these studies was to evaluate the receptor profile of etrasimod and its effects in a UC model in SCID mice. Agonist (S1P1â€5) and antagonist (S1P2â€3) human S1P receptor Î²â€arrestin recruitment assays were used to determine EC50 values for etrasimod. EC50 determinations were performed using 10 concentrations and triplicate determinations made at each test concentration. A recombinant human S1P1 receptor agonist cAMP accumulation assay was developed utilizing the same cell line. Additional studies were performed with rodent S1P1 receptors. In vivo, CD4 + CD45RBhigh T cells were isolated from wildâ€type mice and transferred into SCID mice to induce colon inflammation and pathology reminiscent of human UC. Etrasimod (1 and 3 mg/kg) was administered prophylactically from the day of Tâ€cell transfer until Day 32. The relationship between etrasimod plasma concentrations and lymphocyte counts was explored after oral administration in male BALB/c mice. Using Î²â€arrestin assays, etrasimod demonstrated potent activity at the S1P1 receptor with a mean EC50 of 6.10 nM, and partial activity at S1P4 and S1P5 receptors (EC50 147 nM and 63% efficacy for S1P4 receptor vs. S1P and EC50 24.4 nM and 73% efficacy for S1P5 receptor vs. S1P). Etrasimod was inactive at concentrations up to 10 ÂµM in S1P2 or S1P3 assays performed in agonist or antagonist modes. In the S1P1 receptor cAMP accumulation assay, etrasimod was a potent agonist (mean EC50 0.199 nM). Similar potencies were found for nonâ€human S1P1 receptors. Mice that received CD4+ CD45RBhigh T cells progressively developed symptoms of colitis compared with mice that received unsorted CD4+ T cells. Prophylactic treatment with 3 mg/kg/day etrasimod and 1 mg/kg/day FTY720 significantly inhibited weight loss and colon inflammation versus vehicleâ€treated controls. Effective lymphocyteâ€lowering was shown in normal mice at the same doses. These data demonstrate potent activity of etrasimod at human and nonâ€human S1P1 receptors, with 24X and 4X respective selectivity over human S1P4 and S1P5 and no activity at human S1P2 or S1P3. Etrasimod was also efficacious in an in vivo model of UC in SCID mice. Based on these results, etrasimod may provide systemic and local immune cell modulation in the treatment of UC by selectively targeting S1P receptor subtypes 1, 4 and 5 while avoiding receptors associated with safety issues. Laurent Peyrinâ€Biroulet: Merck, Abbvie, Janssen, Genentech, Mitsubishi, Ferring, Norgine, Tillots, Vifor, Hospira/Pfizer, Celltrion, Takeda, Biogaran, Boerhingerâ€Ingelheim, Lilly, HACâ€Pharma, Index Pharmaceuticals, Amgen, Sandoz, Forward Pharma GmbH, Celgene, Biogen, Lycera, Samsung Bioepis Joel Gatlin: Was an employee of Arena during the conduct of the study. Michelle Soloman: Was an employee of Arena during the conduct of the study. David Unett: Was an employee of Arena during the conduct of the study. Hussien Alâ€Shamma: Was an employee of Arena during the conduct of the study. Dominic Behan: Was an employee of Arena during the conduct of the study. The combination of positron emission tomography (PET) with 18Fâ€fluorodeoxyglucose (18Fâ€FDG) with magnetic resonance imaging (MRI) as integrated PET/MRI in one examination is a new cuttingâ€edge technology for the nonâ€invasive assessment of the inflammatory activity in Crohnâ€™s disease (CD). In CD a manifestation throughout the gastrointestinal tract is possible. Therefore, a comprehensive diagnostic workâ€up is recommended. A number of nonâ€invasive biomarkers like Lactoferrin and Calprotectin are increasingly popular and used in allâ€day patient care. We aimed to compare the performance of nonâ€invasive biomarkers with PET/MRI and colonoscopy in patients with CD. In every patient a PET/MRI including the maximum standardized uptake value ratio gut/liver (SUVQuot) and a colonoscopy including an endoscopy index (SESâ€CD) was performed. The PET/MRI was rated as pivotal for the small bowel and colonoscopy for the colon. Lactoferrin (LF; TechLab), CalprotectinIMUN (CalI; Immundiagnostik), CalprotectinCALPREST (CalP; Eurospital), PMNâ€elastasis (PMNâ€e; mmundiagnostik), S100A12 (Immundiagnostik) as well as CRP (?0.5 mg/dl) were correlated to the SUVQuot and the SESâ€CD using nonparametric correlation analyses. Sensitivity, specificity and diagnostic accuracy were calculated using optimized cutâ€offs. 50 patients (32 female), mean age 43.1 Â± 13.4 years (range 20â€“67) with known CD were included in the study. N = 28 patients showed signs of active disease in colonoscopy and/or PET/MRI, n = 14 patients showed at least one stenosis. SUVQuot was correlated significantly with the SESâ€CD (r (45) = .43; p = .003) and with LF (r (50) = .32; p = .024) and S100A12 (r (50) = .30; p = .037), but not with PMNâ€e, CalP, CalI and CRP (all p > .05). The median levels (inactive/active) of LF were: 4.6/10.4 Âµg/g, CalI: 87.9/125.5 Âµg/g, CalP: 195.0/238.9 Âµg/g; PMNâ€E: 0.130/0.150 Âµg/g, S100A12: 55.48/53.45, CRP: 0.45/0.35 mg/dl,. Using optimized cutâ€offs sensitivity, specificity and diagnostic accuracy (confidence interval) for LF was 57.1%/59.1%/60.6% (CI 44.9â€“76.2%); optimized cutâ€off: 7.7 Âµg/g; CALP: 57.1%/59.1%/63.0% (CI 47.5â€“78.4%); 207.63 Âµg/g; CALI: 60.7%/63.6%/70.0% (CI 55.5â€“84.5%); 107.19 Âµg/g; PMNâ€e: 42.9%/54.5%/51.8% (CI 35.4â€“68.2%); 0.1550 Âµg/g; S100A12: 50.0%/50.0%/58.1% (CI 42.2â€“74.0%); 54.32; and CRP: 50.0%/59.1%/59.7% (CI 43.8â€“75.5%); 0.45 mg/dl. Fecal biomarkers Lactoferrin and Calprotectin outperform CRP for detecting active CD using combined PET/MR enterography and endoscopy. Lactoferrin and S100A12 levels were significantly correlated to the SUVQuot which was significantly correlated with SESâ€CD. Further studies are needed to assess the performance of fecal biomarkers in Crohnâ€™s disease identified by PET/MRI. Colonoscopy is the gold standard investigation of the colon but is imperfect due to variation in operator detection rates.1 Low adenoma detection rates (ADR) are associated with increased risk of interval colorectal cancer2 and therefore devices to enhance mucosal visualisation and improve ADR have been developed. Endocuffâ€Visionâ„¢ is an endoscopic cuff with "fingerâ€like" projections which fold forwards during scope withdrawal to open up the field of view. The ADENOMA study (Accuracy of Detection Using Endocuffâ€Visionâ„¢ Optimisation of Mucosal Abnormalities) aimed to compare the adenoma detection rate between Endocuffâ€Visionâ„¢â€assisted colonoscopy (EAC) and standard colonoscopy (SC). This was a multiâ€centre, randomised controlled trial conducted in seven United Kingdom hospitals (academic and community). Patients were undergoing colonoscopy for symptoms, surveillance, or as part of the Bowel Cancer Screening Programme (BCSP) following an initial positive screening faecal occult blood (FOB) test. Patients underwent stratified randomisation based on age, gender, hospital and group (BCSP or nonâ€BCSP). All colonoscopists were trained in Endocuffâ€Visionâ„¢ use and patients were followed up at 21 days to assess for adverse events. Results were analysed globally and then separately for BCSP and nonâ€BCSP patients. Caecal intubation rate, insertion time, withdrawal time and patient satisfaction scores were measured on a nonâ€inferiority basis. 1772 patients (57% male, mean age 62) were recruited from seven sites over a 15â€month period. There were 975 nonâ€BCSP and 797 BCSP patients and patient characteristics were comparable between arms. Endocuffâ€Visionâ„¢ increased ADR from 36.2% (SC) to 40.9% (EAC) (p = 0.02), which was largely driven by the BCSP subgroup. Mean adenomas per procedure (MAP) was higher with EAC. In addition, EAC detected more leftâ€sided adenomas, sessile serrated adenomas, diminutive adenomas, small adenomas and cancers. Cuff removal rate was 4.1%, mostly due to angulation in a fixed sigmoid colon. Median intubation time was a minute quicker in the EAC arm (p = 0.001). Overall patient comfort scores were similar in both groups but anal intubation was rated as more uncomfortable with EAC. Adverse events were equivalent in both arms with no significant cuffâ€related adverse events. For insertion and withdrawal times, EAC was nonâ€inferior to SC in both subgroups. Standard colonoscopy Endocuffâ€Visionâ„¢â€ assisted colonoscopy p â€value Adenoma detection rate Global 36.2% 40.9% 0.02 BCSP 50.9% 61.7% 0.001 Nonâ€BCSP 23.9% 24.3% 0.44 Mean adenomas detected per procedure Global 0.75 0.95 0.02 BCSP 1.20 1.59 0.004 Nonâ€BCSP 0.37 0.44 0.42 Left colon adenomas Global 22.2% 26.1% 0.03 BCSP 32.8% 40.9% 0.009 Nonâ€BCSP 13.3% 14.4% 0.31 Right colon adenomas Global 24.8% 27.5% 0.10 BCSP 38.0% 43.2% 0.07 Nonâ€BCSP 13.7% 15.0% 0.29 Small adenomas (6â€“9 mm) Global 7.7% 10.6% 0.02 BCSP 10.7% 19.0% <0.001 Nonâ€BCSP 5.2% 3.9% 0.85 Diminutive adenomas (5 mm or less) Global 30.8% 34.6% 0.04 BCSP 44.7% 52.0% 0.02 Nonâ€BCSP 19.1% 20.7% 0.28 Sessile serrated adenomas Global 1.1% 2.3% 0.03 BCSP 1.2% 2.0% 0.19 Nonâ€BCSP 1.0% 2.4% 0.05 Cancers Global 2.3% 4.1% 0.02 BCSP 3.7% 6.6% 0.03 Nonâ€BCSP 1.0% 2.0% 0.11 Caecal intubation rate Global 96.4% 96.7% â€“ BCSP 97.8% 97.7% â€“ Nonâ€BCSP 95.2% 96.0% â€“ This large randomised controlled trial demonstrates that Endocuffâ€Visionâ„¢ significantly improves ADR. This improvement was driven by a large increase in ADR in FOB positive BCSP patients. Endocuffâ€Visionâ„¢ allowed quicker colonic intubation and was nonâ€inferior in all aspects other than discomfort on anal intubation. This study demonstrates that in a population where adenomas are likely to be found the Endocuffâ€Visionâ„¢ improves mucosal visualisation and adenoma detection. Endoscopic hemostasis with throughâ€theâ€scope clips (TTSC) plus injection therapy is highly effective for bleeding peptic ulcers. However, reâ€bleeding is difficult to treat and associated with substantial morbidity and mortality. There is growing evidence that application of overâ€theâ€scope clips (OTSC) is an effective treatment modality for severe upper gastrointestinal bleeding. Prospective randomized trials are lacking to date. We are currently conducting a randomized controlled trial (RCT) at nine academic referral centers. Patients with recurrent peptic ulcer bleeding after initial successful hemostasis were randomly assigned to receive endoscopic hemostasis with either OTSC or TTSC plus injection therapy (with diluted adrenaline). Primary endpoint was a composite endpoint of successful initial hemostasis and freedom of reâ€bleeding within 7 days. From 03/2013 to 08/2016, 60 patients were enrolled into the study; 59 were eligible for analysis. 26 patients were randomized in the TTSC arm and 33 patients were randomized in the OTSC arm. Initial hemostasis was achieved in 15/26 patients (57.6%) in the TTSC group and 30/33 patients (90.9%) in the OTSC group (p = 0.005). Reâ€bleeding within 7 days was observed in 10 patients (38%) in the TTSC and 6 patients (18%) in the OTSC group (p = 0.139). The composite endpoint of successful initial hemostasis and freedom of reâ€bleeding was reached in eight patients (30.8%) in the TTSC group and 26 patients in the OTSC Group (78%) (p = 0.0004). Two patients in the TTSC (7.6%) and one patient in the OTSC group (3.0%) required surgical therapy. 30dâ€mortalitiy was 7.6% in the TTSC (two patients) and 3% in the OTSC group (one patient). Preliminary results of this RCT indicate that endoscopic hemostasis with OTSCs is superior to TTSCs plus injection therapy for recurrent peptic ulcer bleeding (STING study, Clinical trials.gov: NCT1836900). Arthur Schmidt has recieved lecture fees from Ovesco Endoscopy Karel Caca: Prof. Caca has recieved lecture fees from Ovesco Endoscopy. Endoscopic transâ€mural drainage of pancreatic walledâ€off necrosis (WON) can be performed using double pigtail plastic (DP), fully covered selfâ€expanding metal (FCSEMS) or lumenâ€apposing selfâ€expanding metal stents (LAMS). Although researchers have felt that the higher price of the LAMS is compensated by its increased efficacy compared with other stents, a rigorous analysis has to date not been completed.We thus aimed to evaluate which type of stent (DP, FCSEMS, or LAMS) was superior for the therapy of WON with regard to clinical outcomes, associated costs and quality of life. A 6â€month, multicenter trial evaluating consecutive patients with WON managed by EUSâ€guided drainage was previously conducted. The results of this trial were used to inform a populationâ€level, deterministic mathematical model to estimate treatment cost effectiveness in patients treated with DP, FCSEMS, or LAMS. Clinical data was based on the experience of 313 patients, including only those for whom 6â€month or greater followâ€up was availabel; from which costing figures of adverse events were estimated from previously published peerâ€reviewed data, procedural reimbursement calculated using 2016 Medicare hospital IPPS/OPPS reimbursement data for relevant CPT & ICDâ€10 codes, and analysis of endoscopic necrosectomy coding. In the original study, quality of life estimates were not directly measured; in this current effort, these values were based on conditionâ€specific estimates from previously published costâ€effectiveness studies. The analysis was conducted from the US payer perspective over a period of 7 months. Patients who failed endoscopic WON drainage were referred to interventional radiology (IR) or surgery and were assumed to have recovered in 1 month. From a US hospital perspective, the initial placement of DP stents ($125) and FCSEMS ($2,500) cost was significantly less than placement of LAMS ($4900) (p = 0.01). On 6â€month followâ€up, complete resolution of WON using DP stents was lower compared with those who underwent drainage with FCSEMS and LAMS (81% vs. 95% vs. 90%; p = 0.001). Based upon on increased rate of stent occlusion and migration, severity of adverse events, and greater need for concomitant surgical or percutaneous drainage, DP stents were significantly more expensive than FCSEMS and LAMS from a 7â€month payer perspective ($26,695 vs. $16,475.52 vs. $10,968.17 respectively; p < 0.01). DP stents also produced fewer qualityâ€adjusted lifeâ€years (QALYs) per patient than FCSEMS or LAMS (approximately 0.47, 0.49 and 0.51, respectively). Compared with DP and FCSEMS, LAMS is clinically superior and the more costâ€effective treatment for the endoscopic management of WON. LAMS results in the reduction in cost as a result of reduced endoscopic reâ€interventions and costly adverse events. In addition, use of LAMS leads to an improvement in QOL due to reduced endoscopic reinterventions and overall mortality. Mr. Wilson is an employee and nonâ€significant stock holder of Boston Scientific, a company which markets lumenâ€opposing metal stents. Mr. Cangelosi is an employee and nonâ€significant stock holder of Boston Scientific, a company which markets lumenâ€opposing metal stents. The purpose of the study was to obtain safety and performance data for the microwave coagulation modality of the CROMA Electrosurgical System (Creo Medical Ltd, UK) and RS2 polyp resection device to complement existing data and support a CE marking application.1 This study was a dualâ€centre, nonâ€randomised, prospective, nonâ€comparative, prospective pivotal trial. The primary objective was to assess the safety and efficacy of microwave coagulation of visible blood vessels and the treatment of oozing/actively bleeding vessels (?1mm in diameter) during endoscopic resection of complex colorectal polyps. The secondary objective was the identification and incidence of periâ€and postâ€procedural complications. Patients completed part 1 of the study if vessels were treated with microwave during polypecytomy and part 2 of the study when they attended for endoscopic followâ€up at 3â€“6 months. A total of 38 patients were screened; one patient declined to participate; 37 patients were enrolled; 31 patients completed Part 1 of the study; 30 patients completed Part 2 of the study. Six patients were withdrawn from the study on completion of their endoscopic procedure; either for clinical reasons (diagnosis of cancer, Ã—2) or absence of suitable blood vessels for coagulation (Ã—4). One patient was withdrawn following their telephone followâ€up subsequent to a diagnosis of cancer. The majority of participants in the study were male (67.7% vs. 32.3%). Mean age, height, weight and BMI was 66.4y (SD 13.2), 1.7 m (SD 0.1), 79.7 kg (SD 16.9) and 27.7 kg.m2 (SD 4.9), respectively. There were no protocol violations in the study. Patient compliance with the visit schedule for the study was 100%. Patient recruitment was completed in 8.3 months. A total of 51 blood vessels meeting the required inclusion criteria were treated in Part 1 of the study. The majority of blood vessels, 98% (50/51) were successfully coagulated by the microwave device (95% exact CI 89.6% to 100%). The majority of study patients (30/31, 96.8%) had all blood vessels meeting the study inclusion criteria successfully coagulated (95% exact CI 83.3% to 99.9%). The majority of these blood vessels (29/51, 56.9%) were successfully coagulated at first attempt (95% exact CI 42.3% to 70.7%). One blood vessel could not be coagulated by two applications of microwave energy, but was successfully treated by the use of an endoclip, as per protocol. There were no significant complications or deviceâ€related events in the study. Of those patients completing Part 2 of the study, 97% had a completely healed scar at the site of their polypectomy and one patient had some granulation tissue. Five patients displayed small recurrence of <1 cm and one patient had a recurrence of >1 cm (recurrence rate 17%), all easily treatable with further endoscopic therapy. Microwave coagulation appears to be a safe and easy modality to apply. No serious adverse events were recorded. No disruption of the normal healing process was noted. Small vessels (?1mm in diameter) were successfully treated at a preâ€coagulation or bleeding phase using the microwave coagulation. Earlier diagnosis of colorectal cancer (CRC) is associated with better patient outcomes so, between 2001 and 2014, the National Health Service (NHS) in England ran four national programmes intended to promote it. These included a reduction in waiting times for colonoscopy to a maximum of 2 weeks for urgent cases and 6 weeks for routine (2004â€“07), a national colonoscopy training programme (2003â€“2008), an FOBTâ€based screening programme (2006 onwards) and national symptom awareness campaigns (2011â€“12). This study hypothesised that if these initiatives were successful there would initially be a rise in the ageâ€standardised incidence (ASI) of CRC due to the earlier diagnosis of tumours already present in the population, achieved by earlier, more frequent and higher quality colonoscopies. A drop in incidence would then follow this rise, as there would be fewer patients presenting late. In addition, it would be expected that with more widespread and higherâ€quality colonoscopy, fewer cancers would occur because of the â€˜preventativeâ€™ effect of colonoscopy. We hypothesised that the speed and depth of any drop in incidence would reflect the impact of the interventions. We aimed, therefore to compare CRC incidence rates with colonoscopy use (both inside and outside of the screening programme) and emergency presentation rates (as a surrogate for late diagnosis). Information on all CRCs diagnosed between 2001 and 2014 was extracted from the National Cancer Registration and Analysis Service (NCRAS) and directly ageâ€standardised incidence rates were calculated for each year. In addition, data were taken from Hospital Episode Statistics (HES) on all colonoscopy and flexible sigmoidoscopy procedures undertaken in the English NHS in each financial year between 2001/02 and 2014/15. Emergency presentation rates were derived from linked NCRAS and HES data. Incidence rates of CRC, the use of colonoscopic procedures and emergency presentation rates were then compared. CRC incidence rates rose steadily in England during the period of the national initiatives, until 2012, when there was a sudden and statistically significant 5.6% reduction. The drop was greatest in men. Over this period the numbers of colonoscopic procedures undertaken more than doubled from 303,790 in 2001/2 to 778,292 in 2014/15. From 2010 onwards, around 7% of these colonoscopies were undertaken as part of the national screening programme. The proportion of cases presenting urgently dropped from 33.6% in 2001 to 23.7% in 2014. Overall ageâ€standardised incidence (ASI) rates of colorectal cancer 2001â€“2014 Year ASI (95%CI) 2001 70.5 (60.7â€“71.4) 2002 69.5 (68.7â€“70.4) 2003 69.9 (69.0â€“70.7) 2004 71.6 (70.8â€“72.5) 2005 72.2 (71.3â€“73.0) 2006 72.8 (71.9â€“73.6) 2007 73.4 (72.6â€“74.3) 2008 75.0 (74.1â€“75.8) 2009 75.6 (74.8â€“76.4) 2010 75.4 (74.6â€“76.2) 2011 76.2 (75.4â€“77.0) 2012 76.0 (75.2â€“76.8) 2013 72.4 (71.6â€“73.2) 2014 70.4 (69.7â€“71.2) This study demonstrates a change in ASI of CRC, increasing use of colonoscopic procedures and a reduction in emergency presentation rates that all occurred concurrently with national initiatives aimed at improving outcomes through earlier diagnosis. These associations suggest the initiatives have had the intended impact. The ASI changes were greatest in men, which is important as they have been more likely to present with lateâ€stage disease and less likely to engage in screening. We propose that ASI should be a key outcome measure to monitor the impact of national interventions aimed at improving the timing of diagnosis of CRC and the quality of the diagnostic test. All authors have declared no conflicts of interest. Endoscopic ultrasound (EUS)â€guided pancreatic cyst ablation (PCA) has been shown to be effective, but longâ€term durability is not well understood. The aim of this study was to investigate the longâ€term efficacy and durability of EUSâ€PCA for pancreatic cystic tumors. In this prospective cohort study, patients with pancreatic cystic lesions (PCLs) underwent EUSâ€PCA using ethanol with or without paclitaxel. The inclusion criteria were as follows: unilocular or oligolocular cysts without definite communication to the main pancreatic duct; clinically indeterminate pancreatic cystic tumors; and patients who refused surgery or were at high risk. Outcomes included efficacy, safety, and durability of response. Multivariate analysis was used for variables to demonstrate the independent factors related to complete cyst resolution. A total of 164 patients with pancreatic cysts underwent one (n = 159) or two (n = 5) EUSâ€PCA procedures, and five patients were excluded from the analysis due to followâ€up loss. The median cyst diameter at baseline was 32 mm, and the median level of carcinoembryonic antigen was 157 ng/ml. Presumed clinical diagnoses were mucinous cystic neoplasm in 95 patients, serous cystadenoma in 23 patients, intraductal papillary mucinous neoplasm in 22 patients, and indeterminate cyst in 24 patients. Complete resolution (CR) was achieved in 115 patients (72.2%), partial resolution was achieved in 40 patients, and 13 patients had persistent cysts. Twelve of 13 patients with persistent cysts underwent surgical resection. Procedureâ€related adverse events occurred in 18 (11%) patients. During further followâ€up (median 72 months) of the 106 patients with initial successful CR, only two patients (2%) showed cyst recurrence. Based on multivariate analysis, the absence of septa and nonâ€mucinous cysts predicted cyst resolution. In patients with PCLs, EUSâ€PCA has an acceptable safety profile, is durable, and is associated with a low rate of recurrence for up to 6 years. Unilocular and nonâ€mucinous cysts predicted CR. The cause of acute pancreatitis (AP) remains elusive even after extensive workâ€up in up to 30 % of cases. Finding a treatable cause may help to prevent recurrent episodes. The aim of our study was to determine the efficacy of early endoscopic ultrasound (EUS), performed within 24 h of admission, in evaluating the etiology of idiopathic acute pancreatitis (IAP) after a first attack of AP. During the study period (2010â€“2016), 850 cases of AP were admitted. Out of these, aetiology was determined in 666 (78.35 %). There were 184 cases of IAP. EUS examination was done using a linear echo endoscope. Out of 158 cases (90 males; age range: 15â€“70 years) of IAP, (26 were excluded) EUS was able to clinch the diagnosis in 110 patients (69.6 %). The most common causes of IAP included biliary disease (gallbladder microlithiaisis, common bile ductmicrolithiasis/stone/sludge) (n = 60) followed by chronic pancreatitis (CP) (n = 25), pancreatic tumour (n = 11) and pancreaticobiliary ascariasis (n = 9). No cause was found in 48 patients. Occult biliary stone/sludge was the predominant cause for IAP followed by CP. EUS is a safe investigation with a high diagnostic yield for determining the aetiology of IAP and an early EUS can influence important therapeutic decisions and prevent further attacks of AP which may occur if a delayed EUS is performed, and thus improve longâ€term prognosis. An early EUS also has an additional advantage of making an early diagnosis of pancreatic tumours. It also prevents making the wrong diagnosis of sludge as aetiological factor for AP, which may occur in patients undergoing a delayed EUS since sludge may be secondary to AP due to prolonged fasting, total parenteral nutrition or antibiotics like ceftriaxone. All authors have declared no conflicts of interest.},
 author = {Deterding, K. and Spinner, C.D. and Schott, E. and Welzel, T.M. and Gerken, G. and Klinker, H. and Spengler, U. and Wiegand, J. and Wiesch, J. Schulze zur and Pathil, A. and Cornberg, M. and Umgelter, A. and ZÃ¶llner, C. and Zeuzem, S. and Papkalla, A. and Weber, K. and Hardtke, S. and Leyen, H. and Koch, A. and Witzendorff, D. and Manns, M. and Wedemeyer, H. and Preda, C.M. and Popescu, C.P. and Baicus, C. and Manuc, M. and Voiosu, R. and Ceausu, E. and Fulger, L. and Nisanian, A. and Pop, C.S. and Oproiu, A. and Arezzo, A. and Passera, R. and Bullano, A. and Mintz, Y. and KEDAR, A. and Boni, L. and Cassinotti, E. and Rosati, R. and Fumagalli, U. and Sorrentino, M. and Brizzolari, M. and Di Lorenzo, N. and Gaspari, A.L. and Andreone, D. and De Stefani, E. and Navarra, G. and Lazzara, S. and Degiuli, M. and Shishin, K. and Khatkov, I. and Kazakov, I. and Schrittwieser, R. and Carus, T. and Corradi, A. and Sitzman, G. and Lacy, A. and Uranues, S. and Szold, A. and Bonino, M.A. and Morino, M. and StrÃ¶mberg, J. and Sandblom, G. and Coelen, R. and Gaspersz, M. and Labeur, T. and Vugt, J. and Dieren, S. and Willemssen, F. and Nio, C.Y. and IJzermans, J. and KlÃ¼mpen, H.â€J. and Koerkamp, B. Groot and Gulik, T. and Sturgess, R. and Palmer, D. and Trojan, J. and Hoffmeister, A. and Neu, B. and Kasper, S. and DechÃªne, A. and JÃ¼rgensen, C. and Schirra, J. and Jakobs, R. and HÃ¸gset, A. and Finnesand, L. and Elrazek, A.E. Abd and Saab, S. and Salem, T. and Abdelâ€Aty, M. and Hawary, B. and Ismail, A. and Zayied, M. and Alboraie, M. and Orenstein, R. and Dubberke, E. and Lee, C.H. and Khanna, S. and Hecht, G. and Wong, S. and Kwong, T. and Wang, X. and Tang, R.S.Y. and Ng, S.C. and Sung, J.J.Y. and Yu, J. and Ott, S. and Waetzig, G.H. and Rehmann, A. and Moltzauâ€Anderson, J. and Bharti, R. and Grasis, J.A. and Cassidy, L. and Tholey, A. and Fickenscher, H. and Seegert, D. and Rosenstiel, P. and Schreiber, S. and Mazzawi, T. and Lied, G.A. and Elâ€Salhy, M. and Gilja, O.H. and Hatlebakk, J.G. and Hausken, T. and Witt, S.T. and Bednarska, O. and Icenhour, A. and Elsenbruch, S. and StrÃ¶m, M. and SÃ¶derholm, J.D. and EngstrÃ¶m, M. and Mayer, E.A. and Keita, Ã…. and Walter, S. and Kump, P.K. and Wurm, P. and GrÃ¶chenig, H.P. and Wenzl, H.H. and Petritsch, W. and Halwachs, B. and Wagner, M. and Stadlbauerâ€KÃ¶llner, V. and Eherer, A.J. and Hoffmann, K.M. and Deutschmann, A. and Reicht, G. and Reiter, L. and Slawitsch, P. and Gorkiewicz, G. and Hoegenauer, C. and Zhou, Y. and Kakuturu, R. and Jung, D. and JÃ¸rgensen, K.K. and Olsen, I.C. and Goll, G.L. and Lorentzen, M. and Bolstad, N. and Haavardsholm, E.A. and Lundin, K.E. and MÃ¸rk, C. and Jahnsen, J. and Kvien, T.K. and Feagan, B.G. and Sands, B.E. and Rossiter, G. and Li, X. and Usiskin, K. and Zhan, X. and Colombel, J.â€F. and Feagan, B.G. and Sandborn, W.J. and PanÃ©s, J. and Ferrante, M. and Louis, E. and D'Haens, G. and Franchimont, D. and Kaser, A. and Dewit, O. and Seidler, U. and Kim, K.â€J. and Neurath, M.F. and Scholl, P. and Visvanathan, S. and Padula, S.J. and Herichova, I. and Sha, N. and Hall, D. and BÃ¶cher, W.O. and Bloemendaal, F. and Levin, A. and Wildenberg, M. and Koelink, P. and Verbeek, S. and Claassens, J. and D'Haens, G. and Mcrae, B. and Vidarsson, G. and Brink, G.R. and Badke, M. and Roseâ€John, S. and Spehlmann, M.E. and Peyrinâ€Biroulet, L. and Gatlin, J. and Soloman, M. and Unett, D. and Alâ€Shamma, H. and Behan, D. and Langhorst, J. and Boone, J. and Koch, A. and Rueffer, A. and Dobos, G. and Beiderwellen, K. and Lauenstein, T. and Ngu, W.S. and Bevan, R. and Tsiamoulos, Z.P. and Bassett, P. and Hoare, Z. and Rutter, M. and Totton, N. and Lee, T.J. and Ramadas, A.V. and Silcock, J. and Painter, J. and Neilson, L.J. and Saunders, B.P. and Rees, C.J. and Schmidt, A. and Goelder, S. and Messmann, H. and Goetz, M. and Kratt, T. and Meining, A. and Birk, M. and Delius, J. and Albert, M. and Escher, J.Y.W. and Lau, A. and Hoffman, R. and Wiest, K. and Caca and Siddiqui, A. and Wilson, D. and Cangelosi, M. and Tsiamoulos, Z.P. and Rameshshanker, R. and Wall, P. and Cocks, K. and Doulton, T. and Yusuf, A. and Hancock, C. and Saunders, B.P. and Valori, R. and Rutter, M. and Aravani, A. and Rashbass, J. and Vernon, S. and Morris, E.J.A. and â€Choi, J.H. and Seo, D.â€W. and Song, T.J. and Park, D.H. and Lee, S.S. and Lee, S.K. and Kim, â€H. and Somani, P. and Sharma, M.},
 doi = {10.1177/2050640616678364},
 journal = {UEG Journal},
 keywords = {},
 note = {https://journals.sagepub.com/doi/pdf/10.1177/2050640616678364},
 number = {6},
 pages = {800-811},
 title = {LB01 SIX WEEKS OF SOFOSBUVIR/LEDIPASVIR TREATMENT OF ACUTE HEPATITIS C VIRUS GENOTYPE 1 MONOINFECTION: FINAL RESULTS OF THE THE GERMAN HEPNET ACUTE HCV IV STUDY},
 url = {https://app.dimensions.ai/details/publication/pub.1002410606},
 volume = {4},
 year = {2016}
}

@article{pub.1004492325,
 abstract = {A new scan that matches an aided Inertial Navigation System (INS) with a low-cost LiDAR is proposed as an alternative to GNSS-based navigation systems in GNSS-degraded or -denied environments such as indoor areas, dense forests, or urban canyons. In these areas, INS-based Dead Reckoning (DR) and Simultaneous Localization and Mapping (SLAM) technologies are normally used to estimate positions as separate tools. However, there are critical implementation problems with each standalone system. The drift errors of velocity, position, and heading angles in an INS will accumulate over time, and on-line calibration is a must for sustaining positioning accuracy. SLAM performance is poor in featureless environments where the matching errors can significantly increase. Each standalone positioning method cannot offer a sustainable navigation solution with acceptable accuracy. This paper integrates two complementary technologies-INS and LiDAR SLAM-into one navigation frame with a loosely coupled Extended Kalman Filter (EKF) to use the advantages and overcome the drawbacks of each system to establish a stable long-term navigation process. Static and dynamic field tests were carried out with a self-developed Unmanned Ground Vehicle (UGV) platform-NAVIS. The results prove that the proposed approach can provide positioning accuracy at the centimetre level for long-term operations, even in a featureless indoor environment. },
 author = {Tang, Jian and Chen, Yuwei and Niu, Xiaoji and Wang, Li and Chen, Liang and Liu, Jingbin and Shi, Chuang and HyyppÃ¤, Juha},
 doi = {10.3390/s150716710},
 journal = {Sensors (Basel, Switzerland)},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/15/7/16710/pdf},
 number = {7},
 pages = {16710-16728},
 title = {LiDAR Scan Matching Aided Inertial Navigation System in GNSS-Denied Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1004492325},
 volume = {15},
 year = {2015}
}

@book{pub.1006123589,
 abstract = {The fully automated estimation of the 6 degrees of freedom camera motion and the imaged 3D scenario using as the only input the pictures taken by the camera has been a long term aim in the computer vision community. The associated line of research has been known as Structure from Motion (SfM). An intense research effort during the latest decades has produced spectacular advances; the topic has reached a consistent state of maturity and most of its aspects are well known nowadays. 3D vision has immediate applications in many and diverse fields like robotics, videogames and augmented reality; and technological transfer is starting to be a reality. This book describes one of the first systems for sparse point-based 3D reconstruction and egomotion estimation from an image sequence; able to run in real-time at video frame rate and assuming quite weak prior knowledge about camera calibration, motion or scene. Its chapters unify the current perspectives of the robotics and computer vision communities on the 3D vision topic: As usual in robotics sensing, the explicit estimation and propagation of the uncertainty hold a central role in the sequential video processing and is shown to boost the efficiency and performance of the 3D estimation. On the other hand, some of the most relevant topics discussed in SfM by the computer vision scientists are addressed under this probabilistic filtering scheme; namely projective models, spurious rejection, model selection and self-calibration.},
 author = {Civera, Javier and Davison, Andrew J. and MartÃ­nez Montiel, JosÃ© MarÃ­a},
 doi = {10.1007/978-3-642-24834-4},
 editor = {},
 keywords = {},
 pages = {},
 title = {Structure from Motion using the Extended Kalman Filter},
 url = {https://app.dimensions.ai/details/publication/pub.1006123589},
 year = {2012}
}

@article{pub.1008771254,
 abstract = {This paper reports on an active SLAM framework for performing large-scale inspections with an underwater robot. We propose a path planning algorithm integrated with visual SLAM that plans loop-closure paths in order to decrease navigation uncertainty. While loop-closing revisit actions bound the robotâ€™s uncertainty, they also lead to redundant area coverage and increased path length. Our proposed opportunistic framework leverages sampling-based techniques and information filtering to plan revisit paths that are coverage efficient. We employ Gaussian process regression for modeling the prediction of camera registrations and use a two-step optimization procedure for selecting revisit actions. We show that the proposed method offers many benefits over existing solutions and good performance for bounding navigation uncertainty in long-term autonomous operations with hybrid simulation experiments and real-world field trials performed by an underwater inspection robot.},
 author = {Chaves, Stephen M. and Kim, Ayoung and Galceran, Enric and Eustice, Ryan M.},
 doi = {10.1007/s10514-016-9597-6},
 journal = {Autonomous Robots},
 keywords = {},
 number = {7},
 pages = {1245-1265},
 title = {Opportunistic sampling-based active visual SLAM for underwater inspection},
 url = {https://app.dimensions.ai/details/publication/pub.1008771254},
 volume = {40},
 year = {2016}
}

@article{pub.1009792780,
 abstract = {We address the problems of localization, mapping, and guidance for robots with limited computational resources by combining vision with the metrical information given by the robot odometry. We propose in this article a novel light and robust topometric simultaneous localization and mapping framework using appearance-based visual loop-closure detection enhanced with the odometry. The main advantage of this combination is that the odometry makes the loop-closure detection more accurate and reactive, while the loop-closure detection enables the long-term use of odometry for guidance by correcting the drift. The guidance approach is based on qualitative localization using vision and odometry, and is robust to visual sensor occlusions or changes in the scene. The resulting framework is incremental, real-time, and based on cheap sensors provided on many robots (a camera and odometry encoders). This approach is, moreover, particularly well suited for low-power robots as it is not dependent on the image processing frequency and latency, and thus it can be applied using remote processing. The algorithm has been validated on a Pioneer P3DX mobile robot in indoor environments, and its robustness is demonstrated experimentally for a large range of odometry noise levels.},
 author = {Bazeille, Stephane and Battesti, Emmanuel and Filliat, David},
 doi = {10.1515/jisys-2014-0116},
 journal = {Journal of Intelligent Systems},
 keywords = {},
 note = {https://hal-ensta-paris.archives-ouvertes.fr//hal-01122633/file/Bazeille_2015.pdf},
 number = {4},
 pages = {505-524},
 title = {A Light Visual Mapping and Navigation Framework for Low-Cost Robots},
 url = {https://app.dimensions.ai/details/publication/pub.1009792780},
 volume = {24},
 year = {2015}
}

@article{pub.1012410463,
 abstract = {Robots that use vision for localization need to handle environments that are subject to seasonal and structural change, and operate under changing lighting and weather conditions. We present a framework for lifelong localization and mapping designed to provide robust and metrically accurate online localization in these kinds of changing environments. Our system iterates between offline map building, map summary, and online localization. The offline mapping fuses data from multiple visually varied datasets, thus dealing with changing environments by incorporating new information. Before passing these data to the online localization system, the map is summarized, selecting only the landmarks that are deemed useful for localization. This Summary Map enables online localization that is accurate and robust to the variation of visual information in natural environments while still being computationally efficient. We present a number of summary policies for selecting useful features for localization from the multisession map, and we explore the tradeoff between localization performance and computational complexity. The system is evaluated on 77 recordings, with a total length of 30 kilometers, collected outdoors over 16 months. These datasets cover all seasons, various times of day, and changing weather such as sunshine, rain, fog, and snow. We show that it is possible to build consistent maps that span data collected over an entire year, and cover day-to-night transitions. Simple statistics computed on landmark observations are enough to produce a Summary Map that enables robust and accurate localization over a wide range of seasonal, lighting, and weather conditions.},
 author = {MÃ¼hlfellner, Peter and BÃ¼rki, Mathias and Bosse, Michael and Derendarz, Wojciech and Philippsen, Roland and Furgale, Paul},
 doi = {10.1002/rob.21595},
 journal = {Journal of Field Robotics},
 keywords = {},
 number = {5},
 pages = {561-590},
 title = {Summary Maps for Lifelong Visual Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1012410463},
 volume = {33},
 year = {2016}
}

@article{pub.1012432501,
 abstract = {This paper reports on a system for an autonomous underwater vehicle to perform in situ, multiple session hull inspection using long-term simultaneous localization and mapping (SLAM). Our method assumes very little a priori knowledge, and it does not require the aid of acoustic beacons for navigation, which is a typical mode of navigation in this type of application. Our system combines recent techniques in underwater saliency-informed visual SLAM and a method for representing the ship hull surface as a collection of many locally planar surface features. This methodology produces accurate maps that can be constructed in real-time on consumer-grade computing hardware. A single-session SLAM result is initially used as a prior map for later sessions, where the robot automatically merges the multiple surveys into a common hull-relative reference frame. To perform the relocalization step, we use a particle filter that leverages the locally planar representation of the ship hull surface, and a fast visual descriptor matching algorithm. Finally, we apply the recently developed graph sparsification tool, generic linear constraints, as a way to manage the computational complexity of the SLAM system as the robot accumulates information across multiple sessions. We show results for 20 SLAM sessions for two large vessels over the course of days, months, and even up to three years, with a total path length of approximately 10.2 km.},
 author = {Ozog, Paul and Carlevarisâ€Bianco, Nicholas and Kim, Ayoung and Eustice, Ryan M.},
 doi = {10.1002/rob.21582},
 journal = {Journal of Field Robotics},
 keywords = {},
 note = {http://robots.engin.umich.edu/publications/pozog-2015a.pdf},
 number = {3},
 pages = {265-289},
 title = {Longâ€term Mapping Techniques for Ship Hull Inspection and Surveillance using an Autonomous Underwater Vehicle},
 url = {https://app.dimensions.ai/details/publication/pub.1012432501},
 volume = {33},
 year = {2016}
}

@article{pub.1014931235,
 abstract = {RatSLAM is a navigation system based on the neural processes underlying navigation in the rodent brain, capable of operating with low resolution monocular image data. Seminal experiments using RatSLAM include mapping an entire suburb with a web camera and a long term robot delivery trial. This paper describes OpenRatSLAM, an open-source version of RatSLAM with bindings to the Robot Operating System framework to leverage advantages such as robot and sensor abstraction, networking, data playback, and visualization. OpenRatSLAM comprises connected ROS nodes to represent RatSLAMâ€™s pose cells, experience map, and local view cells, as well as a fourth node that provides visual odometry estimates. The nodes are described with reference to the RatSLAM model and salient details of the ROS implementation such as topics, messages, parameters, class diagrams, sequence diagrams, and parameter tuning strategies. The performance of the system is demonstrated on three publicly available open-source datasets.},
 author = {Ball, David and Heath, Scott and Wiles, Janet and Wyeth, Gordon and Corke, Peter and Milford, Michael},
 doi = {10.1007/s10514-012-9317-9},
 journal = {Autonomous Robots},
 keywords = {},
 number = {3},
 pages = {149-176},
 title = {OpenRatSLAM: an open source brain-based SLAM system},
 url = {https://app.dimensions.ai/details/publication/pub.1014931235},
 volume = {34},
 year = {2013}
}

@article{pub.1016451346,
 abstract = {Visual localization systems that are practical for autonomous vehicles in outdoor industrial applications must perform reliably in a wide range of conditions. Changing outdoor conditions cause difficulty by drastically altering the information available in the camera images. To confront the problem, we have developed a visual localization system that uses a surveyed three-dimensional (3D)-edge map of permanent structures in the environment. The map has the invariant properties necessary to achieve long-term robust operation. Previous 3D-edge map localization systems usually maintain a single pose hypothesis, making it difficult to initialize without an accurate prior pose estimate and also making them susceptible to misalignment with unmapped edges detected in the camera image. A multihypothesis particle filter is employed here to perform the initialization procedure with significant uncertainty in the vehicle's initial pose. A novel observation function for the particle filter is developed and evaluated against two existing functions. The new function is shown to further improve the abilities of the particle filter to converge given a very coarse estimate of the vehicle's initial pose. An intelligent exposure control algorithm is also developed that improves the quality of the pertinent information in the image. Results gathered over an entire sunny day and also during rainy weather illustrate that the localization system can operate in a wide range of outdoor conditions. The conclusion is that an invariant map, a robust multihypothesis localization algorithm, and an intelligent exposure control algorithm all combine to enable reliable visual localization through challenging outdoor conditions. Â© 2009 Wiley Periodicals, Inc.},
 author = {Nuske, Stephen and Roberts, Jonathan and Wyeth, Gordon},
 doi = {10.1002/rob.20306},
 journal = {Journal of Field Robotics},
 keywords = {},
 number = {9},
 pages = {728-756},
 title = {Robust outdoor visual localization using a threeâ€dimensionalâ€edge map},
 url = {https://app.dimensions.ai/details/publication/pub.1016451346},
 volume = {26},
 year = {2009}
}

@article{pub.1016451629,
 abstract = {Hybrid maps where local metric submaps are kept in the nodes of a graph-based topological structure are gaining relevance as the focus of robot Simultaneous Localization and Mapping (SLAM) shifts towards spatial scalability and long-term operation. In this paper we examine the applicability of spectral graph partitioning techniques to the automatic generation of metric submaps by establishing groups in the sequence of observations gathered by the robot. One of the main aims of this work is to provide a probabilistically grounded interpretation of such a partitioning technique in the context of generating local maps. We also discuss how to apply it to different kinds of sensory data (landmarks extracted from stereo images and laser range scans) and how to consider them simultaneously. An important feature of our approach is that the partitioning takes into account the intrinsic characteristics of the sensors, such as the sensor field of view, instead of applying heuristics supplied by a human as in other works. Thus the robot builds â€œsubjectiveâ€ local maps whose size will be determined by the nature of the sensors. The ideas presented here are supported by experimental results from a real mobile robot as well as simulations for statistical analysis. We discuss the effects of considering different combinations of sensors in the resulting clustering of the environment.},
 author = {Blanco, J.L. and GonzÃ¡lez, J. and FernÃ¡ndez-Madrigal, J.-A.},
 doi = {10.1016/j.robot.2008.02.002},
 journal = {Robotics and Autonomous Systems},
 keywords = {},
 number = {1},
 pages = {64-74},
 title = {Subjective local maps for hybrid metric-topological SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1016451629},
 volume = {57},
 year = {2009}
}

@article{pub.1018233021,
 abstract = {In this paper, we propose a robust pose tracking method for mobile robot localization with an incomplete map in a highly non-static environment. This algorithm will work with a simple map that does not include complete information about the non-static environment. With only an initial incomplete map, a mobile robot cannot estimate its pose because of the inconsistency between the real observations from the environment and the predicted observations on the incomplete map. The proposed localization algorithm uses the approach of sampling from a non-corrupted window, which allows the mobile robot to estimate its pose more robustly in a non-static environment even when subjected to severe corruption of observations. The algorithm sequence involves identifying the corruption by comparing the real observations with the corresponding predicted observations of all particles, sampling particles from a non-corrupted window that consists of multiple non-corrupted sets, and filtering sensor measurements to provide weights to particles in the corrupted sets. After localization, the estimated path may still contain some errors due to long-term corruption. These errors can be corrected using nonlinear constrained least-squares optimization. The incomplete map is then updated using both the corrected path and the stored sensor information. The performance of the proposed algorithm was verified via simulations and experiments in various highly non-static environments. Our localization algorithm can increase the success rate of tracking its pose to more than 95% compared to estimates made without its use. After that, the initial incomplete map is updated based on the localization result.},
 author = {Lee, Jung-Suk and Chung, Wan Kyun},
 doi = {10.1007/s10514-010-9184-1},
 journal = {Autonomous Robots},
 keywords = {},
 number = {1},
 pages = {1-16},
 title = {Robust mobile robot localization in highly non-static environments},
 url = {https://app.dimensions.ai/details/publication/pub.1018233021},
 volume = {29},
 year = {2010}
}

@inbook{pub.1018462235,
 abstract = {To effectively act on the same physical space, robots must first communicate to share and fuse the map of the area in which they operate. For long-term online operation, the merging of maps from heterogeneous devices must be fast and allow for scalable growth in both the number of clients and the size of the map. This paper presents a system which allows multiple clients to share and merge maps built from a state-of-the-art relative SLAM system. Maps can also be augmented with virtual elements that are consistently shared by all the clients. The visual-inertial mapping framework which underlies this system is discussed, along with the server architecture and novel integrated multi-session loop closure system. We show quantitative results of the system. The map fusion benefits are demonstrated with an example augmented reality application.},
 author = {Morrison, John G. and GÃ¡lvez-LÃ³pez, Dorian and Sibley, Gabe},
 booktitle = {Distributed Autonomous Robotic Systems},
 doi = {10.1007/978-4-431-55879-8_9},
 keywords = {},
 pages = {119-132},
 publisher = {},
 title = {MOARSLAM: Multiple Operator Augmented RSLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1018462235},
 year = {2016}
}

@article{pub.1018639570,
 abstract = {Real-world environments such as houses and offices change over time, meaning that a mobile robotâ€™s map will become out of date. In this work, we introduce a method to update the reference views in a hybrid metric-topological map so that a mobile robot can continue to localize itself in a changing environment. The updating mechanism, based on the multi-store model of human memory, incorporates a spherical metric representation of the observed visual features for each node in the map, which enables the robot to estimate its heading and navigate using multi-view geometry, as well as representing the local 3D geometry of the environment. A series of experiments demonstrate the persistence performance of the proposed system in real changing environments, including analysis of the long-term stability.},
 author = {Dayoub, Feras and Cielniak, Grzegorz and Duckett, Tom},
 doi = {10.1016/j.robot.2011.02.013},
 journal = {Robotics and Autonomous Systems},
 keywords = {},
 note = {http://eprints.lincoln.ac.uk/id/eprint/6046/1/FGTRAS.pdf},
 number = {5},
 pages = {285-295},
 title = {Long-term experiments with an adaptive spherical view representation for navigation in changing environments},
 url = {https://app.dimensions.ai/details/publication/pub.1018639570},
 volume = {59},
 year = {2011}
}

@inbook{pub.1019026086,
 abstract = {This paper presents a novel method for incorporating unstructured, natural terrain information into the process of tracking of underwater vehicles. Terrain-aided navigation promises to revolutionise the ability of marine systems to track underwater bodies in deepwater applications. This work represents a crucial step in the development of underwater technologies capable of long-term, reliable deployment. A particle based estimator is used to incorporate observations of altitude into the estimation process using a priori map information. Results of the application of this technique to the tracking of a towed body and a ship operating in Sydney Harbour are shown.},
 author = {Williams, Stefan and Mahon, Ian},
 booktitle = {Field and Service Robotics},
 doi = {10.1007/10991459_10},
 keywords = {},
 pages = {93-102},
 publisher = {},
 title = {A Terrain-Aided Tracking Algorithm for Marine Systems},
 url = {https://app.dimensions.ai/details/publication/pub.1019026086},
 year = {}
}

@article{pub.1019140063,
 abstract = {We present a bio-inspired approach to deal with the localization and spatial mapping problem, extending the successful previous RatSLAM approach from 2D ground vehicles to the 3D underwater environments. Our approach, called DolphinSLAM, is a SLAM system based on mammals navigation. Experiments in simulation and real environments were conducted involving long-term navigation tasks with different robots and sensors. Our proposal is open- source, being integrated with the Robot Operating System (ROS).},
 author = {Silveira, Luan and Guth, Felipe and Drews-Jr, Paulo and Ballester, Pedro and Machado, Matheus and Codevilla, Felipe and Duarte-Filho, Nelson and Botelho, Silvia},
 doi = {10.1016/j.ifacol.2015.06.035},
 journal = {IFAC-PapersOnLine},
 keywords = {},
 note = {https://doi.org/10.1016/j.ifacol.2015.06.035},
 number = {2},
 pages = {212-217},
 title = {An Open-source Bio-inspired Solution to Underwater SLAMâ˜…},
 url = {https://app.dimensions.ai/details/publication/pub.1019140063},
 volume = {48},
 year = {2015}
}

@inbook{pub.1023893171,
 abstract = {Multi-robot systems play an important role in many robotic applications. A prerequisite for a team of robots is the capability of building and maintaining updated maps of the environment. The simultaneous estimation of the trajectory and the map of the environment (known as SLAM) requires many computational resources. Moreover, SLAM is generally performed in environments that do not vary over time (called static environments), whereas real applications commonly require navigation services in dynamic environments. This paper focuses on long term mapping operativity in presence of variations in the map, as in the case of robotic applications in logistic spaces, where rovers have to track the presence of goods in given areas. In this context classical SLAM approaches are generally not directly applicable, since they usually apply in static environments or in dynamic environments where it is possible to model the environment dynamics. This paper proposes a methodology that allows the robots to detect variations in the environment, generate maps containing only the persistent variations, propagate thiem to the team and finally merge the received information in a consistent way. The team of robots is also exploited to assure the coverage of areas not visited for long time, thus improving the knowledge on the present status of the map. The map updating process is demonstrated to be computationally light, in order to be performed in parallel with other tasks (e.g., team coordination and planning, surveillance).},
 author = {Abrate, Fabrizio and Bona, Basilio and Indri, Marina and Rosa, Stefano and Tibaldi, Federico},
 booktitle = {Distributed Autonomous Robotic Systems},
 doi = {10.1007/978-3-642-32723-0_11},
 keywords = {},
 pages = {147-160},
 publisher = {},
 title = {Multi-robot Map Updating in Dynamic Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1023893171},
 year = {2013}
}

@article{pub.1025759780,
 abstract = {This paper presents a system for long-term SLAM (simultaneous localization and mapping) by mobile service robots and its experimental evaluation in a real dynamic environment. To deal with the stability-plasticity dilemma (the trade-off between adaptation to new patterns and preservation of old patterns), the environment is represented by multiple timescales simultaneously (five in our experiments). A sample-based representation is proposed, where older memories fade at different rates depending on the timescale and robust statistics are used to interpret the samples. The dynamics of this representation are analyzed in a five-week experiment, measuring the relative influence of short- and long-term memories over time and further demonstrating the robustness of the approach.},
 author = {Biber, Peter and Duckett, Tom},
 doi = {10.1177/0278364908096286},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 note = {http://eprints.lincoln.ac.uk/id/eprint/2095/1/Biber-Duckett-IJRR-2009.pdf},
 number = {1},
 pages = {20-33},
 title = {Experimental Analysis of Sample-Based Maps for Long-Term SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1025759780},
 volume = {28},
 year = {2009}
}

@article{pub.1026654951,
 abstract = {Robot localization systems typically assume that the environment is static, ignoring the dynamics inherent in most real-world settings. Corresponding scenarios include households, offices, warehouses and parking lots, where the location of certain objects such as goods, furniture or cars can change over time. These changes typically lead to inconsistent observations with respect to previously learned maps and thus decrease the localization accuracy or even prevent the robot from globally localizing itself. In this paper we present a sound probabilistic approach to lifelong localization in changing environments using a combination of a Rao-Blackwellized particle filter with a hidden Markov model. By exploiting several properties of this model, we obtain a highly efficient map management approach for dynamic environments, which makes it feasible to run our algorithm online. Extensive experiments with a real robot in a dynamically changing environment demonstrate that our algorithm reliably adapts to changes in the environment and also outperforms the popular Monte-Carlo localization approach.},
 author = {Tipaldi, Gian Diego and Meyer-Delius, Daniel and Burgard, Wolfram},
 doi = {10.1177/0278364913502830},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 number = {14},
 pages = {1662-1678},
 title = {Lifelong localization in changing environments},
 url = {https://app.dimensions.ai/details/publication/pub.1026654951},
 volume = {32},
 year = {2013}
}

@article{pub.1026813880,
 abstract = {Long-term autonomous mobile robot operation requires considering place recognition decisions with great caution. A single incorrect decision that is not detected and reconsidered can corrupt the environment model that the robot is trying to build and maintain. This work describes a consensus-based approach to robust place recognition over time, that takes into account all the available information to detect and remove past incorrect loop closures. The main novelties of our work are: (1) the ability of realizing that, in light of new evidence, an incorrect past loop closing decision has been made; the incorrect information can be removed thus recovering the correct estimation with a novel algorithm; (2) extending our proposal to incremental operation; and (3) handling multi-session, spatially related or unrelated scenarios in a unified manner. We demonstrate our proposal, the RRR algorithm, on different odometry systems, e.g. visual or laser, using different front-end loop-closing techniques. For our experiments we use the efficient graph optimization framework g2o as back-end. We back our claims up with several experiments carried out on real data, in single and multi-session experiments showing better results than those obtained by state-of-the-art methods, comparisons against whom are also presented.},
 author = {Latif, Yasir and Cadena, CÃ©sar and Neira, JosÃ©},
 doi = {10.1177/0278364913498910},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 note = {http://webdiis.unizar.es/%7Eylatif/papers/IJRR.pdf},
 number = {14},
 pages = {1611-1626},
 title = {Robust loop closing over time for pose graph SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1026813880},
 volume = {32},
 year = {2013}
}

@article{pub.1028707941,
 abstract = {Long-term autonomy in robotics requires perception systems that are resilient to unusual but realistic conditions that will eventually occur during extended missions. For example, unmanned ground vehicles (UGVs) need to be capable of operating safely in adverse and low-visibility conditions, such as at night or in the presence of smoke. The key to a resilient UGV perception system lies in the use of multiple sensor modalities, e.g., operating at different frequencies of the electromagnetic spectrum, to compensate for the limitations of a single sensor type. In this paper, visual and infrared imaging are combined in a Visual-SLAM algorithm to achieve localization. We propose to evaluate the quality of data provided by each sensor modality prior to data combination. This evaluation is used to discard low-quality data, i.e., data most likely to induce large localization errors. In this way, perceptual failures are anticipated and mitigated. An extensive experimental evaluation is conducted on data sets collected with a UGV in a range of environments and adverse conditions, including the presence of smoke (obstructing the visual camera), fire, extreme heat (saturating the infrared camera), low-light conditions (dusk), and at night with sudden variations of artificial light. A total of 240 trajectory estimates are obtained using five different variations of data sources and data combination strategies in the localization method. In particular, the proposed approach for selective data combination is compared to methods using a single sensor type or combining both modalities without preselection. We show that the proposed framework allows for camera-based localization resilient to a large range of low-visibility conditions.},
 author = {Brunner, Christopher and Peynot, Thierry and Vidalâ€Calleja, Teresa and Underwood, James},
 doi = {10.1002/rob.21464},
 journal = {Journal of Field Robotics},
 keywords = {},
 note = {https://eprints.qut.edu.au/67607/1/ROB-12-0094-FINAL.pdf},
 number = {4},
 pages = {641-666},
 title = {Selective Combination of Visual and Thermal Imaging for Resilient Localization in Adverse Conditions: Day and Night, Smoke and Fire},
 url = {https://app.dimensions.ai/details/publication/pub.1028707941},
 volume = {30},
 year = {2013}
}

@article{pub.1030433417,
 abstract = {An online incremental method of vision-only loop-closure detection for long-term robot navigation is proposed. The method is based on the scheme of direct feature matching which has recently become more efficient than the Bag-of-Words scheme in many challenging environments. The contributions of the paper are the application of hierarchical k-means to speed-up feature matching time and the improvement of the score calculation technique used to determine the loop-closing location. As a result, the presented method runs quickly in long term while retaining high accuracy. The searching cost has been markedly reduced. The proposed method requires no any off-line dictionary generation processes. It can start anywhere at any times. The evaluation has been done on standard well-known datasets being challenging in perceptual aliasing and dynamic changes. The results show that the proposed method offers high precision-recall in large-scale different environments with real-time computation comparing to other vision-only loop-closure detection methods.},
 author = {Kawewong, Aram and Tongprasit, Noppharit and Hasegawa, Osamu},
 doi = {10.1080/01691864.2013.826410},
 journal = {Advanced Robotics},
 keywords = {},
 number = {17},
 pages = {1325-1336},
 title = {A speeded-up online incremental vision-based loop-closure detection for long-term SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1030433417},
 volume = {27},
 year = {2013}
}

@inbook{pub.1032246777,
 abstract = {This session presents four recent researches of humanoid. The first one introduces new SONY humanoid â€œQRIOâ€, developed as a small entertainer humanoid in home environment. People expect a partner robot to behave a spontaneous motion, memory of interactions, multi-modal dialogue, reflective behavior to the world and user situations. The system consists of 5 parts: Perception, Memory, Internal State Model, Behavior Control, and Motion Control. The memory part of the system has two aspects; the short-term memory for perception composition with limited view angles, and the long-term memory, which provides some level of intelligence in the dialogue with human. In order to act properly in a real environment, QRIO system employs the situated behaviorbased architecture. Fujita discusses how the software architecture should be designed, and describes the autonomous behavior control architecture which serves to integrate multi-modal recognition and motion control technologies. In addition to the presentation at the session, Fujita demonstrates the real QRIO interactions with audiences. Small size humanoid is rather easy to handle than full-size humanoid, it provides very effective tool to examine the software development in real environment for behavior, intelligence , and interactions with humans.},
 author = {Inoue, Hirochika},
 booktitle = {Robotics Research. The Eleventh International Symposium},
 doi = {10.1007/11008941_37},
 keywords = {},
 pages = {353-354},
 publisher = {},
 title = {Session Summary},
 url = {https://app.dimensions.ai/details/publication/pub.1032246777},
 year = {2005}
}

@inbook{pub.1032357987,
 abstract = {A central clue for implementation of visual memory based navigation strategies relies on efficient point matching between the current image and the key images of the memory. However, the visual memory may become out of date after some times because the appearance of real-world environments keeps changing. It is thus necessary to remove obsolete information and to add new data to the visual memory over time. In this paper, we propose a method based on short-term and long term memory concepts to update the visual memory of mobile robots during navigation. The results of our experiments show that using this method improves the robustness of the localization and path-following steps.},
 author = {Courbon, Jonathan and Korrapati, Hemanth and Mezouar, Youcef},
 booktitle = {Intelligent Autonomous Systems 12},
 doi = {10.1007/978-3-642-33926-4_12},
 keywords = {},
 pages = {133-142},
 publisher = {},
 title = {Visual Memory Update for Life-Long Mobile Robot Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1032357987},
 year = {2013}
}

@article{pub.1034002871,
 abstract = {In graph-based simultaneous localization and mapping (SLAM), the pose graph grows over time as the robot gathers information about the environment. An ever growing pose graph, however, prevents long-term mapping with mobile robots. In this paper, we address the problem of efficient information-theoretic compression of pose graphs. Our approach estimates the mutual information between the laser measurements and the map to discard the measurements that are expected to provide only a small amount of information. Our method subsequently marginalizes out the nodes from the pose graph that correspond to the discarded laser measurements. To maintain a sparse pose graph that allows for efficient map optimization, our approach applies an approximate marginalization technique that is based on Chowâ€“Liu trees. Our contributions allow the robot to effectively restrict the size of the pose graph. Alternatively, the robot is able to maintain a pose graph that does not grow unless the robot explores previously unobserved parts of the environment. Real-world experiments demonstrate that our approach to pose graph compression is well suited for long-term mobile robot mapping.},
 author = {Kretzschmar, Henrik and Stachniss, Cyrill},
 doi = {10.1177/0278364912455072},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 note = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/kretzschmar12ijrr.pdf},
 number = {11},
 pages = {1219-1230},
 title = {Information-theoretic compression of pose graphs for laser-based SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1034002871},
 volume = {31},
 year = {2012}
}

@inbook{pub.1034078452,
 abstract = {Accurate localization of a micro aerial vehicle (MAV) with respect to a scene is important for a wide range of applications, in particular autonomous navigation, surveillance, and inspection. In this context, visual localization in urban outdoor environments is gaining importance as common methods such as GPS positioning are often not accurate enough or even fail. We present recent approaches and results for robust 3D reconstruction of suitable visual landmarks, for the alignment in a world coordinate system, and for fast, high-accuracy monocular localization. We introduce a scalable representation of the prior knowledge about the scene and demonstrate how in-flight information can be integrated to facilitate long-term operation. Our method outperforms a state-of-the-art visual SLAM approach and achieves localization accuracies comparable to differential GPS.},
 author = {Wendel, Andreas and Bischof, Horst},
 booktitle = {Advanced Topics in Computer Vision},
 doi = {10.1007/978-1-4471-5520-1_7},
 keywords = {},
 pages = {181-214},
 publisher = {},
 title = {Visual Localization for Micro Aerial Vehicles in Urban Outdoor Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1034078452},
 year = {2013}
}

@article{pub.1035394869,
 abstract = {FastSLAM is a framework for simultaneous localisation and mapping (SLAM) using a Rao-Blackwellised particle filter. In FastSLAM, particle filter is used for the robot pose (position and orientation) estimation, and parametric filter (i.e. EKF and UKF) is used for the feature location's estimation. However, in the long term, FastSLAM is an inconsistent algorithm. In this paper, a new approach to SLAM based on hybrid auxiliary marginalised particle filter and differential evolution (DE) is proposed. In the proposed algorithm, the robot pose is estimated based on auxiliary marginal particle filter that operates directly on the marginal distribution, and hence avoids performing importance sampling on a space of growing dimension. In addition, static map is considered as a set of parameters that are learned using DE. Compared to other algorithms, the proposed algorithm can improve consistency for longer time periods and also, improve the estimation accuracy. Simulations and experimental results indicate that the proposed algorithm is effective.},
 author = {Havangi, R. and Nekoui, M.A. and Teshnehlab, M. and Taghirad, H.D.},
 doi = {10.1080/00207721.2012.759299},
 journal = {International Journal of Systems Science},
 keywords = {},
 number = {9},
 pages = {1913-1926},
 title = {A SLAM based on auxiliary marginalised particle filter and differential evolution},
 url = {https://app.dimensions.ai/details/publication/pub.1035394869},
 volume = {45},
 year = {2014}
}

@inbook{pub.1035598974,
 abstract = {This paper presents experimental results of the application of terrain aided localisation and mapping algorithms to vehicle deployments in marine environments. The application of a terrain aided navigation filter to the tracking of a ship operating on Sydney Harbour is described. This approach allows highly unstructured seafloor bathymetric information to be incorporated into the navigation solution. In addition, experimental validation of the Simultaneous Localisation and Mapping algorithm using data collected by an Unmanned Underwater Vehicle operating on the Great Barrier Reef in Australia is reported. By fusing information from the vehicleâ€™s on-board sonar and vision systems, it is possible to use the highly textured reef to provide estimates of the vehicle motion as well as to generate models of the gross structure of the underlying reefs. Terrain-aided navigation promises to revolutionise the ability of marine systems to track underwater bodies in many applications. This work represents a crucial step in the development of underwater technologies capable of long-term, reliable deployment.},
 author = {Williams, Stefan and Mahon, Ian},
 booktitle = {Experimental Robotics IX},
 doi = {10.1007/11552246_11},
 keywords = {},
 pages = {111-120},
 publisher = {},
 title = {Terrain Aided Localisation and Mapping for Marine Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1035598974},
 year = {2006}
}

@article{pub.1035737575,
 abstract = {In this paper we present a novel method for online and incremental appearance-based localization and mapping in a highly dynamic environment. Using position-invariant robust features (PIRFs), the method can achieve a high rate of recall with 100% precision. It can handle both strong perceptual aliasing and dynamic changes of places efficiently. Its performance also extends beyond conventional images; it is applicable to omnidirectional images for which the major portions of scenes are similar for most places. The proposed PIRF-based Navigation method named PIRF-Nav is evaluated by testing it on two standard datasets in a similar manner as in FAB-MAP and on an additional omnidirectional image dataset that we collected. This extra dataset was collected on 2 days with different specific events, i.e. an open-campus event, to present challenges related to illumination variance and strong dynamic changes, and to test assessment of dynamic scene changes. Results show that PIRF-Nav outperforms FAB-MAP; PIRF-Nav at precision-1 yields a recall rate about twice as high (approximately 80% increase) than that of FAB-MAP. Its computation time is sufficiently short for real-time applications. The method is fully incremental, and requires no offline process for dictionary creation. Additional testing using combined datasets proves that PIRF-Nav can function over the long term and can solve the kidnapped robot problem.},
 author = {Kawewong, Aram and Tongprasit, Noppharit and Tangruamsub, Sirinart and Hasegawa, Osamu},
 doi = {10.1177/0278364910371855},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 number = {1},
 pages = {33-55},
 title = {Online and Incremental Appearance-based SLAM in Highly Dynamic Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1035737575},
 volume = {30},
 year = {2011}
}

@article{pub.1036016691,
 abstract = {Markov localization and its variants are widely used for mobile robot localization. These methods assume Markov independence of observations, implying that the observations can be entirely explained by a map. However, in real human environments, robots frequently make unexpected observations due to unmapped static objects like chairs and tables, and dynamic objects like humans. We therefore introduce Episodic non-Markov Localization (EnML), which reasons about the world as consisting of three classes of objects: long-term features corresponding to permanent mapped objects, short-term features corresponding to unmapped static objects, and dynamic features corresponding to unmapped moving objects. Long-term features are represented by a static map, while short-term features are detected and tracked in real-time. To reason about unexpected observations and their correlations across poses, we augment the Dynamic Bayesian Network for Markov localization to include varying edges and nodes, resulting in a novel Varying Graphical Network representation. The maximum likelihood estimate of the belief is incrementally computed by non-linear functional optimization. By detecting timesteps along the robotâ€™s trajectory where unmapped observations prior to such time steps are unrelated to those afterwards, EnML limits the history of observations and pose estimates to â€œepisodesâ€ over which the belief is computed. We demonstrate EnML using different types of sensors including laser rangefinders and depth cameras, and over multiple datasets, comparing it with alternative approaches. We further include results of a team of indoor autonomous service mobile robots traversing hundreds of kilometers using EnML.},
 author = {Biswas, Joydeep and Veloso, Manuela M.},
 doi = {10.1016/j.robot.2016.09.005},
 journal = {Robotics and Autonomous Systems},
 keywords = {},
 number = {},
 pages = {162-176},
 title = {Episodic non-Markov localization},
 url = {https://app.dimensions.ai/details/publication/pub.1036016691},
 volume = {87},
 year = {2017}
}

@article{pub.1040736228,
 abstract = {In this work, we present a novel approach that allows a robot to improve its own navigation performance through introspection and then targeted data retrieval. It is a step in the direction of life-long learning and adaptation and is motivated by the desire to build robots that have plastic competencies which are not baked in. They should react to and benefit from use. We consider a particular instantiation of this problem in the context of place recognition. Based on a topic-based probabilistic representation for images, we use a measure of perplexity to evaluate how well a working set of background images explain the robotâ€™s online view of the world. Offline, the robot then searches an external resource to seek out additional background images that bolster its ability to localize in its environment when used next. In this way the robot adapts and improves performance through use. We demonstrate this approach using data collected from a mobile robot operating in outdoor workspaces.},
 author = {Paul, Rohan and Newman, Paul},
 doi = {10.1177/0278364913509859},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 number = {14},
 pages = {1742-1766},
 title = {Self-help: Seeking out perplexing images for ever improving topological mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1040736228},
 volume = {32},
 year = {2013}
}

@article{pub.1043187743,
 abstract = {Autonomous ground vehicles navigating on road networks require robust and accurate localization over long-term operation and in a wide range of adverse weather and environmental conditions. GPS/INS (inertial navigation system) solutions, which are insufficient alone to maintain a vehicle within a lane, can fail because of significant radio frequency noise or jamming, tall buildings, trees, and other blockage or multipath scenarios. LIDAR and camera map-based vehicle localization can fail when optical features become obscured, such as with snow or dust, or with changes to gravel or dirt road surfaces. Localizing ground penetrating radar (LGPR) is a new mode of a priori map-based vehicle localization designed to complement existing approaches with a low sensitivity to failure modes of LIDAR, camera, and GPS/INS sensors due to its low-frequency RF energy, which couples deep into the ground. Most subsurface features detected are inherently stable over time. Significant research, discussed herein, remains to prove general utility. We have developed a novel low-profile ultra-low power LGPR system and demonstrated real-time operation underneath a passenger vehicle. A correlation maximizing optimization technique was developed to allow real-time localization at 126Â Hz. Here we present the detailed design and results from highway testing, which uses a simple heuristic for fusing LGPR estimates with a GPS/INS system. Cross-track localization accuracies of 4.3Â cm RMS relative to a â€œtruthâ€ RTK GPS/INS unit at speeds up to 100Â km/h (60Â mph) are demonstrated. These results, if generalizable, introduce a widely scalable real-time localization method with cross-track accuracy as good as or better than current localization methods.},
 author = {Cornick, Matthew and Koechling, Jeffrey and Stanley, Byron and Zhang, Beijia},
 doi = {10.1002/rob.21605},
 journal = {Journal of Field Robotics},
 keywords = {},
 note = {https://doi.org/10.1002/rob.21605},
 number = {1},
 pages = {82-102},
 title = {Localizing Ground Penetrating RADAR: A Step Toward Robust Autonomous Ground Vehicle Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1043187743},
 volume = {33},
 year = {2016}
}

@article{pub.1045086761,
 abstract = {This paper presents an alternative approach to the problem of outdoor, persistent visual localisation against a known map. Instead of blindly applying a feature detector/descriptor combination over all images of all places, we leverage prior experiences of a place to learn place-dependent feature detectors (i.e., features that are unique to each place in our map and used for localisation). Furthermore, as these features do not represent low-level structure, like edges or corners, but are in fact mid-level patches representing distinctive visual elements (e.g., windows, buildings, or silhouettes), we are able to localise across extreme appearance changes. Note that there is no requirement that the features posses semantic meaning, only that they are optimal for the task of localisation. This work is an extension on previous work (McManus et al. in Proceedings of robotics science and systems, 2014b) in the following ways: (i) we have included a landmark refinement and outlier rejection step during the learning phase, (ii) we have implemented an asynchronous pipeline design, (iii) we have tested on data collected in an urban environment, and (iv) we have implemented a purely monocular system. Using over 100Â km worth of data for training, we present localisation results from Begbroke Science Park and central Oxford.},
 author = {McManus, Colin and Upcroft, Ben and Newman, Paul},
 doi = {10.1007/s10514-015-9463-y},
 journal = {Autonomous Robots},
 keywords = {},
 number = {3},
 pages = {363-387},
 title = {Learning place-dependant features for long-term vision-based localisation},
 url = {https://app.dimensions.ai/details/publication/pub.1045086761},
 volume = {39},
 year = {2015}
}

@article{pub.1045476971,
 abstract = {We address the place recognition problem, which we define as the problem of establishing whether an observed location has been previously seen, and if so, determining the transformation aligning the current observations to an existing map. In the contexts of robot navigation and mapping, place recognition amounts to globally localizing a robot or map segment without being given any prior estimate. An efficient method of solving this problem involves first selecting a set of keypoints in the scene which store an encoding of their local region, and then utilizing a sublinear-time search into a database of keypoints previously generated from the global map to identify places with common features. We present an algorithm to embed arbitrary keypoint descriptors in a reduced-dimension metric space, in order to frame the problem as an efficient nearest neighbor search. Given that there are a multitude of possibilities for keypoint design, we propose a general methodology for comparing keypoint location selection heuristics and descriptor models that describe the region around the keypoint. With respect to selecting keypoint locations, we introduce a metric that encodes how likely it is that the keypoint will be found in the presence of noise and occlusions during mapping passes. Metrics for keypoint descriptors are used to assess the distinguishability between the distributions of matches and non-matches and the probability the correct match will be found in an approximate k-nearest neighbors search. Verification of the test outcomes is done by comparing the various keypoint designs on a kilometers-scale place recognition problem. We apply our design evaluation methodology to three keypoint selection heuristics and six keypoint descriptor models. A full place recognition system is presented, including a series of match verification algorithms which effectively filter out false positives. Results from city-scale and long-term mapping problems illustrate our approach for both offline and online SLAM, map merging, and global localization and demonstrate that our algorithm is able to produce accurate maps over trajectories of hundreds of kilometers.},
 author = {Bosse, Michael and Zlot, Robert},
 doi = {10.1016/j.robot.2009.07.009},
 journal = {Robotics and Autonomous Systems},
 keywords = {},
 number = {12},
 pages = {1211-1224},
 title = {Keypoint design and evaluation for place recognition in 2D lidar maps},
 url = {https://app.dimensions.ai/details/publication/pub.1045476971},
 volume = {57},
 year = {2009}
}

@inbook{pub.1045558255,
 abstract = {Corner and edge based robotic vision systems have achieved enormous success in various applications. To quantify and thereby improve the system performance, the standard method is to conduct cross comparisons using benchmark datasets. Such datasets, however, are usually generated for validating specific vision algorithms (e.g. monocular SLAMÂ [1] and stereo odometryÂ [2]). In addition, they are not capable of evaluating robotic systems which require visual feedback signals for motion control (e.g. visual servoingÂ [3]). To develop a more generalised framework to evaluate ordinary corner and edge based robotic vision systems, we propose a novel Monte-Carlo simulation which contains various real-world geometric uncertainty sources. An edge-based global localisation algorithm is evaluated and optimised using the proposed simulation via a large scale Monte-Carlo analysis. During a long-term optimisation, the system performance is improved by around 230 times, while preserving high robustness towards all the simulated uncertainty sources.},
 author = {Tian, Jingduo and Thacker, Neil and Stancu, Alexandru},
 booktitle = {Advances in Visual Computing},
 doi = {10.1007/978-3-319-50832-0_53},
 keywords = {},
 pages = {544-554},
 publisher = {},
 title = {Quantitative Performance Optimisation for Corner and Edge Based Robotic Vision Systems: A Monte-Carlo Simulation},
 url = {https://app.dimensions.ai/details/publication/pub.1045558255},
 year = {2016}
}

@article{pub.1045905566,
 abstract = {A typical indoor environment can be divided into three categories; office (or room), hallway, and wide-open space such as lobby and hall. There have been numerous approaches for solving simultaneous localization and mapping (SLAM) problem in office (or room) and hallway. However, direct application of the existing approaches to wide-open space may be failed, because it has some distinguished features compared to other indoor places. To solve this problem, this paper proposes a new ceiling vision-based active SLAM framework, with an emphasis on practical deployment of service robot for commercial use in dynamically changing and wide-open environments by adopting the ceiling vision. First, for defining ceiling feature which can be extracted regardless of complexity of ceiling pattern we introduce a model-free landmark, i.e., visual node descriptor, which consists of edge points and their orientations in image space. Second, a recursive â€˜explore and exploitâ€™ is proposed for autonomous mapping. It is recursively performed by spreading out mapped area gradually while the robot is actively localized in the map. It can improve map accuracy due to frequent small loop closing. Third, a dynamic edge link (DEL) is proposed to cope with environmental changes in the map. Owing to DEL, we do not need to filter out corrupted sensor data and to distinguish moving object from static one. Also, a self-repairing map mechanism is introduced to deal with unexpected installation or removal of inner structures. We therefore achieve long-term navigation. Several simulations and real experiments in various places show that the proposed active SLAM framework could build a topologically consistent map, and demonstrated that it can be applied well to real environments such as wide-open space in a city hall and railway station.},
 author = {An, Su-Yong and Lee, Lae-Kyoung and Oh, Se-Young},
 doi = {10.1007/s10514-015-9453-0},
 journal = {Autonomous Robots},
 keywords = {},
 number = {2},
 pages = {291-324},
 title = {Ceiling vision-based active SLAM framework for dynamic and wide-open environments},
 url = {https://app.dimensions.ai/details/publication/pub.1045905566},
 volume = {40},
 year = {2016}
}

@article{pub.1048012151,
 abstract = {This paper proposes extending Monte Carlo Localization methods with visual place recognition information in order to build a robust robot localization system. This system is aimed to work in crowded and non-planar scenarios, where 2D laser rangefinders may not always be enough to match the robot position within the map. Thus, visual place recognition will be used in order to obtain robot position clues that can be used to detect when the robot is lost and also to reset its positions to the right one. The paper presents experimental results based on datasets gathered with a real robot in challenging scenarios.},
 author = {PÃ©rez, Javier and Caballero, Fernando and Merino, Luis},
 doi = {10.1007/s10846-015-0198-y},
 journal = {Journal of Intelligent & Robotic Systems},
 keywords = {},
 number = {3-4},
 pages = {641-656},
 title = {Enhanced Monte Carlo Localization with Visual Place Recognition for Robust Robot Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1048012151},
 volume = {80},
 year = {2015}
}

@article{pub.1049416172,
 abstract = {For the last three years, we have developed and researched multiple collaborative robots, CoBots, which have been autonomously traversing our multi-floor buildings. We pursue the goal of long-term autonomy for indoor service mobile robots as the ability for them to be deployed indefinitely while they perform tasks in an evolving environment. The CoBots include several levels of autonomy, and in this paper we focus on their localization and navigation algorithms. We present the Corrective Gradient Refinement (CGR) algorithm, which refines the proposal distribution of the particle filter used for localization with sensor observations using analytically computed state space derivatives on a vector map. We also present the Fast Sampling Plane Filtering algorithm that extracts planar regions from depth images in real time. These planar regions are then projected onto the 2D vector map of the building, and along with the laser rangefinder observations, used with CGR for localization. For navigation, we present a hierarchical planner, which computes a topological policy using a graph representation of the environment, computes motion commands based on the topological policy, and then modifies the motion commands to side-step perceived obstacles. We started logging the deployments of the CoBots one and a half years ago, and have since collected logs of the CoBots traversing more than 130 km over 1082 deployments and a total run time of 182 h, which we publish as a dataset consisting of more than 10 million laser scans. The logs show that although there have been continuous changes in the environment, the robots are robust to most of them, and there exist only a few locations where changes in the environment cause increased uncertainty in localization.},
 author = {Biswas, Joydeep and Veloso, Manuela M.},
 doi = {10.1177/0278364913503892},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 note = {http://www.cs.cmu.edu/afs/cs/user/mmv/www/papers/13ijrr-BiswasVeloso.pdf},
 number = {14},
 pages = {1679-1694},
 title = {Localization and navigation of the CoBots over long-term deployments},
 url = {https://app.dimensions.ai/details/publication/pub.1049416172},
 volume = {32},
 year = {2013}
}

@article{pub.1049526038,
 abstract = {In this paper, we address the problem of lifelong map learning in static environments with mobile robots using the graph-based formulation of the simultaneous localization and mapping problem. The pose graph, which stores the poses of the robot and spatial constraints between them, is the central data structure in graph-based SLAM. The size of the pose graph has a direct influence on the runtime and the memory complexity of the SLAM system and typically grows over time. AÂ robot that performs lifelong mapping in a bounded environment has to limit the memory and computational complexity of its mapping system. We present a novel approach to prune the pose graph so that it only grows when the robot acquires relevant new information about the environment in terms of expected information gain. As a result, our approach scales with the size of the environment and not with the length of the trajectory, which is an important prerequisite for lifelong map learning. The experiments presented in this paper illustrate the properties of our method using real robots.},
 author = {Kretzschmar, Henrik and Grisetti, Giorgio and Stachniss, Cyrill},
 doi = {10.1007/s13218-010-0034-2},
 journal = {KI - KÃ¼nstliche Intelligenz},
 keywords = {},
 number = {3},
 pages = {199-206},
 title = {Lifelong Map Learning for Graph-based SLAM in Static Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1049526038},
 volume = {24},
 year = {2010}
}

@article{pub.1049804089,
 abstract = {In mobile robot's localization, it is well known that odometry can provide a reliable accuracy in short term navigation and a very high sampling rate. However, odometry produces cumulative error because of uneven terrains or wheel slippage and this error increases proportionally with the distance traveled by the mobile robot. Therefore, it is necessary to augment odometry with other sensors to improve its accuracy. This paper proposes an estimation method of mobile robot orientation using an environmental magnetic field (magnetic field that occurs in the environment). A three-axis magnetic sensor is utilized to scan the environmental magnetic field to build a magnetic database on a grid map called â€œa magnetic mapâ€ with the mobile robot operated with a joystick on a desired route. The mobile robot then estimates its orientation by comparing the magnetic sensor readings with the magnetic data stored in the magnetic map. However, even if the proposed method can improve the accuracy of the odometry, positioning error still remains as a major problem in long term navigation. In this work, a localization method using Monte Carlo Localization (MCL) based on a Light Detection and Ranging (LIDAR) is utilized to fix the positioning error at the areas where landmark can be observed. The experimental results showed that the mobile robot could localize robustly in any environments with the proposed method.},
 author = {SHINOHARA, Masatoshi and RAHOK, Sam Ann and INOUE, Kazumichi and OZAKI, Koichi},
 doi = {10.9746/sicetr.49.795},
 journal = {Transactions of the Society of Instrument and Control Engineers},
 keywords = {},
 note = {https://www.jstage.jst.go.jp/article/sicetr/49/8/49_795/_pdf},
 number = {8},
 pages = {795},
 title = {Development of Localization Method Using Magnetic Sensor and LIDAR},
 url = {https://app.dimensions.ai/details/publication/pub.1049804089},
 volume = {49},
 year = {2013}
}

@article{pub.1051503513,
 abstract = {An approach for long term localization, stabilization, and navigation of micro-aerial vehicles (MAVs) in unknown environment is presented in this paper. The proposed method relies strictly on onboard sensors of employed MAVs and does not require any external positioning system. The core of the method consists in extraction of information from pictures consequently captured using a camera carried by the particular MAV. Visual features are obtained from images of the surface under the MAV, and stored into a map that is represented by these features. The position of the MAV is then obtained through matching with previously stored features. An important part of the proposed system is a novel approach for exploration and mapping of the workspace of robots. This method enables efficient exploring of the unknown environment, while keeping the iteratively built map of features consistent. The proposed algorithm is suitable for mapping of surfaces, both outdoor and indoor, with various density of the image features. The sufficient precision and long term persistence of the method allows its utilization for stabilization of large MAV groups that work in formations with small relative distances between particular vehicles. Numerous experiments with quadrotor helicopters and various numerical simulations have been realized for verification of the entire system and its components.},
 author = {Chudoba, Jan and Kulich, Miroslav and Saska, Martin and BÃ¡Äa, TomÃ¡Å¡ and PÅ™euÄil, Libor},
 doi = {10.1007/s10846-016-0358-8},
 journal = {Journal of Intelligent & Robotic Systems},
 keywords = {},
 number = {1-4},
 pages = {351-369},
 title = {Exploration and Mapping Technique Suited for Visual-features Based Localization of MAVs},
 url = {https://app.dimensions.ai/details/publication/pub.1051503513},
 volume = {84},
 year = {2016}
}

@inbook{pub.1052255199,
 abstract = {A central clue for implementation of visual memory based navigation strategies relies on efficient point matching between the current image and the key images of the memory. However, the visual memory may become out of date after some times because the appearance of real-world environments keeps changing. It is thus necessary to remove obsolete information and to add new data to the visual memory over time. In this paper, we propose a method based on short-term and long term memory concepts to update the visual memory of mobile robots during navigation. The results of our experiments show that using this method improves the robustness of the localization and path-following steps.},
 author = {Courbon, Jonathan and Korrapati, Hemanth and Mezouar, Youcef},
 booktitle = {Frontiers of Intelligent Autonomous Systems},
 doi = {10.1007/978-3-642-35485-4_4},
 keywords = {},
 pages = {43-52},
 publisher = {},
 title = {Visual Memory Update for Life-Long Mobile Robot Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1052255199},
 year = {2013}
}

@inbook{pub.1052960374,
 abstract = {In order to assist elderly people and disabled people, this paper describes development of autonomous wheelchair to travel in indoor and outdoor environments for providing traveling ability to any where. For this aim, the autonomous wheelchairs should have traveling-capability without choosing indoor and outdoor environments. Position detection is a key technology for autonomous driving since people decides a traveling direction from a current position and a destination. GPS is a fundamental technology for position detection. However GPS is not available in indoor cases, and GPS is not always available in outdoor cases when tall buildings occlude satellites. In these cases autonomous wheelchair has to detect a self-position by other sensor systems. In this study we have adopted a localization system utilizing 3D maps and a 3D laser range finder. By the 3D localization system our wheelchair system can detect a self-position robustly if the wheelchair is surrounded by obstacles such as pedestrians. To avoid collision our wheelchair system uses short and long term planning. The short planning finds a safe motion-pattern from every conceivable pattern by the simulation on a map. The long term planning generates a feasible route to destination. If the route generated by the long-term planner collides to some obstacles our wheelchair avoids collision by the short term planning. By the localization system and the planning system our wheelchair could operate in public spaces.},
 author = {Yokozuka, Masashi and Hashimoto, Naohisa and Tomita, Kohji and Matsumoto, Osamu},
 booktitle = {Internet of Things. IoT Infrastructures},
 doi = {10.1007/978-3-319-19743-2_14},
 keywords = {},
 pages = {91-96},
 publisher = {},
 title = {Development of Autonomous Wheelchair for Indoor and Outdoor Traveling},
 url = {https://app.dimensions.ai/details/publication/pub.1052960374},
 year = {2015}
}

@inbook{pub.1053038391,
 abstract = {In this paper we describe the autonomous behavior control architecture of SDR-4X II, which serves to integrate multi-modal recognition and motion control technologies. We overview the entire software architecture of SDR-4X II, which is composed of perception, short and long term memory, behavior control, and motion control parts. Regarding autonomous behavior control, we further focus on issues such as spontaneous behavior generation using a homeostasis regulation mechanism, and a behavior control/selection mechanism with tree-structured situated behavior modules. In the autonomous behavior control architecture, we achieve three basic requirements, which are the concurrent evaluation of the situation of each behavior module, concurrent execution of multiple behavior modules, and preemption (behavior interruption/resume capability). Using the autonomous behavior control architecture described, we demonstrate that SDR-4X II can spontaneously and passively interact with a human.},
 author = {Fujita, Masahiro and Sabe, Kohtaro and Kuroki, Yoshihiro and Ishida, Tatsuzo and Doi, Toshi T.},
 booktitle = {Robotics Research. The Eleventh International Symposium},
 doi = {10.1007/11008941_38},
 keywords = {},
 pages = {355-364},
 publisher = {},
 title = {SDR-4X II: A Small Humanoid as an Entertainer in Home Environment},
 url = {https://app.dimensions.ai/details/publication/pub.1053038391},
 year = {2005}
}

@article{pub.1053561422,
 abstract = {Abstract. Effective and accurate localization method in three-dimensional indoor environments is a key requirement for indoor navigation and lifelong robotic assistance. So far, Monte Carlo Localization (MCL) has given one of the promising solutions for the indoor localization methods. Previous work of MCL has been mostly limited to 2D motion estimation in a planar map, and a few 3D MCL approaches have been recently proposed. However, their localization accuracy and efficiency still remain at an unsatisfactory level (a few hundreds millimetre error at up to a few FPS) or is not fully verified with the precise ground truth. Therefore, the purpose of this study is to improve an accuracy and efficiency of 6DOF motion estimation in 3D MCL for indoor localization. Firstly, a terrestrial laser scanner is used for creating a precise 3D mesh model as an environment map, and a professional-level depth camera is installed as an outer sensor. GPU scene simulation is also introduced to upgrade the speed of prediction phase in MCL. Moreover, for further improvement, GPGPU programming is implemented to realize further speed up of the likelihood estimation phase, and anisotropic particle propagation is introduced into MCL based on the observations from an inertia sensor. Improvements in the localization accuracy and efficiency are verified by the comparison with a previous MCL method. As a result, it was confirmed that GPGPU-based algorithm was effective in increasing the computational efficiency to 10-50 FPS when the number of particles remain below a few hundreds. On the other hand, inertia sensor-based algorithm reduced the localization error to a median of 47mm even with less number of particles. The results showed that our proposed 3D MCL method outperforms the previous one in accuracy and efficiency.},
 author = {Kanai, S. and Hatakeyama, R. and Date, H.},
 doi = {10.5194/isprsarchives-xl-4-w5-61-2015},
 journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
 keywords = {},
 note = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-4-W5/61/2015/isprsarchives-XL-4-W5-61-2015.pdf},
 number = {},
 pages = {61-66},
 title = {IMPROVEMENT OF 3D MONTE CARLO LOCALIZATION USING A DEPTH CAMERA AND TERRESTRIAL LASER SCANNER},
 url = {https://app.dimensions.ai/details/publication/pub.1053561422},
 volume = {XL-4/W5},
 year = {2015}
}

@article{pub.1053938787,
 abstract = {We present a challenging new dataset for autonomous driving: the Oxford RobotCar Dataset. Over the period of May 2014 to December 2015 we traversed a route through central Oxford twice a week on average using the Oxford RobotCar platform, an autonomous Nissan LEAF. This resulted in over 1000 km of recorded driving with almost 20 million images collected from 6 cameras mounted to the vehicle, along with LIDAR, GPS and INS ground truth. Data was collected in all weather conditions, including heavy rain, night, direct sunlight and snow. Road and building works over the period of a year significantly changed sections of the route from the beginning to the end of data collection. By frequently traversing the same route over the period of a year we enable research investigating long-term localization and mapping for autonomous vehicles in real-world, dynamic urban environments. The full dataset is available for download at: http://robotcar-dataset.robots.ox.ac.uk},
 author = {Maddern, Will and Pascoe, Geoffrey and Linegar, Chris and Newman, Paul},
 doi = {10.1177/0278364916679498},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 note = {https://ora.ox.ac.uk/objects/uuid:c5266bad-e0f8-49f1-918e-1602ef935990/download_file?safe_filename=1%2BYear%252C%2B1000km-%2BThe%2BOxford%2BRobotCar%2BDataset.pdf&file_format=application%2Fpdf&type_of_work=Journal+article},
 number = {1},
 pages = {3-15},
 title = {1 year, 1000 km: The Oxford RobotCar dataset},
 url = {https://app.dimensions.ai/details/publication/pub.1053938787},
 volume = {36},
 year = {2017}
}

@article{pub.1061419655,
 abstract = {Part II of the tutorial has summarized the remaining building blocks of the VO pipeline: specifically, how to detect and match salient and repeatable features across frames and robust estimation in the presence of outliers and bundle adjustment. In addition, error propagation, applications, and links to publicly available code are included. VO is a well understood and established part of robotics. VO has reached a maturity that has allowed us to successfully use it for certain classes of applications: space, ground, aerial, and underwater. In the presence of loop closures, VO can be used as a building block for a complete SLAM algorithm to reduce motion drift. Challenges that still remain are to develop and demonstrate large-scale and long-term implementations, such as driving autonomous cars for hundreds of miles. Such systems have recently been demonstrated using Lidar and Radar sensors [86]. However, for VO to be used in such systems, technical issues regarding robustness and, especially, long-term stability have to be resolved. Eventually, VO has the potential to replace Lidar-based systems for egomotion estimation, which are currently leading the state of the art in accuracy, robustness, and reliability. VO offers a cheaper and mechanically easier-to-manufacture solution for egomotion estimation, while, additionally, being fully passive. Furthermore, the ongoing miniaturization of digital cameras offers the possibility to develop smaller and smaller robotic systems capable of ego-motion estimation.},
 author = {Fraundorfer, Friedrich and Scaramuzza, Davide},
 doi = {10.1109/mra.2012.2182810},
 journal = {IEEE Robotics & Automation Magazine},
 keywords = {},
 note = {https://doi.org/10.1109/mra.2012.2182810},
 number = {2},
 pages = {78-90},
 title = {Visual Odometry : Part II: Matching, Robustness, Optimization, and Applications},
 url = {https://app.dimensions.ai/details/publication/pub.1061419655},
 volume = {19},
 year = {2012}
}

@article{pub.1061485525,
 abstract = {To enable navigation of miniature aerial vehicles (MAVs) with a low-quality inertial measurement unit (IMU), external sensors are typically fused with the information generated by the low-quality IMU. Most commercial systems for MAVs currently fuse GPS measurements with IMU information to navigate the MAV. However there are many scenarios in which an MAV might prove useful, but GPS is not available (e.g., indoors, urban terrain, etc.). Therefore several approaches have recently been introduced that couple information from an IMU with visual information (usually captured by an electro-optical camera). In general the methods for fusing visual information with an IMU utilizes one of two techniques: 1) applying rigid body constraints on where landmarks should appear in a set of two images (constraint-based fusion) or 2) simultaneously estimating the location of features that are observed by the camera (mapping) and the location of the camera (simultaneous localization and mapping-SLAM-based fusion). While each technique has some nuances associated with its implementation in a true MAV environment (i.e., computational requirements, real-time implementation, feature tracking, etc.), this paper focuses solely on answering the question "Which fusion technique (constraint- or SLAM-based) enables more accurate long-term MAV navigation?" To answer this question, specific implementations of a constraint- and SLAM-based fusion technique, with novel modifications for improved results on MAVs, are described. A basic simulation environment is used to perform a comparison of the constraint- and SLAM-based fusion methods. We demonstrate the superiority of SLAM-based techniques in specific MAV flight scenarios and discuss the relative weaknesses and strengths of each fusion approach.},
 author = {Taylor, Clark N and Veth, Michael J and Raquet, John F and Miller, Mikel M},
 doi = {10.1109/taes.2011.5751236},
 journal = {IEEE Transactions on Aerospace and Electronic Systems},
 keywords = {},
 number = {2},
 pages = {946-958},
 title = {Comparison of Two Image and Inertial Sensor Fusion Techniques for Navigation in Unmapped Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1061485525},
 volume = {47},
 year = {2011}
}

@article{pub.1061515458,
 abstract = {Detection of cracks on bridge decks is a vital task for maintaining the structural health and reliability of concrete bridges. Robotic imaging can be used to obtain bridge surface image sets for automated on-site analysis. We present a novel automated crack detection algorithm, the STRUM (spatially tuned robust multifeature) classifier, and demonstrate results on real bridge data using a state-of-the-art robotic bridge scanning system. By using machine learning classification, we eliminate the need for manually tuning threshold parameters. The algorithm uses robust curve fitting to spatially localize potential crack regions even in the presence of noise. Multiple visual features that are spatially tuned to these regions are computed. Feature computation includes examining the scale-space of the local feature in order to represent the information and the unknown salient scale of the crack. The classification results are obtained with real bridge data from hundreds of crack regions over two bridges. This comprehensive analysis shows a peak STRUM classifier performance of 95% compared with 69% accuracy from a more typical image-based approach. In order to create a composite global view of a large bridge span, an image sequence from the robot is aligned computationally to create a continuous mosaic. A crack density map for the bridge mosaic provides a computational description as well as a global view of the spatial patterns of bridge deck cracking. The bridges surveyed for data collection and testing include Long-Term Bridge Performance program's (LTBP) pilot project bridges at Haymarket, VA, USA, and Sacramento, CA, USA. Note to Practitionersâ€”The automated crack detection algorithm can analyze an image sequence with full video coverage of the region of interest at high resolution (approximately 0.6 mm pixel size). The image sequence can be acquired with a robotic measurement device with attached cameras or with a mobile cart equipped with surface imaging cameras. The automated algorithm can provide a crack map from this video sequence that creates a seamless photographic panorama with annotated crack regions. Crack density (the number of cracks per region) is illustrated in the crack map because individual cracks are difficult to see at the magnification required to view large regions of the bridge deck.},
 author = {Prasanna, Prateek and Dana, Kristin J. and Gucunski, Nenad and Basily, Basily B. and La, Hung M. and Lim, Ronny Salim and Parvardeh, Hooman},
 doi = {10.1109/tase.2014.2354314},
 journal = {IEEE Transactions on Automation Science and Engineering},
 keywords = {},
 number = {2},
 pages = {591-599},
 title = {Automated Crack Detection on Concrete Bridges},
 url = {https://app.dimensions.ai/details/publication/pub.1061515458},
 volume = {13},
 year = {2016}
}

@article{pub.1061785933,
 abstract = {Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
 author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jos and Reid, Ian and Leonard, John J.},
 doi = {10.1109/tro.2016.2624754},
 journal = {IEEE Transactions on Robotics},
 keywords = {},
 note = {https://digital.library.adelaide.edu.au/dspace/bitstream/2440/107554/2/RA_hdl_107554.pdf},
 number = {6},
 pages = {1309-1332},
 title = {Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age},
 url = {https://app.dimensions.ai/details/publication/pub.1061785933},
 volume = {32},
 year = {2016}
}

@article{pub.1068093660,
 abstract = {A novel Position-Invariant Robust Feature, designated as PIRF, is presented to address the problem of highly dynamic scene recognition. The PIRF is obtained by identifying existing local features (i.e. SIFT) that have a wide baseline visibility within a place (one place contains more than one sequential images). These wide-baseline visible features are then represented as a single PIRF, which is computed as an average of all descriptors associated with the PIRF. Particularly, PIRFs are robust against highly dynamical changes in scene: a single PIRF can be matched correctly against many features from many dynamical images. This paper also describes an approach to using these features for scene recognition. Recognition proceeds by matching an individual PIRF to a set of features from test images, with subsequent majority voting to identify a place with the highest matched PIRF. The PIRF system is trained and tested on 2000+ outdoor omnidirectional images and on COLD datasets. Despite its simplicity, PIRF offers a markedly better rate of recognition for dynamic outdoor scenes (ca. 90%) than the use of other features. Additionally, a robot navigation system based on PIRF (PIRF-Nav) can outperform other incremental topological mapping methods in terms of time (70% less) and memory. The number of PIRFs can be reduced further to reduce the time while retaining high accuracy, which makes it suitable for long-term recognition and localization.},
 author = {KAWEWONG, Aram and TANGRUAMSUB, Sirinart and HASEGAWA, Osamu},
 doi = {10.1587/transinf.e93.d.2587},
 journal = {IEICE Transactions on Information and Systems},
 keywords = {},
 note = {https://www.jstage.jst.go.jp/article/transinf/E93.D/9/E93.D_9_2587/_pdf},
 number = {9},
 pages = {2587-2601},
 title = {Position-Invariant Robust Features for Long-Term Recognition of Dynamic Outdoor Scenes},
 url = {https://app.dimensions.ai/details/publication/pub.1068093660},
 volume = {E93.D},
 year = {2010}
}

@article{pub.1068819923,
 abstract = {We propose a discriminative and compact scene descriptor for single-view place recognition that facilitates long-term visual SLAM in familiar, semi-dynamic, and partially changing environments. In contrast to popular bag-of-words scene descriptors, which rely on a library of vector quantized visual features, our proposed scene descriptor is based on a library of raw image data (such as an available visual experience, images shared by other colleague robots, and publicly available image data on the Web) and directly mine it to find visual phrases (VPs) that discriminatively and compactly explain an input query/database image. Our mining approach is motivated by recent success achieved in the field of common pattern discovery â€“ specifically mining of common visual patterns among scenes â€“ and requires only a single library of raw images that can be acquired at different times or on different days. Experimental results show that, although our scene descriptor is significantly more compact than conventional descriptors, its recognition performance is relatively high.},
 author = {Tanaka, Kanji and Chokushi, Yuuto and Ando, Masatoshi},
 doi = {10.20965/jaciii.2016.p0057},
 journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
 keywords = {},
 note = {https://doi.org/10.20965/jaciii.2016.p0057},
 number = {1},
 pages = {57-65},
 title = {Mining Visual Phrases for Visual Robot Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1068819923},
 volume = {20},
 year = {2016}
}

@article{pub.1068822123,
 abstract = {Since the dawning of the Robotics age, mobile robots have been important objectives of research and development. Working from such aspects as locomotion mechanisms, path and motion planning algorithms, navigation, map building and localization, and system architecture, researchers are working long and hard. Despite the fact that mobile robotics has a shorter history than conventional mechanical engineering, it has already accumulated a major, innovative, and rich body of R&D work. Rapid progress in modern scientific technology had advanced to where down-sized low-cost electronic devices, especially highperformance computers, can now be built into such mobile robots. Recent trends in ever higher performance and increased downsizing have enabled those working in the field of mobile robotics to make their models increasingly intelligent, versatile, and dexterous. The down-sized computer systems implemented in mobile robots must provide high-speed calculation for complicated motion planning, real-time image processing in image recognition, and sufficient memory for storing the huge amounts of data required for environment mapping. Given the swift progress in electronic devices, new trends are now emerging in mobile robotics. This special issue on ""Modern Trends in Mobile Robotics"" provides a diverse collection of distinguished papers on modern mobile robotics research. In the area of locomotion mechanisms, Huang et al. provide an informative paper on control of a 6-legged walking robot and Fujiwara et al. contribute progressive work on the development of a practical omnidirectional cart. Given the importance of vision systems enabling robots to survey their environments, Doi et al., Tang et al., and Shimizu present papers on cutting-edge vision-based navigation. On the crucial subject of how to equip robots with intelligence, Hashimoto et al. present the latest on sensor fault detection in dead-reckoning, Miura et al. detail the probabilistic modeling of obstacle motion during mobile robot navigation, Hada et al. treat long-term mobile robot activity, and Lee et al. explore mobile robot control in intelligent space. As guest editors, we are sure readers will find these articles both informative and interesting concerning current issues and new perspectives in modern trends in mobile robotics.},
 author = {Tsubouchi, Takashi and Nagatani, Keiji},
 doi = {10.20965/jrm.2002.p0323},
 journal = {Journal of Robotics and Mechatronics},
 keywords = {},
 number = {4},
 pages = {323-323},
 title = {Special Issue on Modern Trends in Mobile Robotics},
 url = {https://app.dimensions.ai/details/publication/pub.1068822123},
 volume = {14},
 year = {2002}
}

@article{pub.1083507257,
 abstract = {Place recognition, or loop closure detection, is an essential component to address the problem of visual simultaneous localization and mapping (SLAM). Long-term navigation of robots in outdoor environments introduces new challenges to enable life-long SLAM, including the strong appearance change resulting from vegetation, weather, and illumination variations across various times of the day, different days, months, or even seasons. In this paper, we propose a new shared representative appearance learning (SRAL) approach to address long-term visual place recognition. Different from previous methods using a single feature modality or a concatenation of multiple features, our SRAL method autonomously learns representative features that are shared in all scene scenarios, and then fuses the features together to represent the long-term appearance of environments observed by a robot during life-long navigation. By formulating SRAL as a regularized optimization problem, we use structured sparsity-inducing norms to model interrelationships of feature modalities. In addition, an optimization algorithm is developed to efficiently solve the formulated optimization problem, which holds a theoretical convergence guarantee. Extensive empirical study was performed to evaluate the SRAL method using large-scale benchmark datasets, including St Lucia, CMU-VL, and Nordland datasets. Experimental results have shown that our SRAL method obtains superior performance for life-long place recognition using individual images, outperforms previous single image-based methods, and is capable of estimating the importance of feature modalities.},
 author = {Han, Fei and Yang, Xue and Deng, Yiming and Rentschler, Mark and Yang, Dejun and Zhang, Hao},
 doi = {10.1109/lra.2017.2662061},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 number = {2},
 pages = {1172-1179},
 title = {SRAL: Shared Representative Appearance Learning for Long-Term Visual Place Recognition},
 url = {https://app.dimensions.ai/details/publication/pub.1083507257},
 volume = {2},
 year = {2017}
}

@article{pub.1084207528,
 abstract = {We present a new approach to long-term mobile robot mapping in dynamic indoor environments. Unlike traditional world models that are tailored to represent static scenes, our approach explicitly models environmental dynamics. We assume that some of the hidden processes that influence the dynamic environment states are periodic and model the uncertainty of the estimated state variables by their frequency spectra. The spectral model can represent arbitrary timescales of environment dynamics with low memory requirements. Transformation of the spectral model to the time domain allows for the prediction of the future environment states, which improves the robot's long-term performance in changing environments. Experiments performed over time periods of months to years demonstrate that the approach can efficiently represent large numbers of observations and reliably predict future environment states. The experiments indicate that the model's predictive capabilities improve mobile robot localization and navigation in changing environments.},
 author = {Krajnk, Tom and Fentanes, Jaime P. and Santos, Joo M. and Duckett, Tom},
 doi = {10.1109/tro.2017.2665664},
 journal = {IEEE Transactions on Robotics},
 keywords = {},
 note = {http://eprints.lincoln.ac.uk/id/eprint/26196/1/krajnik_TRO.pdf},
 number = {4},
 pages = {964-977},
 title = {FreMEn: Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1084207528},
 volume = {33},
 year = {2017}
}

@article{pub.1085405095,
 abstract = {The autonomy of robotic underwater vehicles is dependent on the ability to perform long-term and long-range missions without need of human intervention. While current state-of-the-art underwater navigation techniques are able to provide sufficient levels of precision in positioning, they require the use of support vessels or acoustic beacons. This can pose limitations on the size of the survey area, but also on the whole cost of the operations.Terrain Based Navigation is a sensor-based navigation technique that bounds the error growth of dead-reckoning using a map with terrain information, provided that there is enough terrain variability. An obvious advantage of Terrain Based Navigation is the fact that no external aiding signals or devices are required. Because of this unique feature, terrain navigation has the potential to dramatically improve the autonomy of Autonomous Underwater Vehicles (AUVs).This paper consists on a comprehensive survey on the recent developments for Terrain Based Navigation methods proposed for AUVs. The survey includes a brief introduction to the original Terrain Based Navigation formulations, as well as a description of the algorithms, and a list of the different implementation alternatives found in the literature. Additionally, and due to the relevance, Bathymetric SLAM techniques will also be discussed.},
 author = {Melo, JosÃ© and Matos, AnÃ­bal},
 doi = {10.1016/j.oceaneng.2017.04.047},
 journal = {Ocean Engineering},
 keywords = {},
 note = {http://repositorio.inesctec.pt/bitstreams/3570a091-41fb-4110-b412-658eb7ccc733/download},
 number = {},
 pages = {250-264},
 title = {Survey on advances on terrain based navigation for autonomous underwater vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1085405095},
 volume = {139},
 year = {2017}
}

@article{pub.1085407702,
 abstract = {This paper presents the design of an Unmanned Underwater Vehicle (UUV) used for surveying coral reef environments. Application of the Simultaneous Localisation and Mapping algorithm are shown using data collected by the vehicle operating on the Great Barrier Reef in Australia. By fusing information from the vehicle's on-board sonar and vision systems, it is possible to use the highly textured reef to provide estimates of the vehicle motion as well as to generate models of the gross structure of the underlying reefs. Terrain-aided navigation promises to revolutionise the ability of marine systems to track underwater bodies in many applications. This work represents a crucial step in the development of underwater technologies capable of long-term, reliable deployment. Results of the application of this technique to the tracking of the vehicle position are shown.},
 author = {Williams, Stefan B. and Mahon, Ian},
 doi = {10.1016/s1474-6670(17)31100-x},
 journal = {IFAC Proceedings Volumes},
 keywords = {},
 number = {14},
 pages = {175-180},
 title = {Design of an Unmanned Underwater Vehicle for Reef Surveying},
 url = {https://app.dimensions.ai/details/publication/pub.1085407702},
 volume = {37},
 year = {2004}
}

@article{pub.1085457515,
 abstract = {In this work, we focus on key topics related to underwater Simultaneous Localization and Mapping (SLAM) applications. Moreover, a detailed review of major studies in the literature and our proposed solutions for addressing the problem are presented. The main goal of this paper is the enhancement of the accuracy and robustness of the SLAM-based navigation problem for underwater robotics with low computational costs. Therefore, we present a new method called AEKF-SLAM that employs an Augmented Extended Kalman Filter (AEKF)-based SLAM algorithm. The AEKF-based SLAM approach stores the robot poses and map landmarks in a single state vector, while estimating the state parameters via a recursive and iterative estimation-update process. Hereby, the prediction and update state (which exist as well in the conventional EKF) are complemented by a newly proposed augmentation stage. Applied to underwater robot navigation, the AEKF-SLAM has been compared with the classic and popular FastSLAM 2.0 algorithm. Concerning the dense loop mapping and line mapping experiments, it shows much better performances in map management with respect to landmark addition and removal, which avoid the long-term accumulation of errors and clutters in the created map. Additionally, the underwater robot achieves more precise and efficient self-localization and a mapping of the surrounding landmarks with much lower processing times. Altogether, the presented AEKF-SLAM method achieves reliably map revisiting, and consistent map upgrading on loop closure.},
 author = {Yuan, Xin and MartÃ­nez-Ortega, JosÃ©-FernÃ¡n and FernÃ¡ndez, JosÃ© Antonio SÃ¡nchez and Eckert, Martina},
 doi = {10.3390/s17051174},
 journal = {Sensors},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/17/5/1174/pdf},
 number = {5},
 pages = {1174},
 title = {AEKF-SLAM: A New Algorithm for Robotic Underwater Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1085457515},
 volume = {17},
 year = {2017}
}

@inproceedings{pub.1086226601,
 abstract = {The author shows how it is possible for a mobile robot to exploit the visual information obtained by scanning a room to determine its size and shape, and to orient itself continually within it. The equipment used is a very simple camera setup whose detailed initial configuration is not known but can be deduced as the algorithm runs. The approach does not require any special environment, nor is it sensitive to changes in the physical aspect of the room being inspected such as moved furniture or roaming people. The long-term goal of the project is for the robot to use the information thus acquired in order to build maps of its environment, presumed to be a single floor of an office building, and to localize itself within this framework.<>},
 author = {Sarachik, K.B.},
 booktitle = {Proceedings, 1989 International Conference on Robotics and Automation},
 doi = {10.1109/robot.1989.100109},
 keywords = {},
 pages = {984-989 vol.2},
 title = {Characterising an indoor environment with a mobile robot and uncalibrated stereo},
 url = {https://app.dimensions.ai/details/publication/pub.1086226601},
 year = {1989}
}

@inproceedings{pub.1086272536,
 abstract = {Discusses a significant open problem in mobile robotics: simultaneous map building and localization, which the authors define as long-term globally referenced position estimation without a priori information. This problem is difficult because of the following paradox: to move precisely, a mobile robot must have an accurate environment map; however, to build an accurate map, the mobile robot's sensing locations must be known precisely. In this way, simultaneous map building and localization can be seen to present a question of 'which came first, the chicken or the egg?' (The map or the motion?) When using ultrasonic sensing, to overcome this issue the authors equip the vehicle with multiple servo-mounted sonar sensors, to provide a means in which a subset of environment features can be precisely learned from the robot's initial location and subsequently tracked to provide precise positioning.<>},
 author = {Leonard, J.J. and Durrant-Whyte, H.F.},
 booktitle = {Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91},
 doi = {10.1109/iros.1991.174711},
 keywords = {},
 pages = {1442-1447 vol.3},
 title = {Simultaneous map building and localization for an autonomous mobile robot},
 url = {https://app.dimensions.ai/details/publication/pub.1086272536},
 year = {1991}
}

@article{pub.1087291676,
 abstract = {With recent progress in lifelong map-learning as well as in information sharing networks, compact representation of a large-size landmark database has become crucial. In this paper, we propose a compact binary code (e.g. 32bit code) landmark representation by employing the semantic hashing technique from web-scale image retrieval. We show how well such a binary representation achieves compactness of a landmark database while maintaining efficiency of the localization system. In our contribution, we investigate the cost-performance, the semantic gap, the saliency evaluation using the presented techniques as well as challenge to further reduce the resources (#bits) per landmark. Experiments using a high-speed car-like robot show promising results.},
 author = {å‰›ä¸€éƒŽ, æ± ç”° and å®Œçˆ¾, ç”°ä¸­ and è³¢ä½‘, è¿‘è—¤ and è²´ä¹‹, éˆ´æœ¨ and è­·, è¦‹æµª},
 doi = {10.1299/jsmermd.2011._2a1-m12_1},
 journal = {The Proceedings of JSME annual Conference on Robotics and Mechatronics (Robomec)},
 keywords = {},
 number = {0},
 pages = {_2a1-m12_1-_2a1-m12_4},
 title = {2A1-M12 åœ§ç¸®Gistã‚·ãƒ¼ãƒ³ç‰¹å¾´ã«åŸºã¥ãè‡ªå·±ä½ç½®æŽ¨å®š(ç§»å‹•ãƒ­ãƒœãƒƒãƒˆã®è‡ªå·±ä½ç½®æŽ¨å®šã¨åœ°å›³æ§‹ç¯‰)},
 url = {https://app.dimensions.ai/details/publication/pub.1087291676},
 volume = {2011},
 year = {2011}
}

@article{pub.1091297886,
 abstract = {This paper describes our approach to perform robust monocular camera metric localization in the dynamic environments of Tsukuba Challenge 2016. We address two issues related to vision-based navigation. First, we improved the coverage by building a custom vocabulary out of the scene and improving upon place recognition routine which is key for global localization. Second, we established possibility of lifelong localization by using previous yearâ€™s map. Experimental results show that localization coverage was higher than 90% for six different data sets taken in different years, while localization average errors were under 0.2 m. Finally, the average of coverage for data sets tested with maps taken in different years was of 75%.},
 author = {Sujiwo, Adi and Takeuchi, Eijiro and Morales, Luis Yoichi and Akai, Naoki and Darweesh, Hatem and Ninomiya, Yoshiki and Edahiro, Masato},
 doi = {10.20965/jrm.2017.p0685},
 journal = {Journal of Robotics and Mechatronics},
 keywords = {},
 note = {https://doi.org/10.20965/jrm.2017.p0685},
 number = {4},
 pages = {685-696},
 title = {Robust and Accurate Monocular Vision-Based Localization in Outdoor Environments of Real-World Robot Challenge},
 url = {https://app.dimensions.ai/details/publication/pub.1091297886},
 volume = {29},
 year = {2017}
}

@inbook{pub.1091403541,
 abstract = {The range of applications for autonomous guided carts (AGC) is increasingly growing. Especially in industrial environments ensuring high safety standards in combination with high availability and flexibility are major requirements. For this reason, knowledge about its own position in the environments becomes particularly important. For AGC with low vehicle height localization approaches based on contour observations are widespread. However, in over-time-changing environments the robustness of these techniques is limited. This paper proposes an approach for updating the underlying map in real time during operation. This map update allows for a long-term robust localization. The proposed approach is evaluated for a dynamic test scenario using a cellular transport vehicle.},
 author = {Hansen, Christoph and Fuerstenberg, Kay},
 booktitle = {Advanced Microsystems for Automotive Applications 2017},
 doi = {10.1007/978-3-319-66972-4_5},
 keywords = {},
 pages = {47-57},
 publisher = {},
 title = {Enabling Robust Localization for Automated Guided Carts in Dynamic Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1091403541},
 year = {2018}
}

@article{pub.1091781372,
 abstract = {The goal of this proposal is to develop novel representations and techniques for localization, mapping and target recognition from videos of indoors and urban outdoors environments. The proposed techniques will facilitate enhanced navigation capabilities by means of visual sensing and enable scalable, long-term navigation and target detection in outdoors and indoors environments. The attained representations will also be applicable towards human-robot interaction, enhancement of human navigational and decision making capabilities and provide compact semantically meaningful summaries of the acquired sensory experience. The proposed representations will be governed by principles of compositionality, facilitate bottom-up learning, enable efficient inference and could be adapted to a task at hand. The main novelty of the approach will be the use both 3D and 2D geometric and photometric cues computed either from video sequence or from novel RBG-D cameras, which provide synchronized video and range data at frame rate. Video poses challenges related to more extreme variations in viewpoint and scale, dramatic changes in lighting and large amount of clutter and occlusions, but also enables computation of 3D structure and motion cues, which can aid segmentation and recognition of object and non-object categories. As a part of this proposal we have developed techniques for semantic labeling of outdoors and indoors environments using photometric and geometric cues from video. The proposed approach is informed by novel features and representations for learning models of objects and non-object categories from video, works effectively with multiple sensing modalities and can be deployed on static frames as well as video in a recursive setting. We have tested the approach extensively on benchmark sequences of indoors and outdoors environments.},
 author = {Kosecka, Jana},
 doi = {10.21236/ada622601},
 journal = {},
 keywords = {},
 number = {},
 pages = {},
 title = {Acquiring Semantically Meaningful Models for Robotic Localization, Mapping and Target Recognition},
 url = {https://app.dimensions.ai/details/publication/pub.1091781372},
 volume = {},
 year = {2014}
}

@inbook{pub.1091832706,
 abstract = {Autonomous robots need to perceive and represent their environments and act accordingly. Using simultaneous localization and mapping (SLAM) methods, robots can build maps of the environment which are efficient for localization and path planning as long as the environment remains unchanged. However, facility logistics environments are not static because pallets and other obstacles are stored temporarily.This paper proposes a novel solution for updating maps of changing environments (i.e. environments with low-dynamic or semi-static objects) in real-time with multiple robots. Each robot is equipped with a laser range sensor and runs localization to estimate its position. Each robot senses the change in the environment with respect to a current map, initially built with a SLAM method, and constructs a temporary map which will be merged into the current map using localization information and line features of the map. This procedure enables the creation of long-term mapping robot systems for facility logistics.},
 author = {Shaik, Nayabrasul and Liebig, Thomas and Kirsch, Christopher and MÃ¼ller, Heinrich},
 booktitle = {KI 2017: Advances in Artificial Intelligence},
 doi = {10.1007/978-3-319-67190-1_19},
 keywords = {},
 pages = {249-261},
 publisher = {},
 title = {Dynamic Map Update of Non-static Facility Logistics Environment with a Multi-robot System},
 url = {https://app.dimensions.ai/details/publication/pub.1091832706},
 year = {2017}
}

@inbook{pub.1092140720,
 abstract = {Visual SLAM is widely used in robotics and computer vision. Although there have been many excellent achievements over the past few decades, there are still some challenges. 2D feature-based SLAM algorithm has been suffering from the inaccurate or insufficient correspondences while dealing with the case of textureless or frequently repeating regions. Furthermore, most of the SLAM systems cannot be used for long-term localization in a wide range of environment because of the heavy burden of calculating and memory. In this paper, we propose a robust RGB-D keyframe-based SLAM algorithm. The novelty of proposed approach lies in using both 2D and 3D features for tracking, pose estimation and bundle adjustment. By using 2D and 3D features, the SLAM system can achieve high accuracy and robustness in some challenging environments. The experimental results on TUM RGB-D dataset [1] and ICL-NUIM dataset [2] verify the effectiveness of our algorithm.},
 author = {Pan, Liangliang and Cheng, Jun and Feng, Wei and Ji, Xiaopeng},
 booktitle = {Computer Vision Systems},
 doi = {10.1007/978-3-319-68345-4_11},
 keywords = {},
 pages = {120-130},
 publisher = {},
 title = {A Robust RGB-D Image-Based SLAM System},
 url = {https://app.dimensions.ai/details/publication/pub.1092140720},
 year = {2017}
}

@inbook{pub.1092140721,
 abstract = {Remarkable performance has been achieved using the state-of-the-art monocular Simultaneous Localization and Mapping (SLAM) algorithms. However, tracking failure is still a challenging problem during the monocular SLAM process, and it seems to be even inevitable when carrying out long-term SLAM in large-scale environments. In this paper, we propose an active loop closure based relocalization system, which enables the monocular SLAM to detect and recover from tracking failures automatically even in previously unvisited areas where no keyframe exists. We test our system by extensive experiments including using the most popular KITTI dataset, and our own dataset acquired by a hand-held camera in outdoor large-scale and indoor small-scale real-world environments where man-made shakes and interruptions were added. The experimental results show that the least recovery time (within 5Â ms) and the longest success distance (up to 46Â m) were achieved comparing to other relocalization systems. Furthermore, our system is more robust than others, as it can be used in different kinds of situations, i.e., tracking failures caused by the blur, sudden motion and occlusion. Besides robots or autonomous vehicles, our system can also be employed in other applications, like mobile phones, drones, etc.},
 author = {Chen, Xieyuanli and Lu, Huimin and Xiao, Junhao and Zhang, Hui and Wang, Pan},
 booktitle = {Computer Vision Systems},
 doi = {10.1007/978-3-319-68345-4_12},
 keywords = {},
 pages = {131-143},
 publisher = {},
 title = {Robust Relocalization Based on Active Loop Closure for Real-Time Monocular SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1092140721},
 year = {2017}
}

@article{pub.1092727335,
 abstract = {For long-term simultaneous planning, localization and mapping (SPLAM), a robot should be able to continuously update its map according to the dynamic changes of the environment and the new areas explored. With limited onboard computation capabilities, a robot should also be able to limit the size of the map used for online localization and mapping. This paper addresses these challenges using a memory management mechanism, which identifies locations that should remain in a Working Memory (WM) for online processing from locations that should be transferred to a Long-Term Memory (LTM). When revisiting previously mapped areas that are in LTM, the mechanism can retrieve these locations and place them back in WM for online SPLAM. The approach is tested on a robot equipped with a short-range laser rangefinder and a RGB-D camera, patrolling autonomously 10.5Â km in an indoor environment over 11 sessions while having encountered 139 people.},
 author = {LabbÃ©, Mathieu and Michaud, FranÃ§ois},
 doi = {10.1007/s10514-017-9682-5},
 journal = {Autonomous Robots},
 keywords = {},
 number = {6},
 pages = {1133-1150},
 title = {Long-term online multi-session graph-based SPLAM with memory management},
 url = {https://app.dimensions.ai/details/publication/pub.1092727335},
 volume = {42},
 year = {2018}
}

@inproceedings{pub.1092949171,
 abstract = {In carrying out simultaneous localization and mapping, a mobile vehicle is used to simultaneously estimate its position and build a map of the environment. The long-term goal of this work is to build an autonomous inspection mobile vehicle for oil storage tanks and pipelines. The harsh environmental conditions in storage tanks and pipelines limit the types of feature extraction sensors and vehicle pose estimation sensors that one can use. Here, a SOund Navigation And Ranging (SONAR) sensor will be used for feature extraction, and a gyroscope and an encoder will be used for vehicle pose estimation. The integration of these sensors (SONAR, encoder, and gyroscope) will be discussed in this paper, along with the use of a recently developed algorithm fusion for SONAR sensors. The integration of the sensors represents a step towards implementation of concurrent localization and mapping progress in harsh environments.Copyright Â© 2015 by ASME},
 author = {Ismail, Hesham and Balachandran, Balakumar},
 booktitle = {Volume 4A: Dynamics, Vibration, and Control},
 doi = {10.1115/imece2015-52427},
 keywords = {},
 pages = {v04at04a024-v04at04a024},
 title = {Vehicle Pose Estimation and SONAR Sensor Based Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1092949171},
 year = {2015}
}

@inproceedings{pub.1093174789,
 abstract = {Life-long and robust operation are important challenges to be solved towards everyday usability of service robots. Global localization is of particular interest for real-world applications. If a robot would not be able to relocalize itself within a known map, all positions stored by the robot (rooms, objects, etc.) would become obsolete. Although Simultaneous Localization and Mapping (SLAM) allows to initially map new and unknown environments and to keep track of environmental changes, it does not solve the global localization problem. Each time SLAM is restarted at different locations, it introduces a new map and a new frame of reference. In this paper, we propose a solution to the global localization problem which uses a SLAM generated feature map. The approach is demonstrated with an omnicam and bearing-only features. A new way to weight hypotheses and to sort out false hypotheses results in fast convergence even with arbitrary relocalization paths. The combined approach is a further step towards life-long operation of service robots and covers every part of a robot lifecycle, ranging from a setup via SLAM to efficient global localization for reuse of maps and object poses after restart.},
 author = {Lutz, Matthias and Hochdorfer, Siegfried and Schlegel, Christian},
 booktitle = {2011 IEEE Conference on Technologies for Practical Robot Applications},
 doi = {10.1109/tepra.2011.5753494},
 keywords = {},
 pages = {127-132},
 title = {Global Localization using Multiple Hypothesis Tracking: A real-world Approach},
 url = {https://app.dimensions.ai/details/publication/pub.1093174789},
 year = {2011}
}

@inproceedings{pub.1093227787,
 abstract = {This paper presents an analysis of FastSLAMâ€”a Rao-Blackwellised particle filter formulation of simultaneous localisation and mapping. It shows that the algorithm degenerates with time, regardless of the number of particles used or the density of landmarks within the environment, and will always produce optimistic estimates of uncertainty in the long-term. In essence, FastSLAM behaves like a non-optimal local search algorithm; in the short-term it may produce consistent uncertainty estimates but, in the long-term, it is unable to adequately explore the state-space to be a reasonable Bayesian estimator. However, the number of particles and landmarks does affect the accuracy of the estimated mean and, given sufficient particles, FastSLAM can produce good non-stochastic estimates in practice. FastSLAM also has several practical advantages, particularly with regard to data association, and will probably work well in combination with other versions of stochastic SLAM, such as EKF-based SLAM.},
 author = {Bailey, Tim and Nieto, Juan and Nebot, Eduardo},
 booktitle = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},
 doi = {10.1109/robot.2006.1641748},
 keywords = {},
 pages = {424-429},
 title = {Consistency of the FastSLAM Algorithm},
 url = {https://app.dimensions.ai/details/publication/pub.1093227787},
 year = {2006}
}

@inproceedings{pub.1093237285,
 abstract = {Acting in everyday-life environments is still a great challenge in service robotics. Although algorithms and solutions already exist for many relevant subproblems, in particular the aspect of robustness and suitability for everyday use has been neglected so far very often. Robustness and suitability for everyday use are features affecting not only the overall system design but have impact on each single algorithm of each component. Although an overwhelming amount of work is available to address the SLAM problem, the challenge of applying a SLAM algorithm over the whole life cycle of a service robot, perhaps even in different environments, has not been brought into focus very often. An obvious problem to be solved is the continuously growing number of landmarks. A lifelong running SLAM approach requires means to select landmarks such that they best cover the working environment given bounded SLAM resources like the maximum number of manageable landmarks. This paper proposes a novel solution for selecting appropriate landmarks to limit the number of landmarks. The idea is to quantify the contribution of a landmark to the ability of the robot to localize itself in its working environment. Thus, the core contribution is to base the landmark selection process upon the landmarks' coverage of the working environment. Real-world experiments on a P3DX-platform with a bearing-only SLAM approach and an omnicam confirm that the addressed question and the proposed first approach might be another step towards the overall goal of suitability for everyday use.},
 author = {Hochdorfer, Siegfried and Schlegel, Christian},
 booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2009.5354433},
 keywords = {},
 pages = {382-387},
 title = {Landmark rating and selection according to localization coverage: Addressing the challenge of lifelong operation of SLAM in service robots},
 url = {https://app.dimensions.ai/details/publication/pub.1093237285},
 year = {2009}
}

@inproceedings{pub.1093259055,
 abstract = {Vision-based, Vision-based, route-following algorithms enable autonomous robots to repeat manually taught paths over long distances using inexpensive vision sensors. However, these methods struggle with long-term, outdoor operation due to the challenges of environmental appearance change caused by lighting, weather, and seasons. While techniques exist to address appearance change by using multiple experiences over different environmental conditions, they either provide topological-only localization, require several manually taught experiences in different conditions, or require extensive offline mapping to produce metric localization. For real-world use, we would like to localize metrically to a single manually taught route and gather additional visual experiences during autonomous operations. Accordingly, we propose a novel multi-experience localization (MEL) algorithm developed specifically for route-following applications; it provides continuous, six-degree-of-freedom (6DoF) localization with relative uncertainty to a privileged (manually taught) path using several experiences simultaneously. We validate our algorithm through two experiments: i) an offline performance analysis on a 9km subset of a challenging 27km route-traversal dataset and ii) an online field trial where we demonstrate autonomy on a small 250m loop over the course of a sunny day. Both exhibit significant appearance change due to lighting variation. Through these experiments we show that safe localization can be achieved by bridging the appearance gap.},
 author = {Paton, Michael and Tavish, Kirk Mac and Warren, Michael and Barfoot, Timothy D.},
 booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2016.7759303},
 keywords = {},
 pages = {1918-1925},
 title = {Bridging the Appearance Gap: Multi-Experience Localization for Long-Term Visual Teach and Repeat},
 url = {https://app.dimensions.ai/details/publication/pub.1093259055},
 year = {2016}
}

@inproceedings{pub.1093261872,
 abstract = {Simultaneous Localization and Mapping, SLAM, is an important topic in the field of robotics and autonomous navigation. The metric SLAM suffers from sensor inaccuracies and thus cannot be used for long-term navigation. In such case, Visual SLAM or a Hybrid SLAM based on both metric and visual approach is a good alternative. In this paper, in order to speed up a Visual SLAM, we propose a novel concept of dynamic dictionary generated on the results of triangulation done on RF, radio frequency, signals from nearest cell towers of a cellular network. This dynamic dictionary efficiently manages the scalability of a Visual SLAM and make it possible to work in a large-scale environment. A framework is proposed along with triangulation data of a city and with simulations to support the concept.},
 author = {Anwar, Shahzad and Zhao, Qingjie and Qadeer, Nouman and Khan, Saqib Ishaq},
 booktitle = {Proceedings of 2013 10th International Bhurban Conference on Applied Sciences & Technology (IBCAST)},
 doi = {10.1109/ibcast.2013.6512139},
 keywords = {},
 pages = {103-108},
 title = {A Framework for RF-Visual SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1093261872},
 year = {2013}
}

@inproceedings{pub.1093278134,
 abstract = {This paper reports on the use of generic linear constraint (GLC) node removal as a method to control the computational complexity of long-term simultaneous localization and mapping. We experimentally demonstrate that GLC provides a principled and flexible tool enabling a wide variety of complexity management schemes. Specifically, we consider two main classes: batch multi-session node removal, in which nodes are removed in a batch operation between mapping sessions, and online node removal, in which nodes are removed as the robot operates. Results are shown for 34.9 h of realworld indoor-outdoor data covering 147.4 km collected over 27 mapping sessions spanning a period of 15 months.},
 author = {Carlevaris-Bianco, Nicholas and Eustice, Ryan M.},
 booktitle = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2013.6696478},
 keywords = {},
 note = {http://robots.engin.umich.edu/publications/ncarlevaris-2013b.pdf},
 pages = {1034-1041},
 title = {Long-Term Simultaneous Localization and Mapping with Generic Linear Constraint Node Removal},
 url = {https://app.dimensions.ai/details/publication/pub.1093278134},
 year = {2013}
}

@inproceedings{pub.1093319419,
 abstract = {This work presents our results on 3D robot localization, mapping and path planning for the latest joint exercise of the European project â€œLong-Term Human-Robot Teaming for Robots Assisted Disaster Responseâ€ (TRADR)1. The full system is operated and evaluated by firemen end-users in real-world search and rescue experiments. We demonstrate that the system is able to plan a path to a goal position desired by the fireman operator in the TRADR Operational Control Unit (OCU), using a persistent 3D map created by the robot during previous sorties.},
 author = {Dube, R. and Gawel, A. and Cadena, C. and Siegwart, R. and Freda, L. and Gianni, M.},
 booktitle = {2016 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)},
 doi = {10.1109/ssrr.2016.7784311},
 keywords = {},
 note = {https://www.research-collection.ethz.ch/bitstream/20.500.11850/127840/1/eth-50472-01.pdf},
 pages = {272-273},
 title = {3D Localization, Mapping and Path Planning for Search and Rescue Operations},
 url = {https://app.dimensions.ai/details/publication/pub.1093319419},
 year = {2016}
}

@inproceedings{pub.1093349796,
 abstract = {We demonstrate the viability of using 2D LIDAR data as the sole means for accurate, robust, long-term roadvehicle localization within a prior map in a complex, dynamic real-world setting. We utilize a dual-LIDAR system-one oriented horizontally, in order to infer vehicle linear and rotational velocity, and one declined to capture a dense view of the surrounds-that allows us to estimate both velocity and position within a prior map. We show how probabilistically modelling the noisy local velocity estimates from the horizontal laser feed, fusing these estimates with data from the declined LIDAR to form a dense 3D swathe and matching this swathe statistically within a map will allow for robust, long-term position estimation. We accommodate estimation errors induced by passing vehicles, pedestrians, ground-strike etc., by learning a positional-dependent sensor model-that is, a sensor-model that varies spatially-and show that learning such a model for LIDAR data allows us to deal gracefully with the complexities of real-world data. We validate the concept over more than 9 kilometres of driven distance in and around the town of Woodstock, Oxfordshire.},
 author = {Baldwin, Ian and Newman, Paul},
 booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2012.6385677},
 keywords = {},
 pages = {2490-2497},
 title = {Laser-only road-vehicle localization with dual 2D push-broom LIDARS and 3D priors},
 url = {https://app.dimensions.ai/details/publication/pub.1093349796},
 year = {2012}
}

@inproceedings{pub.1093355991,
 abstract = {In the problem of simultaneously localization and mapping (SLAM) for a mobile robot, it is required to detect previously visited locations so the estimation error shall be reduced. Sensor observations are compared by a similarity metric to detect loops. In long term navigation or exploration, the number of observations increases and so the complexity of the loop closure detection. Several techniques are proposed in order to reduce the complexity of loop closure detection. Few algorithms have considered the loop closure detection from a subset of sensor observations. In this paper, the compressed sensing approach is exploited to detect loops from few sensor measurements. In the basic compressed sensing it is assumed that a signal has a sparse representation is a basis which means that only a few elements of the signal are non-zero. Based on the compressed sensing approach a sparse signal can be recovered from few linear noisy projections by $l$ 1 minimization. The difference matrix which is widely used for loop detection has a sparse structure, where similar observations are shown by zero distance and different locations are indicated by ones. Based on the multiple measurement vector technique which is an extension of the basic compressed sensing, the loop closure detection is performed by comparison of few sensor observations. The applicability of the proposed algorithm is investigated in some outdoor environments through some publicly available data sets. It has been shown by some experiments that the proposed method can detect loops effectively.},
 author = {Ravari, Alireza Norouzzadeh and Taghirad, Hamid D.},
 booktitle = {2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM)},
 doi = {10.1109/icrom.2015.7367836},
 keywords = {},
 pages = {511-516},
 title = {Loop Closure Detection by Compressed Sensing for Exploration of Mobile Robots in Outdoor Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1093355991},
 year = {2015}
}

@inproceedings{pub.1093365981,
 abstract = {Robust long term positioning for autonomous mobile robots is essential for many applications. Key to a successful visual SLAM system is correctly recognizing the objects and labeling where the robot is. Local image features are popular with constructing object recognition system, which are invariant to image scaling, translation, rotation, and partially invariant to illumination changes and affine. In this paper, we proposed an object recognition method based on the bag of word model, mainly idea includes three steps as follows: firstly, a set of local image patches are sampled using a key point detector, and each patch is a descriptor based on scale invariant feature transform. Then outliers are removed by RANSAC algorithm, and the resulting distribution of descriptors is quantified by using vector quantization against a pre-specified codebook to convert it to a histogram of votes for codebook centers. Finally, a KNN algorithm is used to classify images through the resulting global descriptor vector. The experimental results show that our proposed method has a better performance against the previous methods.},
 author = {Yang, Jin-fu and Wang, Kai and Li, Ming-ai and Liu, Lu},
 booktitle = {2011 IEEE International Conference on Mechatronics and Automation},
 doi = {10.1109/icma.2011.5986295},
 keywords = {},
 pages = {1735-1740},
 title = {Research on Object Recognition Using Bag of Word Model for Mobile Robot Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1093365981},
 year = {2011}
}

@inproceedings{pub.1093374018,
 abstract = {The paper describes the general idea, the application scenario, and selected methodological approaches of our long term research project PERSES (PERsonal SErvice System). The aim of the project consists of the development of an interactive mobile shopping assistant that allows a continuous and intuitively understandable interaction with a customer in a home improvement store. Typical tasks we have to tackle are to detect and contact potential users in the operation area, to guide them to desired areas or articles within the store or to follow them as a mobile information kiosk while continuously observing their behavior. Due to the specificity of the interaction-oriented scenario and the characteristics of the operation area, we have focused on vision based methods for both human-robot interaction and robot navigation. Besides some methodological approaches, we present preliminary results of experiments achieved with our mobile robot PERSES in the store with an emphasis on vision based methods for user localization, map building and self-localization.},
 author = {Gross, H.-M. and Boehme, H.-J.},
 booktitle = {SMC 2000 Conference Proceedings. 2000 IEEE International Conference on Systems, Man and Cybernetics. 'Cybernetics Evolving to Systems, Humans, Organizations, and their Complex Interactions' (Cat. No.00CH37166)},
 doi = {10.1109/icsmc.2000.884968},
 keywords = {},
 pages = {80-85 vol.1},
 title = {PERSES-a vision-based interactive mobile shopping assistant},
 url = {https://app.dimensions.ai/details/publication/pub.1093374018},
 year = {2000}
}

@inproceedings{pub.1093398080,
 abstract = {This paper presents a work on mapping the use of space by humans in long periods of time. Daily geometric maps with the same coordinate frame were generated with SLAM, and in a similar manner, daily affordance density maps (places people use) were generated with the output of a human tracker running on the robot. The contribution of the paper is two-fold: an approach to detect geometric changes to cluster them in similar geometric configurations and the building of geometric and affordance composite maps on each cluster. This approach avoids the loss of long term retrieved information. Geometric similarity was computed using a normal distance approach on the maps. The analysis was performed on data collected by a mobile robot for a period of 4 months accumulating data equivalent to 70 days. Experimental results show that the system is capable of detecting geometric changes in the environment and clustering similar geometric configurations.},
 author = {Limosani, R. and Morales, L. Yoichi and Even, J. and Ferreri, F. and Watanabe, A. and Cavallo, F. and Dario, P. and Hagita, N.},
 booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2015.7354193},
 keywords = {},
 note = {https://www.iris.sssup.it/bitstream/11382/511937/1/C033%20-%20Long-term%20human%20affordance%20maps.pdf},
 pages = {5748-5754},
 title = {Long-Term Human Affordance Maps},
 url = {https://app.dimensions.ai/details/publication/pub.1093398080},
 year = {2015}
}

@inproceedings{pub.1093477411,
 abstract = {Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need to extend beyond geometry and appearance â€” they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state-of-the-art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondences between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of â‰ˆ25Hz.},
 author = {McCormac, John and Handa, Ankur and Davison, Andrew and Leutenegger, Stefan},
 booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2017.7989538},
 keywords = {},
 note = {http://arxiv.org/pdf/1609.05130},
 pages = {4628-4635},
 title = {SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks},
 url = {https://app.dimensions.ai/details/publication/pub.1093477411},
 year = {2017}
}

@inproceedings{pub.1093497029,
 abstract = {This paper presents a new approach for topological localisation of service robots in dynamic indoor environments. In contrast to typical localisation approaches that rely mainly on static parts of the environment, our approach makes explicit use of information about changes by learning and modelling the spatio-temporal dynamics of the environment where the robot is acting. The proposed spatio-temporal world model is able to predict environmental changes in time, allowing the robot to improve its localisation capabilities during longterm operations in populated environments. To investigate the proposed approach, we have enabled a mobile robot to autonomously patrol a populated environment over a period of one week while building the proposed model representation. We demonstrate that the experience learned during one week is applicable for topological localization even after a hiatus of three months by showing that the localization error rate is significantly lower compared to static environment representations.},
 author = {KrajnÃ­k, TomÃ¡Å” and Fentanes, Jaime P. and Mozos, Oscar M. and Duckett, Tom and Ekekrantz, Johan and Hanheide, Marc},
 booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2014.6943205},
 keywords = {},
 note = {http://eprints.lincoln.ac.uk/id/eprint/14423/1/iros_2014_public.pdf},
 pages = {4537-4542},
 title = {Long-Term Topological Localisation for Service Robots in Dynamic Environments Using Spectral Maps},
 url = {https://app.dimensions.ai/details/publication/pub.1093497029},
 year = {2014}
}

@inproceedings{pub.1093508297,
 abstract = {WITU (Wearable Indoor Tracking Unit) is a wearable robotic device that aids indoor navigation by building maps and localizing the user within them. Applications of such a device include search and rescue, travel aid in large and complex buildings, museum guides among others where external localization information such as from a GPS is not available. However, WITU relies on human intelligence both to maintain long term consistency of its location estimates and to efficiently manage its limited memory and processing capacity. This alludes to a symbiotic relationship between the user and the device and here we look at this symbiotic relationship from an end user perspective. Thus, in order to have a successful interaction, we argue that the user needs to feel comfortable wearing the device while carrying out the intended tasks. We hypothesize that this perceived comfort is dependent on the context in which the device is used. We test our hypothesis on three different scenarios; search and rescue worker, dementia patient in a long care facility and a person at a party which acts as the baseline. Results indicate an important consequence for the development of such wearable robotic systems.},
 author = {Herath, Damith C. and Chapman, Tawna and Tomkins, Alethea and Elliott, Larissa and David, Michelle and Cooper, Amy and Burnham, Denis and Kodagoda, Sarath},
 booktitle = {2011 IEEE International Conference on Robotics and Biomimetics},
 doi = {10.1109/robio.2011.6181757},
 keywords = {},
 pages = {2969-2974},
 title = {A Study on Wearable Robotics - Comfort is in the Context},
 url = {https://app.dimensions.ai/details/publication/pub.1093508297},
 year = {2011}
}

@inproceedings{pub.1093534359,
 abstract = {Localization and modeling of stairways by mobile robots can enable multi-floor exploration for those platforms capable of stair traversal. Existing approaches focus on eiThe R stairway detection or traversal, but do not address these problems in the context of path planning for the autonomous exploration of multi-floor buildings. We propose a system for detecting and modeling ascending stairways while performing simultaneous localization and mapping, such that the traversability of each stairway can be assessed by estimating its physical properties. The long-term objective of our approach is to enable exploration of multiple floors of a building by allowing stairways to be considered during path planning as traversable portals to new frontiers. We design a generative model of a stairway as a single object. We localize these models with respect to the map, and estimate the dimensions of the stairway as a whole, as well as its steps. With these estimates, a robot can determine if the stairway is traversable based on its climbing capabilities. Our system consists of two parts: a computationally efficient detector that leverages geometric cues from dense depth imagery to detect sets of ascending stairs, and a stairway modeler that uses multiple detections to infer the location and parameters of a stairway that is discovered during exploration. We demonstrate the performance of this system when deployed on several mobile platforms using a Microsof T Kinect sensor.},
 author = {Delmerico, Jeffrey A. and Baran, David and David, Philip and Ryde, Julian and Corso, Jason J.},
 booktitle = {2013 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/icra.2013.6630886},
 keywords = {},
 note = {http://www.cse.buffalo.edu/~jcorso/pubs/delmerico_ICRA2013_stairs.pdf},
 pages = {2283-2290},
 title = {Ascending Stairway Modeling from Dense Depth Imagery for Traversability Analysis},
 url = {https://app.dimensions.ai/details/publication/pub.1093534359},
 year = {2013}
}

@inproceedings{pub.1093548656,
 abstract = {This paper presents methods for path planning and obstacle avoidance for the humanoid robot QRIO, allowing the robot to autonomously walk around in a home environment, For an autonomous robot, obstacle detection and localization as well as representing them in a map are crucial tasks for the success of the robot. Our approach is based on plane extraction from data captured by a stereo-vision system that has been developed specifically for QRIO. We briefly overview the general software architecture composed of perception, short and long term memory, behavior control, and motion control, and emphasize on our methods for obstacle detection by plane extraction, occupancy grid mapping, and path planning. Experimental results complete the description of our system.},
 author = {Sabe, Kohtaro and Fukuchi, Masaki and Gutmann, Jens-Steffen and Ohashi, Takeshi and Kawamoto, Kenta and Yoshigahara, Takayuki},
 booktitle = {IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004},
 doi = {10.1109/robot.2004.1307213},
 keywords = {},
 pages = {592-597},
 title = {Obstacle Avoidance and Path Planning for Humanoid Robots using Stereo Vision},
 url = {https://app.dimensions.ai/details/publication/pub.1093548656},
 year = {2004}
}

@inproceedings{pub.1093568327,
 abstract = {Maintaining a map of an environment that changes over time is a critical challenge in the development of persistently autonomous mobile robots. Many previous approaches to mapping assume a static world. In this work we incorporate the time dimension into the mapping process to enable a robot to maintain an accurate map while operating in dynamical environments. This paper presents Dynamic Pose Graph SLAM (DPG-SLAM), an algorithm designed to enable a robot to remain localized in an environment that changes substantially over time. Using incremental smoothing and mapping (iSAM) as the underlying SLAM state estimation engine, the Dynamic Pose Graph evolves over time as the robot explores new places and revisits previously mapped areas. The approach has been implemented for planar indoor environments, using laser scan matching to derive constraints for SLAM state estimation. Laser scans for the same portion of the environment at different times are compared to perform change detection; when sufficient change has occurred in a location, the dynamic pose graph is edited to remove old poses and scans that no longer match the current state of the world. Experimental results are shown for two real-world dynamic indoor laser data sets, demonstrating the ability to maintain an up-to-date map despite long-term environmental changes.},
 author = {Walcott-Bryant, Aisha and Kaess, Michael and Johannsson, Hordur and Leonard, John J.},
 booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2012.6385561},
 keywords = {},
 note = {https://dspace.mit.edu/bitstream/1721.1/78911/1/Leonard_Dymanic%20pose.pdf},
 pages = {1871-1878},
 title = {Dynamic Pose Graph SLAM: Long-term Mapping in Low Dynamic Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1093568327},
 year = {2012}
}

@inproceedings{pub.1093573802,
 abstract = {This paper presents a novel method to use optical flow navigation for long term navigation. Unlike standard SLAM approaches for augmented reality, OFLAAM is designed for Micro Air Vehicles (MAV). It uses a optical flow camera pointing downwards, a IMU and a monocular camera pointing frontwards. That configuration avoids the computational expensive mapping and tracking of the 3D features. It only maps these features in a vocabulary list by a localization module to tackle the optical flow drift and the lose of the navigation estimation. That module, based on the well established algorithm DBoW2, will be also used to close the loop and allow long-term navigation in previously visited areas. The combination of high speed optical flow navigation with a low rate localization algorithm allows fully autonomous navigation for MAV, at the same time it reduces the overall computational load. This framework is implemented in ROS (Robot Operating System) and tested attached to a laptop. A representative scenario is used to validate and analyze the performance of the system.},
 author = {Pastor-Moreno, Daniel and Shin, Hyo-Sang and Waldock, Antony},
 booktitle = {2015 International Conference on Unmanned Aircraft Systems (ICUAS)},
 doi = {10.1109/icuas.2015.7152387},
 keywords = {},
 pages = {980-988},
 title = {Optical Flow Localisation and Appearance Mapping (OFLAAM) for Long-Term Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1093573802},
 year = {2015}
}

@inproceedings{pub.1093626771,
 abstract = {Localization and mapping are fundamental problems in service robotics since representations of the environment and knowledge about the own pose significantly simplify the implementation of a series of high-level applications. ToF (time-of-flight) cameras are a relatively new kind of sensors in robotics. They enable the real-time capture of the distance and the grayscale information of a scene. Due to the increase of the image resolution of ToF cameras, now highlevel computer vision algorithms for visual feature extraction (e.g. SIFT [1] or SURF [2]) can be applied to the captured images. These visual features combined with the corresponding distance information give a full measurement of 3D landmarks. An obvious problem to be solved is the continuously growing number of landmarks. So far, all ever seen landmarks are just accumulated irrespective of their utility and the then required resources. Rather, one should keep only really useful landmarks, e.g. such that localization quality in the whole operational area is kept above a given threshold. In fact a lifelong running SLAM approach is dependent on means to select and discard landmarks. That is even more acute in case of feature-rich sensor data as provided with high update rates by sensors like a ToF camera. We run our SLAM approach in a real-world experiment within an indoor environment. The experiment was performed on a P3DX-platform equipped with a PMD CamCube 2.0 and a Xsens IMU.},
 author = {Hochdorfer, Siegfried and Schlegel, Christian},
 booktitle = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2010.5651229},
 keywords = {},
 pages = {3981-3986},
 title = {6 DoF SLAM Using a ToF Camera: The Challenge of a Continuously Growing Number of Landmarks},
 url = {https://app.dimensions.ai/details/publication/pub.1093626771},
 year = {2010}
}

@inproceedings{pub.1093635536,
 abstract = {SLAM (Simultaneous Localization and Mapping) for underwater vehicles is a challenging research topic due to the limitations of underwater localization sensors and error accumulation over long-term operations. Furthermore, acoustic sensors for mapping often provide noisy and distorted images or low-resolution ranging, while video images provide highly detailed images but are often limited due to turbidity and lighting. This paper presents a review of the approaches used in state-of-the-art SLAM techniques: Extended Kalman Filter SLAM (EKF-SLAM), FastSLAM, GraphSLAM and its application in underwater environments.},
 author = {Hidalgo, Franco and BrÃ¤unl, Thomas},
 booktitle = {2015 6th International Conference on Automation, Robotics and Applications (ICARA)},
 doi = {10.1109/icara.2015.7081165},
 keywords = {},
 pages = {306-311},
 title = {Review of Underwater SLAM Techniques},
 url = {https://app.dimensions.ai/details/publication/pub.1093635536},
 year = {2015}
}

@inproceedings{pub.1093670309,
 abstract = {This paper proposes extending Monte Carlo Localization methods with visual information in order to build a long term robot localization system. This system is aimed to work in crowded and non-planar scenarios, where 2D laser rangefinders may not always be enough to match the robot position with the map. Thus, visual place recognition will be used in order to obtain robot position clues that can be used to detect when the robot is lost and also to reset its positions to the right one. The paper presents experimental results based on datasets gathered with a real robot in challenging scenarios.},
 author = {PÃ©rez, Javier and Caballero, Fernando and Merino, Luis},
 booktitle = {2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
 doi = {10.1109/icarsc.2014.6849767},
 keywords = {},
 pages = {85-91},
 title = {Integration of Monte Carlo Localization and Place Recognition for Reliable Long-Term Robot Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1093670309},
 year = {2014}
}

@inproceedings{pub.1093716058,
 abstract = {Precisely estimating the pose of an agent in a global reference frame is a crucial goal that unlocks a multi-tude of robotic applications, including autonomous navigation and collaboration. In order to achieve this, current state-of-the-art localization approaches collect data provided by one or more agents and create a single, consistent localization map, maintained over time. However, with the introduction of lengthier sorties and the growing size of the environments, data transfers between the backend server where the global map is stored and the agents are becoming prohibitively large. While some existing methods partially address this issue by building compact summary maps, the data transfer from the agents to the backend can still easily become unmanageable. In this paper, we propose a method that is designed to reduce the amount of data that needs to be transferred from the agent to the backend, functioning in large-scale, multi-session mapping scenarios. Our approach is based upon a landmark selection method that exploits information coming from multiple, possibly weak and correlated, landmark utility predictors; fused using learned feature coefficients. Such a selection yields a drastic reduction in data transfer while maintaining localization performance and the ability to efficiently summarize environments over time. We evaluate our approach on a data set that was autonomously collected in a dynamic indoor environment over a period of several months.},
 author = {Dymczyk, Marcin and Schneider, Thomas and Gilitschenski, Igor and Siegwart, Roland and Stumm, Elena},
 booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2016.7759673},
 keywords = {},
 pages = {4572-4579},
 title = {Erasing Bad Memories: Agent-Side Summarization for Long-Term Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1093716058},
 year = {2016}
}

@inproceedings{pub.1093779970,
 abstract = {We demonstrate distributed, online, and real-time cooperative localization and mapping between multiple robots operating throughout an unknown environment using indirect measurements. We present a novel Expectation Maximization (EM) based approach to efficiently identify inlier multi-robot loop closures by incorporating robot pose uncertainty, which significantly improves the trajectory accuracy over long-term navigation. An EM and hypothesis based method is used to determine a common reference frame. We detail a 2D laser scan correspondence method to form robust correspondences between laser scans shared amongst robots. The implementation is experimentally validated using teams of aerial vehicles, and analyzed to determine its accuracy, computational efficiency, scalability to many robots, and robustness to varying environ-ments. We demonstrate through multiple experiments that our method can efficiently build maps of large indoor and outdoor environments in a distributed, online, and real-time setting.},
 author = {Dong, Jing and Nelson, Erik and Indelman, Vadim and Michael, Nathan and Dellaert, Frank},
 booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2015.7140012},
 keywords = {},
 pages = {5807-5814},
 title = {Distributed Real-Time Cooperative Localization and Mapping Using an Uncertainty-Aware Expectation Maximization Approach},
 url = {https://app.dimensions.ai/details/publication/pub.1093779970},
 year = {2015}
}

@inproceedings{pub.1093786134,
 abstract = {New applications of mobile robotics in dynamic urban areas require more than the single-session geometric maps that have dominated simultaneous localization and mapping (SLAM) research to date; maps must be updated as the environment changes and include a semantic layer (such as road network information) to aid motion planning in dynamic environments. We present an algorithm for long-term localization and mapping in real time using a three-dimensional (3D) laser scanner. The system infers the static or dynamic state of each 3D point in the environment based on repeated observations. The velocity of each dynamic point is estimated without requiring object models or explicit clustering of the points. At any time, the system is able to produce a most-likely representation of underlying static scene geometry. By storing the time history of velocities, we can infer the dominant motion paerns within the map. The result is an online mapping and localization system specifically designed to enable long-term autonomy within highly dynamic environments. We validate the approach using data collected around the campus of ETH Zurich over seven months and several kilometers of navigation. To the best of our knowledge, this is the first work to unify long-term map update with tracking of dynamic objects. Index Terms-Long-term mapping, dynamic obstacles, ICP, kd-tree, registration, scan matching, robot, SLAM.},
 author = {Pomerleau, FranÃ§ois and KrÃ¼si, Philipp and Colas, Francis and Furgale, Paul and Siegwart, Roland},
 booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2014.6907397},
 keywords = {},
 note = {https://hal.archives-ouvertes.fr/hal-01143106/file/2014_Pomerleau_ICRA_Long-term.pdf},
 pages = {3712-3719},
 title = {Long-term 3D map maintenance in dynamic environments},
 url = {https://app.dimensions.ai/details/publication/pub.1093786134},
 year = {2014}
}

@inproceedings{pub.1093790310,
 abstract = {Robotic systems typically require memory recall mechanisms for a variety of tasks including localization, mapping, planning, visualization etc. We argue for a novel memory recall framework that enables more complex inference schemas by separating the computation from its associated data. In this work we propose a shared, centralized data persistence layer that maintains an ensemble of online, situationally-aware robot states. This is realized through a queryable graph-database with an accompanying key-value store for larger data. In turn, this approach is scalable and enables a multitude of capabilities such as experience-based learning and long-term autonomy. Using multi-modal simultaneous localization and mapping and a few example use-cases, we demonstrate the versatility and extensible nature that centralized persistence and SLAMinDB can provide. In order to support the notion of life-long autonomy, we envision robots to be endowed with such a persistence model, enabling them to revisit previous experiences and improve upon their existing task-specific capabilities.},
 author = {Fourie, Dehann and Claassens, Samuel and Pillai, Sudeep and Mata, Roxana and Leonard, John},
 booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2017.7989749},
 keywords = {},
 pages = {6331-6337},
 title = {SLAMinDB: Centralized Graph Databases for Mobile Robotics},
 url = {https://app.dimensions.ai/details/publication/pub.1093790310},
 year = {2017}
}

@inproceedings{pub.1093820994,
 abstract = {One of the main issues in mobile robotics is the autonomous navigation of a mobile robot in an unknown environment. Concurrent mapping and localisation or simultaneous localisation and mapping is a stochastic map building method which permits consistent robot navigation without requiring an a priori map. The governing idea which guides autonomous robotics consists in saying that the vehicle builds its chart progressively during exploration enabling it to evolve in the long term in unknown places in advance. When the robot environment chart is not known a priori, a generation module of incremental chart must obligatorily be integrated into the navigation system. The map is built incrementally as the robot observes the environment with its on-board sensors and, at the same time, is used to localise the robot. Unfortunately, the inaccuracy of the odometric sensors does not allow a sufficiently correct positioning of the robot. In this paper, simultaneous localisation and map building is performed with a metric approach which permits both precision and robustness. The most important innovation of the approach is the way how errors in the robot localisation control are handled by map building using the landmarks localisation information. The method uses data from a laser scanner to extract distances and orientations of landmarks and combines control localisation and metric paradigm. The metric approach, based on the Kalman filter, uses a new concept to avoid the problem of the drift in odometry. The simulation section will validate the maps representation approach and presents different aspect of environments.},
 author = {SLIMANE, Noureddine and KHIREDDINE, Mohamed Salah and CHAFAA, Kheireddine},
 booktitle = {2013 International Conference on Control, Decision and Information Technologies (CoDIT)},
 doi = {10.1109/codit.2013.6689619},
 keywords = {},
 pages = {647-652},
 title = {A metric approach for Environments mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1093820994},
 year = {2013}
}

@inproceedings{pub.1093901675,
 abstract = {Recently, there has been significant progress towards lifelong, autonomous operation of mobile robots, especially in the field of localization and mapping. One important challenge in this context is visual localization under substantial perceptual changes, for example, coming from different seasons. In this paper, we present an approach to localize a mobile robot with a low frequency camera with respect to an image sequence, recorded previously within a different season. Our approach uses a discrete Bayes filter and a sensor model based on whole image descriptors. Thereby it exploits sequential information to model the dynamics of the system. Since we compute a probability distribution over the whole state space, our approach can handle more complex trajectories that may include same season loop-closures as well as fragmented sub-sequences. Throughout an extensive experimental evaluation on challenging datasets, we demonstrate that our approach outperforms state-of-the-art techniques.},
 author = {Naseer, Tayyab and Suger, Benjamin and Ruhnke, Michael and Burgard, Wolfram},
 booktitle = {2015 European Conference on Mobile Robots (ECMR)},
 doi = {10.1109/ecmr.2015.7324181},
 keywords = {},
 pages = {1-6},
 title = {Vision-Based Markov Localization Across Large Perceptual Changes},
 url = {https://app.dimensions.ai/details/publication/pub.1093901675},
 year = {2015}
}

@inproceedings{pub.1093921673,
 abstract = {Visual information is a valuable asset in any perception scheme designed for an intelligent transportation system. In this regard, the camera-based recognition of locations provides a higher situational awareness of the environment, which is very useful for varied localization solutions typically needed in long-term autonomous navigation, such as loop closure detection and visual odometry or SLAM correction. In this paper we present OpenABLE, an open-source toolbox contributed to the community with the aim of helping researchers in the application of these kinds of life-long localization algorithms. The implementation follows the philosophy of the topological place recognition method named ABLE, including several new features and improvements. These functionalities allow to match locations using different global image description methods and several configuration options, which enable the users to control varied parameters in order to improve the performance of place recognition depending on their specific problem requisites. The applicability of our toolbox in visual localization purposes for intelligent vehicles is validated in the presented results, jointly with comparisons to the main state-of-the-art methods.},
 author = {Arroyo, Roberto and Alcantarilla, Pablo F. and Bergasa, Luis M. and Romera, Eduardo},
 booktitle = {2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)},
 doi = {10.1109/itsc.2016.7795672},
 keywords = {},
 pages = {965-970},
 title = {OpenABLE: An open-source toolbox for application in life-long visual localization of autonomous vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1093921673},
 year = {2016}
}

@inproceedings{pub.1093986218,
 abstract = {This paper is about life-long vast-scale localisation in spite of changes in weather, lighting and scene structure. Building upon our previous work in Experience-based Navigation [1], we continually grow and curate a visual map of the world that explicitly supports multiple representations of the same place. We refer to these representations as experiences, where a single experience captures the appearance of an environment under certain conditions. Pedagogically, an experience can be thought of as a visual memory. By accumulating experiences we are able to handle cyclic appearance change (diurnal lighting, seasonal changes, and extreme weather conditions) and also adapt to slow structural change. This strategy, although elegant and effective, poses a new challenge: In a region with many stored representations - which one(s) should we try to localise against given finite computational resources? By learning from our previous use of the experience-map, we can make predictions about which memories we should consider next, conditioned on how the robot is currently localised in the experience-map. During localisation, we prioritise the loading of past experiences in order to minimise the expected computation required. We do this in a probabilistic way and show that this memory policy significantly improves localisation efficiency, enabling long-term autonomy on robots with limited computational resources. We demonstrate and evaluate our system over three challenging datasets, totalling 206km of outdoor travel. We demonstrate the system in a diverse range of lighting and weather conditions, scene clutter, camera occlusions, and permanent structural change in the environment.},
 author = {Linegar, Chris and Churchill, Winston and Newman, Paul},
 booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2015.7138985},
 keywords = {},
 pages = {90-97},
 title = {Work Smart, Not Hard: Recalling Relevant Experiences for Vast-Scale but Time-Constrained Localisation},
 url = {https://app.dimensions.ai/details/publication/pub.1093986218},
 year = {2015}
}

@inproceedings{pub.1094026240,
 abstract = {We propose a discriminative and compact scene descriptor for single-view place recognition that facilitates long-term visual SLAM in familiar, semi-dynamic and partially changing environments. In contrast to popular bag-of-words scene descriptors, which rely on a library of vector quantized visual features, our proposed scene descriptor is based on a library of raw image data (such as an available visual experience, images shared by other colleague robots, and publicly available image data on the web) and directly mine it to find visual phrases (VPs) that discriminatively and compactly explain an input query/database image. Our mining approach is motivated by recent success in the field of common pattern discoveryâ€”specifically mining of common visual patterns among scenesâ€”and requires only a single library of raw images that can be acquired at different time or day. Experimental results show that even though our scene descriptor is significantly more compact than conventional descriptors it has a relatively higher recognition performance.},
 author = {Kanji, Tanaka and Yuuto, Chokushi and Masatoshi, Ando},
 booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2014.6942552},
 keywords = {},
 pages = {136-142},
 title = {Mining Visual Phrases for Long-Term Visual SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1094026240},
 year = {2014}
}

@inproceedings{pub.1094030759,
 abstract = {Life-long visual localization is one of the most challenging topics in robotics over the last few years. The difficulty of this task is in the strong appearance changes that a place suffers due to dynamic elements, illumination, weather or seasons. In this paper, we propose a novel method (ABLE-M) to cope with the main problems of carrying out a robust visual topological localization along time. The novelty of our approach resides in the description of sequences of monocular images as binary codes, which are extracted from a global LDB descriptor and efficiently matched using FLANN for fast nearest neighbor search. Besides, an illumination invariant technique is applied. The usage of the proposed binary description and matching method provides a reduction of memory and computational costs, which is necessary for long-term performance. Our proposal is evaluated in different life-long navigation scenarios, where ABLE-M outperforms some of the main state-of-the-art algorithms, such as WI-SURF, BRIEF-Gist, FAB-MAP or SeqSLAM. Tests are presented for four public datasets where a same route is traversed at different times of day or night, along the months or across all four seasons.},
 author = {Arrayo, Roberto and Alcantarilla, Pablo F. and Bergasa, Luis M. and Romera, Eduardo},
 booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2015.7140088},
 keywords = {},
 note = {http://www.robesafe.com/personal/roberto.arroyo/docs/Arroyo15icra.pdf},
 pages = {6328-6335},
 title = {Towards Life-Long Visual Localization Using an Efficient Matching of Binary Sequences from Images},
 url = {https://app.dimensions.ai/details/publication/pub.1094030759},
 year = {2015}
}

@inproceedings{pub.1094034914,
 abstract = {As robots continue to create long-term maps, the amount of information that they need to handle increases over time. In terms of place recognition, this implies that the number of images being considered may increase until exceeding the computational resources of the robot. In this paper we consider a scenario where, given multiple independent large maps, possibly from different cities or locations, a robot must effectively and in real time decide whether it can localize itself in one of those known maps. Since the number of images to be handled by such a system is likely to be extremely large, we find that it is beneficial to decompose the set of images into independent groups or environments. This raises a new question: Given a query image, how do we select the best environment? This paper proposes a similarity criterion that can be used to solve this problem. It is based on the observation that, if each environment is described in terms of its co-occurrent features, similarity between environments can be established by comparing their co-occurrence matrices. We show that this leads to a novel place recognition algorithm that divides the collection of images into environments and arranges them in a hierarchy of inverted indices. By selecting first the relevant environment for the operating robot, we can reduce the number of images to perform the actual loop detection, reducing the execution time while preserving the accuracy. The practicality of this approach is shown through experimental results on several large datasets covering a combined distance of more than 750Km.},
 author = {Mohan, Mahesh and GÃ¡lvez-LÃ³pez, Dorian and Monteleoni, Claire and Sibley, Gabe},
 booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2015.7139966},
 keywords = {},
 pages = {5487-5494},
 title = {Environment Selection and Hierarchical Place Recognition},
 url = {https://app.dimensions.ai/details/publication/pub.1094034914},
 year = {2015}
}

@inproceedings{pub.1094064764,
 abstract = {Continuous localization is a technique that allows a robot to maintain an accurate estimate of its location by performing regular small corrections to its odometry. Continuous localization uses an evidence grid representation, a common representation scheme that is used by other map-dependent processes, such as path planning. Although techniques exist for building evidence grid maps, most are not adaptive to changes in the environment. In this research, we extend the continuous localization technique by adding a learning component. This allows continuous localization to update the long-term map (evidence grid) with current sensor readings. Results show that the addition of the learning behavior to continuous localization allows the system to adapt to changes in its environment without a loss in its ability to remain localized. This system was tested on a Nomad 200 mobile robot.},
 author = {Graves, K. and Adams, W. and Schultz, A.},
 booktitle = {Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation'},
 doi = {10.1109/cira.1997.613834},
 keywords = {},
 pages = {28-33},
 title = {Continuous localization in changing environments},
 url = {https://app.dimensions.ai/details/publication/pub.1094064764},
 year = {1997}
}

@inproceedings{pub.1094083494,
 abstract = {The intelligent application of a mobile robot, outside the experimental laboratory, requires a robust locomotive strategy that is rarely conducive to stringent kinematic modeling. Localisation methods that rely upon such modeling often fail, as model boundaries succumb to unpredictable events. This paper presents the development of a self-contained localisation system that purposely obviates the need for odometric information, and an associated kinematic model, to provide robot anonymity. Without odometry, the system is oblivious to the non-systematic vagaries of the robotic platform interacting with a natural domain. The proposed system hypothesises about the robot's absolute pose by algorithmically solving the kidnapped robot problem using exteroceptive based perception. Since no a priori information is assumed, long-term pose fixes are derived within a simultaneous localisation and mapping (SLAM) framework. Preliminary results were gathered using a skid steering mobile robot, equipped with a scanning laser range-finder, in an outdoor environment. This novel localisation approach was found to be efficient and robust, while exhibiting the capacity for widespread applicability.},
 author = {Spero, Dorian J. and Jarvis, Ray A.},
 booktitle = {IEEE Conference on Robotics, Automation and Mechatronics, 2004.},
 doi = {10.1109/ramech.2004.1438024},
 keywords = {},
 pages = {822-827},
 title = {Towards Exteroceptive Based Localisation},
 url = {https://app.dimensions.ai/details/publication/pub.1094083494},
 year = {2005}
}

@inproceedings{pub.1094111662,
 abstract = {Playbot [1], [13] is a long-term, large-scale research project, whose goal is to provide a vision-based computer controlled wheelchair that enables children and adults with mobility impairments to become more independent. Within this context, we show how Playbot can actively search an indoor environment to localize a door, approach the door, use a mounted robotic arm to open the door, and go through the door, using exclusively vision-based sensors and without using a map of the environment. We demonstrate the effectiveness of active vision for localizing objects that are too large to fall within a single camera's field of view and show that well-calibrated vision-based sensors are sufficient to safely pass through a door frame that is narrow enough to tolerate a wheelchair localization error of at most a few centimetres. We provide experimental results demonstrating near perfect performance in an indoor environment.},
 author = {Andreopoulos, Alexander and Tsotsos, John K.},
 booktitle = {2008 Canadian Conference on Computer and Robot Vision},
 doi = {10.1109/crv.2008.23},
 keywords = {},
 pages = {3-10},
 title = {Active Vision for Door Localization and Door Opening using Playbot: A Computer Controlled Wheelchair for People with Mobility Impairments},
 url = {https://app.dimensions.ai/details/publication/pub.1094111662},
 year = {2008}
}

@inproceedings{pub.1094132896,
 abstract = {This paper describes a an SLAM algorithm for the navigation for an indoor autonomous mobile robot. The main emphasis of this paper is on the ability of line extraction. A recognition method based on straight line extraction is proposed for extracting the key features on the office ceiling, in an effort to estimate the pose of mobile robot. Random Sample Consensus (RANSAC) paradigm is used to group the line segments. During the navigation, onboard odometry is used at the beginning stage to estimate the information of environment for visual reckoning, while lamps on the ceiling act as beacons for positioning to eliminate accumulation of errors after a long-term run. The data captured from infrared sensors is used for constructing a map. The proposed method scales well with respect to the size of the input image and the number and size of the shapes within the data. Moreover the algorithm is conceptually simple and easy to implement. Simulation and experimental results show that good recognition and localization can be achieved using the proposed method, allowing for the interested region correspondence matching and mapping between images from different sensors or the same sensor indifferent time phrase.},
 author = {Fu, Siyao and Yang, Guosheng},
 booktitle = {2009 International Conference on Networking, Sensing and Control},
 doi = {10.1109/icnsc.2009.4919356},
 keywords = {},
 pages = {663-668},
 title = {Uncalibrated Monocular based Simultaneous Localization and Mapping for Indoor Autonomous Mobile Robot Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1094132896},
 year = {2009}
}

@inproceedings{pub.1094215402,
 abstract = {For large-scale and long-term simultaneous localization and mapping (SLAM), a robot has to deal with unknown initial positioning caused by either the kidnapped robot problem or multi-session mapping. This paper addresses these problems by tying the SLAM system with a global loop closure detection approach, which intrinsically handles these situations. However, online processing for global loop closure detection approaches is generally influenced by the size of the environment. The proposed graph-based SLAM system uses a memory management approach that only consider portions of the map to satisfy online processing requirements. The approach is tested and demonstrated using five indoor mapping sessions of a building using a robot equipped with a laser rangefinder and a Kinect.},
 author = {LabbÃ©, Mathieu and Michaud, FranÃ§ois},
 booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2014.6942926},
 keywords = {},
 note = {https://introlab.3it.usherbrooke.ca/mediawiki-introlab/images/e/eb/Labbe14-IROS.pdf},
 pages = {2661-2666},
 title = {Online Global Loop Closure Detection for Large-Scale Multi-Session Graph-Based SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1094215402},
 year = {2014}
}

@inproceedings{pub.1094240669,
 abstract = {Simultaneous Localization and Mapping (SLAM) is a fundamental component of all autonomous robotics systems, which probabilisticaly fuses information from an exteroceptive sensor and a proprioceptive sensor to simultaneously estimate the robot's trajectory and the map. Inputs from the proprioceptive sensor are fed into the estimation algorithm via a process model corresponding with the vehicle kinematics, while a measurement model is used to process inputs from the exteroceptive sensor. Most SLAM algorithms assume known, fixed model estimate bias. This assumption does not hold true for systems with wrongly modeled estimate bias, or those affected by component fatigue due to applications requiring long term autonomy. This paper will display the adverse effects of mismodeled process model bias using a simulation. An adaptive algorithm employing Adaptive Gaussian Particle Filter based process model bias compensation will be deployed in tandem with a particle filter based FastSLAM algorithm. The algorithm will be compared favourably with existing state of the art SLAM algorithms in controlled simulations. Experimental data from a marine environment will be used to validate the efficacy of the algorithm.},
 author = {Rao, Akshay and Han, Wang},
 booktitle = {2015 IEEE 7th International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)},
 doi = {10.1109/iccis.2015.7274622},
 keywords = {},
 pages = {210-215},
 title = {An Adaptive Gaussian Particle Filter Based Simultaneous Localization and Mapping with Dynamic Process Model Noise Bias Compensation},
 url = {https://app.dimensions.ai/details/publication/pub.1094240669},
 year = {2015}
}

@inproceedings{pub.1094286969,
 abstract = {We present a lifelong mapping and localisation system for long-term autonomous operation of mobile robots in changing environments. The core of the system is a spatiotemporal occupancy grid that explicitly represents the persistence and periodicity of the individual cells and can predict the probability of their occupancy in the future. During navigation, our robot builds temporally local maps and integrates then into the global spatio-temporal grid. Through re-observation of the same locations, the spatio-temporal grid learns the long-term environment dynamics and gains the ability to predict the future environment states. This predictive ability allows to generate time-specific 2d maps used by the robot's localisation and planning modules. By analysing data from a long-term deployment of the robot in a human-populated environment, we show that the proposed representation improves localisation accuracy and the efficiency of path planning. We also show how to integrate the method into the ROS navigation stack for use by other roboticists.},
 author = {KrajnÃ­k, TomÃ¡Å¡ and Fentanes, Jaime Pulido and Hanheide, Marc and Duckett, Tom},
 booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2016.7759671},
 keywords = {},
 note = {http://eprints.lincoln.ac.uk/id/eprint/24088/1/camera_ready.pdf},
 pages = {4558-4563},
 title = {Persistent Localization and Life-Long Mapping in Changing Environments Using the Frequency Map Enhancement},
 url = {https://app.dimensions.ai/details/publication/pub.1094286969},
 year = {2016}
}

@inproceedings{pub.1094310172,
 abstract = {The paper describes progress achieved in our longterm research project SHopBoT, which aims at the development of an intelligent and interactive mobile shopping assistant for everyday use in shopping centers or home improvement stores. It is focusing on recent progress concerning two important methodological aspects: (i) the on-line building of maps of the operation area by means of advanced Rao-Blackwellized SLAM approaches using both sonar-based gridmaps as well as vision-based graph maps as representations, and (ii) a probabilistic approach to multi-modal user detection and tracking during the guidance tour. Experimental results of both the map building characteristics and the person tracking behavior achieved in an ordinary home improvement store demonstrate the reliability of both approaches. Moreover, we present first very encouraging results of long-term field trials which have been executed with three robotic shopping assistants in another home improvement store in Bavaria since March 2008. In this field test, the robots could demonstrate their suitability for this challenging real-world application, as well as the necessary user acceptance.1},
 author = {Gross, H.-M. and Boehme, H.-J. and Schroeter, C. and Mueller, S. and Koenig, A. and Martin, Ch. and Merten, M. and Bley, A.},
 booktitle = {2008 IEEE International Conference on Systems, Man and Cybernetics},
 doi = {10.1109/icsmc.2008.4811835},
 keywords = {},
 pages = {3471-3478},
 title = {ShopBot: Progress in Developing an Interactive Mobile Shopping Assistant for Everyday Use},
 url = {https://app.dimensions.ai/details/publication/pub.1094310172},
 year = {2008}
}

@inproceedings{pub.1094348027,
 abstract = {To effectively examine ocean processes we must often sample over the duration of long (weeks to months) oscillation patterns. Such sampling requires persistent autonomous underwater vehicles, that have a similarly long deployment duration. Actively actuated (propeller-driven) underwater vehicles have proven effective in multiple sampling scenarios, however they have limited deployment endurance. The emergence of less actuated vehicles, i.e., underwater gliders, has enabled greater energy savings and thus increased endurance. Due to reduced actuation, these vehicles are more susceptible to external forces, e.g., ocean currents, causing them to have poor navigational and localization accuracy underwater. This is exacerbated in coastal regions, where current velocities are the same order of magnitude as the vehicle velocity. In this paper, we examine a method of reducing navigation and localization error, not only for navigation, but more so for more accurately reconstructing the path that the glider traversed to contextualize the gathered data, with respect to the science question at hand. We present a set of algorithms for offline processing that accurately localizes the traversed path of an underwater glider over long-term, ocean deployments. The proposed method utilizes terrain-based navigation with only depth, altimeter and compass data compared to local bathymetry maps to provide accurate reconstructions of traversed paths in the ocean.},
 author = {Stuntz, Andrew and Liebel, David and Smith, Ryan N.},
 booktitle = {OCEANS 2015 - Genova},
 doi = {10.1109/oceans-genova.2015.7271751},
 keywords = {},
 pages = {1-10},
 title = {Enabling Persistent Autonomy for Underwater Gliders Through Terrain Based Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1094348027},
 year = {2015}
}

@inproceedings{pub.1094363270,
 abstract = {Robust scalable place recognition is a core competency for many robotic applications. However, when revisiting places over and over, many state-of-the-art approaches exhibit reduced performance in terms of computation and memory complexity and in terms of accuracy. For successful deployment of robots over long time scales, we must develop algorithms that get better with repeated visits to the same environment, while still working within a fixed computational budget. This paper presents and evaluates an algorithm that alternates between online place recognition and offline map maintenance with the goal of producing the best performance with a fixed map size. At the core of the algorithm is the concept of a Summary Map, a reduced map representation that includes only the landmarks that are deemed most useful for place recognition. To assign landmarks to the map, we use a scoring function that ranks the utility of each landmark and a sampling policy that selects the landmarks for each place. The Summary Map can then be used by any descriptor-based inference method for constant-complexity online place recognition. We evaluate a number of scoring functions and sampling policies and show that it is possible to build and maintain maps of a constant size and that place-recognition performance improves over multiple visits.},
 author = {Dymczyk, Marcin and Lynen, Simon and Cioslewski, Titus and Bosse, Michael and Roland, Siegwart and Furgale, Paul},
 booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2015.7139575},
 keywords = {},
 pages = {2767-2773},
 title = {The Gist of Maps - Summarizing Experience for Lifelong Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1094363270},
 year = {2015}
}

@inproceedings{pub.1094367488,
 abstract = {This paper is about underpinning longterm operations of fleets of vehicles using visual localisation. In particular it examines ways in which vehicles, considered as independent agents, can share, update and leverage each others' visual experiences in a mutually beneficial way. We draw on our previous work in Experience-based Navigation (EBN) [1], in which a visual map supporting multiple representations of the same place is built, yielding real-time localisation capability for a solitary vehicle. We now consider how any number of such agents might operate in concert via data sharing policies that are germane to the shared task of lifelong localisation. We rapidly construct considerable maps by the conjoining of work distributed to asynchronous processes, and share expertise amongst the team by the selective dispensing of mission-specific map contents. We demonstrate and evaluate our system against 100km of data collected in North Oxford over a period of a month featuring diverse deviation in appearance due to atmospheric, lighting, and structural dynamics. We show that our framework is capable of creating maps in a fraction of the time required by single-agent EBN, with no significant loss in localisation robustness, and is able to furnish robots on realworld forays with maps which require much less storage.},
 author = {Gadd, Matthew and Newman, Paul},
 booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2016.7759843},
 keywords = {},
 note = {https://ora.ox.ac.uk/objects/uuid:7b7d68fa-5941-4932-9d30-d10746aa1e15/download_file?safe_filename=Checkout%2BMy%2BMap-%2BVersion%2BControl%2Bfor%2BFleetwide%2BVisual%2BLocalisation.pdf&file_format=application%2Fpdf&type_of_work=Conference+item},
 pages = {5729-5736},
 title = {Checkout My Map: Version Control for Fleetwide Visual Localisation},
 url = {https://app.dimensions.ai/details/publication/pub.1094367488},
 year = {2016}
}

@inproceedings{pub.1094389109,
 abstract = {In graph-based SLAM, the pose graph encodes the poses of the robot during data acquisition as well as spatial constraints between them. The size of the pose graph has a substantial influence on the runtime and the memory requirements of a SLAM system, which hinders long-term mapping. In this paper, we address the problem of efficient information-theoretic compression of pose graphs. Our approach estimates the expected information gain of laser measurements with respect to the resulting occupancy grid map. It allows for restricting the size of the pose graph depending on the information that the robot acquires about the environment or based on a given memory limit, which results in an any-space SLAM system. When discarding laser scans, our approach marginalizes out the corresponding pose nodes from the graph. To avoid a densely connected pose graph, which would result from exact marginalization, we propose an approximation to marginalization that is based on local Chow-Liu trees and maintains a sparse graph. Real world experiments suggest that our approach effectively reduces the growth of the pose graph while minimizing the loss of information in the resulting grid map.},
 author = {Kretzschmar, Henrik and Stachniss, Cyrill and Grisetti, Giorgio},
 booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2011.6094414},
 keywords = {},
 note = {http://www.informatik.uni-freiburg.de/%7Ekretzsch/pdf/kretzschmar11iros.pdf},
 pages = {1-7},
 title = {Efficient Information-Theoretic Graph Pruning for Graph-Based SLAM with Laser Range Finders},
 url = {https://app.dimensions.ai/details/publication/pub.1094389109},
 year = {2011}
}

@inproceedings{pub.1094426212,
 abstract = {This paper is an invitation to use mono-vision techniques on stereo-vision equipped robots. By using monocular algorithms on both cameras, the advantages of mono-vision (bearing-only, with infinity range but no 3D instant information) and stereo-vision (3D information only up to a limited range) naturally add up to provide interesting possibilities, that are here developed and demonstrated using an EKF-based monocular SLAM algorithm. Mainly we obtain: a) fast 3D mapping with long term, absolute angular references; b) great landmark updating flexibility; and c) the possibility of stereo rig extrinsic self-calibration, providing a much more robust and accurate sensor. Experimental results show the pertinence of the proposed ideas, which should be easily exportable (and we encourage to do so) to other, more performing, vision-based SLAM algorithms.},
 author = {SolÃ , Joan and Monin, AndrÃ© and Devy, Michel},
 booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2007.364218},
 keywords = {},
 pages = {4795-4800},
 title = {BiCamSLAM: Two times mono is more than stereo},
 url = {https://app.dimensions.ai/details/publication/pub.1094426212},
 year = {2007}
}

@inproceedings{pub.1094487364,
 abstract = {Migration birds are able to navigate themselves during a long-distance journey without getting lost. They actually achieve just what is being sought for in the field of Unmanned Aerial Vehicles (UAVs): long-term autonomous navigation. This paper proposes an approach that combines the migration birdsâ€™ sense principles with Micro-Electro-Mechanical System (MEMS) sensors to estimate UAVs position within GPS-denied environments. Camera, orientation and web-based maps (such as Google/Baidu Maps) are chosen to simulate the birdsâ€™ localization cues: vision, earth magnetic field and mental maps. The visual odometry, Particle Filter theories are used in the proposed approach to integrate multiple sensor measurements. Real flying experiments are conducted both in indoor and outdoor environments. The results validate that the proposed migration-inspired visual odometry system can estimate the UAV localization effectively.},
 author = {Zhang, Yu and Chao, Ainong and Zhao, Boxin and Liu, Huawei and Zhao, Xiaolin},
 booktitle = {2016 IEEE International Conference on Information and Automation (ICIA)},
 doi = {10.1109/icinfa.2016.7831835},
 keywords = {},
 pages = {276-281},
 title = {Migratory Birds-Inspired Navigation System for Unmanned Aerial Vehicles* *This work is partially supported by National Natural Science Foundation of China #61503405},
 url = {https://app.dimensions.ai/details/publication/pub.1094487364},
 year = {2016}
}

@inproceedings{pub.1094527049,
 abstract = {Within the context of Simultaneous Localisation and Mapping (SLAM), â€œloop closingâ€ is the task of deciding whether or not a vehicle has, after an excursion of arbitrary length, returned to a previously visited area. Reliable loop closing is both essential and hard. It is without doubt one of the greatest impediments to long term, robust SLAM. This paper illustrates how visual features, used in conjunction with scanning laser data, can be used to a great advantage. We use the notion of visual saliency to focus the selection of suitable (affine invariant) image-feature descriptors for storage in a database. When queried with a recently taken image the database returns the capture time of matching images. This time information is used to discover loop closing events. Crucially this is achieved independently of estimated map and vehicle location. We integrate the above technique into a SLAM algorithm using delayed vehicle states and scan matching to form interpose geometric constraints. We present initial results using this system to close loops (around 100m) in an indoor environment.},
 author = {Newman, Paul and Ho, Kin},
 booktitle = {Proceedings of the 2005 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2005.1570189},
 keywords = {},
 pages = {635-642},
 title = {Slam-Loop Closing with Visually Salient Features},
 url = {https://app.dimensions.ai/details/publication/pub.1094527049},
 year = {2005}
}

@inproceedings{pub.1094543172,
 abstract = {This work considers a mobile service robot which uses an appearance-based representation of its workplace as a map, where the current view and the map are used to estimate the current position in the environment. Due to the nature of real-world environments such as houses and offices, where the appearance keeps changing, the internal representation may become out of date after some time. To solve this problem the robot needs to be able to adapt its internal representation continually to the changes in the environment. This paper presents a method for creating an adaptive map for long-term appearance-based localization of a mobile robot using longterm and short-term memory concepts, with omni-directional vision as the external sensor.},
 author = {Dayoub, Feras and Duckett, Tom},
 booktitle = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2008.4650701},
 keywords = {},
 note = {http://eprints.lincoln.ac.uk/id/eprint/1679/1/IROS08.pdf},
 pages = {3364-3369},
 title = {An Adaptive Appearance-based Map for Long-Term Topological Localization of Mobile Robots},
 url = {https://app.dimensions.ai/details/publication/pub.1094543172},
 year = {2008}
}

@inproceedings{pub.1094572980,
 abstract = {SLAM (Simultaneous Localization and Mapping) mechanisms are a key component towards advanced service robotics applications. Currently, a major hurdle on the way to lifelong localization is the handling of the ever growing amount of landmarks over time. Therefore, the required resources in terms of memory and processing power are also growing over time. An approach to restrict the absolute number of landmarks by an upper bound was presented in [1]. The key was a method to specifically select and replace landmarks once an upper bound has been reached. In this paper, we extend that landmark rating and selection approach. The here presented extension improves the landmark rating and selection process. Landmarks are kept such that their visibility regions better approximate the robot's operational area. A landmark with a low information content in a sparsely known region is often more useful than a landmark with a higher information content in a well-known region. Clustering algorithms are used to identify regions in the environment with a high landmark density. Removing a landmark from a cluster with high localization support will have the smallest degradation of robot localization quality. Real-world experiments are used to demonstrate the performance of our approach. These experiments are performed on a P3DX-platform with a bearing-only SLAM approach. All three approaches of handling landmarks (the standard approach without upper bound on the number of landmarks, the improved and the previous landmark rating and selection process) are compared against each other.},
 author = {Hochdorfer, Siegfried and Lutz, Matthias and Schlegel, Christian},
 booktitle = {2009 IEEE International Conference on Technologies for Practical Robot Applications},
 doi = {10.1109/tepra.2009.5339626},
 keywords = {},
 pages = {161-166},
 title = {Lifelong Localization of a Mobile Service-Robot in Everyday Indoor Environments Using Omnidirectional Vision},
 url = {https://app.dimensions.ai/details/publication/pub.1094572980},
 year = {2009}
}

@inproceedings{pub.1094609648,
 abstract = {Extended missions in unknown regions present a significant navigational challenge for autonomous underwater vehicles (AUV). This paper investigates the long-term performance of a concurrent mapping and localization (CML) algorithm for the scenario of an AUV making observations of point features in the environment with a forward look sonar. Simulation results demonstrate that position estimates with long-term bounded errors of a few meters can be achieved under realistic assumptions about the vehicle, its sensors, and the environment. Potential failure modes of the algorithm, such as divergence and map slip, are discussed. CML technology can provide a significant improvement in the navigational capabilities of AUVs and can enable new missions in unmapped regions without reliance on acoustic beacons or surfacing for GPS resets.},
 author = {Feder, H.J.S. and Leonard, J.J. and Smith, C.M.},
 booktitle = {Proceedings of the 1998 Workshop on Autonomous Underwater Vehicles (Cat. No.98CH36290)},
 doi = {10.1109/auv.1998.744447},
 keywords = {},
 pages = {115-122},
 title = {Incorporating environmental measurements in navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1094609648},
 year = {1998}
}

@inproceedings{pub.1094628018,
 abstract = {This paper is concerned with the problem of mobile robot localization using a novel compact representation of visual landmarks. With recent progress in lifelong map-learning as well as in information sharing networks, compact representation of a large-size landmark database has become crucial. In this paper, we propose a compact binary code (e.g. 32bit code) landmark representation by employing the semantic hashing technique from web-scale image retrieval. We show how well such a binary representation achieves compactness of a landmark database while maintaining efficiency of the localization system. In our contribution, we investigate the cost-performance, the semantic gap, the saliency evaluation using the presented techniques as well as challenge to further reduce the resources (#bits) per landmark. Experiments using a high-speed car-like robot show promising results.},
 author = {Ikeda, Kouichirou and Tanaka, Kanji},
 booktitle = {2010 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2010.5509579},
 keywords = {},
 pages = {4397-4403},
 title = {Visual Robot Localization Using Compact Binary Landmarks},
 url = {https://app.dimensions.ai/details/publication/pub.1094628018},
 year = {2010}
}

@inproceedings{pub.1094633542,
 abstract = {This paper reports on an algorithm for underwater visual place recognition in the presence of dramatic appearance change. Long-term visual place recognition is challenging underwater due to biofouling, corrosion, and other effects that lead to dramatic visual appearance change, which often causes traditional point-based feature methods to perform poorly. Building upon the authors' earlier work, this paper presents an algorithm for underwater vehicle place recognition and relocalization that enables an autonomous underwater vehicle (AUV) to relocalize itself to a previously-built simultaneous localization and mapping (SLAM) graph. High-level structural features are learned using a supervised learning framework that retains features that have a high potential to persist in the underwater environment. Combined with a particle filtering framework, these features are used to provide a probabilistic representation of localization confidence. The algorithm is evaluated on real data, from multiple years, collected by a Hovering Autonomous Underwater Vehicle (HAUV) for ship hull inspection.},
 author = {Li, Jie and Eustice, Ryan M. and Johnson-Roberson, Matthew},
 booktitle = {OCEANS 2015 - MTS/IEEE Washington},
 doi = {10.23919/oceans.2015.7404369},
 keywords = {},
 note = {http://robots.engin.umich.edu/publications/jli-2015b.pdf},
 pages = {1-6},
 title = {Underwater robot visual place recognition in the presence of dramatic appearance change},
 url = {https://app.dimensions.ai/details/publication/pub.1094633542},
 year = {2015}
}

@inproceedings{pub.1094656989,
 abstract = {One of the greatest challenges nowadays in robotics is the advancement of robots from industrial tools to companions and helpers of humans, operating in natural, populated environments. In this respect, the Autonomous City Explorer (ACE) project aims to combine the research fields of autonomous mobile robot navigation and human robot interaction. A robot has been created that is capable of navigating in an unknown, highly populated, urban environment, based only on information extracted through interaction with passers-by and its local perception capabilities. This paper describes the algorithms and architecture that make up the navigation subsystem of ACE. More specifically, the algorithms used for Simultaneous Localization and Mapping (SLAM), path planning in dynamic environments and behavior selection are presented, as well as the system architecture that integrates them to a complete working system. Results from an extended field experiment, where the robot navigated autonomously through the downtown city area of Munich, are analyzed and show that the robot is capable of long-term, safe navigation in real-world settings.},
 author = {Lidoris, Georgios and RohrmÃ¼ller, Florian and Wollherr, Dirk and Buss, Martin},
 booktitle = {2009 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2009.5152534},
 keywords = {},
 pages = {1416-1422},
 title = {The Autonomous City Explorer (ACE) Project â€“ Mobile Robot Navigation in Highly Populated Urban Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1094656989},
 year = {2009}
}

@inproceedings{pub.1094727900,
 abstract = {This paper reports on a factor graph simultaneous localization and mapping framework for autonomous underwater vehicle localization based on terrain-aided navigation. The method requires no prior bathymetric map and only assumes that the autonomous underwater vehicle has the ability to sparsely sense the local water column depth, such as with a bottom-looking Doppler velocity log. Since dead-reckoned navigation is accurate in short time windows, the vehicle accumulates several water column depth point clouds- or submaps-during the course of its survey. We propose an xy-alignment procedure between these submaps in order to enforce consistent bathymetric structure over time, and therefore attempt to bound long-term navigation drift. We evaluate the submap alignment method in simulation and present performance results from multiple autonomous underwater vehicle field trials.},
 author = {Bichucher, Vittorio and Walls, Jeffrey M. and Ozog, Paul and Skinner, Katherine A. and Eustice, Ryan M.},
 booktitle = {OCEANS 2015 - MTS/IEEE Washington},
 doi = {10.23919/oceans.2015.7404433},
 keywords = {},
 note = {http://robots.engin.umich.edu/publications/vbichucher-2015a.pdf},
 pages = {1-7},
 title = {Bathymetric factor graph SLAM with sparse point cloud alignment},
 url = {https://app.dimensions.ai/details/publication/pub.1094727900},
 year = {2015}
}

@inproceedings{pub.1094728866,
 abstract = {This paper considers the trajectory planning problem for line-feature based SLAM in structured indoor environments. The robot poses and line features are estimated using Smooth and Mapping (SAM) which is found to provide more consistent estimates than the Extended Kalman Filter (EKF). The objective of trajectory planning is to minimise the uncertainty of the estimates and to maximise coverage. Trajectory planning is performed using Model Predictive Control (MPC) with an attractor incorporating long term goals. This planning is demonstrated both in simulation and in a real-time experiment with a Pioneer2DX robot.},
 author = {Leung, Cindy and Huang, Shoudong and Dissanayake, Gamini},
 booktitle = {2008 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2008.4543484},
 keywords = {},
 note = {https://opus.lib.uts.edu.au/bitstream/10453/11075/1/2008002085.pdf},
 pages = {1898-1903},
 title = {Active SLAM in Structured Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1094728866},
 year = {2008}
}

@inproceedings{pub.1094790973,
 abstract = {Ego-motion estimation and 3D scene reconstruction from image data has been a long term aim both in the Robotics and Computer Vision communities. Nevertheless, while both visual SLAM and Structure from Motion already provide an accurate ego-motion estimation, visual scene estimation does not offer yet such a satisfactory result; being in most cases limited to a sparse set of salient points. In this paper we propose an algorithm to densify a sparse point-based reconstruction into a dense multi-plane based one, from the only input of a set of sparse images. The method starts by recovering a sparse set of 3D salient points and uses them to robustly estimate the dominant planes of the scene. The number of planes is not known in advance and there may exist outliers from the planes in the point cloud. In a second step, the image data and the estimated 3D structure are combined to determine which parts of each plane actually belong to the scene exploiting photo consistency and geometrical constraints. Experimental results with real images show that the described approach achieves accurate and dense estimation results in man-made environments. Moreover, the method is able to recover areas without texture, where usually there are no salient points.},
 author = {Argiles, Alberto and Civera, Javier and Montesano, Luis},
 booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2011.6094458},
 keywords = {},
 note = {http://webdiis.unizar.es/~jcivera/papers/argiles_etal_iros11.pdf},
 pages = {4448-4454},
 title = {Dense Multi-Planar Scene Estimation from a Sparse Set of Images},
 url = {https://app.dimensions.ai/details/publication/pub.1094790973},
 year = {2011}
}

@inproceedings{pub.1094870306,
 abstract = {Highly accurate localization of a micro aerial vehicle (MAV) with respect to a scene is important for a wide range of applications, in particular surveillance and inspection. Most existing approaches to visual localization focus on indoor environments, while such tasks require outdoor navigation. Within this work, we introduce a novel algorithm for monocular visual localization for MAVs based on the concept of virtual views in 3D space. Under the assumption that significant parts of the scene do not alter their geometry and serve as natural landmarks, the accuracy of our visual approach outperforms consumer grade GPS systems. In an experimental setup we compare our approach to a state-of-the-art visual SLAM algorithm and evaluate the performance by geometric validation from an observer's view. As our method directly allows global registration, it is neither prone to drift nor bias. This makes it well suited for long-term autonomous navigation.},
 author = {Wendel, Andreas and Irschara, Arnold and Bischof, Horst},
 booktitle = {2011 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/icra.2011.5980317},
 keywords = {},
 pages = {5792-5799},
 title = {Natural Landmark-based Monocular Localization for MAVs},
 url = {https://app.dimensions.ai/details/publication/pub.1094870306},
 year = {2011}
}

@inproceedings{pub.1094955754,
 abstract = {Proper place recognition on an environment that can change over time is fundamental for longterm SLAM. In such scenarios the observations obtained in the same region can drastically differ due to changes caused by semi-static objects, such as doors, furniture, etc. In this work, we extend a strategy that represents environment regions using words, based on spatial density information extracted from laser readings. This time, in order to deal with changes in the environment, our method not only builds words representing the real observations made by the robot, but also alternative multi-level words to account for possible changes in a place's observations generated by non-static objects. Place recognition is made by searching matches of sequences of N consecutive words (both real or alternatives). Experiments performed in real and simulated scenarios are shown, and demonstrate the advantages associated to the use of multi-level words.},
 author = {Maffei, Renan and Jorge, Vitor A.M. and Rey, Vitor F. and Kolberg, Mariana and Prestes, Edson},
 booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2016.7759504},
 keywords = {},
 pages = {3269-3274},
 title = {Long-Term Place Recognition Using Multi-Level Words of Spatial Densities},
 url = {https://app.dimensions.ai/details/publication/pub.1094955754},
 year = {2016}
}

@inproceedings{pub.1094969040,
 abstract = {This paper presents results of the application of the Simultaneous Localisation and Mapping algorithm to data collected by an Unmanned Underwater Vehicle operating on the Great Barrier Reef in Australia By fusing information from the vehicle's on-board sonar and vision systems, it is possible to use the highly textured reef to provide estimates of the vehicle motion as well as to generate models of the gross structure of the underlying reefs. Terrain-aided navigation promises to revolutionise the ability of marine systems to track underwater bodies in many applications. This work represents a crucial step in the development of underwater technologies capable of long-term, reliable deployment. Results of the application of this technique to the tracking of the vehicle position are shown.},
 author = {Williams, Stefan and Mahon, Ian},
 booktitle = {IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004},
 doi = {10.1109/robot.2004.1308080},
 keywords = {},
 pages = {1771-1776},
 title = {Simultaneous Localisation and Mapping on the Great Barrier Reef},
 url = {https://app.dimensions.ai/details/publication/pub.1094969040},
 year = {2004}
}

@inproceedings{pub.1094979985,
 abstract = {Over the past decade, challenging applications for autonomous robots have been identified, in the areas of servicing crowded, built-up areas, mining, search and rescue operations, underwater exploration and airborne surveillance. Autonomous navigation arguably remains the key enabling issue behind any realistic commercial success in these areas. Consequently, autonomous robotic research has focused on large scale and long term navigation algorithms, sensing technologies, robust sensor data interpretation and map building. The recent breakthroughs which contribute to the success of outdoor field robotics, and remaining fundamental research issues involved, will be the theme of this presentation. The most successful robot navigation algorithms to-date, have been derived from a probabilistic perspective, which takes into account vehicle motion and terrain uncertainty and sensor noise. Over the past decade, an explosion of interest in the estimation of an autonomous robot's location state, and that of its surroundings, known as simultaneous localisation and map building (SLAM), is evident. New algorithms which represent uncertain information based on particle filters and Gaussian mixture models, as well as the more classical Kalman filter based techniques, are advancing the progress of a robot's long term navigation abilities. This has been significantly aided by recently affordable sensor technologies, including GPS and inertial measurement units (IMUs) as well as fast and reliable laser range finders. To demonstrate the state of the art in autonomous navigation, outdoor research experiments will be demonstrated within complex, built environments, jungle terrain and in the under water domain with an array of vehicles, using RADAR, underwater SONAR and laser range finders. Finally, new research, which correctly addresses the, often previously ignored, fundamental issues of observability, sensed target classification and sensor biases will also be presented. With affordable new hardware, and a wealth of new estimation techniques, autonomous robot technology is entering a new era in which applications within complex environments becomes achievable.},
 author = {Adams, Martin E.},
 booktitle = {2006 IEEE International Conference on Robotics and Biomimetics},
 doi = {10.1109/robio.2006.340225},
 keywords = {},
 pages = {1-1},
 title = {Autonomous Navigation: Achievements in Complex Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1094979985},
 year = {2006}
}

@inproceedings{pub.1094986903,
 abstract = {Exploring an unknown space and building maps is a fundamental capability for mobile robots. For fully autonomous systems, the robot would further need to actively plan its paths during exploration. The problem of designing robot trajectories to actively explore an unknown environment and minimize the map error is referred to as active simultaneous localization and mapping (active SLAM). Existing work has focused on planning paths with occupancy grid maps, which do not scale well and suffer from long term drift. This work proposes a Topological Feature Graph (TFG) representation that scales well and develops an active SLAM algorithm with it. The TFG uses graphical models, which utilize independences between variables, and enables a unified quantification of exploration and exploitation gains with a single entropy metric. Hence, it facilitates a natural and principled balance between map exploration and refinement. A probabilistic roadmap path-planner is used to generate robot paths in real time. Experimental results demonstrate that the proposed approach achieves better accuracy than a standard grid-map based approach while requiring orders of magnitude less computation and memory resources.},
 author = {Mu, Beipeng and Giamou, Matthew and Paull, Liam and Agha-Mohammadi, Ali-Akbar and Leonard, John and How, Jonathan},
 booktitle = {2016 IEEE 55th Conference on Decision and Control (CDC)},
 doi = {10.1109/cdc.2016.7799127},
 keywords = {},
 note = {https://dspace.mit.edu/bitstream/1721.1/123811/1/2015_ActiveSLAM.pdf},
 pages = {5583-5590},
 title = {Information-Based Active SLAM via Topological Feature Graphs},
 url = {https://app.dimensions.ai/details/publication/pub.1094986903},
 year = {2016}
}

@inproceedings{pub.1094992151,
 abstract = {In this paper, we present an online landmark selection method for distributed long-term visual localization systems in bandwidth-constrained environments. Sharing a common map for online localization provides a fleet of autonomous vehicles with the possibility to maintain and access a consistent map source, and therefore reduce redundancy while increasing efficiency. However, connectivity over a mobile network imposes strict bandwidth constraints and thus the need to minimize the amount of exchanged data. The wide range of varying appearance conditions encountered during long-term visual localization offers the potential to reduce data usage by extracting only those visual cues which are relevant at the given time. Motivated by this, we propose an unsupervised method of adaptively selecting landmarks according to how likely these landmarks are to be observable under the prevailing appearance condition. The ranking function this selection is based upon exploits landmark co-observability statistics collected in past traversals through the mapped area. Evaluation is performed over different outdoor environments, large time-scales and varying appearance conditions, including the extreme transition from day-time to night-time, demonstrating that with our appearance-dependent selection method, we can significantly reduce the amount of landmarks used for localization while maintaining or even improving the localization performance.},
 author = {BÃ¼rki, Mathias and Gilitschenski, Igor and Stumm, Elena and Siegwart, Roland and Nieto, Juan},
 booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2016.7759609},
 keywords = {},
 note = {http://arxiv.org/pdf/1808.02656},
 pages = {4137-4143},
 title = {Appearance-Based Landmark Selection for Efficient Long-Term Visual Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1094992151},
 year = {2016}
}

@inproceedings{pub.1094992200,
 abstract = {Terms of mobile robots can be deployed in search and rescue missions to explore previously unknown environments. Methods for joint localization and mapping constitute the basis for (semi-)autonomous cooperative action, in particular when navigating in GPS-denied areas. As communication losses may occur, a decentralized solution is required. With these challenges in mind, we designed a submap-based SLAM system that relies on inertial measurements and stereo-vision to create multi-robot dense 3D maps. For online pose and map estimation, we integrate the results of keyframe-based local reference filters through incremental graph SLAM. To the best of our knowledge, we are the first to combine these two methods to benefit from their particular advantages for 6D multi-robot localization and mapping: Local reference filters on each robot provide real-time, long-term stable state estimates that are required for stabilization, control and fast obstacle avoidance, whereas online graph optimization provides global multi-robot pose and map estimates needed for cooperative planning. We propose a novel graph topology for a decoupled integration of local filter estimates from multiple robots into a SLAM graph according to the filters' uncertainty estimates and independence assumptions and evaluated its benefits on two different robots in indoor, outdoor and mixed scenarios. Further, we performed two extended experiments in a multi-robot setup to evaluate the full SLAM system, including visual robot detections and submap matches as inter-robot loop closure constraints.},
 author = {Schuster, Martin J. and Brandl, Christoph and Hirschmuller, Hetko and Suppa, Michael and Beetz, Michael},
 booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2015.7354094},
 keywords = {},
 note = {https://elib.dlr.de/100757/1/multirobot_slam_v2.4_href_header.pdf},
 pages = {5093-5100},
 title = {Multi-Robot 6D Graph SLAM Connecting Decoupled Local Reference Filters},
 url = {https://app.dimensions.ai/details/publication/pub.1094992200},
 year = {2015}
}

@inproceedings{pub.1094997890,
 abstract = {Localization for low cost humanoid or animal-like personal robots has to rely on cheap sensors and has to be robust to user manipulations of the robot. We present a visual localization and map-learning system that relies on vision only and that is able to incrementally learn to recognize the different rooms of an apartment from any robot position. This system is inspired by visual categorization algorithms called bag of words methods that we modified to make fully incremental and to allow a user-interactive training. Our system is able to reliably recognize the room in which the robot is after a short training time and is stable for long term use. Empirical validation on a real robot and on an image database acquired in real environments are presented.},
 author = {FILLIAT, David},
 booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2007.364080},
 keywords = {},
 note = {https://hal-ensta-paris.archives-ouvertes.fr//hal-00640996/file/Filliat_ICRA07.pdf},
 pages = {3921-3926},
 title = {A visual bag of words method for interactive qualitative localization and mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1094997890},
 year = {2007}
}

@inproceedings{pub.1094999774,
 abstract = {Long-term visual SLAM, in familiar, semi-dynamic, and partially changing environments is an important area of research in robotics. The main problem we faced is the question of how to describe a scene discriminatively and compactlyâ€”both of which are necessary in order to cope with changes in appearance and a large amount of visual information. In this study, we address the above issues by mining visual experience. Our strategy is to mine a library of raw visual images, termed visual experience, to find the relevant visual patterns to effectively explain the input scene. From a practical point of view, our work offers three main contributions over the previous work. First, it is the first application of discriminative visual features from deep convolutional neural networks (DCNN) to the task of visual landmark mining. Second, we show how to interpret a high-dimensional DCNN feature to a compact semantic representation of visual word. Third, we show that our approach can turn the scene description task with any feature (including the DCNN feature) into the task of mining visual experience. Experiments on a challenging cross-domain visual place recognition validate efficacy of the proposed approach.},
 author = {Taisho, Tsukamoto and Kanji, Tanaka},
 booktitle = {2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
 doi = {10.1109/robio.2016.7866383},
 keywords = {},
 pages = {570-576},
 title = {Mining DCNN Landmarks for Long-Term Visual SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1094999774},
 year = {2016}
}

@inproceedings{pub.1095017239,
 abstract = {Appearance-based localization is increasingly used for loop closure detection in metric SLAM systems. Since it relies only upon the appearance-based similarity between images from two locations, it can perform loop closure regardless of accumulated metric error. However, the computation time and memory requirements of current appearance-based methods scale linearly not only with the size of the environment but also with the operation time of the platform. These properties impose severe restrictions on long-term autonomy for mobile robots, as loop closure performance will inevitably degrade with increased operation time. We present a set of improvements to the appearance-based SLAM algorithm CAT-SLAM to constrain computation scaling and memory usage with minimal degradation in performance over time. The appearance-based comparison stage is accelerated by exploiting properties of the particle observation update, and nodes in the continuous trajectory map are removed according to minimal information loss criteria. We demonstrate constant time and space loop closure detection in a large urban environment with recall performance exceeding FAB-MAP by a factor of 3 at 100% precision, and investigate the minimum computational and memory requirements for maintaining mapping performance.},
 author = {Maddern, Will and Milford, Michael and Wyeth, Gordon},
 booktitle = {2012 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/icra.2012.6224622},
 keywords = {},
 pages = {822-827},
 title = {Capping Computation Time and Storage Requirements for Appearance-based Localization with CAT-SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1095017239},
 year = {2012}
}

@inproceedings{pub.1095026765,
 abstract = {Future Unmanned Aerial Vehicle (UAV) applications will require high-accuracy localisation in environments in which navigation infrastructure such as the Global Positioning System (GPS) and prior terrain maps may be unavailable or unreliable. In these applications, long-term operation requires the vehicle to build up a spatial map of theenvironment while whlsiutneul localising itself within vironment map, a task known as Simultaneous Localisation And Mapping (SLAM). In the first part of this paper we present an architecture archtecure erfomin inetia-senor based SLAM on an aerial vehicle. We demonstrate an on-line path planning scheme that intelligently plans the vehicle's trajectory while exploring unknown terrain in order to maximise the quality of both the resulting SLAM map and localisation estimates necessary for the autonomous control of the UAV. Two important performance properties and their relationship to the dynamic motion and path planning systems on-board the UAV are analysed. Firstly we analyse information-based measures such as Entropy. Secondly we perform an observability analysis of inertial SLAM by recasting the algorithms into an indirect error model form. Qualitative knowledge gained from the observability analysis is used to assist in the design of an information-based trajectory planner for the UAV. Results of the online path planning algorithm are presented using a high-fidelity 6-DoF simulation of a UAV during a simulated navigation and mapping task.},
 author = {Bryson, Mitch and Sukkarieh, Salah},
 booktitle = {2006 IEEE Aerospace Conference},
 doi = {10.1109/aero.2006.1655801},
 keywords = {},
 pages = {1-13},
 title = {Active Airborne Localisation and Exploration in Unknown Environments using Inertial SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1095026765},
 year = {2006}
}

@inproceedings{pub.1095040805,
 abstract = {One aim of ambient assistive technologies is to reduce long term hospitalization for elderly people, especially with pathologies such as Mild Cognitive Impairment (MCI). The smart environment assists these people and their families with safety and cognitive stimulation, so they stay as long as possible at home. The originality comes from using the robot in the elderly person's home. This robot is remote controlled by a distant user, a therapist or a relative, for determining alarming situations or for participating in stimulation exercises. Several modes are available for controlling the robot. This paper deals with an assisted control mode in which the remote user gives to the robot one goal and the robot reaches the goal by itself. During the robot movement, the user can dynamically change the current goal. An important hypothesis is that the robot has no a priori knowledge of its environment at the beginning. The knowledge will increase with time and the planned trajectory will be refreshed at two levels: a local one - faster but not always sufficient - and a global one - slower but which always finds a path if one exists. The idea is to work only with local information, using the robot sensors, the operator keeping the high level control. To assure that control, the remote operator uses video feedback and information from a laser range scanner.},
 author = {Devaux, Jean-ClÃ©ment and Nadrag, Paul and Colle, Etienne and Hoppenot, Philippe},
 booktitle = {2011 15th International Conference on Advanced Robotics (ICAR)},
 doi = {10.1109/icar.2011.6088615},
 keywords = {},
 note = {https://hal.archives-ouvertes.fr/hal-00604479/file/2011_ICAR_perso.pdf},
 pages = {186-191},
 title = {High level assisted control mode based on SLAM for a remotely controlled robot},
 url = {https://app.dimensions.ai/details/publication/pub.1095040805},
 year = {2011}
}

@inproceedings{pub.1095047829,
 abstract = {Markov localization and its variants are widely used for localization of mobile robots. These methods assume Markov independence of observations, implying that observations made by a robot correspond to a static map. However, in real human environments, observations include occlusions due to unmapped objects like chairs and tables, and dynamic objects like humans. We introduce an episodic non-Markov localization algorithm that maintains estimates of the belief over the trajectory of the robot while explicitly reasoning about observations and their correlations arising from unmapped static objects, moving objects, as well as objects from the static map. Observations are classified as arising from longterm features, short-term features, or dynamic features, which correspond to mapped objects, unmapped static objects, and unmapped dynamic objects respectively. By detecting time steps along the robot's trajectory where unmapped observations prior to such time steps are unrelated to those afterwards, non-Markov localization limits the history of observations and pose estimates to â€œepisodesâ€ over which the belief is computed. We demonstrate non-Markov localization in challenging real world indoor and outdoor environments over multiple datasets, comparing it with alternative state-of-the-art approaches, showing it to be robust as well as accurate.},
 author = {Biswas, Joydeep and Veloso, Manuela},
 booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2014.6907435},
 keywords = {},
 note = {http://www.cs.cmu.edu/afs/cs/user/mmv/www/papers/14icra-BiswasVeloso.pdf},
 pages = {3969-3974},
 title = {Episodic Non-Markov Localization: Reasoning About Short-Term and Long-Term Features},
 url = {https://app.dimensions.ai/details/publication/pub.1095047829},
 year = {2014}
}

@inproceedings{pub.1095231585,
 abstract = {This paper explores the issues involved in deployment of mobile robots in real-world situations and presents solutions and approaches under development at the Australian National University. For deployment of mobile robots outside of the laboratory, long-term operation is required. Hence, we have developed an automatic recharging system. In addition, a web-based teleoperation system is used to provide missions to test the long-teNn reliability of the robot. The final aspect of real-world operation that is explored here is operations in dynamic environments. To date, researchers have assumed static environments for mapping and localisation. Here we propose methods to avoid this restriction.},
 author = {Austin, David and Fletcher, Luke and Zelinsky, Alexander},
 booktitle = {Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)},
 doi = {10.1109/iros.2001.976237},
 keywords = {},
 pages = {613-618},
 title = {Mobile Robotics in the Long Term},
 url = {https://app.dimensions.ai/details/publication/pub.1095231585},
 year = {2001}
}

@inproceedings{pub.1095254970,
 abstract = {In this work, we describe an autonomous mobile robotic system for finding and investigating ambient noise sources in the environment. Motivated by the large negative effect of ambient noise sources on robot audition, the long-term goal is to provide awareness of the auditory scene to a robot, so that it may more effectively act to filter out the interference or re-position itself to increase the signal-to-noise ratio. Here, we concentrate on the discovery of new sources of sound through the use of mobility and directed investigation. This is performed in a two-step process. In the first step, a mobile robot first explores the surrounding acoustical environment, creating evidence grid representations to localize the most influential sound sources in the auditory scene. Then in the second step, the robot investigates each potential sound source location in the environment so as to improve the localization result, and identify volume and directionality characteristics of the sound source. Once every source has been investigated, a noise map of the entire auditory scene is created for use by the robot in avoiding areas of loud ambient noise when performing an auditory task.},
 author = {Martinson, E. and Schultz, A.},
 booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2007.363825},
 keywords = {},
 note = {http://vigir.missouri.edu/%7Egdesouza/Research/Conference_CDs/IEEE_ICRA_2007/data/papers/1160.pdf},
 pages = {1-6},
 title = {Robotic Discovery of the Auditory Scene},
 url = {https://app.dimensions.ai/details/publication/pub.1095254970},
 year = {2007}
}

@inproceedings{pub.1095331563,
 abstract = {This paper presents a vision based SLAM method by using stereo SFM technique. The proposed method is based on the stereo SFM presented in our former paper. The method do not need stereo correspondence but, nevertheless, can determine absolute scale factor, which is an important factor for long term navigation and SLAM. The method use only motion correspondence basically, which is easier to solve than stereo correspondence because the SFM algorithm can use a sequence of images taken at short time intervals. However, we infer the stereo correspondence inversely from the absolute estimates of structure and motion parameters and utilize this information to improve the performance of our method. Consequently, the method maintain the robustness to the stereo correspondence ambiguity and can avoid the degenerate configuration reported in the former paper. We also propose a simple initialization technique for the proposed method based on extended Kalman filter, which is critical issue for the methods using bearing-only measurements. The experimental results demonstrate the effectiveness of the algorithm.},
 author = {Kim, Jae-Hean and Chung, Myung Jin},
 booktitle = {Proceedings of the 2005 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2005.1570629},
 keywords = {},
 pages = {3360-3365},
 title = {Absolute Stereo SFM without Stereo Correspondence for Vision based SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1095331563},
 year = {2005}
}

@inproceedings{pub.1095339919,
 abstract = {Appearance-based mapping and localisation is especially challenging when separate processes of mapping and localisation occur at different times of day. The problem is exacerbated in the outdoors where continuous change in sun angle can drastically affect the appearance of a scene. We confront this challenge by fusing the probabilistic local feature based data association method of FAB-MAP with the pose cell filtering and experience mapping of RatSLAM. We evaluate the effectiveness of our amalgamation of methods using five datasets captured throughout the day from a single camera driven through a network of suburban streets. We show further results when the streets are re-visited three weeks later, and draw conclusions on the value of the system for lifelong mapping.},
 author = {Glover, Arren J. and Maddern, William P. and Milford, Michael J. and Wyeth, Gordon F.},
 booktitle = {2010 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2010.5509547},
 keywords = {},
 pages = {3507-3512},
 title = {FAB-MAP + RatSLAM: Appearance-based SLAM for Multiple Times of Day},
 url = {https://app.dimensions.ai/details/publication/pub.1095339919},
 year = {2010}
}

@inproceedings{pub.1095384745,
 abstract = {The paper presents vision-based robot navigation and user localization techniques of our long-term research project PERSES (PERsonal SErvice System), which aims to develop an interactive mobile shopping assistant that allows a continuous and intuitively understandable interaction with customers in a home store. Against this background, the paper describes a number of new or improved approaches, addressing challenges arising from the characteristics of the operation area, and from the need to continuously interact with users in a complex envirorunent. With our approaches to vision-based or visually-controlled map building, self-localization and navigation as well as user localization and tracking, we want to make a contribution to the real-world suitability of interactive mobile service-robots in non-trivial application areas and demanding human-robot interaction scenarios.},
 author = {Gross, Horst-Michael and Boehme, Hans-Joachim and Wilhelm, Torsten},
 booktitle = {2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)},
 doi = {10.1109/icsmc.2001.972991},
 keywords = {},
 pages = {672-677},
 title = {A Contribution to Vision-based Localization, Tracking and Navigation Methods for an Interactive Mobile Service-Robot},
 url = {https://app.dimensions.ai/details/publication/pub.1095384745},
 year = {2001}
}

@inproceedings{pub.1095490087,
 abstract = {The feature-based graphical approach to robotic mapping provides a representationally rich and computationally efficient framework for an autonomous agent to learn a model of its environment. However, this formulation does not naturally support long-term autonomy because it lacks a notion of environmental change; in reality, â€œeverything changes and nothing stands still;â€ and any mapping and localization system that aims to support truly persistent autonomy must be similarly adaptive. To that end, in this paper we propose a novel feature-based model of environmental evolution over time. Our approach is based upon the development of an expressive probabilistic generative feature persistence model that describes the survival of abstract semi-static environmental features over time. We show that this model admits a recursive Bayesian estimator, the persistence filter, that provides an exact online method for computing, at each moment in time, an explicit Bayesian belief over the persistence of each feature in the environment. By incorporating this feature persistence estimation into current state-of-the-art graphical mapping techniques, we obtain a flexible, computationally efficient, and information-theoretically rigorous framework for lifelong environmental modeling in an ever-changing world.},
 author = {Rosen, David M. and Mason, Julian and Leonard, John J.},
 booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2016.7487237},
 keywords = {},
 note = {https://dspace.mit.edu/bitstream/1721.1/107620/1/Leonard_Towards%20lifelong.pdf},
 pages = {1063-1070},
 title = {Towards Lifelong Feature-Based Mapping in Semi-Static Environments $t\in(25,75))$},
 url = {https://app.dimensions.ai/details/publication/pub.1095490087},
 year = {2016}
}

@inproceedings{pub.1095497723,
 abstract = {Self-driving car's navigation requires a very precise localization covering wide areas and long distances. Moreover, they have to do it at faster speeds than conventional mobile robots. This paper reports on an efficient technique to optimize the position of a sequence of maps along a journey. We take advantage of the short-term precision and reduced space on disk of the localization using 2D occupancy grid maps, from now on called sub-maps, as well as, the long-term global consistency of a Kalman filter that fuses odometry and GPS measurements. In our approach, horizontal planar LiDARs and odometry measurements are used to perform 2D-SLAM generating the sub-maps, and the EKF to generate the trajectory followed by the car in global coordinates. During the trip, after finishing each sub-map, a relaxation process is applied to a set of the last sub-maps to position them globally using both, global and map's local path. The importance of this method lies on its performance, expending low computing resources, so it can work in real time on a computer with conventional characteristics and on its robustness which makes it suitable for being used on a self-driving car as it doesn't depend excessively on the availability of GPS signal or the eventual appearance of moving objects around the car. Extensive testing has been performed in the suburbs and in the down-town of Nantes (France) covering a distance of 25 kilometers with different traffic conditions obtaining satisfactory results for autonomous driving.},
 author = {Dominguez, Salvador and Khomutenko, Bodgan and Garcia, Gaetan and Martinet, Philippe},
 booktitle = {2015 IEEE 18th International Conference on Intelligent Transportation Systems},
 doi = {10.1109/itsc.2015.433},
 keywords = {},
 note = {https://hal.archives-ouvertes.fr/hal-02459451/file/ITS15-Salvador.pdf},
 pages = {2694-2699},
 title = {An Optimization Technique for Positioning Multiple Maps for Self-Driving Car's Autonomous Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1095497723},
 year = {2015}
}

@inproceedings{pub.1095498083,
 abstract = {Underwater localization faces many constrains and long-term persistent global localization for autonomous underwater vehicles (AUVs) is very difficult. In this paper, we propose a novel AUV localization method taking advantage of the recent progress in ocean general circulation models (OGCMs). During navigation, the AUV performs intermittent local background flow velocity measurements or estimates using on-board sensors. A series of preloaded flow velocity forecast maps generated by OGCMs are referred by a particle filter in updating particle weights based on resemblance between forecasts and local estimation. A rigorous derivation of the problem in probability theory is presented to reveal the recursive structure of the target distribution function. Simulations in a simple double-gyre velocity field exhibit satisfactory converging localization error. Further simulations in a flow field with local flow fluctuations that are not resolved by OGCMs show similar convergent localization error with a slower converging rate. As a first step towards a new set of underwater localization methods, this work presents promising results and reveals the possibility of realizing converging global underwater localization through partial utilization of the background flow information that is easily accessible.},
 author = {Song, Zhuoyuan and Mohseni, Kamran},
 booktitle = {53rd IEEE Conference on Decision and Control},
 doi = {10.1109/cdc.2014.7040480},
 keywords = {},
 pages = {6945-6950},
 title = {Towards Background Flow Based AUV Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1095498083},
 year = {2014}
}

@inproceedings{pub.1095536717,
 abstract = {This paper reports on a method for an autonomous underwater vehicle to perform real-time visual simultaneous localization and mapping (SLAM) on large ship hulls over multiple sessions. Along with a monocular camera, our method uses a piecewise-planar model to explicitly optimize the ship hull surface in our factor-graph framework, and anchor nodes to co-register multiple surveys. To enable realtime performance for long-term SLAM, we use the recent Generic Linear Constraints (G LC) framework to sparsify our factor-graph. This paper analyzes how our single-session SLAM techniques can be used in the GLC framework, and describes a particle filter reacquisition algorithm so that an underwater session can be automatically re-localized to a previously built SLAM graph. We provide real-world experimental results involving automated ship hull inspection, and show that our localization filter out-performs Fast Appearance-Based Mapping (FAB-MAP), a popular place-recognition system. Using our approach, we can automatically align surveys that were taken days, months, and even years apart.},
 author = {Ozog, Paul and Eustice, Ryan M.},
 booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2014.6907415},
 keywords = {},
 pages = {3832-3839},
 title = {Toward Long-Term, Automated Ship Hull Inspection with Visual SLAM, Explicit Surface Optimization, and Generic Graph-Sparsification},
 url = {https://app.dimensions.ai/details/publication/pub.1095536717},
 year = {2014}
}

@inproceedings{pub.1095538380,
 abstract = {The ability to reconsider information over time allows to detect failures and is crucial for long term robust autonomous robot applications. This applies to loop closure decisions in localization and mapping systems. This paper describes a method to analyze all available information up to date in order to robustly remove past incorrect loop closures from the optimization process. The main novelties of our algorithm are: 1. incrementally reconsidering loop closures and 2. handling multi-session, spatially related or unrelated experiments. We validate our proposal in real multi-session experiments showing better results than those obtained by state of the art methods.},
 author = {Latif, Yasir and Cadena, CÃ©sar and Neira, JosÃ©},
 booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/iros.2012.6385879},
 keywords = {},
 note = {http://robots.unizar.es/new/data/documentos/yasir_iros12.pdf},
 pages = {4211-4217},
 title = {Realizing, Reversing, Recovering: Incremental Robust Loop Closing over time using the iRRR algorithm},
 url = {https://app.dimensions.ai/details/publication/pub.1095538380},
 year = {2012}
}

@inproceedings{pub.1095546723,
 abstract = {This paper presents a method to enable a mobile robot working in non-stationary environments to plan its path and localize within multiple map hypotheses simultaneously. The maps are generated using a long-term and short-term memory mechanism that ensures only persistent configurations in the environment are selected to create the maps. In order to evaluate the proposed method, experimentation is conducted in an office environment. Compared to navigation systems that use only one map, our system produces superior path planning and navigation in a non-stationary environment where paths can be blocked periodically, a common scenario which poses significant challenges for typical planners.},
 author = {Morris, Timothy and Dayoub, Feras and Corke, Peter and Wyeth, Gordon and Upcroft, Ben},
 booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2014.6907255},
 keywords = {},
 note = {https://eprints.qut.edu.au/72278/1/Morris_icra2014.pdf},
 pages = {2765-2770},
 title = {Multiple map hypotheses for planning and navigating in non-stationary environments},
 url = {https://app.dimensions.ai/details/publication/pub.1095546723},
 year = {2014}
}

@inproceedings{pub.1095572569,
 abstract = {This paper investigates sound source mapping in a real environment using a mobile robot. Our approach is based on audio ray tracing which integrates occupancy grids and sound source localization using a laser range finder and a microphone array. Previous audio ray tracing approaches rely on all observed rays and grids. As such observation errors caused by sound reflection, sound occlusion, wall occlusion, sounds at misdetected grids, etc. can significantly degrade the ability to locate sound sources in a map. A three-layered selective audio ray tracing mechanism is proposed in this work. The first layer conducts frame-based unreliable ray rejection (sensory rejection) considering sound reflection and wall occlusion. The second layer introduces triangulation and audio tracing to detect falsely detected sound sources, rejecting audio rays associated to these misdetected sounds sources (short-term rejection). A third layer is tasked with rejecting rays using the whole history (long-term rejection) to disambiguate sound occlusion. Experimental results under various situations are presented, which proves the effectiveness of our method.},
 author = {Su, Daobilige and Nakamura, Keisuke and Nakadai, Kazuhiro and Miro, Jaime Valls},
 booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2016.7759430},
 keywords = {},
 note = {https://opus.lib.uts.edu.au/bitstream/10453/60693/4/iros2016_manuscript_submitted.pdf},
 pages = {2771-2777},
 title = {Robust Sound Source Mapping Using Three-layered Selective Audio Rays for Mobile Robots},
 url = {https://app.dimensions.ai/details/publication/pub.1095572569},
 year = {2016}
}

@inproceedings{pub.1095593872,
 abstract = {Autonomous service mobile robots need to consistently, accurately, and robustly localize in human environments despite changes to such environments over time. Episodic non-Markov Localization addresses the challenge of localization in such changing environments by classifying observations as arising from Long-Term, Short-Term, or Dynamic Features. However, in order to do so, EnML relies on an estimate of the Long-Term Vector Map (LTVM) that does not change over time. In this paper, we introduce a recursive algorithm to build and update the LTVM over time by reasoning about visibility constraints of objects observed over multiple robot deployments. We use a signed distance function (SDF) to filter out observations of short-term and dynamic features from multiple deployments of the robot. The remaining longterm observations are used to build a vector map by robust local linear regression. The uncertainty in the resulting LTVM is computed via Monte Carlo resampling the observations arising from longterm features. By combining occupancy-grid based SDF filtering of observations with continuous space regression of the filtered observations, our proposed approach builds, updates, and amends LTVMs over time, reasoning about all observations from all robot deployments in an environment. We present experimental results demonstrating the accuracy, robustness, and compact nature of the extracted LTVMs from several longterm robot datasets.},
 author = {Nashed, Samer and Biswas, Joydeep},
 booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2016.7759683},
 keywords = {},
 note = {http://arxiv.org/pdf/2007.15736},
 pages = {4643-4648},
 title = {Curating Long-Term Vector Maps},
 url = {https://app.dimensions.ai/details/publication/pub.1095593872},
 year = {2016}
}

@inproceedings{pub.1095656489,
 abstract = {One of the fundamental requirements of an autonomous vehicle is the ability to determine its location on a map. Frequently, solutions to this localization problem rely on GPS information or use expensive three dimensional (3D) sensors. In this paper, we describe a method for long-term vehicle localization based on visual features alone. Our approach utilizes a combination of topological and metric mapping, which we call topometric localization, to encode the coarse topology of the route as well as detailed metric information required for accurate localization. A topometric map is created by driving the route once and recording a database of visual features. The vehicle then localizes by matching features to this database at runtime. Since individual feature matches are unreliable, we employ a discrete Bayes filter to estimate the most likely vehicle position using evidence from a sequence of images along the route. We illustrate the approach using an 8.8 km route through an urban and suburban environment. The method achieves an average localization error of 2.7 m over this route, with isolated worst case errors on the order of 10 m.},
 author = {Badino, H. and Huber, D. and Kanade, T.},
 booktitle = {2011 IEEE Intelligent Vehicles Symposium (IV)},
 doi = {10.1109/ivs.2011.5940504},
 keywords = {},
 note = {http://www.ri.cmu.edu/pub_files/2011/6/IV2011.pdf},
 pages = {794-799},
 title = {Visual topometric localization},
 url = {https://app.dimensions.ai/details/publication/pub.1095656489},
 year = {2011}
}

@inproceedings{pub.1095816338,
 abstract = {Early detection and removal of small pulmonary nodules significantly improves long term survival rates for patients with lung cancer. To aid in the localization of these tumors during video assisted thoracoscopic surgery (VATS), a Tactile Imaging System (TIS) is presented. The system consists of a capacitive array sensor mounted on a minimally invasive surgical probe that is integrated with the thoracoscopic imaging. A vision-based algorithm localizes the probe in the live video and overlays a registered pseudo-color map of the measured pressure distribution on the streaming images. The surgeon can locate the hard nodules by scanning the tactile sensor head across the surface of the lung and observing the spatial variation in contact pressures caused by the elasticity differences in the underlying tissue. A validation experiment was conducted to compare the system to a current localization technique using a rigid rod. Results indicate that subjects could localize stiff lumps in lung phantoms more quickly and accurately using the TIS.},
 author = {Miller, Andrew P. and Peine, William J. and Son, Jae S. and Hammoud, Zane T.M.D.},
 booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
 doi = {10.1109/robot.2007.363927},
 keywords = {},
 pages = {2996-3001},
 title = {Tactile Imaging System for Localizing Lung Nodules during Video Assisted Thoracoscopic Surgery},
 url = {https://app.dimensions.ai/details/publication/pub.1095816338},
 year = {2007}
}

@inproceedings{pub.1096992256,
 abstract = { Abstract If deepwater field developments are to continue to integrate complex subsea equipment for seafloor processing, sometimes in harsh environments and remote locations, the offshore industry has to find new means for Inspection, Maintenance and Repair (IMR) interventions that ensure and improve both integrity management and the flexibility and reactivity of the interventions. " SWIMMER?? is a new AUV incorporating a light work-class ROV with tooling designed for long-term subsea deployment - 3 months minimum - and able to perform light IMR operations such as:valve operation: manual or hydraulic failing valvesequipment survey: subsea production systems, pipelines and risers, umbilicalsprocess monitoring: hydrate localization, thermal leak identification,â€¦light equipment replacement: hydraulic or electric flying leads, â€¦trouble shooting assistance, using an ROV commanded from the surface facilities through the production control umbilical, rather than a dedicated service vessel. In early 2007, CYBERNETIX, TOTAL and STATOILHYDRO joined their engineering forces to work on a SWIMMER prototype, and set themselves the target of deploying it offshore by the end of 2011. This paper presents the project's industrial objectives, highlights key component specifications along with challenges such as AUV system, ROV/TMS, AUV payload, docking station, control and power umbilical, topside interface, and puts forward conclusions on feasibility, technological gaps, associated qualification programs and views on industrial integration on subsea oil and gas fields. Reliability, flexibility and autonomy are the key words in support of the integration of this concept in future field development architectures which could be fine-tuned with a SWIMMER-based operating intervention philosophy.  Introduction The offshore industry is facing new challenges with new field discoveries. Reservoirs are often deeper, smaller and in remote locations. The fluids are more complex, often heavy, acid and with low energy. To improve oil recovery, operators are seeking to install complex subsea equipment for boosting, or more generally processing production. Consequently, the capital expenditure figures for future developments are soaring and in the meantime, the costs of IMR operations, mainly driven by ROV support vessels and associated resources, have risen significantly. More than ever, innovative solutions have to be found, not only to decrease the operational expenditure but also to improve flexibility and reactivity. Over the past years, the use of AUV has increased extensively in various industrial sectors such as oil and gas, military and research. AUV are currently employed in seabed mapping where they have proved to be a choice answer. A couple of programs have attempted to demonstrate that AUVs can perform pipeline surveys but so far, no autonomous vehicle has been really used in inspection, maintenance or repair applications. No intervention AUV are currently available on the market.  },
 author = {Tito, Nicolas and Rambaldi, Eric},
 booktitle = {All Days},
 doi = {10.4043/19930-ms},
 keywords = {},
 pages = {},
 title = {SWIMMER: Innovative IMR AUV},
 url = {https://app.dimensions.ai/details/publication/pub.1096992256},
 year = {2009}
}

@inproceedings{pub.1098821924,
 abstract = {Reliable indoor navigation has been a goal of mobile robotics research in recent years. Probabilistic belief update, occupancy grid methods, and multi-tiered hybrid architectures have all been implemented in the pursuit of autonomous navigation systems that can function in the face of danger and incomplete information. The conventional approach applies AI techniques to the localization problem in order to attain reasonable navigation reliability. In this paper, we describe a simple navigation system that is neither multi-tiered nor AI-based. The system has been implemented on a Nomad 150 mobile robot that has demonstrated statistically significant navigation reliability during working hours in several buildings in the San Francisco Bay Area. In addition to providing navigation benchmarks based on long-term empirical tests, NaviGates demonstrates implementations of robot skills that are essential to robot autonomy: robust dynamic obstacle avoidance, path replanning in case of obstruction, automatic avoidance of dangerous regions such as staircases, and automatic human-guided mapping for a new building, allowing the robot to be introduced to a new office building without the need for time consuming manual mapping. Simplicity itself is an important thesis of this research, as a robot with an extremely simple control algoritm is shown to perform admirably in a variety of environments.},
 author = {Knotts, Ryan M. and Nourbakhsh, Illah R. and Morris, Robert C.},
 booktitle = {Robotics 98},
 doi = {10.1061/40337(205)6},
 keywords = {},
 pages = {36-42},
 title = {NaviGates: A Benchmark for Indoor Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1098821924},
 year = {1998}
}

@inproceedings{pub.1099219224,
 abstract = {Mobile robot localization in the GPS denied environments is increasingly exerting fundamental roles in a wide range of applications such as SFM and SLAM. However, the traditional single sensor based positioning methods are either unreliable or inaccurate in the long term. This paper presents a novel moving agent localizing approach that combines both...},
 author = {Gao, Enyang and Chen, Zhaohua and Gao, Qizhuhui},
 booktitle = {Proceedings of the 6th International Conference on Electronic, Mechanical, Information and Management Society},
 doi = {10.2991/emim-16.2016.309},
 keywords = {},
 note = {https://download.atlantis-press.com/article/25853465.pdf},
 pages = {},
 title = {Particle Filter Based Robot Self-localization Using RGBD Cues and Wheel Odometry Measurements},
 url = {https://app.dimensions.ai/details/publication/pub.1099219224},
 year = {2016}
}

@inproceedings{pub.1099729105,
 abstract = {Robust state estimation and real-time dense mapping are two core capabilities for autonomous navigation of mobile robots. Global Navigation Satellite System (GNSS) and visual odometry/SLAM are popular methods for state estimation. However, when working between tall buildings or in indoor environments, GNSS fails due to limited sky view or obstruction from buildings. Visual odometry/SLAM are prone to long-term drifting in the absence of reliable loop closure detection. A state estimation method with global-consistent guarantee is desirable for navigation applications. As for real-time mapping, SLAM methods usually get a sparse map that is not good enough for obstacle avoidance and path-planning, and high-quality dense mapping is often computationally too demanding for mobile devices. Realizing the availability of city-scale 3D models, in this work, we improve our previous work on model-based global localization, and propose a model-aided monocular visual-inertial state estimation and dense mapping solution. We first develop a global-consistent state estimator by fusing visual-inertial odometry with the model-based localization results. Utilizing depth prior from the model, we perform motion stereo with semi-global disparity smoothing. Our dense mapping pipeline is capable of online detection of obstacles that are originally not included in the offline 3D model. Our method runs onboard an embedded computer in real-time. We validate both the state estimation and mapping accuracy in real-world experiments.},
 author = {Qiu, Kejie and Shen, Shaojie},
 booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2017.8205992},
 keywords = {},
 pages = {1783-1789},
 title = {Model-Aided Monocular Visual-Inertial State Estimation and Dense Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1099729105},
 year = {2017}
}

@inproceedings{pub.1100254996,
 abstract = {This paper presents a novel method of visual simultaneous localization and mapping (SLAM), which is a method of real-time localization and mapping. It is important for a mobile robot to build a map while autonomously navigation. Due to the complexity of the robot work scene, the SLAM method proposed in this paper optimizes map management. It will cost a lot of time and space when a robot long-term works in a same large scene. Therefore, we propose a method in this paper to save a detail map as an offline map in advance. At the same time in order to facilitate the follow-up optimization, the offline map can be divided into several sub-graphs according to the similarity of the scene. Since the segmented offline map has been saved to local system, it can be loaded at any time to localization and obtain the pose of current frame.},
 author = {Shen, Qihui and Sun, Hanxu and Ye, Ping},
 booktitle = {2017 4th International Conference on Systems and Informatics (ICSAI)},
 doi = {10.1109/icsai.2017.8248292},
 keywords = {},
 pages = {215-219},
 title = {Research of Large-Scale Offline Map Management in Visual SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1100254996},
 year = {2017}
}

@article{pub.1100799671,
 abstract = {Abstract  INTRODUCTION Patients with pharmacotherapy-resistant localization-related epilepsy (LRE) may be candidates for surgical intervention if the seizure onset zone can be well localized. Long used in Europe, intracranial recording with stereoelectroencephalography (sEEG) is emerging as an alternative to subdural strip and grid techniques in North American centers.   METHODS We reviewed our initial experience in a consecutive cohort of patients who underwent sEEG for extraoperative monitoring of LRE between May 2014 and September 2016.   RESULTS >Fifty patients (37 adult, 13 pediatric) were implanted with 536 depth electrodes (mean 10.7 per patient, 7.9 per implanted hemisphere). Among 18 patients with suspected lesional epilepsy (including 3 with bilateral and 4 with multiple unilateral lesions), sEEG identified lesional foci in 16 (89%) cases (15 unifocal, 1 bitemporal). Two patients required further localization with subdural grids. Of 20 patients with nonlesional epilepsy, sEEG localized foci in 16 (80%) cases (13 unifocal, 2 bitemporal, 1 multifocal). Two patients had foci near eloquent cortex requiring grid placement for further mapping and two could not be focally localized. Finally, of 12 patients who had previous resections or ablations, sEEG localized foci in 11 (92%) cases (10 peri-cavity, 1 multifocal) and 1 was not focally localized. Complications were minor and rare. In 536 electrodes, there were no (0.0%) infections or symptomatic hemorrhages and 3 (0.6%) small, asymptomatic hemorrhages. One electrode was deflected into the subdural space during placement and 1 patient required replacement of 2 electrodes that were broken during seizures in the monitoring unit.   CONCLUSION Robot-assisted sEEG is a safe and useful method for localizing epileptogenic foci in patients with lesional, nonlesional, and previously treated LRE. The success of seizure onset localization and safety compare favorably with invasive subdural monitoring. Longer clinical follow up will be required to determine whether sEEG monitoring improves long-term seizure freedom in these challenging epilepsy patients. },
 author = {Youngerman, Brett E and Oh, Justin and Pathak, Yagna and Banks, Garrett P and Sheth, Sameer A and Feldstein, Neil A and McKhann, Guy M},
 doi = {10.1093/neuros/nyx417.350},
 journal = {Neurosurgery},
 keywords = {},
 number = {CN_suppl_1},
 pages = {279-280},
 title = {350 Stereoelectroencephalography for Refractory Localization-related Epilepsy: Initial Experience in 50 Patients},
 url = {https://app.dimensions.ai/details/publication/pub.1100799671},
 volume = {64},
 year = {2017}
}

@article{pub.1101276502,
 abstract = {Input uncertainty, e.g., noise on the on-board camera and inertial measurement unit, in vision-based control of unmanned aerial vehicles (UAVs) is an inevitable problem. In order to handle input uncertainties as well as further analyze the interaction between the input and the antecedent fuzzy sets (FSs) of nonsingleton fuzzy logic controllers (NSFLCs), an input uncertainty sensitivity enhanced NSFLC has been developed in robot operating system using the C++ programming language. Based on recent advances in nonsingleton inference, the centroid of the intersection of the input and antecedent FSs (Cen-NSFLC) is utilized to calculate the firing strength of each rule instead of the maximum of the intersection used in traditional NSFLC (Tra-NSFLC). An 8-shaped trajectory, consisting of straight and curved lines, is used for the real-time validation of the proposed controllers for a trajectory following problem. An accurate monocular keyframe-based visual-inertial simultaneous localization and mapping (SLAM) approach is used to estimate the position of the quadrotor UAV in GPS-denied unknown environments. The performance of the Cen-NSFLC is compared with a conventional proportional-integral derivative (PID) controller, a singleton FLC and a Tra-NSFLC. All controllers are evaluated for different flight speeds, thus introducing different levels of uncertainty into the control problem. Visual-inertial SLAM-based real-time quadrotor UAV flight tests demonstrate that not only does the Cen-NSFLC achieve the best control performance among the four controllers, but it also shows better control performance when compared to their singleton counterparts. Considering the bias in the use of model-based controllers, e.g., PID, for the control of UAVs, this paper advocates an alternative method, namely Cen-NSFLCs, in uncertain working environments.},
 author = {Fu, Changhong and Sarabakha, Andriy and Kayacan, Erdal and Wagner, Christian and John, Robert and Garibaldi, Jonathan M.},
 doi = {10.1109/tmech.2018.2810947},
 journal = {IEEE/ASME Transactions on Mechatronics},
 keywords = {},
 note = {https://nottingham-repository.worktribe.com/preview/929538/Input%20Uncertainty%20Sensitivity%20Cen-NSFLC.pdf},
 number = {2},
 pages = {725-734},
 title = {Input Uncertainty Sensitivity Enhanced Nonsingleton Fuzzy Logic Controllers for Long-Term Navigation of Quadrotor UAVs},
 url = {https://app.dimensions.ai/details/publication/pub.1101276502},
 volume = {23},
 year = {2018}
}

@article{pub.1101544826,
 abstract = {Robust place recognition plays a key role for the long-term autonomy of unmanned ground vehicles (UGVs) working in indoor or outdoor environments. Although most of the state-of-the-art that approaches for place recognition are vision-based, visual sensors lack adaptability in environments with poor or dynamically changing illumination. In this paper, a 3-D-laser-based place recognition algorithm is proposed to accomplish loop closure detection for simultaneous localization and mapping. An image model named bearing angle (BA) is adopted to convert 3-D laser points to 2-D images, and then ORB features extracted from BA images are utilized to perform scene matching. Since the computational cost for matching a query BA image with all the BA images in a database is too high to meet the requirement of performing real-time place recognition, a visual bag of words approach is used to improve search efficiency. Furthermore, a speed normalization algorithm and a 3-D geometry-based verification algorithm are proposed to complete the proposed place recognition algorithm. Experiments were conducted on two self-developed UGV platforms to verify the performance of the proposed method.},
 author = {Cao, Fengkui and Zhuang, Yan and Zhang, Hong and Wang, Wei},
 doi = {10.1109/jsen.2018.2815956},
 journal = {IEEE Sensors Journal},
 keywords = {},
 number = {10},
 pages = {4242-4252},
 title = {Robust Place Recognition and Loop Closing in Laser-Based SLAM for UGVs in Urban Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1101544826},
 volume = {18},
 year = {2018}
}

@article{pub.1101834397,
 abstract = {Long term mapping and localization are the primary components for mobile robots in real world application deployment, of which the crucial challenge is the robustness and stability. In this paper, we introduce a topological local-metric framework (TLF), aiming at dealing with environmental changes, erroneous measurements and achieving constant complexity. TLF organizes the sensor data collected by the robot in a topological graph, of which the geometry is only encoded in the edge, i.e. the relative poses between adjacent nodes, relaxing the global consistency to local consistency. Therefore the TLF is more robust to unavoidable erroneous measurements from sensor information matching since the error is constrained in the local. Based on TLF, as there is no global coordinate, we further propose the localization and navigation algorithms by switching across multiple local metric coordinates. Besides, a lifelong memorizing mechanism is presented to memorize the environmental changes in the TLF with constant complexity, as no global optimization is required. In experiments, the framework and algorithms are evaluated on 21-session data collected by stereo cameras, which are sensitive to illumination, and compared with the state-of-art global consistent framework. The results demonstrate that TLF can achieve similar localization accuracy with that from global consistent framework, but brings higher robustness with lower cost. The localization performance can also be improved from sessions because of the memorizing mechanism. Finally, equipped with TLF, the robot navigates itself in a 1Â km session autonomously.},
 author = {Tang, Li and Wang, Yue and Ding, Xiaqing and Yin, Huan and Xiong, Rong and Huang, Shoudong},
 doi = {10.1007/s10514-018-9724-7},
 journal = {Autonomous Robots},
 keywords = {},
 note = {https://opus.lib.uts.edu.au/bitstream/10453/131696/4/10.1007s10514-018-9724-7%20am.pdf},
 number = {1},
 pages = {197-211},
 title = {Topological local-metric framework for mobile robots navigation: a long term perspective},
 url = {https://app.dimensions.ai/details/publication/pub.1101834397},
 volume = {43},
 year = {2019}
}

@article{pub.1103214284,
 abstract = {This paper presents a sophisticated vision-aided flocking system for unmanned aerial vehicles (UAVs), which is able to operate in GPS-denied unknown environments for exploring and searching missions, and also able to adopt two types of vision sensors, day and thermal cameras, to measure relative motion between UAVs in different lighting conditions without using wireless communication. In order to realize robust vision-aided flocking, an integrated framework of tracking-learning-detection on the basis of multifeature coded correlation filter has been developed. To achieve long-term tracking, a redetector is trained online to adaptively reinitialize target for global sensing. An advanced flocking strategy is developed to address the autonomous multi-UAVs' cooperative flight. Light detection and ranging (LiDAR)-based navigation modules are developed for autonomous localization, mapping, and obstacle avoidance. Flight experiments of a team of UAVs have been conducted to verify the performance of this flocking system in a GPS-denied environment. The extensive experiments validate the robustness of the proposed vision algorithms in challenging scenarios.},
 author = {Tang, Yazhe and Hu, Yuchao and Cui, Jinqiang and Liao, Fang and Lao, Mingjie and Lin, Feng and Teo, Rodney S. H.},
 doi = {10.1109/tie.2018.2824766},
 journal = {IEEE Transactions on Industrial Electronics},
 keywords = {},
 number = {1},
 pages = {616-626},
 title = {Vision-Aided Multi-UAV Autonomous Flocking in GPS-Denied Environment},
 url = {https://app.dimensions.ai/details/publication/pub.1103214284},
 volume = {66},
 year = {2019}
}

@article{pub.1103722677,
 abstract = {In this paper, heading reference-assisted pose estimation (HRPE) has been proposed to compensate inherent drift of visual odometry (VO) on ground vehicles, where an estimation error is prone to grow while the vehicle is making turns or in environments with poor features. By introducing a particular orientation as â€œheading reference,â€ a pose estimation framework has been presented to incorporate measurements from heading reference sensors into VO. A graph formulation is then proposed to represent the pose estimation problem under the commonly used graph optimization model. Simulations and experiments on KITTI data set and our self-collected sequences have been conducted to verify the accuracy and robustness of the proposed scheme. KITTI sequences and manually generated heading measurement with Gaussian noises are used in simulation, where rotational drift error is observed to be bounded. Compared with a pure VO, the proposed approach greatly reduces average translational localization error from 153.85 to 24.29 m and 23.80 m in self-collected stereo visual sequences with traveling distance over 4.5 km at the processing rates of 19.7 and 11.1 Hz, for the loosely coupled and tightly coupled models, respectively. Note to Practitionersâ€”When the Global Positioning System is not available or reliable, visual odometry (VO) on ground vehicles is an efficient tool for estimating the pose, which involves translation and rotation. However, VO inherently suffers from drifting issue due to constant iterations. By adding a low-cost heading reference sensor, this paper first introduces graph optimization formulation of pose estimation and then presents a pose estimation framework which incorporates heading measurements to VO such that long-term translation and rotation estimation errors can be greatly reduced in real-time computation. As a supplementary to VO, performance may still deteriorate in environments with poor illumination conditions and high-frequency movements. The proposed approach may be further improved by fusing heading measurements from more sensors or being used to build a heading reference-assisted simultaneous localization and mapping (SLAM) system on any off-the-shelf SLAM framework.},
 author = {Wang, Han and Jiang, Rui and Zhang, Handuo and Ge, Shuzhi Sam},
 doi = {10.1109/tase.2018.2828078},
 journal = {IEEE Transactions on Automation Science and Engineering},
 keywords = {},
 note = {https://dr.ntu.edu.sg/bitstream/10356/143622/2/Heading%20Reference-Assisted%20Pose%20Estimation%20for%20Ground%20Vehicles.pdf},
 number = {1},
 pages = {448-458},
 title = {Heading Reference-Assisted Pose Estimation for Ground Vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1103722677},
 volume = {16},
 year = {2019}
}

@inproceedings{pub.1103851661,
 abstract = {Swarm robotics is receiving increasing interest, because the collaborative completion of tasks, such as the exploration of unknown environments, leads to improved performance and reduced effort. The ability to exchange map information is an essential requirement for collaborative exploration. When moving to large-scale environments, where the communication data rate between the swarm participants is typically limited, efficient compression algorithms and an approach for discarding less informative parts of the map are key for a successful long-term operation. In this paper, we present a novel compression approach for environment maps obtained from a visual SLAM system. We apply feature coding to the visual information to compress the map efficiently. We make use of a minimum spanning tree to connect all features that serve as observations of a single map point. Thereby, we can exploit inter-feature dependencies and obtain an optimal coding order. Additionally, we add a map sparsification step to keep only useful map points by solving a linear integer programming problem, which preserves the map points that exhibit both good compression properties and high observability. We evaluate the proposed method on a standard dataset and show that our approach outperforms state-of-the-art techniques.},
 author = {van Opdenbosch, Dominik and Aykut, Tamay and Oelsch, Martin and Alt, Nicolas and Steinbach, Eckehard},
 booktitle = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
 doi = {10.1109/wacv.2018.00114},
 keywords = {},
 pages = {992-1000},
 title = {Efficient Map Compression for Collaborative Visual SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1103851661},
 year = {2018}
}

@article{pub.1103999893,
 abstract = {In this paper we propose a simultaneous localization and mapping (SLAM) back-end solution called the exactly sparse delayed state filter on Lie groups (LG-ESDSF). We derive LG-ESDSF and demonstrate that it retains all the good characteristics of the classic Euclidean ESDSF, the main advantage being the exact sparsity of the information matrix. The key advantage of LG-ESDSF in comparison with the classic ESDSF lies in the ability to respect the state space geometry by negotiating uncertainties and employing filtering equations directly on Lie groups. We also exploit the special structure of the information matrix in order to allow long-term operation while the robot is moving repeatedly through the same environment. To prove the effectiveness of the proposed SLAM solution, we conducted extensive experiments on two different publicly available datasets, namely the KITTI and EuRoC datasets, using two front-ends: one based on the stereo camera and the other on the 3D LIDAR. We compare LG-ESDSF with the general graph optimization framework ([Formula: see text]) when coupled with the same front-ends. Similarly to [Formula: see text] the proposed LG-ESDSF is front-end agnostic and the comparison demonstrates that our solution can match the accuracy of [Formula: see text], while maintaining faster computation times. Furthermore, the proposed back-end coupled with the stereo camera front-end forms a complete visual SLAM solution dubbed LG-SLAM. Finally, we evaluated LG-SLAM using the online KITTI protocol and at the time of writing it achieved the second best result among the stereo odometry solutions and the best result among the tested SLAM algorithms.},
 author = {Lenac, Kruno and Ä†esiÄ‡, Josip and MarkoviÄ‡, Ivan and PetroviÄ‡, Ivan},
 doi = {10.1177/0278364918767756},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 number = {6},
 pages = {585-610},
 title = {Exactly sparse delayed state filter on Lie groups for long-term pose graph SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1103999893},
 volume = {37},
 year = {2018}
}

@article{pub.1104171590,
 abstract = {A novel multi-sensor fusion indoor localization algorithm based on ArUco marker is designed in this paper. The proposed ArUco mapping algorithm can build and correct the map of markers online with Grubbs criterion and K-mean clustering, which avoids the map distortion due to lack of correction. Based on the conception of multi-sensor information fusion, the federated Kalman filter is utilized to synthesize the multi-source information from markers, optical flow, ultrasonic and the inertial sensor, which can obtain a continuous localization result and effectively reduce the position drift due to the long-term loss of markers in pure marker localization. The proposed algorithm can be easily implemented in a hardware of one Raspberry Pi Zero and two STM32 micro controllers produced by STMicroelectronics (Geneva, Switzerland). Thus, a small-size and low-cost marker-based localization system is presented. The experimental results show that the speed estimation result of the proposed system is better than Px4flow, and it has the centimeter accuracy of mapping and positioning. The presented system not only gives satisfying localization precision, but also has the potential to expand other sensors (such as visual odometry, ultra wideband (UWB) beacon and lidar) to further improve the localization performance. The proposed system can be reliably employed in Micro Aerial Vehicle (MAV) visual localization and robotics control.},
 author = {Xing, Boyang and Zhu, Quanmin and Pan, Feng and Feng, Xiaoxue},
 doi = {10.3390/s18061706},
 journal = {Sensors},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/18/6/1706/pdf},
 number = {6},
 pages = {1706},
 title = {Marker-Based Multi-Sensor Fusion Indoor Localization System for Micro Air Vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1104171590},
 volume = {18},
 year = {2018}
}

@inproceedings{pub.1104307778,
 abstract = {Simultaneous localization and mapping (SLAM) is the first step for enabling autonomous operation in unknown and changing environments. Many applications such as service and assistive robots require constant movement between different regions along with accurate navigation and localization at any point in time. Algorithms for SLAM have matured greatly over the last few years and can accommodate different sensors, computing requirements as well as environments for use. However, for long-term autonomy indoors, reasoning with a large volume of RGB-D data is still a major challenge. In this work, we propose a pipeline that attributes semantics, more specifically cuboidal structure, to observed objects, uses them as landmarks for mapping and thereby reduces the dimensionality of the represented map greatly. We chose cuboids, because many common urban scenes (such as offices, homes, malls) contain cuboidal objects (such as cabinets, tables, shelves). We develop a metric to perform such attribution consistently so they can be used as landmarks for mapping/navigation. We have tested our pipeline on three different datasets and show that we can reduce the map representation significantly while maintaining localization accuracy in all of them. Our vision is that attributing low-level semantics such as one presented in this work would make long-term autonomy computationally tractable.},
 author = {Hashemifar, Zakieh and Lee, Kyung Won and Napp, Nils and Dantu, Karthik},
 booktitle = {Proceedings of the 1st International Workshop on Internet of People, Assistive Robots and Things},
 doi = {10.1145/3215525.3215531},
 keywords = {},
 pages = {19-24},
 title = {Geometric Mapping for Sustained Indoor Autonomy},
 url = {https://app.dimensions.ai/details/publication/pub.1104307778},
 year = {2018}
}

@article{pub.1104981024,
 abstract = {In this article, we propose a distributed and collaborative monocular simultaneous localization and mapping system for the multi-robot system in large-scale environments, where monocular vision is the only exteroceptive sensor. Each robot estimates its pose and reconstructs the environment simultaneously using the same monocular simultaneous localization and mapping algorithm. Meanwhile, they share the results of their incremental maps by streaming keyframes through the robot operating system messages and the wireless network. Subsequently, each robot in the group can obtain the global map with high efficiency. To build the collaborative simultaneous localization and mapping architecture, two novel approaches are proposed. One is a robust relocalization method based on active loop closure, and the other is a vision-based multi-robot relative pose estimating and map merging method. The former is used to solve the problem of tracking failures when robots carry out long-term monocular simultaneous localization and mapping in large-scale environments, while the latter uses the appearance-based place recognition method to determine multi-robot relative poses and build the large-scale global map by merging each robotâ€™s local map. Both KITTI data set and our own data set acquired by a handheld camera are used to evaluate the proposed system. Experimental results show that the proposed distributed multi-robot collaborative monocular simultaneous localization and mapping system can be used in both indoor small-scale and outdoor large-scale environments.},
 author = {Zhang, Hui and Chen, Xieyuanli and Lu, Huimin and Xiao, Junhao},
 doi = {10.1177/1729881418780178},
 journal = {International Journal of Advanced Robotic Systems},
 keywords = {},
 note = {https://journals.sagepub.com/doi/pdf/10.1177/1729881418780178},
 number = {3},
 pages = {1729881418780178},
 title = {Distributed and collaborative monocular simultaneous localization and mapping for multi-robot systems in large-scale environments},
 url = {https://app.dimensions.ai/details/publication/pub.1104981024},
 volume = {15},
 year = {2018}
}

@article{pub.1105010013,
 abstract = {SUMMARY
                  A robot should be able to estimate an accurate and dense 3D model of its environment (a map), along with its pose relative to it, all of it in real time, in order to be able to navigate autonomously without collisions.
                  As the robot moves from its starting position and the estimated map grows, the computational and memory footprint of a dense 3D map increases and might exceed the robot capabilities in a short time. However, a global map is still needed to maintain its consistency and plan for distant goals, possibly out of the robot field of view.
                  In this work, we address such problem by proposing a real-time stereo mapping pipeline, feasible for standard CPUs, which is locally dense and globally sparse and accurate. Our algorithm is based on a graph relating poses and salient visual points, in order to maintain a long-term accuracy with a small cost. Within such framework, we propose an efficient dense fusion of several stereo depths in the locality of the current robot pose.
                  We evaluate the performance and the accuracy of our algorithm in the public datasets of Tsukuba and KITTI, and demonstrate that it outperforms single-view stereo depth. We release the code as open-source, in order to facilitate the system use and comparisons.},
 author = {Pire, TaihÃº and Baravalle, Rodrigo and D'Alessandro, Ariel and Civera, Javier},
 doi = {10.1017/s0263574718000528},
 journal = {Robotica},
 keywords = {},
 number = {10},
 pages = {1510-1526},
 title = {Real-time dense map fusion for stereo SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1105010013},
 volume = {36},
 year = {2018}
}

@article{pub.1105266181,
 abstract = {Molecular imaging with a fluorescent version of Tilmanocept may permit an accurate and facile detection of sentinel nodes of endometrial cancer. Tilmanocept accumulates in sentinel lymph nodes (SLN) by binding to a cell surface receptor unique to macrophages and dendritic cells. Four female Yorkshire pigs underwent cervical stromal injection of IRDye800-Tilmanocept, a molecular imaging agent tagged with near-infrared fluorescent dye and radiolabeled with gallium-68 and technetium-99m. PET/CT scans 1.5 hours post-injection provided pre-operative SLN mapping. Robotic-assisted lymphadenectomy was performed two days after injection, using the FireFly imaging system to identify nodes demonstrating fluorescent signal. After removal of fluorescent nodes, pelvic and periaortic node dissections were performed. Nodes were assayed for technetium-99m activity, and SLNs were established using the "10%-rule", requiring that the radioactivity of additional SLNs be greater than 10% of the "hottest" SLN. Thirty-four nodal samples were assayed ex vivo for radioactivity. All the SLNs satisfying the "10%-rule" were detected using the FireFly system. Five fluorescent nodes were detected, corresponding with preoperative PET/CT scan. Three pigs had one SLN and one pig had two SLNs, with 100% concordance between fluorescence and radioactivity. Fluorescent-labeled Tilmanocept permits real-time intraoperative detection of SLNs during robotic-assisted lymphadenectomy for endometrial cancer in a porcine model. When radiolabeled with gallium-68, Tilmanocept allows for preoperative localization of SLNs using PET/CT, and shows specificity to SLNs with persistent fluorescent signal, detectable using the FireFly system, for two days post-injection. In conclusion, these findings suggest that a phase I trial in human subjects is warranted, and that a long-term goal of an intra-operative administration of non-radioactive fluorescent-labeled Tilmanocept is possible.},
 author = {Anderson, Kristen M. and Barback, Christopher V. and Qin, Zhengtao and Hall, David J. and Hoh, Carl K. and Vera, David R. and McHale, Michael T.},
 doi = {10.1371/journal.pone.0197842},
 journal = {PLoS ONE},
 keywords = {},
 note = {https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0197842&type=printable},
 number = {7},
 pages = {e0197842},
 title = {Molecular Imaging of endometrial sentinel lymph nodes utilizing fluorescent-labeled Tilmanocept during robotic-assisted surgery in a porcine model},
 url = {https://app.dimensions.ai/details/publication/pub.1105266181},
 volume = {13},
 year = {2018}
}

@article{pub.1105344993,
 abstract = {In this paper, we novelly consider visual localization in active and passive two ways, with simple definition that active localization assists device to estimate location of its interest while passive localization aids device to estimate its own location in environment. Expecting to indicate some insights into visual localization, we specifically performed two explorations on active localization and more importantly explored to upgrade them from active to passive localization with extra geometry information available. In order to produce unconstrained and accurate 2D location estimation of interested object, we constructed an active localization system by fusing detection, tracking and recognition. Based on recognition, we proposed a collaborative strategy making mutual enhancement between detection and tracking possible to obtain better performance on 2D location estimation. Meanwhile, to actively estimate semantic location of interested visual region, we employed latest state-of-the-art light weight CNN models specifically designed for efficiency and trained two of them with large place dataset in perspective of scene recognition. Whatâ€™s more, using depth information available from RGB-D camera, we improved the active system for 2D location of interested object to a passive system for relative 3D location of device to the interested object. Firstly estimated was the 3D location of the interested object in the coordinate system of device, then relative location of device to the interested object in world coordinate system was deduced with appropriate assumption. Evaluations both subjectively on a RGB-D sequence obtained in a lab environment and practically on a robotic platform in an office environment indicated that the improved system was suitable for autonomous following robot. As well, the active system for rough semantic location estimation of interested visual region was promoted to a passive system for fine location estimation of device, with available 3D map describing the visited environment. In perspective of place recognition, we first adopted one of the efficient CNN models previously trained for semantic location estimation as a base to generate CNN features for both retrieval of candidate loops in the map and geometrical consistency checking of retrieved loops, then true loops were used to deduce fine location of device itself in environment. Comparison with state-of-the-art results reflected that the promoted system was adequate for long-term robotic autonomy. Achieving favorable performances, the presented four explorations have implied adequacy for elaborating on some insights into visual localization.},
 author = {Yang, Yongquan and Wu, Yang and Chen, Ning},
 doi = {10.1007/s11042-018-6347-0},
 journal = {Multimedia Tools and Applications},
 keywords = {},
 number = {2},
 pages = {2269-2309},
 title = {Explorations on visual localization from active to passive},
 url = {https://app.dimensions.ai/details/publication/pub.1105344993},
 volume = {78},
 year = {2019}
}

@article{pub.1105861649,
 abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this letter we present DynaSLAM, a visual SLAM system that, building on ORB-SLAM2, adds the capabilities of dynamic object detection and background inpainting. DynaSLAM is robust in dynamic scenarios for monocular, stereo, and RGB-D configurations. We are capable of detecting the moving objects either by multiview geometry, deep learning, or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo, and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynaSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments.},
 author = {Bescos, Berta and Facil, Jose M. and Civera, Javier and Neira, Jose},
 doi = {10.1109/lra.2018.2860039},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 note = {https://zaguan.unizar.es/record/71183/files/texto_completo.pdf},
 number = {4},
 pages = {4076-4083},
 title = {DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes},
 url = {https://app.dimensions.ai/details/publication/pub.1105861649},
 volume = {3},
 year = {2018}
}

@article{pub.1106322761,
 abstract = { Bathymetric simultaneous localization and mapping (BSLAM) technique could provide long-term underwater navigation results for autonomous underwater vehicles (AUVs) and produce a self-consistent bathymetric map simultaneously. However, the inter-frame motion inside BSLAM is still difficult to estimate, and BSLAM might fail catastrophically with invalid loop closures caused by the measurement errors of vehicle states and bathymetric data. To deal with these problems, an AUV robust BSLAM algorithm is proposed based on graph SLAM. In this algorithm, weak data association is constructed via sparse pseudo-input Gaussian process (SPGP) regression to predict inter-frame motion, and a multi-window consistency method (MCM) is introduced to identify invalid loop closures. Various simulation experiments are conducted under different environments. Comparisons are made between more standard approaches, and our proposed algorithm is shown to be viable, accurate, and could robustly handle invalid loop closures.},
 author = {Ma, Teng and Li, Ye and Wang, Rupeng and Cong, Zheng and Gong, Yusen},
 doi = {10.1016/j.oceaneng.2018.08.029},
 journal = {Ocean Engineering},
 keywords = {},
 number = {},
 pages = {336-349},
 title = {AUV robust bathymetric simultaneous localization and mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1106322761},
 volume = {166},
 year = {2018}
}

@inproceedings{pub.1107086892,
 abstract = {Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first large-scale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.},
 author = {Siva, Sriram and Zhang, Hao},
 booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2018.8461042},
 keywords = {},
 pages = {1-9},
 title = {Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition},
 url = {https://app.dimensions.ai/details/publication/pub.1107086892},
 year = {2018}
}

@inproceedings{pub.1107088545,
 abstract = {Robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles. In this paper, we exploit recent advances in semantic segmentation of images, i.e., where each pixel is assigned a label related to the type of object it represents, to attack the problem of long-term visual localization. We show that semantically labeled 3D point maps of the environment, together with semantically segmented images, can be efficiently used for vehicle localization without the need for detailed feature descriptors (SIFT, SURF, etc.), Thus, instead of depending on hand-crafted feature descriptors, we rely on the training of an image segmenter. The resulting map takes up much less storage space compared to a traditional descriptor based map. A particle filter based semantic localization solution is compared to one based on SIFT-features, and even with large seasonal variations over the year we perform on par with the larger and more descriptive SIFT-features, and are able to localize with an error below 1 m most of the time.},
 author = {Stenborg, Erik and Toft, Carl and Hammarstrand, Lars},
 booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2018.8463150},
 keywords = {},
 note = {http://arxiv.org/pdf/1801.05269},
 pages = {6484-6490},
 title = {Long-Term Visual Localization Using Semantically Segmented Images},
 url = {https://app.dimensions.ai/details/publication/pub.1107088545},
 year = {2018}
}

@inproceedings{pub.1107090020,
 abstract = {This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.},
 author = {Sun, Li and Yan, Zhi and Mellado, Sergi Molina and Hanheide, Marc and Duckett, Tom},
 booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2018.8461228},
 keywords = {},
 note = {https://eprints.whiterose.ac.uk/161365/1/1710.00126v1.pdf},
 pages = {1-7},
 title = {3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data},
 url = {https://app.dimensions.ai/details/publication/pub.1107090020},
 year = {2018}
}

@article{pub.1107216671,
 abstract = {Due to the need of zigzag overlay strategy, long-term linear motion is essential for sweep robot. However, the existing indoor sweep robot navigation algorithm has many problems; for instance, algorithm with high complexity demands high hardware performance and is incapable of working at night. To overcome those problems, in this paper, a new method for indoor robot Straight Line Movement via Navigator (SLMN) is proposed to ensure long linear motion of robot with an acceptable error threshold and realize multiroom navigation. Firstly, in a short time, robot runs a suitable distance when it is covered by navigatorâ€™s ultrasonic sensor. We can obtain a triangle with twice the distance between navigator and robot and the distance of robot motion. The forward angle of the robot can be conveniently obtained by the trigonometric functions. Comparing the robotâ€™s current angle with expected angle, the robot could correct itself and realize the indoor linear navigation. Secondly, discovering dozens of the magnitude gaps between the distance of robot run and the distance between navigator and robot, we propose an optimized method using approximate scaling which increases efficiency by nearly 70.8%. Finally, to realize multiroom navigation, we introduce the conception of the depth-first search stack and a unique encode rule on rooms and navigators. It is proved by extensive quantitative evaluations that the proposed method realizes indoor full coverage at a lower cost than other state-of-the-art indoor vision navigation schemes, such as ORB-SLAM.},
 author = {Zhu, Chaozheng and He, Ming and Chen, Pan and Sun, Kang and Wang, Jinglei and Huang, Qian},
 doi = {10.1155/2018/8419384},
 journal = {Mathematical Problems in Engineering},
 keywords = {},
 note = {http://downloads.hindawi.com/journals/mpe/2018/8419384.pdf},
 number = {},
 pages = {1-10},
 title = {Navigation for Indoor Robot: Straight Line Movement via Navigator},
 url = {https://app.dimensions.ai/details/publication/pub.1107216671},
 volume = {2018},
 year = {2018}
}

@article{pub.1107666963,
 abstract = {Joint simultaneous localization and mapping (SLAM) constitutes the basis for cooperative action in multiâ€robot teams. We designed a stereo visionâ€based 6D SLAM system combining local and global methods to benefit from their particular advantages: (1) Decoupled local reference filters on each robot for realâ€time, longâ€term stable state estimation required for stabilization, control and fast obstacle avoidance; (2) Online graph optimization with a novel graph topology and intraâ€ as well as interâ€robot loop closures through an improved submap matching method to provide global multiâ€robot pose and map estimates; (3) Distribution of the processing of highâ€frequency and highâ€bandwidth measurements enabling the exchange of aggregated and thus compacted map data. As a result, we gain robustness with respect to communication losses between robots. We evaluated our improved map matcher on simulated and realâ€world datasets and present our full system in five realâ€world multiâ€robot experiments in areas of up 3,000 m2 (bounding box), including visual robot detections and submap matches as loopâ€closure constraints. Further, we demonstrate its application to autonomous multiâ€robot exploration in a challenging roughâ€terrain environment at a Moonâ€analogue site located on a volcano.},
 author = {Schuster, Martin J. and Schmid, Korbinian and Brand, Christoph and Beetz, Michael},
 doi = {10.1002/rob.21812},
 journal = {Journal of Field Robotics},
 keywords = {},
 note = {https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rob.21812},
 number = {2},
 pages = {305-332},
 title = {Distributed stereo visionâ€based 6D localization and mapping for multiâ€robot teams},
 url = {https://app.dimensions.ai/details/publication/pub.1107666963},
 volume = {36},
 year = {2019}
}

@article{pub.1107736287,
 abstract = {In the classical context of map relative localization, map-matching (MM) is typically defined as the task of finding a rigid transformation (i.e., 3DOF rotation/translation on the 2D moving plane) that aligns two maps, the query and reference maps built by mobile robots. This definition is valid in loop-rich trajectories that enable a mapper robot to close many loops, for which precise maps can be assumed. The same cannot be said about the newly emerging vision only autonomous navigation systems, which typically operate in loop-less trajectories that have no loop (e.g., straight paths). In this paper, we address this limitation by merging the two maps. Our study is motivated by the observation that even when there is no loop in either the query or reference map, many loops can often be obtained in the merged map. We add two new aspects to MM: (1) map retrieval with compact and discriminative binary features powered by deep convolutional neural network (DCNN), which efficiently generates a small number of good initial alignment hypotheses; and (2) map merge, which jointly deforms the two maps to minimize differences in shape between them. A preemption scheme is introduced to avoid excessive evaluation of useless MM hypotheses. For experimental investigation, we created a novel collection of uncertain loop-less maps by utilizing the recently published North Campus Long-Term (NCLT) dataset and its ground-truth GPS data. The results obtained using these map collections confirm that our approach improves on previous MM approaches.},
 author = {Tanaka, Kanji},
 doi = {10.20965/jaciii.2018.p0915},
 journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
 keywords = {},
 note = {https://doi.org/10.20965/jaciii.2018.p0915},
 number = {6},
 pages = {915-923},
 title = {Deformable Map Matching to Handle Uncertain Loop-Less Maps},
 url = {https://app.dimensions.ai/details/publication/pub.1107736287},
 volume = {22},
 year = {2018}
}

@inproceedings{pub.1107773592,
 abstract = {We present a complete map management process for a visual localization system designed for multi-vehicle long-term operations in resource constrained outdoor environments. Outdoor visual localization generates large amounts of data that need to be incorporated into a lifelong visual map in order to allow localization at all times and under all appearance conditions. Processing these large quantities of data is non-trivial, as it is subject to limited computational and storage capabilities both on the vehicle and on the mapping backend. We address this problem with a two-fold map update paradigm capable of, either, adding new visual cues to the map, or updating co-observation statistics. The former, in combination with offline map summarization techniques, allows enhancing the appearance coverage of the lifelong map while keeping the map size limited. On the other hand, the latter is able to significantly boost the appearance-based landmark selection for efficient online localization without incurring any additional computational or storage burden. Our evaluation in challenging outdoor conditions shows that our proposed map management process allows building and maintaining maps for precise visual localization over long time spans in a tractable and scalable fashion.},
 author = {BÃ¼rki, Mathias and Dymczyk, Marcin and Gilitschenski, Igor and Cadena, Cesar and Siegwart, Roland and Nieto, Juan},
 booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},
 doi = {10.1109/ivs.2018.8500432},
 keywords = {},
 note = {http://arxiv.org/pdf/1808.02658},
 pages = {682-688},
 title = {Map Management for Efficient Long-Term Visual Localization in Outdoor Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1107773592},
 year = {2018}
}

@article{pub.1107868394,
 abstract = {Distributed as an openâ€source library since 2013, realâ€time appearanceâ€based mapping (RTABâ€Map) started as an appearanceâ€based loop closure detection approach with memory management to deal with largeâ€scale and longâ€term online operation. It then grew to implement simultaneous localization and mapping (SLAM) on various robots and mobile platforms. As each application brings its own set of constraints on sensors, processing capabilities, and locomotion, it raises the question of which SLAM approach is the most appropriate to use in terms of cost, accuracy, computation power, and ease of integration. Since most of SLAM approaches are either visualâ€ or lidarâ€based, comparison is difficult. Therefore, we decided to extend RTABâ€Map to support both visual and lidar SLAM, providing in one package a tool allowing users to implement and compare a variety of 3D and 2D solutions for a wide range of applications with different robots and sensors. This paper presents this extended version of RTABâ€Map and its use in comparing, both quantitatively and qualitatively, a large selection of popular realâ€world datasets (e.g., KITTI, EuRoC, TUM RGBâ€D, MIT Stata Center on PR2 robot), outlining strengths, and limitations of visual and lidar SLAM configurations from a practical perspective for autonomous navigation applications.},
 author = {LabbÃ©, Mathieu and Michaud, FranÃ§ois},
 doi = {10.1002/rob.21831},
 journal = {Journal of Field Robotics},
 keywords = {},
 number = {2},
 pages = {416-446},
 title = {RTABâ€Map as an openâ€source lidar and visual simultaneous localization and mapping library for largeâ€scale and longâ€term online operation},
 url = {https://app.dimensions.ai/details/publication/pub.1107868394},
 volume = {36},
 year = {2019}
}

@article{pub.1108004209,
 abstract = {Visual navigation is a key enabling technology for autonomous mobile vehicles. The ability to provide largeâ€scale, longâ€term navigation using lowâ€cost, lowâ€power vision sensors is appealing for industrial applications. A crucial requirement for longâ€term navigation systems is the ability to localize in environments whose appearance is constantly changing over timeâ€”due to lighting, weather, seasons, and physical changes. This paper presents a multiexperience localization (MEL) system that uses a powerful map representationâ€”storing every visual experience in layersâ€”that does not make assumptions about underlying appearance modalities and generators. Our localization system provides realâ€time performance by selecting online, a subset of experiences against which to localize. We achieve this task through a novel experienceâ€triage algorithm based on collaborative filtering, which selects experiences relevant to the live view, outperforming competing techniques. Based on classical memoryâ€based recommender systems, this technique also enables landmarkâ€level recommendations, is entirely online, and requires no training data. We demonstrate the capabilities of the MEL system in the context of longâ€term autonomous path following in unstructured outdoor environments with a challenging 100â€day field experiment through day, night, snow, spring, and summer. We furthermore provide offline analysis comparing our system to several stateâ€ofâ€theâ€art alternatives. We show that the combination of the novel methods presented in this paper enable full use of incredibly rich multiexperience maps, opening the door to robust longâ€term visual localization.},
 author = {MacTavish, Kirk and Paton, Michael and Barfoot, Timothy D.},
 doi = {10.1002/rob.21838},
 journal = {Journal of Field Robotics},
 keywords = {},
 number = {8},
 pages = {1265-1292},
 title = {Selective memory: Recalling relevant experience for longâ€term visual localization},
 url = {https://app.dimensions.ai/details/publication/pub.1108004209},
 volume = {35},
 year = {2018}
}

@article{pub.1109797701,
 abstract = {Accurate localization is an essential technology for flexible automation. Industrial applications require mobile platforms to be precisely localized in complex environments, often subject to continuous changes and reconfiguration. Most of the approaches use precomputed maps both for localization and for interfacing robots with workers and operators. This results in increased deployment time and costs as mapping experts are required to setup the robotic systems in factory facilities. Moreover, such maps need to be updated whenever significant changes in the environment occur in order to be usable within commanding tools. To overcome those limitations, in this work we present a robust and highly accurate method for long-term LiDAR-based indoor localization that uses CAD-based architectural floor plans. The system leverages a combination of graph-based mapping techniques and Bayes filtering to maintain a sparse and up-to-date globally consistent map that represents the latest configuration of the environment. This map is aligned to the CAD drawing using prior constraints and is exploited for relative localization, thus allowing the robot to estimate its current pose with respect to the global reference frame of the floor plan. Furthermore, the map helps in limiting the disturbances caused by structures and clutter not represented in the drawing. Several long-term experiments in changing real-world environments show that our system outperforms common state-of-the-art localization methods in terms of accuracy and robustness while remaining memory and computationally efficient.},
 author = {Boniardi, Federico and Caselitz, Tim and KÃ¼mmerle, Rainer and Burgard, Wolfram},
 doi = {10.1016/j.robot.2018.11.003},
 journal = {Robotics and Autonomous Systems},
 keywords = {},
 number = {},
 pages = {84-97},
 title = {A pose graph-based localization system for long-term navigation in CAD floor plans},
 url = {https://app.dimensions.ai/details/publication/pub.1109797701},
 volume = {112},
 year = {2019}
}

@inproceedings{pub.1110135080,
 abstract = {Simultaneous Localization and Mapping, commonly known as SLAM, has been an active research area in the field of Robotics over the past three decades. For solving the SLAM problem, every robot is equipped with either a single sensor or a combination of similar/different sensors. This paper attempts to review, discuss, evaluate and compare these sensors. Keeping an eye on future, this paper also assesses the characteristics of these sensors against factors critical to the long-term autonomy challenge.},
 author = {Zaffar, Mubariz and Ehsan, Shoaib and Stolkin, Rustam and Maier, Klaus McDonald},
 booktitle = {2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)},
 doi = {10.1109/ahs.2018.8541483},
 keywords = {},
 note = {http://repository.essex.ac.uk/27398/1/1807.01605v1.pdf},
 pages = {285-290},
 title = {Sensors, SLAM and Long-term Autonomy: A Review},
 url = {https://app.dimensions.ai/details/publication/pub.1110135080},
 year = {2018}
}

@inproceedings{pub.1110562196,
 abstract = {This study addresses the problem of visual change detection using a 3D point cloud (PC) map acquired by a car-like robot. With recent advances in long-term autonomous navigation, change detection under global viewpoint uncertainty has become a topic of considerable interest. In our study, we extend the traditional two-level pipeline of change detection: (1) scene registration and (2) scene comparison, to enable scalable and efficient change detection. In the traditional pipeline, the registration stage is required to align a given scene pair (i.e., query and reference PC maps) that are taken at different times into the same coordinate system, before comparing the two PCs. However, the registration stage is a time-consuming step, which makes it harder to realize a scalable change detection. Our key concept is to transform every query or reference PC beforehand into an invariant coordinate system, which should be predefined and invariant to environment changes (e.g., dynamic objects, clutters, the mapper vehicle's trajectories), so as to enable a direct comparison of spatial layout between the two different maps. The proposed framework employs an efficient bag-of-local-features (BoLF) scene model and realizes a scalable joint viewpoint-change detection. Change detection experiments using a publicly available cross-season NCLT dataset validate the efficacy of the approach.},
 author = {Yoshiki, Takahashi and Kanji, Tanaka and Naiming, Yang},
 booktitle = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
 doi = {10.1109/itsc.2018.8569294},
 keywords = {},
 pages = {1115-1121},
 title = {Scalable Change Detection from 3D Point Cloud Maps: Invariant Map Coordinate for Joint Viewpoint-Change Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1110562196},
 year = {2018}
}

@inproceedings{pub.1110562218,
 abstract = {The precise localization of vehicles is an important requirement for autonomous driving or advanced driver assistance systems. Using common GNSS the ego position can be measured but not with the reliability and precision necessary. An alternative approach to achieve precise localization is the usage of visual landmarks observed by a camera mounted in the vehicle. However, this raises the necessity of reliable visual landmarks that are easily recognizable and persistent. We propose a novel SLAM algorithm that focuses on learning and mapping such visual long-term landmarks (LLamas). The algorithm therefore processes stereo image streams from several recording sessions in the same spatial area. The key part within LLama-SLAM is the assessment of the landmarks with quality values that are inferred as viewpoint dependent probabilities from observation statistics. By adding solely landmarks of high quality to the final LLama Map, it can be kept compact while still allowing reliable localization. Due to the long-term evaluation of the GNSS measurement during the sessions, the landmarks can be positioned precisely in a global referenced coordinate system. For a first assessment of the algorithm's capabilities, we present some experimental results from the mapping process combining three sessions recorded over two months on the same route.},
 author = {Luthardt, Stefan and Willert, Volker and Adamy, JÃ¼rgen},
 booktitle = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
 doi = {10.1109/itsc.2018.8569323},
 keywords = {},
 note = {http://tuprints.ulb.tu-darmstadt.de/8357/1/Luthardt_ITSC_2018_LLama.pdf},
 pages = {2645-2652},
 title = {LLama-SLAM: Learning High-Quality Visual Landmarks for Long-Term Mapping and Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1110562218},
 year = {2018}
}

@inproceedings{pub.1111257582,
 abstract = {This paper presents a 2D B-spline mapping framework for representing unstructured environments in a compact manner. While occupancy-grid and landmark-based maps have been successfully employed by the robotics community in indoor scenarios, outdoor long-term autonomous operations require a more compact representation of the environment. This work tackles this problem by interpolating the data of a high frequency sensor using B-spline curves. Compared to lines and circles, splines are more powerful in the sense that they allow for the description of more complex shapes in the scene. In this work, spline curves are continuously tracked and aligned across multiple sensor readings using lightweight methods, making the proposed framework suitable for robot navigation in outdoor missions. In particular, a Simultaneous Localization and Mapping (SLAM) algorithm specifically tailored for B-spline maps is presented here. The efficacy of the proposed framework is demonstrated by Software-in-the-Loop (SiL) simulations in different scenarios.},
 author = {Rodrigues, RÃ´mulo T. and Aguiar, A. Pedro and Pascoal, AntÃ³nio},
 booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2018.8594456},
 keywords = {},
 pages = {3204-3209},
 title = {A B-Spline Mapping Framework for Long-Term Autonomous Operations},
 url = {https://app.dimensions.ai/details/publication/pub.1111257582},
 year = {2018}
}

@inproceedings{pub.1111261745,
 abstract = {Place recognition is one of the major challenges for the LiDAR-based effective localization and mapping task. Traditional methods are usually relying on geometry matching to achieve place recognition, where a global geometry map need to be restored. In this paper, we accomplish the place recognition task based on an end-to-end feature learning framework with the LiDAR inputs. This method consists of two core modules, a dynamic octree mapping module that generates local 2D maps with the consideration of the robot's motion; and an unsupervised place feature learning module which is an improved adversarial feature learning network with additional assistance for the long-term place recognition requirement. More specially, in place feature learning, we present an additional Generative Adversarial Network with a designed Conditional Entropy Reduction module to stabilize the feature learning process in an unsupervised manner. We evaluate the proposed method on the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental results show that the proposed method outperforms state-of-the-art in place recognition tasks under long-term applications. What's more, the feature size and inference efficiency in the proposed method are applicable in real-time performance on practical robotic platforms.},
 author = {Yin, Peng and Xu, Lingyun and Liu, Zhe and Li, Lu and Salman, Hadi and He, Yuqing and Xu, Weiliang and Wang, Hesheng and Choset, Howie},
 booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2018.8593562},
 keywords = {},
 pages = {1162-1167},
 title = {Stabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition},
 url = {https://app.dimensions.ai/details/publication/pub.1111261745},
 year = {2018}
}

@inproceedings{pub.1111263208,
 abstract = {Reliable long-term localization is key for robotic systems in dynamic environments. In this paper, we propose a novel approach for long-term localization using 3D LiDARs, coined PoseMap. In essence, we extract distinctive features from range measurements and bundle these into local views along with observation poses. The sensor's trajectory is then estimated in a sliding window fashion by matching current and old features and minimizing the distances in-between. The map representation facilitates finding a suitable set of old features, by selecting the closest local map(s) for matching. Similarly to a visibility analysis, this procedure provides a suitable set of features for localization but at a fraction of the computational cost. PoseMap also allows for updates and extensions of the map at any time by replacing and adding local maps when necessary. We evaluate our approach using two platforms both equipped with a 3D LiDAR and an IMU, demonstrating localization at 8 Hz and robustness to changes in the environment such as moving vehicles and changing vegetation. PoseMap was implemented on an autonomous vehicle allowing it to drive autonomously over a period of 18 months through a mix of industrial and unstructured off-road environments, covering more than 100 kms without a single localization failure.},
 author = {Egger, Philipp and Borges, Paulo V K and Catt, Gavin and Pfrunder, Andreas and Siegwart, Roland and DubÃ©, Renaud},
 booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2018.8593854},
 keywords = {},
 pages = {3430-3437},
 title = {PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1111263208},
 year = {2018}
}

@inproceedings{pub.1111267670,
 abstract = {As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.},
 author = {LÃ¡zaro, MarÃ­a T. and Capobianco, Roberto and Grisetti, Giorgio},
 booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2018.8594310},
 keywords = {},
 pages = {153-160},
 title = {Efficient Long-term Mapping in Dynamic Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1111267670},
 year = {2018}
}

@inproceedings{pub.1111268130,
 abstract = {To enable robots to work on real home environments, they have to not only consider common knowledge in the global society, but also be aware of existing rules there. Since such â€œlocal rulesâ€ are not describable beforehand, robot agents must acquire them through their lives after deployment. To achieve this, we developed a framework that a) lets robots record long-term episodic memories in their deployed environments, b) autonomously builds probabilistic object localization map as structurization of logged data and c) make adapted task plans based on the map. We equipped our framework on PR2 and Fetch robots operating and recording episodic memory for 41 days with semantic common knowledge of the environment. We also conducted demonstrations in which a PR2 robot tidied up a room, showing that the robot agent can successfully plan and execute local-rule-aware home assistive tasks by using our proposed framework.},
 author = {Furuta, Yuki and Okada, Kei and Kakiuchi, Yohei and Inaba, Masayuki},
 booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros.2018.8594481},
 keywords = {},
 pages = {1-7},
 title = {An Everyday Robotic System that Maintains Local Rules Using Semantic Map Based on Long-Term Episodic Memory},
 url = {https://app.dimensions.ai/details/publication/pub.1111268130},
 year = {2018}
}

@inproceedings{pub.1111644599,
 abstract = {Long-term localization in dynamic changing environments is still a challenge in robotics. Traditional localization algorithms typically assume that the environment is static. However, in many real-world applications, such as parking lots and industrial plants, there are always dynamic objects (e.g. moving people) and semi-dynamic objects (e.g. parked cars and placed goods). In this paper we address this challenge by introducing a long-term localization algorithm in the environments which combine dynamic objects and semi-dynamic objects. Localizability-based-updating particle filter (LU-P F) algorithm is proposed here. Not only we use localizability matric to build an updating mechanism, but also it is used for localization system. Besides, we propose the dynamic factor as long-memory information to serve as prior knowledge, which improves the robustness of updating process. Experiments in parking lots demonstrate that our approach has better localization results with a more accurate up-to-date map compared to other methods.},
 author = {Hu, Xiaowei and Wang, Jingchuan and Chen, Weidong},
 booktitle = {2018 Chinese Automation Congress (CAC)},
 doi = {10.1109/cac.2018.8623046},
 keywords = {},
 pages = {384-389},
 title = {Long-term Localization of Mobile Robots in Dynamic Changing Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1111644599},
 year = {2018}
}

@inproceedings{pub.1111912992,
 abstract = {This paper presents a novel method which fuse visual, IMU and GPS tightly to realize high-precision real-time localization and mapping simultaneously (SLAM). Our method is based on the bundle adjustment (BA). The confidence of the GPS signal is used to determine the window size in the local mapping thread and judge whether the keyframe is reliable. The long-term unreliable keyframe linking with large uncertainty of GPS which called GPS-restricted or GPS-denied situation will cause the drift when mapping. To eliminate the drift, in contrast to use the closed-loop detection and global optimization which will increase the computational burden extremely with the size of the map enlarged, a semi-global optimization method is proposed to relieve the burden, which make the localization estimated by this method possible to be used to navigate for unmanned vehicles. In our method, the confidence of the GPS signal is significantly important, however, the covariance supplied by the GPS receiver may not be trustworthy sometimes, which cause some unnecessary mistake when optimizing, thus a semi-supervised clustering method taking the information of GPS and IMU into account synthetically is introduced to get that confidence more robustly.},
 author = {Zhang, Zhongyuan and Wang, Hesheng and Chen, Weidong},
 booktitle = {2018 13th World Congress on Intelligent Control and Automation (WCICA)},
 doi = {10.1109/wcica.2018.8630513},
 keywords = {},
 pages = {1397-1402},
 title = {A real-time visual-inertial mapping and localization method by fusing unstable GPS},
 url = {https://app.dimensions.ai/details/publication/pub.1111912992},
 year = {2018}
}

@article{pub.1111913247,
 abstract = {In this letter, we present a long-term localization method that effectively exploits the structural information of an environment via an image format. The proposed method presents a robust year-round localization performance even when learned in just a single day. The proposed localizer learns a point cloud descriptor, named Scan Context Image (SCI), and performs robot localization on a grid map by formulating the place recognition problem as place classification using a convolutional neural network. Our method is faster than existing methods proposed for place recognition because it avoids a pairwise comparison between a query and scans in a database. In addition, we provide thorough validations using publicly available long-term datasets, the NCLT dataset and the Oxford RobotCar dataset, and show that the Scan Context Image (SCI) localization attains consistent performance over a year and outperforms existing methods.},
 author = {Kim, Giseop and Park, Byungjae and Kim, Ayoung},
 doi = {10.1109/lra.2019.2897340},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 number = {2},
 pages = {1948-1955},
 title = {1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using Scan Context Image},
 url = {https://app.dimensions.ai/details/publication/pub.1111913247},
 volume = {4},
 year = {2019}
}

@inproceedings{pub.1112655665,
 abstract = {Simultaneous localization and mapping (SLAM) estimates an agentâ€™s trajectory for all six degrees of freedom (6 DoF) and constructs a 3D map of an unknown surrounding. It is a fundamental kernel that enables head-mounted augmented/virtual reality devices and autonomous navigation of micro aerial vehicles. A noticeable recent trend in visual SLAM is to apply computation- and memory-intensive convolutional neural networks (CNNs) that outperform traditional hand-designed feature-based methods [1]. For each video frame, CNN-extracted features are matched with stored keypoints to estimate the agentâ€™s 6-DoF pose by solving a perspective-n-points (PnP) non-linear optimization problem (Fig. 7.3.1, left). The agentâ€™s long-term trajectory over multiple frames is refined by a bundle adjustment process (BA, Fig. 7.3.1 right), which involves a large-scale ($\sim$120 variables) non-linear optimization. Visual SLAM requires massive computation ($\gt250$ GOP/s) in the CNN-based feature extraction and matching, as well as data-dependent dynamic memory access and control flow with high-precision operations, creating significant low-power design challenges. Software implementations are impractical, resulting in 0.2s runtime with a $\sim$3 GHz CPU + GPU system with $\gt100$ MB memory footprint and $\gt100$ W power consumption. Prior ASICs have implemented either an incomplete SLAM system [2, 3] that lacks estimation of ego-motion or employed a simplified (non-CNN) feature extraction and tracking [2, 4, 5] that limits SLAM quality and range. A recent ASIC [5] augments visual SLAM with an off-chip high-precision inertial measurement unit (IMU), simplifying the computational complexity, but incurring additional power and cost overhead.},
 author = {Li, Ziyun and Chen, Yu and Gong, Luyao and Liu, Lu and Sylvester, Dennis and Blaauw, David and Kim, Hun-Seok},
 booktitle = {2019 IEEE International Solid- State Circuits Conference - (ISSCC)},
 doi = {10.1109/isscc.2019.8662397},
 keywords = {},
 pages = {134-136},
 title = {An 879GOPS 243mW 80fps VGA Fully Visual CNN-SLAM Processor for Wide-Range Autonomous Exploration},
 url = {https://app.dimensions.ai/details/publication/pub.1112655665},
 year = {2019}
}

@inbook{pub.1112814120,
 abstract = {Abstract
This paper presents an outreach program aimed to introduce high school students to the basic concepts of intelligent robotics, internet of things (IoT), digital simulation, and programming in Robot Operating System (ROS). The challenge of teaching the emerging technologies to school students was addressed through collaboration of engineering educators from Technion and MIT and professional engineers from PTC Corp. The participants were 15 high school students majoring in computer science. The program included three parts studied at the Technion. In the robotics course, the students constructed and programmed mobile robotic arms to pick and place objects of unknown weight and communicate via the IoT platform ThingWorx. In the second course, the school students programmed a TurtleBot Waffle robot equipped by LIDAR sensor to navigate in an environment by means of simultaneousÂ localizationÂ and mapping. In the mini project the students programmed two mobile robotic arms and the Waffle to implement a warehouse scenario, in which the robots jointly carried out loading, transportation and warehousing of a cargo. Assessment results indicated that the students acquired initial understanding of the studied concepts. The program raised their interest in intelligent robotics and awareness about technological and social changes associated with Industry 4.0.},
 author = {Verner, Igor and Cuperman, Dan and Romm, Tal and Reitman, Michael and Chong, Shi Kai and Gong, Zoe},
 booktitle = {The Challenges of the Digital Transformation in Education},
 doi = {10.1007/978-3-030-11932-4_76},
 keywords = {},
 pages = {824-832},
 publisher = {},
 title = {Intelligent Robotics in High School: An Educational Paradigm for the Industry 4.0 Era},
 url = {https://app.dimensions.ai/details/publication/pub.1112814120},
 year = {2020}
}

@inproceedings{pub.1113127372,
 abstract = {Most autonomous vehicles rely on some kind of map for localization or navigation. Outdated maps however are a risk to the performance of any map-based localization system applied in autonomous vehicles. It is necessary to update the used maps to ensure stable and long-term operation. We address the problem of computing landmark updates live in the vehicle, which requires efficient use of the computational resources. In particular, we employ a graph-based sliding window approach for simultaneous localization and incremental map refinement. We propose a novel method that approximates sliding window marginalization without inducing fill-in. Our method maintains the exact same sparsity pattern as without performing marginalization, but simultaneously improves the landmark estimates. The main novelty of this work is the derivation of sparse global priors that approximate dense marginalization. In comparison to state-of-the-art work, our approach utilizes global instead of local linearization points, but still minimizes linearization errors. We first approximate marginalization via Kullback-Leibler divergence and then recalculate the mean to compensate linearization errors. We evaluate our approach on simulated and real data from a prototype vehicle and compare our approach to state-of-the-art sliding window marginalization.},
 author = {Wilbers, Daniel and Rumberg, Lars and Stachniss, Cyrill},
 booktitle = {2019 Third IEEE International Conference on Robotic Computing (IRC)},
 doi = {10.1109/irc.2019.00013},
 keywords = {},
 pages = {25-31},
 title = {Approximating Marginalization with Sparse Global Priors for Sliding Window SLAM-Graphs},
 url = {https://app.dimensions.ai/details/publication/pub.1113127372},
 year = {2019}
}

@inproceedings{pub.1113228538,
 abstract = {In recent times, a number of reference implementations of Simultaneous Localization and Mapping (SLAM) and navigation techniques have been made publicly available via the ROS Community. Several implementations have transitioned to commercial products (vacuum robots, drones, warehouse robots, etc.). However, in such cases, they are specialized and optimized for their specific domains of deployment. In particular, their success criteria have been based primarily on mission completion and safety of humans around them. In this light, deployment in any new operational design domain (ODD) requires at least a careful verification of performance and often re-optimization. We seek the technological gaps that need to be addressed to ensure the mobile robots are fit for automotive manufacturing environments. Automotive final assembly environments pose significant additional challenges for mobile robot deployment. They are replete with relatively unstructured tasks with significant uncertainty, involve tasks with skills that require robots to work in collaboration with humans and are time sensitive. Currently, metrics for evaluating mobile robot functionalities have been based on accuracy, functionality and resource consumption. In addition to these, automotive assembly also requires consistency in execution times. This work evaluates the navigational capabilities of mobile robots in environments with static objects for time consistency as required by an automotive assembly process. The evaluation uses ASTM F3244-17 standard test method. It is performed on a simulated robot in Gazebo environment and Clearpath OTTO1500 robot in a laboratory environment.},
 author = {Singh Gill, Jasprit and Tomaszewski, Mark and Jia, Yunyi and Pisu, Pierluigi and Krovi, Venkat N},
 booktitle = {SAE Technical Paper Series},
 doi = {10.4271/2019-01-0505},
 keywords = {},
 pages = {},
 title = {Evaluation of Navigation in Mobile Robots for Long-Term Autonomy in Automotive Manufacturing Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1113228538},
 year = {2019}
}

@inproceedings{pub.1113541223,
 abstract = {In this paper, we present iDVO (inertia-embedded deep visual odometry), a self-supervised learning based monocular visual odometry (VO) for road vehicles. When modelling the geometric consistency within adjacent frames, most deep VO methods ignore the temporal continuity of the camera pose, which results in a very severe jagged fluctuation in the velocity curves. With the observation that road vehicles tend to perform smooth dynamic characteristics in most of the time, we design the inertia loss function to describe the abnormal motion variation, which assists the model to learn the consecutiveness from long-term camera ego-motion. Based on the recurrent convolutional neural network (RCNN) architecture, our method implicitly models the dynamics of road vehicles and the temporal consecutiveness by the extended Long Short-Term Memory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to handle the non-consistency in fast camera motion by blocking the boundary part and which generates more efficiency in the whole non-consistency mask. The proposed method is evaluated on the KITTI dataset, and the results demonstrate state-of-the-art performance with respect to other monocular deep VO and SLAM approaches.},
 author = {Wang, Chengze and Yuan, Yuan and Wang, Qi},
 booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 doi = {10.1109/icassp.2019.8683446},
 keywords = {},
 note = {http://arxiv.org/pdf/1905.01634},
 pages = {2252-2256},
 title = {Learning by Inertia: Self-supervised Monocular Visual Odometry for Road Vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1113541223},
 year = {2019}
}

@article{pub.1113609918,
 abstract = {Intelligent mobile robots need self-localization, map generation, and the ability to explore unknown environments autonomously. Probabilistic processing can be applied to overcome the problems of movement uncertainties and measurement errors. Probabilistic robotics and simultaneous localization and mapping (SLAM) technologies are therefore strongly related, and they have been the focus of many studies. As more and more practical applications are found for intelligent mobile robots, such as for autonomous driving and cleaning, the applicability of these techniques has been increasing.
                  In this special issue, we provide a wide variety of very interesting papers ranging from studies and developments in applied SLAM technologies to fundamental theories for SLAM. There are five academic papers, one each on the following topics: first visit navigation, controls for following rescue clues, indoor localization using magnetic field maps, a new solution for self-localization using downhill simplex method, and object detection for long-term map management through image-based learning. In addition, in the next number, there will be a review paper by Tsukuba Universityâ€™s Prof. Tsubouchi, who is famous for the Tsukuba Challenge and research related to mobile robotics.
                  We editors hope this special issue will help readers to develop mobile robots and use SLAM technologies and probabilistic approaches to produce successful applications.},
 author = {Watanabe, Keigo and Maeyama, Shoichi and Tomizawa, Tetsuo and Ueda, Ryuichi and Tomono, Masahiro},
 doi = {10.20965/jrm.2019.p0179},
 journal = {Journal of Robotics and Mechatronics},
 keywords = {},
 note = {https://doi.org/10.20965/jrm.2019.p0179},
 number = {2},
 pages = {179-179},
 title = {Special Issue on Probabilistic Robotics and SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1113609918},
 volume = {31},
 year = {2019}
}

@article{pub.1113680804,
 abstract = {We present a new approach to assisted intrinsic and extrinsic calibration with an observability-aware visualâ€“inertial calibration system that guides the user through the calibration procedure by suggesting easy-to-perform motions that render the calibration parameters observable. This is done by identifying which subset of the parameter space is rendered observable with a rank-revealing decomposition of the Fisher information matrix, modeling calibration as a Markov decision process and using reinforcement learning to establish which discrete sequence of motions optimizes for the regression of the desired parameters. The goal is to address the assumption common to most calibration solutions: that sufficiently informative motions are provided by the operator. We do not make use of a process model and instead leverage an experience-based approach that is broadly applicable to any platform in the context of simultaneous localization and mapping. This is a step in the direction of long-term autonomy and â€œpower-on-and-goâ€ robotic systems, making repeatable and reliable calibration accessible to the non-expert operator.},
 author = {Nobre, Fernando and Heckman, Christoffer},
 doi = {10.1177/0278364919844824},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 note = {https://journals.sagepub.com/doi/pdf/10.1177/0278364919844824},
 number = {12-13},
 pages = {1388-1402},
 title = {Learning to calibrate: Reinforcement learning for guided calibration of visualâ€“inertial rigs},
 url = {https://app.dimensions.ai/details/publication/pub.1113680804},
 volume = {38},
 year = {2019}
}

@article{pub.1113714366,
 abstract = {This work introduces a new complete Simultaneous Localization and Mapping (SLAM) framework that uses an enriched representation of the world based on sensor fusion and is able to simultaneously provide an accurate localization of the vehicle. A method to create an Evidential grid representation from two very different sensors, laser scanner and stereo camera, allows a better handling of the dynamic aspects of the urban environment and a proper management of errors to create a more reliable map, thus having a more precise localization. A life-long layer with high level states is presented, it maintains a global map of the entire vehicleâ€™s trajectory and distinguishes between static and dynamic obstacles. Finally, we propose a method that at each current map creation estimates the vehicleâ€™s position by a grid matching algorithm based on image registration techniques. Results on a real road dataset show that the environment mapping data can be improved by adding relevant information that could be missed without the proposed approach. Moreover, the proposed localization method is able to reduce the drift and improve the localization compared to other methods using similar configurations.},
 author = {Valente, Michelle and Joly, Cyril and de La Fortelle, Arnaud},
 doi = {10.1142/s2301385019410012},
 journal = {Unmanned Systems},
 keywords = {},
 number = {03},
 pages = {149-159},
 title = {Evidential SLAM Fusing 2D Laser Scanner and Stereo Camera},
 url = {https://app.dimensions.ai/details/publication/pub.1113714366},
 volume = {07},
 year = {2019}
}

@article{pub.1117021385,
 abstract = {Global localization in 3D point clouds is a challenging task for mobile vehicles in outdoor scenarios, which requires the vehicle to localize itself correctly in a given map without prior knowledge of its pose. This is a critical component of autonomous vehicles or robots on the road for handling localization failures. In this paper, based on reduced dimension scan representations learned from neural networks, a solution to global localization is proposed by achieving place recognition first and then metric pose estimation in the global prior map. Specifically, we present a semi-handcrafted feature learning method for 3D Light detection and ranging (LiDAR) point clouds using artificial statistics and siamese network, which transforms the place recognition problem into a similarity modeling problem. Additionally, the sensor data using dimension reduced representations require less storage space and make the searching easier. With the learned representations by networks and the global poses, a prior map is built and used in the localization framework. In the localization step, position only observations obtained by place recognition are used in a particle filter algorithm to achieve precise pose estimation. To demonstrate the effectiveness of our place recognition and localization approach, KITTI benchmark and our multi-session datasets are employed for comparison with other geometric-based algorithms. The results show that our system can achieve both high accuracy and efficiency for long-term autonomy.},
 author = {Yin, Huan and Wang, Yue and Ding, Xiaqing and Tang, Li and Huang, Shoudong and Xiong, Rong},
 doi = {10.1109/tits.2019.2905046},
 journal = {IEEE Transactions on Intelligent Transportation Systems},
 keywords = {},
 number = {4},
 pages = {1380-1392},
 title = {3D LiDAR-Based Global Localization Using Siamese Neural Network},
 url = {https://app.dimensions.ai/details/publication/pub.1117021385},
 volume = {21},
 year = {2020}
}

@article{pub.1117322189,
 abstract = {Visual localization in outdoor environments is subject to varying appearance conditions rendering it difficult to match current camera images against a previously recorded map. Although it is possible to extend the respective maps to allow precise localization across a wide range of differing appearance conditions, these maps quickly grow in size and become impractical to handle on a mobile robotic platform. To address this problem, we present a landmark selection algorithm that exploits appearance coâ€observability for efficient visual localization in outdoor environments. Based on the appearance condition inferred from recently observed landmarks, a small fraction of landmarks useful under the current appearance condition is selected and used for localization. This allows to greatly reduce the bandwidth consumption between the mobile platform and a map backend in a sharedâ€map scenario, and significantly lowers the demands on the computational resources on said mobile platform. We derive a landmark ranking function that exhibits high performance under vastly changing appearance conditions and is agnostic to the distribution of landmarks across the different map sessions. Furthermore, we relate and compare our proposed appearanceâ€based landmark ranking function to popular ranking schemes from information retrieval, and validate our results on the challenging University of Michigan North Campus longâ€term vision and LIDAR data sets (NCLT), including an evaluation of the localization accuracy using groundâ€truth poses. In addition to that, we investigate the computational and bandwidth resource demands. Our results show that by selecting 20â€“30% of landmarks using our proposed approach, a similar localization performance as the baseline strategy using all landmarks is achieved.},
 author = {BÃ¼rki, Mathias and Cadena, Cesar and Gilitschenski, Igor and Siegwart, Roland and Nieto, Juan},
 doi = {10.1002/rob.21870},
 journal = {Journal of Field Robotics},
 keywords = {},
 number = {6},
 pages = {1041-1073},
 title = {Appearanceâ€based landmark selection for visual localization},
 url = {https://app.dimensions.ai/details/publication/pub.1117322189},
 volume = {36},
 year = {2019}
}

@article{pub.1117696058,
 abstract = {Today, and possibly for a long time to come, the full driving task is too complex an activity to be fully formalized as a sensing-acting robotics system that can be explicitly solved through model-based and learning-based approaches in order to achieve full unconstrained vehicle autonomy. Localization, mapping, scene perception, vehicle control, trajectory optimization, and higher-level planning decisions associated with autonomous vehicle development remain full of open challenges. This is especially true for unconstrained, real-world operation where the margin of allowable error is extremely small and the number of edge-cases is extremely large. Until these problems are solved, human beings will remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0% to just under 100% of the driving. The governing objectives of the MIT Advanced Vehicle Technology (MIT-AVT) study are to 1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning-based internal and external perception systems; 2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology; and 3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium-term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for the analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, and CAN messages, and high-definition video streams of the driverâ€™s face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15610 days of participation, 511638 mi, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.},
 author = {Fridman, Lex and Brown, Daniel E. and Glazer, Michael and Angell, William and Dodd, Spencer and Jenik, Benedikt and Terwilliger, Jack and Patsekin, Aleksandr and Kindelsberger, Julia and Ding, Li and Seaman, Sean and Mehler, Alea and Sipperley, Andrew and Pettinato, Anthony and Seppelt, Bobbie D. and Angell, Linda and Mehler, Bruce and Reimer, Bryan},
 doi = {10.1109/access.2019.2926040},
 journal = {IEEE Access},
 keywords = {},
 note = {https://ieeexplore.ieee.org/ielx7/6287639/8600701/08751968.pdf},
 number = {},
 pages = {102021-102038},
 title = {MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving Study of Driver Behavior and Interaction With Automation},
 url = {https://app.dimensions.ai/details/publication/pub.1117696058},
 volume = {7},
 year = {2019}
}

@article{pub.1117809173,
 abstract = {Although topographic mapping missions and geological surveys carried out by Autonomous Underwater Vehicles (AUVs) are becoming increasingly prevalent, the lack of precise navigation in these scenarios still limits their application. This paper deals with the problems of long-term underwater navigation for AUVs and provides new mapping techniques by developing a Bathymetric Simultaneous Localisation And Mapping (BSLAM) method based on graph SLAM technology. To considerably reduce the calculation cost, the trajectory of the AUV is divided into various submaps based on Differences of Normals (DoN). Loop closures between submaps are obtained by terrain matching; meanwhile, maximum likelihood terrain estimation is also introduced to build weak data association within the submap. Assisted by one weight voting method for loop closures, the global and local trajectory corrections work together to provide an accurate navigation solution for AUVs with weak data association and inaccurate loop closures. The viability, accuracy and real-time performance of the proposed algorithm are verified with data collected onboard, including an 8 km planned track recorded at a speed of 4 knots in Qingdao, China.},
 author = {Ma, Teng and Li, Ye and Gong, Yusen and Wang, Rupeng and Sheng, Mingwei and Zhang, Qiang},
 doi = {10.1017/s0373463319000286},
 journal = {Journal of Navigation},
 keywords = {},
 number = {6},
 pages = {1602-1622},
 title = {AUV Bathymetric Simultaneous Localisation and Mapping Using Graph Method},
 url = {https://app.dimensions.ai/details/publication/pub.1117809173},
 volume = {72},
 year = {2019}
}

@inproceedings{pub.1117925717,
 abstract = {Simultaneous Localisation and Mapping (SLAM) algorithms are expensive to run on smaller robotic platforms such as Micro-Aerial Vehicles. Bug algorithms are an alternative that use relatively little processing power, and avoid high memory consumption by not building an explicit map of the environment. In this work we explore the performance of Neuroevolution - specifically NEAT - at evolving control policies for simulated differential drive robots carrying out generalised maze navigation. We compare this performance with respect to one particular bug algorithm known as I-Bug. We extend NEAT to include Gated Recurrent Units (GRUs) to help deal with long term dependencies. We show that both NEAT and our NEAT-GRU can repeatably generate controllers that outperform I-Bug on a test set of 209 indoor maze like environments. We show that NEAT-GRU is superior to NEAT in this task. Moreover, we show that out of the 2 systems, only NEAT-GRU can continuously evolve successful controllers for a much harder task in which no bearing information about the target is provided to the agent.},
 author = {Butterworth, James and Savani, Rahul and Tuyls, Karl},
 booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
 doi = {10.1145/3319619.3321995},
 keywords = {},
 note = {http://arxiv.org/pdf/1904.06239},
 pages = {111-112},
 title = {Evolving indoor navigational strategies using gated recurrent units in NEAT},
 url = {https://app.dimensions.ai/details/publication/pub.1117925717},
 year = {2019}
}

@article{pub.1118095104,
 abstract = {Recent trends have accelerated the development of spatial applications on mobile devices and robots. These include navigation, augmented reality, humanâ€“robot interaction, and others. A key enabling technology for such applications is the understanding of the deviceâ€™s location and the map of the surrounding environment. This generic problem, referred to as Simultaneous Localization and Mapping (SLAM), is an extensively researched topic in robotics. However, visual SLAM algorithms face several challenges including perceptual aliasing and high computational cost. These challenges affect the accuracy, efficiency, and viability of visual SLAM algorithms, especially for long-term SLAM, and their use in resource-constrained mobile devices. A parallel trend is the ubiquity of Wi-Fi routers for quick Internet access in most urban environments. Most robots and mobile devices are equipped with a Wi-Fi radio as well. We propose a method to utilize Wi-Fi received signal strength to alleviate the challenges faced by visual SLAM algorithms. To demonstrate the utility of this idea, this work makes the following contributions: (i) We propose a generic way to integrate Wi-Fi sensing into visual SLAM algorithms, (ii) We integrate such sensing into three well-known SLAM algorithms, (iii) Using four distinct datasets, we demonstrate the performance of such augmentation in comparison to the original visual algorithms and (iv) We compare our work to Wi-Fi augmented FABMAP algorithm. Overall, we show that our approach can improve the accuracy of visual SLAM algorithms by 11% on average and reduce computation time on average by 15% to 25%.},
 author = {Hashemifar, Zakieh S. and Adhivarahan, Charuvahan and Balakrishnan, Anand and Dantu, Karthik},
 doi = {10.1007/s10514-019-09874-z},
 journal = {Autonomous Robots},
 keywords = {},
 note = {http://arxiv.org/pdf/1903.06687},
 number = {8},
 pages = {2245-2260},
 title = {Augmenting visual SLAM with Wi-Fi sensing for indoor applications},
 url = {https://app.dimensions.ai/details/publication/pub.1118095104},
 volume = {43},
 year = {2019}
}

@article{pub.1118360505,
 abstract = {Active SLAM is the task of actively planning robot paths while simultaneously
building a map and localizing within. Existing work has focused on planning
paths with occupancy grid maps, which do not scale well and suffer from long
term drift. This work proposes a Topological Feature Graph (TFG) representation
that scales well and develops an active SLAM algorithm with it. The TFG uses
graphical models, which utilize independences between variables, and enables a
unified quantification of exploration and exploitation gains with a single
entropy metric. Hence, it facilitates a natural and principled balance between
map exploration and refinement. A probabilistic roadmap path-planner is used to
generate robot paths in real time. Experimental results demonstrate that the
proposed approach achieves better accuracy than a standard grid-map based
approach while requiring orders of magnitude less computation and memory
resources.},
 author = {Mu, Beipeng and Giamou, Matthew and Paull, Liam and Agha-mohammadi, Ali-akbar and Leonard, John and How, Jonathan},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Information-based Active SLAM via Topological Feature Graphs},
 url = {https://app.dimensions.ai/details/publication/pub.1118360505},
 volume = {},
 year = {2015}
}

@article{pub.1118374769,
 abstract = {Place recognition is one of the most challenging problems in computer vision,
and has become a key part in mobile robotics and autonomous driving
applications for performing loop closure in visual SLAM systems. Moreover, the
difficulty of recognizing a revisited location increases with appearance
changes caused, for instance, by weather or illumination variations, which
hinders the long-term application of such algorithms in real environments. In
this paper we present a convolutional neural network (CNN), trained for the
first time with the purpose of recognizing revisited locations under severe
appearance changes, which maps images to a low dimensional space where
Euclidean distances represent place dissimilarity. In order for the network to
learn the desired invariances, we train it with triplets of images selected
from datasets which present a challenging variability in visual appearance. The
triplets are selected in such way that two samples are from the same location
and the third one is taken from a different place. We validate our system
through extensive experimentation, where we demonstrate better performance than
state-of-art algorithms in a number of popular datasets.},
 author = {Gomez-Ojeda, Ruben and Lopez-Antequera, Manuel and Petkov, Nicolai and Gonzalez-Jimenez, Javier},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Training a Convolutional Neural Network for Appearance-Invariant Place
  Recognition},
 url = {https://app.dimensions.ai/details/publication/pub.1118374769},
 volume = {},
 year = {2015}
}

@article{pub.1118579337,
 abstract = {In the classical context of robotic mapping and localization, map matching is
typically defined as the task of finding a rigid transformation (i.e., 3DOF
rotation/translation on the 2D moving plane) that aligns the query and
reference maps built by mobile robots. This definition is valid in loop-rich
trajectories that enable a mapper robot to close many loops, for which precise
maps can be assumed. The same cannot be said about the newly emerging
autonomous navigation and driving systems, which typically operate in loop-less
trajectories that have no large loop (e.g., straight paths). In this paper, we
propose a solution that overcomes this limitation by merging the two maps. Our
study is motivated by the observation that even when there is no large loop in
either the query or reference map, many loops can often be obtained in the
merged map. We add two new aspects to map matching: (1) image retrieval with
discriminative deep convolutional neural network (DCNN) features, which
efficiently generates a small number of good initial alignment hypotheses; and
(2) map merge, which jointly deforms the two maps to minimize differences in
shape between them. To realize practical computation time, we also present a
preemption scheme that avoids excessive evaluation of useless map-matching
hypotheses. To verify our approach experimentally, we created a novel
collection of uncertain loop-less maps by utilizing the recently published
North Campus Long-Term (NCLT) dataset and its ground-truth GPS data. The
results obtained using these map collections confirm that our approach improves
on previous map-matching approaches.},
 author = {Tanaka, Kanji},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Deformable Map Matching for Uncertain Loop-Less Maps},
 url = {https://app.dimensions.ai/details/publication/pub.1118579337},
 volume = {},
 year = {2016}
}

@article{pub.1118588114,
 abstract = {Ever more robust, accurate and detailed mapping using visual sensing has
proven to be an enabling factor for mobile robots across a wide variety of
applications. For the next level of robot intelligence and intuitive user
interaction, maps need extend beyond geometry and appearence - they need to
contain semantics. We address this challenge by combining Convolutional Neural
Networks (CNNs) and a state of the art dense Simultaneous Localisation and
Mapping (SLAM) system, ElasticFusion, which provides long-term dense
correspondence between frames of indoor RGB-D video even during loopy scanning
trajectories. These correspondences allow the CNN's semantic predictions from
multiple view points to be probabilistically fused into a map. This not only
produces a useful semantic 3D map, but we also show on the NYUv2 dataset that
fusing multiple predictions leads to an improvement even in the 2D semantic
labelling over baseline single frame predictions. We also show that for a
smaller reconstruction dataset with larger variation in prediction viewpoint,
the improvement over single frame segmentation increases. Our system is
efficient enough to allow real-time interactive use at frame-rates of
approximately 25Hz.},
 author = {McCormac, John and Handa, Ankur and Davison, Andrew and Leutenegger, Stefan},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural
  Networks},
 url = {https://app.dimensions.ai/details/publication/pub.1118588114},
 volume = {},
 year = {2016}
}

@article{pub.1118594926,
 abstract = {Simultaneous Localization and Mapping (SLAM)consists in the concurrent
construction of a model of the environment (the map), and the estimation of the
state of the robot moving within it. The SLAM community has made astonishing
progress over the last 30 years, enabling large-scale real-world applications,
and witnessing a steady transition of this technology to industry. We survey
the current state of SLAM. We start by presenting what is now the de-facto
standard formulation for SLAM. We then review related work, covering a broad
set of topics including robustness and scalability in long-term mapping, metric
and semantic representations for mapping, theoretical performance guarantees,
active SLAM and exploration, and other new frontiers. This paper simultaneously
serves as a position paper and tutorial to those who are users of SLAM. By
looking at the published research with a critical eye, we delineate open
challenges and new research issues, that still deserve careful scientific
investigation. The paper also contains the authors' take on two questions that
often animate discussions during robotics conferences: Do robots need SLAM? and
Is SLAM solved?},
 author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Past, Present, and Future of Simultaneous Localization And Mapping:
  Towards the Robust-Perception Age},
 url = {https://app.dimensions.ai/details/publication/pub.1118594926},
 volume = {},
 year = {2016}
}

@article{pub.1118683893,
 abstract = {In this study, we explore the use of deep convolutional neural networks
(DCNNs) in visual place classification for robotic mapping and localization. An
open question is how to partition the robot's workspace into places to maximize
the performance (e.g., accuracy, precision, recall) of potential DCNN
classifiers. This is a chicken and egg problem: If we had a well-trained DCNN
classifier, it is rather easy to partition the robot's workspace into places,
but the training of a DCNN classifier requires a set of pre-defined place
classes. In this study, we address this problem and present several strategies
for unsupervised discovery of place classes ("time cue," "location cue,"
"time-appearance cue," and "location-appearance cue"). We also evaluate the
efficacy of the proposed methods using the publicly available University of
Michigan North Campus Long-Term (NCLT) Dataset.},
 author = {Xiaoxiao, Fei and Kanji, Tanaka and Kouya, Inamoto},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Unsupervised Place Discovery for Visual Place Classification},
 url = {https://app.dimensions.ai/details/publication/pub.1118683893},
 volume = {},
 year = {2016}
}

@article{pub.1118789888,
 abstract = {Recognizing a previously visited place, also known as place recognition (or
loop closure detection) is the key towards fully autonomous mobile robots and
self-driving vehicle navigation. Augmented with various Simultaneous
Localization and Mapping techniques (SLAM), loop closure detection allows for
incremental pose correction and can bolster efficient and accurate map
creation. However, repeated and similar scenes (perceptual aliasing) and long
term appearance changes (e.g. weather variations) are major challenges for
current place recognition algorithms. We introduce a new dataset Multisensory
Omnidirectional Long-term Place recognition (MOLP) comprising omnidirectional
intensity and disparity images. This dataset presents many of the challenges
faced by outdoor mobile robots and current place recognition algorithms. Using
MOLP dataset, we formulate the place recognition problem as a regularized
sparse convex optimization problem. We conclude that information extracted from
intensity image is superior to disparity image in isolating discriminative
features for successful long term place recognition. Furthermore, when these
discriminative features are extracted from an omnidirectional vision sensor, a
robust bidirectional loop closure detection approach is established, allowing
mobile robots to close the loop, regardless of the difference in the direction
when revisiting a place.},
 author = {Mathur, Ashwin and Han, Fei and Zhang, Hao},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Multisensory Omni-directional Long-term Place Recognition: Benchmark
  Dataset and Analysis},
 url = {https://app.dimensions.ai/details/publication/pub.1118789888},
 volume = {},
 year = {2017}
}

@article{pub.1118880892,
 abstract = {With recent progress in large-scale map maintenance and long-term map
learning, the task of change detection on a large-scale map from a visual image
captured by a mobile robot has become a problem of increasing criticality.
Previous approaches for change detection are typically based on image
differencing and require the memorization of a prohibitively large number of
mapped images in the above context. In contrast, this study follows the recent,
efficient paradigm of change-classifier-learning and specifically employs a
collection of place-specific change classifiers. Our change-classifier-learning
algorithm is based on zero-shot learning (ZSL) and represents a place-specific
change classifier by its training examples mined from an external knowledge
base (EKB). The proposed algorithm exhibits several advantages. First, we are
required to memorize only training examples (rather than the classifier
itself), which can be further compressed in the form of bag-of-words (BoW).
Secondly, we can incorporate the most recent map into the classifiers by
straightforwardly adding or deleting a few training examples that correspond to
these classifiers. Thirdly, we can share the BoW vocabulary with other related
task scenarios (e.g., BoW-based self-localization), wherein the vocabulary is
generally designed as a rich, continuously growing, and domain-adaptive
knowledge base. In our contribution, the proposed algorithm is applied and
evaluated on a practical long-term cross-season change detection system that
consists of a large number of place-specific object-level change classifiers.},
 author = {Kanji, Tanaka},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Zero-Shot Learning to Manage a Large Number of Place-Specific
  Compressive Change Classifiers},
 url = {https://app.dimensions.ai/details/publication/pub.1118880892},
 volume = {},
 year = {2017}
}

@article{pub.1118902764,
 abstract = {This paper presents a novel 3DOF pedestrian trajectory prediction approach
for autonomous mobile service robots. While most previously reported methods
are based on learning of 2D positions in monocular camera images, our approach
uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D
position plus 1D rotation within the world coordinate system). Our approach,
T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using
long-term data from real-world robot deployments and aims to learn
context-dependent (environment- and time-specific) human activities. Our
approach incorporates long-term temporal information (i.e. date and time) with
short-term pose observations as input. A sequence-to-sequence LSTM
encoder-decoder is trained, which encodes observations into LSTM and then
decodes as predictions. For deployment, it can perform on-the-fly prediction in
real-time. Instead of using manually annotated data, we rely on a robust human
detection, tracking and SLAM system, providing us with examples in a global
coordinate system. We validate the approach using more than 15K pedestrian
trajectories recorded in a care home environment over a period of three months.
The experiment shows that the proposed T-Pose-LSTM model advances the
state-of-the-art 2D-based method for human trajectory prediction in long-term
mobile robot deployments.},
 author = {Sun, Li and Yan, Zhi and Mellado, Sergi Molina and Hanheide, Marc and Duckett, Tom},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous
  Mobile Robot Deployment Data},
 url = {https://app.dimensions.ai/details/publication/pub.1118902764},
 volume = {},
 year = {2017}
}

@article{pub.1118939566,
 abstract = {Robust cross-seasonal localization is one of the major challenges in
long-term visual navigation of autonomous vehicles. In this paper, we exploit
recent advances in semantic segmentation of images, i.e., where each pixel is
assigned a label related to the type of object it represents, to attack the
problem of long-term visual localization. We show that semantically labeled 3-D
point maps of the environment, together with semantically segmented images, can
be efficiently used for vehicle localization without the need for detailed
feature descriptors (SIFT, SURF, etc.). Thus, instead of depending on
hand-crafted feature descriptors, we rely on the training of an image
segmenter. The resulting map takes up much less storage space compared to a
traditional descriptor based map. A particle filter based semantic localization
solution is compared to one based on SIFT-features, and even with large
seasonal variations over the year we perform on par with the larger and more
descriptive SIFT-features, and are able to localize with an error below 1 m
most of the time.},
 author = {Stenborg, Erik and Toft, Carl and Hammarstrand, Lars},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Long-term Visual Localization using Semantically Segmented Images},
 url = {https://app.dimensions.ai/details/publication/pub.1118939566},
 volume = {},
 year = {2018}
}

@article{pub.1119010384,
 abstract = {We propose a novel online learning algorithm, called SpCoSLAM 2.0, for
spatial concepts and lexical acquisition with high accuracy and scalability.
Previously, we proposed SpCoSLAM as an online learning algorithm based on
unsupervised Bayesian probabilistic model that integrates multimodal place
categorization, lexical acquisition, and SLAM. However, our original algorithm
had limited estimation accuracy owing to the influence of the early stages of
learning, and increased computational complexity with added training data.
Therefore, we introduce techniques such as fixed-lag rejuvenation to reduce the
calculation time while maintaining an accuracy higher than that of the original
algorithm. The results show that, in terms of estimation accuracy, the proposed
algorithm exceeds the original algorithm and is comparable to batch learning.
In addition, the calculation time of the proposed algorithm does not depend on
the amount of training data and becomes constant for each step of the scalable
algorithm. Our approach will contribute to the realization of long-term spatial
language interactions between humans and robots.},
 author = {Taniguchi, Akira and Hagiwara, Yoshinobu and Taniguchi, Tadahiro and Inamura, Tetsunari},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Improved and Scalable Online Learning of Spatial Concepts and Language
  Models with Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1119010384},
 volume = {},
 year = {2018}
}

@article{pub.1119069296,
 abstract = {In this paper, we present a novel tightly-coupled probabilistic monocular
visual-odometric Simultaneous Localization and Mapping algorithm using wheels
and a MEMS gyroscope, which can provide accurate, robust and long-term
localization for the ground robot moving on a plane. Firstly, we present an
odometer preintegration theory that integrates the wheel encoder measurements
and gyroscope measurements to a local frame. The preintegration theory properly
addresses the manifold structure of the rotation group SO(3) and carefully
deals with uncertainty propagation and bias correction. Then the novel odometer
error term is formulated using the odometer preintegration model and it is
tightly integrated into the visual optimization framework. Furthermore, we
introduce a complete tracking framework to provide different strategies for
motion tracking when (1) both measurements are available, (2) visual
measurements are not available, and (3) wheel encoder experiences slippage,
which leads the system to be accurate and robust. Finally, the proposed
algorithm is evaluated by performing extensive experiments, the experimental
results demonstrate the superiority of the proposed system.},
 author = {Quan, Meixiang and Piao, Songhao and Tan, Minglang and Huang, Shi-Sheng},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Tightly-coupled Monocular Visual-odometric SLAM using Wheels and a MEMS
  Gyroscope},
 url = {https://app.dimensions.ai/details/publication/pub.1119069296},
 volume = {},
 year = {2018}
}

@article{pub.1119111239,
 abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a strong
assumption limits the use of most visual SLAM systems in populated real-world
environments, which are the target of several relevant applications like
service robotics or autonomous vehicles. In this paper we present DynaSLAM, a
visual SLAM system that, building over ORB-SLAM2 [1], adds the capabilities of
dynamic object detection and background inpainting. DynaSLAM is robust in
dynamic scenarios for monocular, stereo and RGB-D configurations. We are
capable of detecting the moving objects either by multi-view geometry, deep
learning or both. Having a static map of the scene allows inpainting the frame
background that has been occluded by such dynamic objects. We evaluate our
system in public monocular, stereo and RGB-D datasets. We study the impact of
several accuracy/speed trade-offs to assess the limits of the proposed
methodology. DynaSLAM outperforms the accuracy of standard visual SLAM
baselines in highly dynamic scenarios. And it also estimates a map of the
static parts of the scene, which is a must for long-term applications in
real-world environments.},
 author = {Bescos, Berta and FÃ¡cil, JosÃ© M. and Civera, Javier and Neira, JosÃ©},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes},
 url = {https://app.dimensions.ai/details/publication/pub.1119111239},
 volume = {},
 year = {2018}
}

@article{pub.1119145918,
 abstract = {Simultaneous Localization and Mapping, commonly known as SLAM, has been an
active research area in the field of Robotics over the past three decades. For
solving the SLAM problem, every robot is equipped with either a single sensor
or a combination of similar/different sensors. This paper attempts to review,
discuss, evaluate and compare these sensors. Keeping an eye on future, this
paper also assesses the characteristics of these sensors against factors
critical to the long-term autonomy challenge.},
 author = {Zaffar, Mubariz and Ehsan, Shoaib and Stolkin, Rustam and Maier, Klaus McDonald},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Sensors, SLAM and Long-term Autonomy: A Review},
 url = {https://app.dimensions.ai/details/publication/pub.1119145918},
 volume = {},
 year = {2018}
}

@article{pub.1119223675,
 abstract = {We present a complete map management process for a visual localization system
designed for multi-vehicle long- term operations in resource constrained
outdoor environments. Outdoor visual localization generates large amounts of
data that need to be incorporated into a lifelong visual map in order to allow
localization at all times and under all appearance conditions. Processing these
large quantities of data is non- trivial, as it is subject to limited
computational and storage capabilities both on the vehicle and on the mapping
backend. We address this problem with a two-fold map update paradigm capable
of, either, adding new visual cues to the map, or updating co-observation
statistics. The former, in combination with offline map summarization
techniques, allows enhancing the appearance coverage of the lifelong map while
keeping the map size limited. On the other hand, the latter is able to
significantly boost the appearance-based landmark selection for efficient
online localization without incurring any additional computational or storage
burden. Our evaluation in challenging outdoor conditions shows that our
proposed map management process allows building and maintaining maps for
precise visual localization over long time spans in a tractable and scalable
fashion.},
 author = {BÃ¼rki, Mathias and Dymczyk, Marcin and Gilitschenski, Igor and Cadena, Cesar and Siegwart, Roland and Nieto, Juan},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Map Management for Efficient Long-Term Visual Localization in Outdoor
  Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1119223675},
 volume = {},
 year = {2018}
}

@article{pub.1119223691,
 abstract = {We present an online landmark selection method for distributed long-term
visual localization systems in bandwidth-constrained environments. Sharing a
common map for online localization provides a fleet of au- tonomous vehicles
with the possibility to maintain and access a consistent map source, and
therefore reduce redundancy while increasing efficiency. However, connectivity
over a mobile network imposes strict bandwidth constraints and thus the need to
minimize the amount of exchanged data. The wide range of varying appearance
conditions encountered during long-term visual localization offers the
potential to reduce data usage by extracting only those visual cues which are
relevant at the given time. Motivated by this, we propose an unsupervised
method of adaptively selecting landmarks according to how likely these
landmarks are to be observable under the prevailing appear- ance condition. The
ranking function this selection is based upon exploits landmark
co-observability statistics collected in past traversals through the mapped
area. Evaluation is per- formed over different outdoor environments, large
time-scales and varying appearance conditions, including the extreme tran-
sition from day-time to night-time, demonstrating that with our
appearance-dependent selection method, we can significantly reduce the amount
of landmarks used for localization while maintaining or even improving the
localization performance.},
 author = {BÃ¼rki, Mathias and Gilitschenski, Igor and Stumm, Elena and Siegwart, Roland and Nieto, Juan},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Appearance-Based Landmark Selection for Efficient Long-Term Visual
  Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1119223691},
 volume = {},
 year = {2018}
}

@article{pub.1119272565,
 abstract = {This paper implements Simultaneous Localization and Mapping (SLAM) technique
to construct a map of a given environment. A Real Time Appearance Based Mapping
(RTAB-Map) approach was taken for accomplishing this task. Initially, a 2d
occupancy grid and 3d octomap was created from a provided simulated
environment. Next, a personal simulated environment was created for mapping as
well. In this appearance based method, a process called Loop Closure is used to
determine whether a robot has seen a location before or not. In this paper, it
is seen that RTAB-Map is optimized for large scale and long term SLAM by using
multiple strategies to allow for loop closure to be done in real time and the
results depict that it can be an excellent solution for SLAM to develop robots
that can map an environment in both 2d and 3d.},
 author = {Das, Sagarnil},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Simultaneous Localization and Mapping (SLAM) using RTAB-MAP},
 url = {https://app.dimensions.ai/details/publication/pub.1119272565},
 volume = {},
 year = {2018}
}

@article{pub.1119303402,
 abstract = {To operate in an urban environment, an automated vehicle must be capable of
accurately estimating its position within a global map reference frame. This is
necessary for optimal path planning and safe navigation. To accomplish this
over an extended period of time, the global map requires long-term maintenance.
This includes the addition of newly observable features and the removal of
transient features belonging to dynamic objects. The latter is especially
important for the long-term use of the map as matching against a map with
features that no longer exist can result in incorrect data associations, and
consequently erroneous localisation. This paper addresses the problem of
removing features from the map that correspond to objects that are no longer
observable/present in the environment. This is achieved by assigning a single
score which depends on the geometric distribution and characteristics when the
features are re-detected (or not) on different occasions. Our approach not only
eliminates ephemeral features, but also can be used as a reduction algorithm
for highly dense maps. We tested our approach using half a year of weekly
drives over the same 500-metre section of road in an urban environment. The
results presented demonstrate the validity of the long-term approach to map
maintenance.},
 author = {Berrio, Julie Stephany and Ward, James and Worrall, Stewart and Nebot, Eduardo},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Identifying robust landmarks in feature-based maps},
 url = {https://app.dimensions.ai/details/publication/pub.1119303402},
 volume = {},
 year = {2018}
}

@article{pub.1119335382,
 abstract = {This paper presents a novel framework for simultaneously implementing
localization and segmentation, which are two of the most important vision-based
tasks for robotics. While the goals and techniques used for them were
considered to be different previously, we show that by making use of the
intermediate results of the two modules, their performance can be enhanced at
the same time. Our framework is able to handle both the instantaneous motion
and long-term changes of instances in localization with the help of the
segmentation result, which also benefits from the refined 3D pose information.
We conduct experiments on various datasets, and prove that our framework works
effectively on improving the precision and robustness of the two tasks and
outperforms existing localization and segmentation algorithms.},
 author = {Wang, Kai and Lin, Yimin and Wang, Luowei and Han, Liming and Hua, Minjie and Wang, Xiang and Lian, Shiguo and Huang, Bill},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {A Unified Framework for Mutual Improvement of SLAM and Semantic
  Segmentation},
 url = {https://app.dimensions.ai/details/publication/pub.1119335382},
 volume = {},
 year = {2018}
}

@article{pub.1119397086,
 abstract = {In this paper, we present a framework for real-time autonomous robot
navigation based on cloud and on-demand databases to address two major issues
of human-like robot interaction and task planning in global dynamic
environment, which is not known a priori. Our framework contributes to make
human-like brain GPS mapping system for robot using spatial information and
performs 3D visual semantic SLAM for independent robot navigation. We
accomplish the feat by separating robot's memory system into Long-Term Memory
(LTM) and Short-Term Memory (STM). We also form robot's behavior and knowledge
system by linking these memories to Autonomous Navigation Module (ANM),
Learning Module (LM), and Behavior Planner Module (BPM). The proposed framework
is assessed through simulation using ROS-based Gazebo-simulated mobile robot,
RGB-D camera (3D sensor) and a laser range finder (2D sensor) in 3D model of
realistic indoor environment. Simulation corroborates the substantial practical
merit of our proposed framework.},
 author = {Joo, Sung-Hyeon and Manzoor, Sumaira and Rocha, Yuri Goncalves and Lee, Hyun-Uk and Kuc, Tae-Yong},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {A Realtime Autonomous Robot Navigation Framework for Human like
  High-level Interaction and Task Planning in Global Dynamic Environment},
 url = {https://app.dimensions.ai/details/publication/pub.1119397086},
 volume = {},
 year = {2019}
}

@article{pub.1119397338,
 abstract = {In order to facilitate long-term localization using a visual simultaneous
localization and mapping (SLAM) algorithm, careful feature selection can help
ensure that reference points persist over long durations and the runtime and
storage complexity of the algorithm remain consistent. We present SIVO
(Semantically Informed Visual Odometry and Mapping), a novel
information-theoretic feature selection method for visual SLAM which
incorporates semantic segmentation and neural network uncertainty into the
feature selection pipeline. Our algorithm selects points which provide the
highest reduction in Shannon entropy between the entropy of the current state
and the joint entropy of the state, given the addition of the new feature with
the classification entropy of the feature from a Bayesian neural network. Each
selected feature significantly reduces the uncertainty of the vehicle state and
has been detected to be a static object (building, traffic sign, etc.)
repeatedly with a high confidence. This selection strategy generates a sparse
map which can facilitate long-term localization. The KITTI odometry dataset is
used to evaluate our method, and we also compare our results against ORB_SLAM2.
Overall, SIVO performs comparably to the baseline method while reducing the map
size by almost 70%.},
 author = {Ganti, Pranav and Waslander, Steven L.},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Network Uncertainty Informed Semantic Feature Selection for Visual SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1119397338},
 volume = {},
 year = {2018}
}

@article{pub.1119398719,
 abstract = {Long-term metric self-localization is an essential capability of autonomous
mobile robots, but remains challenging for vision-based systems due to
appearance changes caused by lighting, weather, or seasonal variations. While
experience-based mapping has proven to be an effective technique for bridging
the `appearance gap,' the number of experiences required for reliable metric
localization over days or months can be very large, and methods for reducing
the necessary number of experiences are needed for this approach to scale.
Taking inspiration from color constancy theory, we learn a nonlinear
RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature
matches for images captured under different lighting and weather conditions,
and use it as a pre-processing step in a conventional single-experience
localization pipeline to improve its robustness to appearance change. We train
this mapping by approximating the target non-differentiable localization
pipeline with a deep neural network, and find that incorporating a learned
low-dimensional context feature can further improve cross-appearance feature
matching. Using synthetic and real-world datasets, we demonstrate substantial
improvements in localization performance across day-night cycles, enabling
continuous metric localization over a 30-hour period using a single mapping
experience, and allowing experience-based localization to scale to long
deployments with dramatically reduced data requirements.},
 author = {Clement, Lee and Gridseth, Mona and Tomasi, Justin and Kelly, Jonathan},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Learning Matchable Image Transformations for Long-term Metric Visual
  Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1119398719},
 volume = {},
 year = {2019}
}

@article{pub.1119409754,
 abstract = {Accurate and robust visual localization under a wide range of viewing
condition variations including season and illumination changes, as well as
weather and day-night variations, is the key component for many computer vision
and robotics applications. Under these conditions, most traditional methods
would fail to locate the camera. In this paper we present a visual localization
algorithm that combines structure-based method and image-based method with
semantic information. Given semantic information about the query and database
images, the retrieved images are scored according to the semantic consistency
of the 3D model and the query image. Then the semantic matching score is used
as weight for RANSAC's sampling and the pose is solved by a standard PnP
solver. Experiments on the challenging long-term visual localization benchmark
dataset demonstrate that our method has significant improvement compared with
the state-of-the-arts.},
 author = {Shi, Tianxin and Shen, Shuhan and Gao, Xiang and Zhu, Lingjie},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Visual Localization Using Sparse Semantic 3D Map},
 url = {https://app.dimensions.ai/details/publication/pub.1119409754},
 volume = {},
 year = {2019}
}

@article{pub.1119420268,
 abstract = {Simultaneous Localisation and Mapping (SLAM) algorithms are expensive to run
on smaller robotic platforms such as Micro-Aerial Vehicles. Bug algorithms are
an alternative that use relatively little processing power, and avoid high
memory consumption by not building an explicit map of the environment. Bug
Algorithms achieve relatively good performance in simulated and robotic maze
solving domains. However, because they are hand-designed, a natural question is
whether they are globally optimal control policies. In this work we explore the
performance of Neuroevolution - specifically NEAT - at evolving control
policies for simulated differential drive robots carrying out generalised maze
navigation. We extend NEAT to include Gated Recurrent Units (GRUs) to help deal
with long term dependencies. We show that both NEAT and our NEAT-GRU can
repeatably generate controllers that outperform I-Bug (an algorithm
particularly well-suited for use in real robots) on a test set of 209 indoor
maze like environments. We show that NEAT-GRU is superior to NEAT in this task
but also that out of the 2 systems, only NEAT-GRU can continuously evolve
successful controllers for a much harder task in which no bearing information
about the target is provided to the agent.},
 author = {Butterworth, James and Savani, Rahul and Tuyls, Karl},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Evolving Indoor Navigational Strategies Using Gated Recurrent Units In
  NEAT},
 url = {https://app.dimensions.ai/details/publication/pub.1119420268},
 volume = {},
 year = {2019}
}

@article{pub.1119442607,
 abstract = {Visual localization is one of the primary capabilities for mobile robots.
Long-term visual localization in real time is particularly challenging, in
which the robot is required to efficiently localize itself using visual data
where appearance may change significantly over time. In this paper, we propose
a cloud-based visual localization system targeting at long-term localization in
real time. On the robot, we employ two estimators to achieve accurate and
real-time performance. One is a sliding-window based visual inertial odometry,
which integrates constraints from consecutive observations and self-motion
measurements, as well as the constraints induced by localization on the cloud.
This estimator builds a local visual submap as the virtual observation which is
then sent to the cloud as new localization constraints. The other one is a
delayed state Extended Kalman Filter to fuse the pose of the robot localized
from the cloud, the local odometry and the high-frequency inertial
measurements. On the cloud, we propose a longer sliding-window based
localization method to aggregate the virtual observations for larger field of
view, leading to more robust alignment between virtual observations and the
map. Under this architecture, the robot can achieve drift-free and real-time
localization using onboard resources even in a network with limited bandwidth,
high latency and existence of package loss, which enables the autonomous
navigation in real-world environment. We evaluate the effectiveness of our
system on a dataset with challenging seasonal and illuminative variations. We
further validate the robustness of the system under challenging network
conditions.},
 author = {Ding, Xiaqing and Wang, Yue and Tang, Li and Yin, Huan and Xiong, Rong},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Communication constrained cloud-based long-term visual localization in
  real time},
 url = {https://app.dimensions.ai/details/publication/pub.1119442607},
 volume = {},
 year = {2019}
}

@article{pub.1119450704,
 abstract = {Recent trends have accelerated the development of spatial applications on
mobile devices and robots. These include navigation, augmented reality,
human-robot interaction, and others. A key enabling technology for such
applications is the understanding of the device's location and the map of the
surrounding environment. This generic problem, referred to as Simultaneous
Localization and Mapping (SLAM), is an extensively researched topic in
robotics. However, visual SLAM algorithms face several challenges including
perceptual aliasing and high computational cost. These challenges affect the
accuracy, efficiency, and viability of visual SLAM algorithms, especially for
long-term SLAM, and their use in resource-constrained mobile devices.
  A parallel trend is the ubiquity of Wi-Fi routers for quick Internet access
in most urban environments. Most robots and mobile devices are equipped with a
Wi-Fi radio as well. We propose a method to utilize Wi-Fi received signal
strength to alleviate the challenges faced by visual SLAM algorithms. To
demonstrate the utility of this idea, this work makes the following
contributions: (i) We propose a generic way to integrate Wi-Fi sensing into
visual SLAM algorithms, (ii) We integrate such sensing into three well-known
SLAM algorithms, (iii) Using four distinct datasets, we demonstrate the
performance of such augmentation in comparison to the original visual
algorithms and (iv) We compare our work to Wi-Fi augmented FABMAP algorithm.
Overall, we show that our approach can improve the accuracy of visual SLAM
algorithms by 11% on average and reduce computation time on average by 15% to
25%.},
 author = {Hashemifar, Zakieh S. and Adhivarahan, Charuvahan and Balakrishnan, Anand and Dantu, Karthik},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Augmenting Visual SLAM with Wi-Fi Sensing For Indoor Applications},
 url = {https://app.dimensions.ai/details/publication/pub.1119450704},
 volume = {},
 year = {2019}
}

@article{pub.1119452008,
 abstract = {In this paper, we present iDVO (inertia-embedded deep visual odometry), a
self-supervised learning based monocular visual odometry (VO) for road
vehicles. When modelling the geometric consistency within adjacent frames, most
deep VO methods ignore the temporal continuity of the camera pose, which
results in a very severe jagged fluctuation in the velocity curves. With the
observation that road vehicles tend to perform smooth dynamic characteristics
in most of the time, we design the inertia loss function to describe the
abnormal motion variation, which assists the model to learn the consecutiveness
from long-term camera ego-motion. Based on the recurrent convolutional neural
network (RCNN) architecture, our method implicitly models the dynamics of road
vehicles and the temporal consecutiveness by the extended Long Short-Term
Memory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to
handle the non-consistency in fast camera motion by blocking the boundary part
and which generates more efficiency in the whole non-consistency mask. The
proposed method is evaluated on the KITTI dataset, and the results demonstrate
state-of-the-art performance with respect to other monocular deep VO and SLAM
approaches.},
 author = {Wang, Chengze and Yuan, Yuan and Wang, Qi},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Learning by Inertia: Self-supervised Monocular Visual Odometry for Road
  Vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1119452008},
 volume = {},
 year = {2019}
}

@article{pub.1119739666,
 abstract = {In this paper, we present a novel tightly coupled probabilistic monocular visual-odometric simultaneous localization and mapping (VOSLAM) algorithm using wheels and a MEMS gyroscope, which can provide accurate, robust, and long-term localization for ground robots. First, we present a novel odometer preintegration theory on manifold; it integrates the wheel encoder measurements and gyroscope measurements to a relative motion constraint that is independent of the linearization point and carefully addresses the uncertainty propagation and gyroscope bias correction. Based on the preintegrated odometer measurement model, we also introduce the odometer error term and tightly integrate it into the visual optimization framework. Then, in order to bootstrap the VOSLAM system, we propose a simple map initialization method. Finally, we present a complete localization mechanism to maximally exploit both sensing cues, which provides different strategies for motion tracking when: 1) both measurements are available; 2) visual measurements are not available; and 3) wheel encoders experience slippage, thereby ensuring the accurate and robust motion tracking. The proposed algorithm is evaluated by performing extensive experiments, and the experimental results demonstrate the superiority of the proposed system.},
 author = {Quan, Meixiang and Piao, Songhao and Tan, Minglang and Huang, Shi-Sheng},
 doi = {10.1109/access.2019.2930201},
 journal = {IEEE Access},
 keywords = {},
 note = {https://doi.org/10.1109/access.2019.2930201},
 number = {},
 pages = {97374-97389},
 title = {Tightly-Coupled Monocular Visual-Odometric SLAM Using Wheels and a MEMS Gyroscope},
 url = {https://app.dimensions.ai/details/publication/pub.1119739666},
 volume = {7},
 year = {2019}
}

@inproceedings{pub.1119942202,
 abstract = {Building ``always-on'' robots to be deployed over extended periods of time in real human environments is challenging for several reasons. 
Some fundamental questions that arise in the process include: 
1) How can the robot reconcile unexpected differences between its observations and its outdated map of the world? 
2) How can we scalably test robots for long-term autonomy?
3) Can a robot learn to predict its own failures, and their corresponding causes?
4) When the robot fails and is unable to recover autonomously, can it utilize partially specified, approximate human corrections to overcome its failures?
We summarize our research towards addressing all of these questions. 
We present 1) Episodic non-Markov Localization to maintain the belief of the robot's location while explicitly reasoning about unmapped observations; 2) a 1,000km challenge to test for long-term autonomy; 3) feature-based and learning-based approaches to predicting failures; and 4) human-in-the-loop SLAM to overcome robot mapping errors, and SMT-based robot transition repair  to overcome state machine failures.},
 author = {Biswas, Joydeep},
 booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence},
 doi = {10.24963/ijcai.2019/893},
 keywords = {},
 note = {https://www.ijcai.org/proceedings/2019/0893.pdf},
 pages = {6388-6392},
 title = {The Quest For "Always-On" Autonomous Mobile Robots},
 url = {https://app.dimensions.ai/details/publication/pub.1119942202},
 year = {2019}
}

@inproceedings{pub.1120057133,
 abstract = {In order to facilitate long-term localization using a visual simultaneous localization and mapping (SLAM) algorithm, careful feature selection can help ensure that reference points persist over long durations and the runtime and storage complexity of the algorithm remain consistent. We present SIVO (Semantically Informed Visual Odometry and Mapping), a novel information-theoretic feature selection method for visual SLAM which incorporates semantic segmentation and neural network uncertainty into the feature selection pipeline. Our algorithm selects points which provide the highest reduction in Shannon entropy between the entropy of the current state and the joint entropy of the state, given the addition of the new feature with the classification entropy of the feature from a Bayesian neural network. Each selected feature significantly reduces the uncertainty of the vehicle state and has been detected to be a static object (building, traffic sign, etc.) repeatedly with a high confidence. This selection strategy generates a sparse map which can facilitate long-term localization. The KITTI odometry dataset is used to evaluate our method, and we also compare our results against ORB_SLAM2. Overall, SIVO performs comparably to the baseline method while reducing the map size by almost 70%.},
 author = {Ganti, Pranav and Waslander, Steven L.},
 booktitle = {2019 16th Conference on Computer and Robot Vision (CRV)},
 doi = {10.1109/crv.2019.00024},
 keywords = {},
 note = {http://arxiv.org/pdf/1811.11946},
 pages = {121-128},
 title = {Network Uncertainty Informed Semantic Feature Selection for Visual SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1120057133},
 year = {2019}
}

@inbook{pub.1120106038,
 abstract = {Many robot applications, such as environmental monitoring, security and surveillance help people to do tasks in day-to-day scenarios. However, the growing security demand for environment perception is a key issue of mapping or frequent updating in the long term, such as fire detection in early stage. A hybrid mapping method is proposed based on fusing RGB, depth and thermal (DT) information from Kinect and infrared sensors equipped in the mobile robot. Firstly, the proposed pipeline will estimate the robotâ€™s pose by extracting and matching ORB features in RGB images successively. Then Poses corresponding to each depth and thermal- Infrared image are estimated through a combination of timestamp synchronization and the result of the extrinsic calibration of the system, and the map with both appearance and the temperature of environment is generated by the combination of The RGB and temperature information. Finally, the depth information is used to project the pixel points to the world coordinate system to generate the RGB-DT map. Extensive results verify the effectiveness of the proposed RGB-DT mapping for environments perception.},
 author = {Zhao, Lijun and Liu, Yu and Jiang, Xinkai and Wang, Ke and Zhou, Zigeng},
 booktitle = {Intelligent Robotics and Applications},
 doi = {10.1007/978-3-030-27538-9_12},
 keywords = {},
 pages = {131-141},
 publisher = {},
 title = {Indoor Environment RGB-DT Mapping for Security Mobile Robots},
 url = {https://app.dimensions.ai/details/publication/pub.1120106038},
 year = {2019}
}

@inproceedings{pub.1120279556,
 abstract = {This paper presents a novel framework for simultaneously implementing localization and segmentation, which are two of the most important vision-based tasks for robotics. While the goals and techniques used for them were considered to be different previously, we show that by making use of the intermediate results of the two modules, their performance can be enhanced at the same time. Our framework is able to handle both the instantaneous motion and long-term changes of instances in localization with the help of the segmentation result, which also benefits from the refined 3D pose information. We conduct experiments on various datasets, and prove that our framework works effectively on improving the precision and robustness of the two tasks and outperforms existing localization and segmentation algorithms.},
 author = {Wang, Kai and Lin, Yimin and Wang, Luowei and Han, Liming and Hua, Minjie and Wang, Xiang and Lian, Shiguo and Huang, Bill},
 booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2019.8793499},
 keywords = {},
 note = {http://arxiv.org/pdf/1812.10016},
 pages = {5224-5230},
 title = {A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation},
 url = {https://app.dimensions.ai/details/publication/pub.1120279556},
 year = {2019}
}

@article{pub.1120290630,
 abstract = {Recent years have witnessed the rapid proliferation of low-power backscatter
technologies that realize the ubiquitous and long-term connectivity to empower
smart cities and smart homes. Localizing such low-power backscatter tags is
crucial for IoT-based smart services. However, current backscatter localization
systems require prior knowledge of the site, either a map or landmarks with
known positions, increasing the deployment cost. To empower universal
localization service, this paper presents Rover, an indoor localization system
that simultaneously localizes multiple backscatter tags with zero start-up cost
using a robot equipped with inertial sensors. Rover runs in a joint
optimization framework, fusing WiFi-based positioning measurements with
inertial measurements to simultaneously estimate the locations of both the
robot and the connected tags. Our design addresses practical issues such as the
interference among multiple tags and the real-time processing for solving the
SLAM problem. We prototype Rover using off-the-shelf WiFi chips and customized
backscatter tags. Our experiments show that Rover achieves localization
accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
 author = {Zhang, Shengkai and Wang, Wei and Tang, Sheyang and Jin, Shi and Jiang, Tao},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Localizing Backscatters by a Single Robot With Zero Start-up Cost},
 url = {https://app.dimensions.ai/details/publication/pub.1120290630},
 volume = {},
 year = {2019}
}

@article{pub.1120551666,
 abstract = {Visual place recognition is essential for large-scale simultaneous localization and mapping (SLAM). Long-term robot operations across different time of the days, months, and seasons introduce new challenges from significant environment appearance variations. In this paper, we propose a novel method to learn a location representation that can integrate the semantic landmarks of a place with its holistic representation. To promote the robustness of our new model against the drastic appearance variations due to long-term visual changes, we formulate our objective to use non-squared â„“2-norm distances, which leads to a difficult optimization problem that minimizes the ratio of the â„“2,1-norms of matrices. To solve our objective, we derive a new efficient iterative algorithm, whose convergence is rigorously guaranteed by theory. In addition, because our solution is strictly orthogonal, the learned location representations can have better place recognition capabilities. We evaluate the proposed method using two large-scale benchmark data sets, the CMU-VL and Nordland data sets. Experimental results have validated the effectiveness of our new method in long-term visual place recognition applications.},
 author = {Liu, Kai and Wang, Hua and Han, Fei and Zhang, Hao},
 doi = {10.1609/aaai.v33i01.33018034},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 keywords = {},
 note = {https://ojs.aaai.org/index.php/AAAI/article/download/4805/4683},
 number = {},
 pages = {8034-8041},
 title = {Visual Place Recognition via Robust â„“2-Norm Distance Based Holism and Landmark Integration},
 url = {https://app.dimensions.ai/details/publication/pub.1120551666},
 volume = {33},
 year = {2019}
}

@inproceedings{pub.1120615224,
 abstract = {Accurate and robust visual localization under a wide range of viewing condition variations including season and illumination changes, as well as weather and day-night variations, is the key component for many computer vision and robotics applications. Under these conditions, most traditional methods would fail to locate the camera. In this paper we present a visual localization algorithm that combines structure-based method and image-based method with semantic information. Given semantic information about the query and database images, the retrieved images are scored according to the semantic consistency of the 3D model and the query image. Then the semantic matching score is used as weight for RANSACâ€™s sampling and the pose is solved by a standard PnP solver. Experiments on the challenging long-term visual localization benchmark dataset demonstrate that our method has significant improvement compared with the state-of-the-arts.},
 author = {Shi, Tianxin and Shen, Shuhan and Gao, Xiang and Zhu, Lingjie},
 booktitle = {2019 IEEE International Conference on Image Processing (ICIP)},
 doi = {10.1109/icip.2019.8802957},
 keywords = {},
 note = {http://arxiv.org/pdf/1904.03803},
 pages = {315-319},
 title = {Visual Localization Using Sparse Semantic 3D Map},
 url = {https://app.dimensions.ai/details/publication/pub.1120615224},
 year = {2019}
}

@inproceedings{pub.1120703833,
 abstract = {Mobile vehicles operating in urban navigation applications can achieve high integrity localisation with high accuracy by using maps of the surroundings. To accomplish this, the map should always have an accurate representation of the environment. Thus, it is necessary to detect and remove the map components that no longer exist in the current environment. This maintains the map compactness and dependability while simplifying the data association problem. This paper addresses the problem of deletion of transient map components by taking advantage of the geometric connection between the map and agent poses in order to establish and update the visibility of each feature. Once the map is created an initial visibility vector is associated with every map element and updated over time. The visibility of a map element which no longer exists is reduced and ultimately removed from the map. We demonstrate our approach in a 2D feature-based map composed of poles and corners extracted from information provided by a Iidar sensor. The experimental results show the map update using a seven-month data set collected in the University of Sydney campus.},
 author = {Berrio, Julie Stephany and Ward, James and Worrall, Stewart and Nebot, Eduardo},
 booktitle = {2019 IEEE Intelligent Vehicles Symposium (IV)},
 doi = {10.1109/ivs.2019.8814189},
 keywords = {},
 pages = {1173-1179},
 title = {Updating the visibility of a feature-based map for long-term maintenance},
 url = {https://app.dimensions.ai/details/publication/pub.1120703833},
 year = {2019}
}

@inproceedings{pub.1120703895,
 abstract = {To operate in an urban environment, an automated vehicle must be capable of accurately estimating its position within a global map reference frame. This is necessary for optimal path planning and safe navigation. To accomplish this over an extended period of time, the global map requires long term maintenance. This includes the addition of newly observable features and the removal of transient features belonging to dynamic objects. The latter is especially important for the long-term use of the map as matching against a map with features that no longer exist can result in incorrect data associations, and consequently erroneous localisation. This paper addresses the problem of removing features from the map that correspond to objects that are no longer observable/present in the environment. This is achieved by assigning a single score which depends on the geometric distribution and characteristics when the features are re-detected (or not) on different occasions. Our approach not only eliminates ephemeral features, but can also be used as a reduction algorithm for highly dense maps. We tested our approach using half a year of weekly drives over the same 500 metre section of road in an urban environment. The results presented demonstrate the validity of the long term approach to map maintenance.},
 author = {Berrio, Julie Stephany and Ward, James and Worrall, Stewart and Nebot, Eduardo},
 booktitle = {2019 IEEE Intelligent Vehicles Symposium (IV)},
 doi = {10.1109/ivs.2019.8814289},
 keywords = {},
 note = {http://arxiv.org/pdf/1809.09774},
 pages = {1166-1172},
 title = {Identifying robust landmarks in feature-based maps},
 url = {https://app.dimensions.ai/details/publication/pub.1120703895},
 year = {2019}
}

@inproceedings{pub.1120979512,
 abstract = {Robust loop-closure detection plays a key role for the long-term robot visual Simultaneous Localization and Mapping(SLAM) in indoor or outdoor environment, due to illumination changes can greatly affect the accuracy of online image matching, and keypoints may fail to match between images taken at the same location but different seasons. In this paper, we propose a robust loop-closure detection method for robot visual SLAM, which adopts invariant representation as image descriptors composed of learned features and adapts to changes in illumination and seasons. We evaluate our method on real datasets and demonstrate its excellent ability to handle illumination changes.},
 author = {Chen, Shilang and Wu, Junjun and Wang, Yanran and Zhou, Lin and Lu, Qinghua and Zhang, Yunzhi},
 booktitle = {2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM)},
 doi = {10.1109/icarm.2019.8833730},
 keywords = {},
 pages = {342-347},
 title = {Robust Loop-Closure Detection with a Learned Illumination Invariant Representation for Robot vSLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1120979512},
 year = {2019}
}

@inproceedings{pub.1121128626,
 abstract = {Mobile robotic systems operate in increasingly realistic scenarios even as users have increased expectations for the duration of autonomous tasks. Mobile robots face unique challenges when operating in environments that change over time, where systems must maintain an accurate representation of the environment with respect to both spatial and temporal dimensions. This paper describes a spatio-temporal technique for extending the autonomy of a mobile robot in a changing environment. This new technique called Obstacle Persistent Adaptive Map Maintenance (OPAMM) uses navigation data collected during normal operations to perform periodic self-maintenance of its environment model. OPAMM implements a probabilistic feature persistence model to predict the survival state of obstacles and update the world model. Maintaining an accurate world model is necessary for extending the long-term autonomy of robots in realistic scenarios. Results show that robots using OPAMM had localizations scores higher than other methods, thus reducing long-term localization degradation.},
 author = {Pitschl, Meredith L. and Pryor, Mitchell W.},
 booktitle = {2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)},
 doi = {10.1109/coase.2019.8843095},
 keywords = {},
 pages = {1023-1028},
 title = {Obstacle Persistent Adaptive Map Maintenance for Autonomous Mobile Robots using Spatio-temporal Reasoning*},
 url = {https://app.dimensions.ai/details/publication/pub.1121128626},
 year = {2019}
}

@article{pub.1121405145,
 abstract = {Map building and map-based relocalization techniques are important for unmanned vehicles operating in urban environments. The existing approaches require expensive high-density laser range finders and suffer from relocalization problems in long-term applications. This study proposes a novel map format called the ClusterMap, on the basis of which an approach to achieving relocalization is developed. The ClusterMap is generated by segmenting the perceived point clouds into different point clusters and filtering out clusters belonging to dynamic objects. A location descriptor associated with each cluster is designed for differentiation. The relocalization in the global map is achieved by matching cluster descriptors between local and global maps. The solution does not require high-density point clouds and high-precision segmentation algorithms. In addition, it prevents the effects of environmental changes on illumination intensity, object appearance, and observation direction. A consistent ClusterMap without any scale problem is built by utilizing a 3D visual-LIDAR simultaneous localization and mapping solution by fusing LIDAR and visual information. Experiments on the KITTI dataset and our mobile vehicle illustrates the effectiveness of the proposed approach.},
 author = {Pan, Zhichen and Chen, Haoyao and Li, Silin and Liu, Yunhui},
 doi = {10.3390/s19194252},
 journal = {Sensors},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/19/19/4252/pdf},
 number = {19},
 pages = {4252},
 title = {ClusterMap Building and Relocalization in Urban Environments for Unmanned Vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1121405145},
 volume = {19},
 year = {2019}
}

@article{pub.1121409380,
 abstract = {Autonomous mobile vehicles are expected to perform persistent and accurate localization with low-cost equipment. To achieve this goal, we propose a stereo camera based visual localization method using a modified laser map, which takes the advantage of both the low cost of camera, and high geometric precision of laser data to achieve long-term performance. Considering that LiDAR and camera give measurements of the same environment in different modalities, the cross-modal invariance is investigated to modify the laser map for visual localization. Specifically, a map learning algorithm is introduced to sample the robust subsets in laser maps that are useful for visual localization using multi-session visual and laser data. Further, a generative map model is derived to describe this cross-modal invariance, based on which two types of measurements are defined to model the laser map points as appropriate visual observations. Tightly coupling these measurements within the local bundle adjustment during online sliding-window based visual odometry, the vehicle can achieve robust localization even one year after the map was built. The effectiveness of the proposed method is evaluated on both the public KITTI datasets and self-collected datasets in our campus, which include seasonal, illumination and object variations. On all experimental localization sessions, our method provides satisfactory results, even when the direction is opposite to that in the mapping session, verifying the superior performance of the laser map based visual localization method.},
 author = {Ding, Xiaqing and Wang, Yue and Xiong, Rong and Li, Dongxuan and Tang, Li and Yin, Huan and Zhao, Liang},
 doi = {10.1109/tits.2019.2942760},
 journal = {IEEE Transactions on Intelligent Transportation Systems},
 keywords = {},
 number = {11},
 pages = {4646-4658},
 title = {Persistent Stereo Visual Localization on Cross-Modal Invariant Map},
 url = {https://app.dimensions.ai/details/publication/pub.1121409380},
 volume = {21},
 year = {2020}
}

@article{pub.1121436741,
 abstract = {Accurate and robust global localization is essential to robotics
applications. We propose a novel global localization method that employs the
map traversability as a hidden observation. The resulting map-corrected
odometry localization is able to provide an accurate belief tensor of the robot
state. Our method can be used for blind robots in dark or highly reflective
areas. In contrast to odometry drift in long-term, our method using only
odometry and the map converges in longterm. Our method can also be integrated
with other sensors to boost the localization performance. The algorithm does
not have any initial state assumption and tracks all possible robot states at
all times. Therefore, our method is global and is robust in the event of
ambiguous observations. We parallel each step of our algorithm such that it can
be performed in real-time (up to ~ 300 Hz) using GPU. We validate our algorithm
in different publicly available floor-plans and show that it is able to
converge to the ground truth fast while being robust to ambiguities.},
 author = {Peng, Cheng and Weikersdorfer, David},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Map as The Hidden Sensor: Fast Odometry-Based Global Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1121436741},
 volume = {},
 year = {2019}
}

@article{pub.1121625697,
 abstract = {As the robot explores the environment, the map grows over time in the
simultaneous localization and mapping (SLAM) system, especially for the large
scale environment. The ever-growing map prevents long-term mapping. In this
paper, we developed a compact cognitive mapping approach inspired by
neurobiological experiments. Inspired from neighborhood cells, neighborhood
fields determined by movement information, i.e. translation and rotation, are
proposed to describe one of distinct segments of the explored environment. The
vertices and edges with movement information below the threshold of the
neighborhood fields are avoided adding to the cognitive map. The optimization
of the cognitive map is formulated as a robust non-linear least squares
problem, which can be efficiently solved by the fast open linear solvers as a
general problem. According to the cognitive decision-making of familiar
environments, loop closure edges are clustered depending on time intervals, and
then parallel computing is applied to perform batch global optimization of the
cognitive map for ensuring the efficiency of computation and real-time
performance. After the loop closure process, scene integration is performed, in
which revisited vertices are removed subsequently to further reduce the size of
the cognitive map. A monocular visual SLAM system is developed to test our
approach in a rat-like maze environment. Our results suggest that the method
largely restricts the growth of the size of the cognitive map over time, and
meanwhile, the compact cognitive map correctly represents the overall layer of
the environment as the standard one. Experiments demonstrate that our method is
very suited for compact cognitive mapping to support long-term robot mapping.
Our approach is simple, but pragmatic and efficient for achieving the compact
cognitive map.},
 author = {Zeng, Taiping and Si, Bailu},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {A Brain-Inspired Compact Cognitive Mapping System},
 url = {https://app.dimensions.ai/details/publication/pub.1121625697},
 volume = {},
 year = {2019}
}

@article{pub.1121628845,
 abstract = {Abstract
                  Abstract 1449
                  Poster Board I-472
                  Osteoblastic cells have been identified as a component of the hematopoietic stem cell (HSC) niche. This identification provides a novel strategy for in vivo expansion of HSCs by stimulating their regulatory microenvironment. This therapeutic approach could potentially expand the clinical use of HSCs in conditions where their number limits use. Activation of osteoblastic cells by Parathyroid Hormone (PTH) increases bone, osteoblastic and osteoclastic cell number and expands the HSC pool. Since HSCs do not express the PTH receptor, and genetic activation of PTH receptors in osteoblastic cells is sufficient to expand HSCs, the HSC increase must be initiated by PTH-dependent activation of osteoblastic cells. The HSC-enriched lineage-, sca-1+, and c-kit+ (LSK) compartment is heterogeneous and contains at least three subpopulations of cells with multilineage potential, but with progressively decreased quiescence and more limited self-renewal. In the initial evaluation of PTH effects on the HSC niche, the effects of osteoblastic activation on HSC subsets were not characterized in detail. The effect of PTH and osteoblastic activation on individual HSC subsets could suggest additional therapeutic uses if PTH increases not only Long Term-HSC (LT-HSC) but also the more proliferative Short-Term HSCs/ Multi Potent Progenitors (ST-HSCs/MPPs). In this study, our goal was to identify changes in the proportion of LT-HSC vs ST-HSC and MPPs after PTH treatment by utilizing flow cytometric analysis and competitive repopulation assays (primary, secondary and tertiary reconstitution). We developed an accelerated PTH treatment regimen in mice (40Î¼g/kg body weight ip three times daily for 10 days), which minimizes the effects of normal aging on bone and hematopoiesis. We first analyzed the effect of this PTH regimen on bone. PTH-treated mice had increased trabecular bone volume (% BV/TV VEH 25Â±2 vs PTH 43Â±4 p<0.001) compared to their vehicle-treated controls when analyzed by micro-CT. Histologic analysis demonstrated a dramatic increase in trabecular bone as well as osteoblastic cells lining the trabecular surfaces and TRAP+ osteoclasts in the PTH-treated animals. PTH-treated mice also had increased primitive hematopoietic cells defined through flow cytometry as LSK (VEH 0.19Â±0.02 vs PTH 0.27Â±0.01 p<0.005). LSKs were further subdivided to phenotypically identify HSC subsets using the SLAM receptors. Surprisingly the frequency of subpopulations was increased in PTH treatment increased not only LT-HSCs (VEH 0.025Â±0.002 vs PTH 0.040Â±0.002 p<0.0001), but also the less quiescent ST-HSCs/MPPs (VEH 0.041Â±0.001 vs PTH 0.065Â±0.004 p<0.0001). HSC subsets can also be identified within the LSK pool based on expression of Flt3 and Thy1.1. This phenotypic analysis of bone marrow cells from mice treated with the intensive PTH regimen demonstrated a significant increase in all LSK subsets including MPPs (VEH 0.043Â±0.003 vs PTH 0.061Â±0.004 p<0.005), identified as LSK Flt3+ Thy1.1low cells, LSK Flt3âˆ’ Thy1.1int (VEH 0.056Â±0.005 vs PTH 0.077Â±0.004 p<0.01), enriched for ST-HSCs, and the LT-HSC enriched LSK Flt3âˆ’ Thy1.1int subset (VEH 0.042Â±0.004 vs PTH 0.054Â±0.002 p<0.05). Taken together, these data suggest that systemic PTH treatment not only expands the most quiescent HSCs, but also increases ST-HSC/MPPs. Since PTH treatment increased all phenotypic HSC subsets, transplantation of hematopoietic bone marrow cells from PTH treated mice would be expected to have superior engraftment both in the short and long term. Accordingly, a significant increase in both early and late hematopoietic reconstitution was observed in primary transplant recipients of bone marrow from PTH-treated animals, as well as secondary and tertiary transplant recipients. These results also suggest that PTH treatment equally expands all HSC subsets. The novel finding that the less quiescent, multipotent ST-HSC and MPPs are also expanded by PTH may be of great therapeutic relevance, as it supports the use of osteoblastic stimulation by PTH not only to increase long term repopulation, but also to critically improve accelerated hematopoietic recovery after myeloablation. Thus, manipulation of the HSC niche may result in beneficial effects beyond their ability to increase the most quiescent HSCs. Further studies will elucidate the specific mechanisms causing this novel global increase of multipotent primitive hematopoietic cells.
                  
                    Disclosures
                    No relevant conflicts of interest to declare.
                  },
 author = {Weber, Jonathan M and Frisch, Benjamin J and Porter, Rebecca L and Bromberg, Olga and Olm-shipman, Adam J and Awad, Hani and Calvi, Laura M},
 doi = {10.1182/blood.v114.22.1449.1449},
 journal = {Blood},
 keywords = {},
 number = {22},
 pages = {1449-1449},
 title = {In Vivo Parathyroid Hormone Treatment Expands All Multipotent Primitive Hematopoietic Cell Subsets.},
 url = {https://app.dimensions.ai/details/publication/pub.1121628845},
 volume = {114},
 year = {2009}
}

@inproceedings{pub.1121804001,
 abstract = {Bathymetric navigation enables the long-term operation of autonomous underwater vehicles by reducing navigation drift errors with no need for GPS position fixes. In the case that a bathymetric map is not available, the simultaneous localization and mapping (SLAM) algorithm is required, but this increases computational complexity and memory requirement. Panel-based bathymetric SLAM could considerably reduce the computational burden. However, it may suffers from incorrect update when the vehicle does not belong to the updated panel. This study proposes a new update method, called weighted grid partitioning, which considers the probability distribution of a vehicle's location, and is more effective in terms of the map accuracy, computational burden, and memory usage compared to standard update methods. The feasibility of the proposed algorithm is verified through simulations.},
 author = {Jang, Junwoo and Kim, Jinwhan},
 booktitle = {OCEANS 2019 - Marseille},
 doi = {10.1109/oceanse.2019.8867531},
 keywords = {},
 pages = {1-6},
 title = {Weighted Grid Partitioning for Panel-Based Bathymetric SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1121804001},
 year = {2019}
}

@inproceedings{pub.1121898729,
 abstract = {Occupancy mapping enables a mobile robot to make intelligent planning decisions to accomplish its tasks. Adaptive local maps is an algorithm which represents the occupancy information as a set of overlapping local maps anchored to poses in the robot's trajectory. At any time, a global occupancy map can be rendered from the local maps to be used for path planning. The advantage of this approach is that the occupancy information stays consistent despite the changes in the pose estimates resulting from loop closures and localization updates. The disadvantage, however, is that the number of local maps grows over time. For long robot runs, or for multiple runs in the same space, this growth will result in redundant occupancy information, which will in turn increase the time it takes to render the global map, as well as the memory footprint of the system. In this paper, we propose a novel approach for the maintenance of an adaptive local maps system, which intelligently prunes redundant local maps, ensuring the robustness and stability required for lifelong mapping.},
 author = {Banerjee, Nandan and Lisin, Dimitri and Briggs, Jimmy and Llofriu, Martin and Munich, Mario E.},
 booktitle = {2019 European Conference on Mobile Robots (ECMR)},
 doi = {10.1109/ecmr.2019.8870347},
 keywords = {},
 pages = {1-8},
 title = {Lifelong Mapping using Adaptive Local Maps},
 url = {https://app.dimensions.ai/details/publication/pub.1121898729},
 year = {2019}
}

@inproceedings{pub.1121898750,
 abstract = {Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation [1]. In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15 months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy.},
 author = {Schaefer, Alexander and BÃ¼scher, Daniel and Vertens, Johan and Luft, Lukas and Burgard, Wolfram},
 booktitle = {2019 European Conference on Mobile Robots (ECMR)},
 doi = {10.1109/ecmr.2019.8870928},
 keywords = {},
 note = {http://arxiv.org/pdf/1910.10550},
 pages = {1-7},
 title = {Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from 3-D Lidar Scans},
 url = {https://app.dimensions.ai/details/publication/pub.1121898750},
 year = {2019}
}

@article{pub.1122023795,
 abstract = {Due to their ubiquity and long-term stability, pole-like objects are well
suited to serve as landmarks for vehicle localization in urban environments. In
this work, we present a complete mapping and long-term localization system
based on pole landmarks extracted from 3-D lidar data. Our approach features a
novel pole detector, a mapping module, and an online localization module, each
of which are described in detail, and for which we provide an open-source
implementation at www.github.com/acschaefer/polex. In extensive experiments, we
demonstrate that our method improves on the state of the art with respect to
long-term reliability and accuracy: First, we prove reliability by tasking the
system with localizing a mobile robot over the course of 15~months in an urban
area based on an initial map, confronting it with constantly varying routes,
differing weather conditions, seasonal changes, and construction sites. Second,
we show that the proposed approach clearly outperforms a recently published
method in terms of accuracy.},
 author = {Schaefer, Alexander and BÃ¼scher, Daniel and Vertens, Johan and Luft, Lukas and Burgard, Wolfram},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from
  3-D Lidar Scans},
 url = {https://app.dimensions.ai/details/publication/pub.1122023795},
 volume = {},
 year = {2019}
}

@article{pub.1122148667,
 abstract = {Localization, or position fixing, is an important problem in robotics
research. In this paper, we propose a novel approach for long-term localization
in a changing environment using 3D LiDAR. We first create the map of a real
environment using GPS and LiDAR. Then, we divide the map into several small
parts as the targets for cloud registration, which can not only improve the
robustness but also reduce the registration time. PointLocalization allows us
to fuse different kinds of odometers, which can optimize the accuracy and
frequency of localization results. We evaluate our algorithm on an unmanned
ground vehicle (UGV) using LiDAR and a wheel encoder, and obtain the
localization results at more than 20 Hz after fusion. The algorithm can also
localize the UGV in a 180-degree field of view (FOV). Using an outdated map
captured six months ago, this algorithm shows great robustness, and the test
results show that it can achieve an accuracy of 10 cm. PointLocalization has
been tested for a period of more than six months in a crowded factory and has
operated successfully over a distance of more than 2000 km.},
 author = {Zhu, Yilong and Xue, Bohuan and Zheng, Linwei and Huang, Huaiyang and Liu, Ming and Fan, Rui},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Real-Time, Environmentally-Robust 3D LiDAR Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1122148667},
 volume = {},
 year = {2019}
}

@inproceedings{pub.1122233498,
 abstract = {Egomotion and scene estimation is a key component in automating robot navigation, as well as in virtual reality applications for mobile phones or head-mounted displays. It is well known, however, that with long exploratory trajectories and multi-session mapping for long-term autonomy or collaborative applications, the maintenance of the ever-increasing size of these maps quickly becomes a bottleneck. With the explosion of data resulting in increasing runtime of the optimization algorithms ensuring the accuracy of the Simultaneous Localization And Mapping (SLAM) estimates, the large quantity of collected experiences is imposing hard limits on the scalability of such techniques. Considering the keyframe-based paradigm of SLAM techniques, this paper investigates the redundancy inherent in SLAM maps, by quantifying the information of different experiences of the scene as encoded in keyframes. Here we propose and evaluate different information-theoretic and heuristic metrics to remove dispensable scene measurements with minimal impact on the accuracy of the SLAM estimates. Evaluating the proposed metrics in two state-of-the-art centralized collaborative SLAM systems, we provide our key insights into how to identify redundancy in keyframe-based SLAM.},
 author = {Schmuck, Patrik and Chli, Margarita},
 booktitle = {2019 International Conference on 3D Vision (3DV)},
 doi = {10.1109/3dv.2019.00071},
 keywords = {},
 note = {https://www.research-collection.ethz.ch/bitstream/20.500.11850/380451.1/1/2019_redundancy_detection.pdf},
 pages = {594-603},
 title = {On the Redundancy Detection in Keyframe-based SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1122233498},
 year = {2019}
}

@article{pub.1122259532,
 abstract = {There are several challenges for search and rescue robots: mobility,
perception, autonomy, and communication. Inspired by the DARPA Subterranean
(SubT) Challenge, we propose an autonomous blimp robot, which has the
advantages of low power consumption and collision-tolerance compared to other
aerial vehicles like drones. This is important for search and rescue tasks that
usually last for one or more hours. However, the underground constrained
passages limit the size of blimp envelope and its payload, making the proposed
system resource-constrained. Therefore, a careful design consideration is
needed to build a blimp system with on-board artifact search and SLAM. In order
to reach long-term operation, a failure-aware algorithm with minimal
communication to human supervisor to have situational awareness and send
control signals to the blimp when needed.},
 author = {Huang, Yi-Wei and Lu, Chen-Lung and Chen, Kuan-Lin and Ser, Po-Sheng and Huang, Jui-Te and Shen, Yu-Chia and Chen, Pin-Wei and Chang, Po-Kai and Lee, Sheng-Cheng and Wang, Hsueh-Cheng},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Duckiefloat: a Collision-Tolerant Resource-Constrained Blimp for
  Long-Term Autonomy in Subterranean Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1122259532},
 volume = {},
 year = {2019}
}

@article{pub.1122336514,
 abstract = {Mobile robots depend on maps for localization, planning, and other
applications. In indoor scenarios, there is often lots of clutter present, such
as chairs, tables, other furniture, or plants. While mapping this clutter is
important for certain applications, for example navigation, maps that represent
just the immobile parts of the environment, i.e. walls, are needed for other
applications, like room segmentation or long-term localization. In literature,
approaches can be found that use a complete point cloud to remove the furniture
in the room and generate a furniture free map. In contrast, we propose a
Simultaneous Localization And Mapping (SLAM)-based mobile laser scanning
solution. The robot uses an orthogonal pair of Lidars. The horizontal scanner
aims to estimate the robot position, whereas the vertical scanner generates the
furniture free map. There are three steps in our method: point cloud
rearrangement, wall plane detection and semantic labeling. In the experiment,
we evaluate the efficiency of removing furniture in a typical indoor
environment. We get $99.60\%$ precision in keeping the wall in the 3D result,
which shows that our algorithm can remove most of the furniture in the
environment. Furthermore, we introduce the application of 2D furniture free
mapping for room segmentation.},
 author = {He, Zhenpeng and Hou, Jiawei and Schwertfeger, SÃ¶ren},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Furniture Free Mapping using 3D Lidars},
 url = {https://app.dimensions.ai/details/publication/pub.1122336514},
 volume = {},
 year = {2019}
}

@article{pub.1122562981,
 abstract = {Service robots should be able to operate autonomously in dynamic and daily
changing environments over an extended period of time. While Simultaneous
Localization And Mapping (SLAM) is one of the most fundamental problems for
robotic autonomy, most existing SLAM works are evaluated with data sequences
that are recorded in a short period of time. In real-world deployment, there
can be out-of-sight scene changes caused by both natural factors and human
activities. For example, in home scenarios, most objects may be movable,
replaceable or deformable, and the visual features of the same place may be
significantly different in some successive days. Such out-of-sight dynamics
pose great challenges to the robustness of pose estimation, and hence a robot's
long-term deployment and operation. To differentiate the forementioned problem
from the conventional works which are usually evaluated in a static setting in
a single run, the term \textit{lifelong SLAM} is used here to address SLAM
problems in an ever-changing environment over a long period of time. To
accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The
data are collected in real-world indoor scenes, for multiple times in each
place to include scene changes in real life. We also design benchmarking
metrics for lifelong SLAM, with which the robustness and accuracy of pose
estimation are evaluated separately. The datasets and benchmark are available
online at https://lifelong-robotic-vision.github.io/dataset/scene.},
 author = {Shi, Xuesong and Li, Dongjiang and Zhao, Pengpeng and Tian, Qinbin and Tian, Yuxin and Long, Qiwei and Zhu, Chunhao and Song, Jingwei and Qiao, Fei and Song, Le and Guo, Yangquan and Wang, Zhigang and Zhang, Yimin and Qin, Baoxing and Yang, Wei and Wang, Fangshi and Chan, Rosa H. M. and She, Qi},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for
  Lifelong SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1122562981},
 volume = {},
 year = {2019}
}

@inproceedings{pub.1122596774,
 abstract = {To empower an autonomous robot to perform long-term navigation in a given area, a concurrent localization and map update algorithm is required. In this paper, we tackle this problem by providing both theoretical analysis and algorithm design for robotic systems equipped with 2D laser range finders. The first key contribution of this paper is that we propose a hybrid signed distance field (SDF) framework for laser based localization. The proposed hybrid SDF integrates two methods with complementary characteristics, namely Euclidean SDF (ESDF) and Truncated SDF (TSDF). With our framework, accurate pose estimation and fast map update can be performed simultaneously. Moreover, we introduce a novel sliding window estimator which attains better accuracy by consistently utilizing sensor and map information with both scan-to-scan and scan-to-map data association. Real-world experimental results demonstrate that the proposed algorithm can be used for commercial robots in various environments with long-term usage. Experiments also show that our approach outperforms competing approaches by a wide margin.},
 author = {Zhang, Mingming and Chen, Yiming and Li, Mingyang},
 booktitle = {2019 American Control Conference (ACC)},
 doi = {10.23919/acc.2019.8814347},
 keywords = {},
 pages = {1997-2004},
 title = {SDF-Loc: Signed Distance Field based 2D Relocalization and Map Update in Dynamic Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1122596774},
 year = {2019}
}

@inproceedings{pub.1123028223,
 abstract = {Accurate localization is a vital prerequisite for future assistance or autonomous driving functions in intelligent vehicles. To achieve the required localization accuracy and availability, long-term visual SLAM algorithms like LLama-SLAM are a promising option. In such algorithms visual feature tracks, i. e. landmark observations over several consecutive image frames, have to be matched to feature tracks recorded days, weeks or months earlier. This leads to a more challenging matching problem than in short-term visual localization and known descriptor matching methods cannot be applied directly. In this paper, we devise several approaches to compare and match feature tracks and evaluate their performance on a long-term data set. With the proposed descriptor combination and masking ("CoMa") method the best track matching performance is achieved with minor computational cost. This method creates a single combined descriptor for each feature track and furthermore increases the robustness by capturing the appearance variations of this track in a descriptor mask.},
 author = {Luthardt, Stefan and Ziegler, Christoph and Willert, Volker and Adamy, JÃ¼rgen},
 booktitle = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
 doi = {10.1109/itsc.2019.8916895},
 keywords = {},
 note = {https://tuprints.ulb.tu-darmstadt.de/9108/1/Luthardt_ITSC_2019_FeatureTracks.pdf},
 pages = {934-941},
 title = {How to Match Tracks of Visual Features for Automotive Long-Term SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1123028223},
 year = {2019}
}

@inproceedings{pub.1123028854,
 abstract = {Accurate localization is one of the fundamental tasks of vehicles visual navigation in parking lots. In this paper, we propose a practical and novel solution, which exploits road marking semantic segmentation to attack the problem of long-term and high-precision visual localization. Based on the semantic data association derived from road markings segmentation, point cloud fusion and loop detection strategies are designed to improve the performance of semantic map building. Applying the generated map, we present a point cloud registration algorithm combining semantic and geometric inference to improve the localization precision. Experiments on real-world indoor parking lots prove that the semantic map created by the proposed method reveals more accurate and consistent performance. Moreover, localization error is no more than 10cm, while running in real-time performance.},
 author = {Hu, Jiaxin and Yang, Ming and Xu, Hanqing and He, Yuesheng and Wang, Chunxiang},
 booktitle = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
 doi = {10.1109/itsc.2019.8917529},
 keywords = {},
 pages = {4068-4073},
 title = {Mapping and Localization using Semantic Road Marking with Centimeter-level Accuracy in Indoor Parking Lots},
 url = {https://app.dimensions.ai/details/publication/pub.1123028854},
 year = {2019}
}

@article{pub.1123512688,
 abstract = {Visual Odometry (VO) accumulates a positional drift in long-term robot
navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in
various aspects, VO still suffers from moving obstacles, discontinuous
observation of features, and poor textures or visual information. While recent
approaches estimate a 6DoF pose either directly from (a series of) images or by
merging depth maps with optical flow (OF), research that combines absolute pose
regression with OF is limited. We propose ViPR, a novel modular architecture
for long-term 6DoF VO that leverages temporal information and synergies between
absolute pose estimates (from PoseNet-like modules) and relative pose estimates
(from FlowNet-based modules) by combining both through recurrent layers.
Experiments on known datasets and on our own Industry dataset show that our
modular design outperforms state of the art in long-term navigation tasks.},
 author = {Ott, Felix and Feigl, Tobias and LÃ¶ffler, Christoffer and Mutschler, Christopher},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {ViPR: Visual-Odometry-aided Pose Regression for 6DoF Camera Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1123512688},
 volume = {},
 year = {2019}
}

@inproceedings{pub.1123674290,
 abstract = {Identifying request or task for assistance in an elderly or retirement home is a challenge for service robot. It is an essential task for service robot to give their services along with human. Identifying human position in an environment, such as elderly home, is the most challenging task, as it involves static and dynamic object. During its operation, a service robot must avoid collision to objects, and also prepares path for its journey. This work makes use of RGB-D camera, such as Kinect, to identify the static and dynamic object via 3D maps. To manage its path finding, the prediction of people location and position is enabled via Long Term and Short Term Memory been implemented in Real-Time Appearance Based Mapping. The loop closure detection with real time constraints wrapping the collected information that uses RGB-D SLAM method. To identify the human position and flow, a technique of pedestrian detection is added to find group of people, which will gain more understanding on how human will interact to robot. We have demonstrated the work of this initial work by generating 3D point cloud for specific environment through an intensive mapping experiments.},
 author = {Murwantara, I. Made and Hardjono, Benny and Putra, Alfa Satya and Tjahyadi, Hendra},
 booktitle = {Proceedings of the 2019 2nd International Conference on Sensors, Signal and Image Processing  - SSIP 2019},
 doi = {10.1145/3365245.3365264},
 keywords = {},
 pages = {62-66},
 title = {Towards Elderly Assistance Identification for Service Robot Using Combination of Mappings},
 url = {https://app.dimensions.ai/details/publication/pub.1123674290},
 year = {2019}
}

@article{pub.1123881997,
 abstract = {To enable long term exploration of extreme environments such as planetary surfaces, heterogeneous robotic teams need the ability to localize themselves on previously built maps. While the Localization and Mapping problem for single sessions can be efficiently solved with many state of the art solutions, place recognition in natural environments still poses great challenges for the perception system of a robotic agent. In this paper we propose a relocalization pipeline which exploits both 3D and visual information from stereo cameras to detect matches across local point clouds of multiple SLAM sessions. Our solution is based on a Bag of Binary Words scheme where binarized SHOT descriptors are enriched with visual cues to recall in a fast and efficient way previously visited places. The proposed relocalization scheme is validated on challenging datasets captured using a planetary rover prototype on Mount Etna, designated as a Moon analogue environment.},
 author = {Giubilato, Riccardo and Vayugundla, Mallikarjuna and Schuster, Martin J. and Strzl, Wolfgang and Wedler, Armin and Triebel, Rudolph and Debei, Stefano},
 doi = {10.1109/lra.2020.2964157},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 note = {https://elib.dlr.de/133896/1/preprint.pdf},
 number = {2},
 pages = {580-587},
 title = {Relocalization With Submaps: Multi-Session Mapping for Planetary Rovers Equipped With Stereo Cameras},
 url = {https://app.dimensions.ai/details/publication/pub.1123881997},
 volume = {5},
 year = {2020}
}

@article{pub.1124243195,
 abstract = {A globally consistent map is the basis of indoor robot localization and navigation. However, map built by Rao-Blackwellized Particle Filter (RBPF) doesnâ€™t have high global consistency which is not suitable for long-term application in large scene. To address the problem, we present an improved RBPF Lidar SLAM system with loop detection and correction named LCPF. The efficiency and accuracy of loop detection depend on the segmentation of submaps. Instead of dividing the submap at fixed number of laser scan like existing method, Dynamic Submap Segmentation is proposed in LCPF. This segmentation algorithm decreases the error inside the submap by splitting the submap where there is high scan match error and later rectifies the error by an improved pose graph optimization between submaps. In order to segment the submap at appropriate point, when to create a new submap is determined by both the accumulation of scan match error and the particle distribution. Furthermore, LCPF uses branch and bound algorithm as basic detector for loop detection and multiple criteria to judge the reliability of a loop. In the criteria, a novel parameter called usable ratio was proposed to measure the useful information that a laser scan containing. Finally, comparisons to existing 2D-Lidar mapping algorithm are performed with a series of open dataset simulations and real robot experiments to demonstrate the effectiveness of LCPF.},
 author = {Nie, Fuyu and Zhang, Weimin and Yao, Zhuo and Shi, Yongliang and Li, Fangxing and Huang, Qiang},
 doi = {10.1109/access.2020.2968353},
 journal = {IEEE Access},
 keywords = {},
 note = {https://doi.org/10.1109/access.2020.2968353},
 number = {},
 pages = {20401-20412},
 title = {LCPF: A Particle Filter Lidar SLAM System With Loop Detection and Correction},
 url = {https://app.dimensions.ai/details/publication/pub.1124243195},
 volume = {8},
 year = {2020}
}

@article{pub.1124244331,
 abstract = {Long-term metric self-localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems due to appearance changes caused by lighting, weather, or seasonal variations. While experience-based mapping has proven to be an effective technique for bridging the appearance gap, the number of experiences required for reliable metric localization over days or months can be very large, and methods for reducing the necessary number of experiences are needed for this approach to scale. Taking inspiration from color constancy theory, we learn a nonlinear RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature matches for images captured under different lighting and weather conditions, and use it as a pre-processing step in a conventional single-experience localization pipeline to improve its robustness to appearance change. We train this mapping by approximating the target non-differentiable localization pipeline with a deep neural network, and find that incorporating a learned low-dimensional context feature can further improve cross-appearance feature matching. Using synthetic and real-world datasets, we demonstrate substantial improvements in localization performance across day-night cycles, enabling continuous metric localization over a 30-hour period using a single mapping experience, and allowing experience-based localization to scale to long deployments with dramatically reduced data requirements.},
 author = {Clement, Lee and Gridseth, Mona and Tomasi, Justin and Kelly, Jonathan},
 doi = {10.1109/lra.2020.2967659},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 note = {http://arxiv.org/pdf/1904.01080},
 number = {2},
 pages = {1492-1499},
 title = {Learning Matchable Image Transformations for Long-Term Metric Visual Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1124244331},
 volume = {5},
 year = {2020}
}

@inproceedings{pub.1124425450,
 abstract = {Accurate vehicle localization is arguably the most critical and fundamental task for autonomous vehicle navigation. While dense 3D point-cloud-based maps enable precise localization, they impose significant storage and transmission burdens when used in city-scale environments. In this paper, we propose a highly compressed representation for LiDAR maps, along with an efficient and robust real-time alignment algorithm for on-vehicle LiDAR scans. The proposed mapping framework, which we refer to as Feature Likelihood Acquisition Map Emulation (FLAME), requires less than 0.1% of the storage space of the original 3D point cloud map. In essence, FLAME emulates an original map through feature likelihood functions. In particular, FLAME models planar, pole and curb features. These three feature classes are long-term stable, distinct and common among vehicular roadways. Multiclass feature points are extracted from LiDAR scans through feature detection. A new multiclass-based point-to-distribution alignment method is proposed to find the association and alignment between the multiclass feature points and the FLAME map. The experimental results show that the proposed framework can achieve the same level of accuracy (less than 10cm) as the 3D point cloud based localization.},
 author = {Pang, Su and Kent, Daniel and Morris, Daniel and Radha, Hayder},
 booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros40897.2019.8968082},
 keywords = {},
 pages = {5312-5319},
 title = {FLAME: Feature-Likelihood Based Mapping and Localization for Autonomous Vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1124425450},
 year = {2019}
}

@inproceedings{pub.1124428340,
 abstract = {With the advance in the field of mobile robots, autonomous robots are required for long-term deployment in dynamic and complex environments. However, the performance of Visual Inertial SLAM systems in long-term operation is not satisfactory, and most long-term SLAM systems assumes periodic changes in the environment. This paper presents a novel solution for long-term monocular VI SLAM system in dynamic environment based on autoregression(AR) modeling and map prediction. Map points are first classified into static and semi-static map points according to a memory model. Modeling and prediction of the different states of semi-static map points are performed that are derived from time series models. The predicted map is then fused with the current map to achieve a better forecast for the next frame if the prediction is not satisfactory enough. Experiments are carried out on an embedded system. The results indicate that the map prediction is reliable and the proposed approach improves the performance of long-term localization and mapping in dynamic environments.},
 author = {Song, Bowen and Chen, Weidong and Wang, Jingchuan and Wang, Hesheng},
 booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros40897.2019.8968017},
 keywords = {},
 pages = {5364-5369},
 title = {Long-Term Visual Inertial SLAM based on Time Series Map Prediction},
 url = {https://app.dimensions.ai/details/publication/pub.1124428340},
 year = {2019}
}

@inproceedings{pub.1124428925,
 abstract = {Visual localization is one of the primary capabilities for mobile robots. Long-term visual localization in real time is particularly challenging, in which the robot is required to efficiently localize itself using visual data where appearance may change significantly over time. In this paper, we propose a cloud-based visual localization system targeting at long-term localization in real time. On the robot, we employ two estimators to achieve accurate and real-time performance. One is a sliding-window based visual inertial odometry, which integrates constraints from consecutive observations and self-motion measurements, as well as the constraints induced by localization results from the cloud. This estimator builds a local visual submap as the virtual observation which is then sent to the cloud as new localization constraints. The other one is a delayed state Extended Kalman Filter to fuse the pose of the robot localized from the cloud, the local odometry and the high-frequency inertial measurements. On the cloud, we propose a longer sliding-window based localization method to aggregate the virtual observations for larger field of view, leading to more robust alignment between virtual observations and the map. Under this architecture, the robot can achieve drift-free and real-time localization using onboard resources even in a network with limited bandwidth, high latency and existence of package loss, which enables the autonomous navigation in real-world environment. We evaluate the effectiveness of our system on a dataset with challenging seasonal and illuminative variations. We further validate the robustness of the system under challenging network conditions.},
 author = {Ding, Xiaqing and Wang, Yue and Tang, Li and Yin, Huan and Xiong, Rong},
 booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros40897.2019.8968550},
 keywords = {},
 note = {http://arxiv.org/pdf/1903.03968},
 pages = {2159-2166},
 title = {Communication constrained cloud-based long-term visual localization in real time},
 url = {https://app.dimensions.ai/details/publication/pub.1124428925},
 year = {2019}
}

@inproceedings{pub.1124430782,
 abstract = {In this paper, we compare different map management techniques for long-term visual navigation in changing environments. In this scenario, the navigation system needs to continuously update and refine its feature map in order to adapt to the environment appearance change. To achieve reliable long-term navigation, the map management techniques have to (i) select features useful for the current navigation task, (ii) remove features that are obsolete, (iii) and add new features from the current camera view to the map. We propose several map management strategies and evaluate their performance with regard to the robot localisation accuracy in long-term teach-and-repeat navigation. Our experiments, performed over three months, indicate that strategies which model cyclic changes of the environment appearance and predict which features are going to be visible at a particular time and location, outperform strategies which do not explicitly model the temporal evolution of the changes.},
 author = {Halodova, Lucie and Dvorrakova, Eliska and Majer, Filip and Vintr, Tomas and Mozos, Oscar Martinez and Dayoub, Feras and Krajnik, Tomas},
 booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros40897.2019.8967994},
 keywords = {},
 pages = {7033-7039},
 title = {Predictive and adaptive maps for long-term visual navigation in changing environments},
 url = {https://app.dimensions.ai/details/publication/pub.1124430782},
 year = {2019}
}

@inproceedings{pub.1124671268,
 abstract = {In this paper, we utilize semantically enhanced feature matching and visual inertial bundle adjustment to improve the robustness of odometry especially in feature-sparse environments. A novel semantically enhanced feature matching algorithm is developed for robust: 1) medium and long-term tracking, and 2) loop-closing. Additionally, a semantic visual inertial bundle adjustment algorithm is introduced to robustly estimate pose in presence of ambiguous correspondences or in feature sparse environment. Our tightly coupled semantic RGB-D odometry approach is demonstrated on a real world indoor dataset collected using our unmanned ground vehicle (UGV). Our approach improves traditional visual odometry relying on low-level geometric features like corners, points, and planes for localization and mapping. Additionally, prior approaches are limited due to their sensitivity to scene geometry and changes in light intensity. The semantic inertial odometry is especially important to significantly reduce drifts in longer intervals.},
 author = {Patel, Naman and Khorrami, Farshad and Krishnamurthy, Prashanth and Tzes, Anthony},
 booktitle = {2019 19th International Conference on Advanced Robotics (ICAR)},
 doi = {10.1109/icar46387.2019.8981658},
 keywords = {},
 pages = {523-528},
 title = {Tightly Coupled Semantic RGB-D Inertial Odometry for Accurate Long-Term Localization and Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1124671268},
 year = {2019}
}

@article{pub.1124681418,
 abstract = {We propose a novel online learning algorithm, called SpCoSLAM 2.0, for spatial concepts and lexical acquisition with high accuracy and scalability. Previously, we proposed SpCoSLAM as an online learning algorithm based on unsupervised Bayesian probabilistic model that integrates multimodal place categorization, lexical acquisition, and SLAM. However, our original algorithm had limited estimation accuracy owing to the influence of the early stages of learning, and increased computational complexity with added training data. Therefore, we introduce techniques such as fixed-lag rejuvenation to reduce the calculation time while maintaining an accuracy higher than that of the original algorithm. The results show that, in terms of estimation accuracy, the proposed algorithm exceeds the original algorithm and is comparable to batch learning. In addition, the calculation time of the proposed algorithm does not depend on the amount of training data and becomes constant for each step of the scalable algorithm. Our approach will contribute to the realization of long-term spatial language interactions between humans and robots.},
 author = {Taniguchi, Akira and Hagiwara, Yoshinobu and Taniguchi, Tadahiro and Inamura, Tetsunari},
 doi = {10.1007/s10514-020-09905-0},
 journal = {Autonomous Robots},
 keywords = {},
 note = {https://link.springer.com/content/pdf/10.1007/s10514-020-09905-0.pdf},
 number = {6},
 pages = {927-946},
 title = {Improved and scalable online learning of spatial concepts and language models with mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1124681418},
 volume = {44},
 year = {}
}

@article{pub.1124938241,
 abstract = {We present a unified representation for actionable spatial perception: 3D
Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent
entities in the scene (e.g. objects, walls, rooms), and edges represent
relations (e.g. inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs)
extend this notion to represent dynamic scenes with moving agents (e.g. humans,
robots), and to include actionable information that supports planning and
decision-making (e.g. spatio-temporal relations, topology at different levels
of abstraction). Our second contribution is to provide the first fully
automatic Spatial PerceptIon eNgine(SPIN) to build a DSG from visual-inertial
data. We integrate state-of-the-art techniques for object and human detection
and pose estimation, and we describe how to robustly infer object, robot, and
human nodes in crowded scenes. To the best of our knowledge, this is the first
paper that reconciles visual-inertial SLAM and dense human mesh tracking.
Moreover, we provide algorithms to obtain hierarchical representations of
indoor environments (e.g. places, structures, rooms) and their relations. Our
third contribution is to demonstrate the proposed spatial perception engine in
a photo-realistic Unity-based simulator, where we assess its robustness and
expressiveness. Finally, we discuss the implications of our proposal on modern
robotics applications. 3D Dynamic Scene Graphs can have a profound impact on
planning and decision-making, human-robot interaction, long-term autonomy, and
scene prediction. A video abstract is available at https://youtu.be/SWbofjhyPzI},
 author = {Rosinol, Antoni and Gupta, Arjun and Abate, Marcus and Shi, Jingnan and Carlone, Luca},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {3D Dynamic Scene Graphs: Actionable Spatial Perception with Places,
  Objects, and Humans},
 url = {https://app.dimensions.ai/details/publication/pub.1124938241},
 volume = {},
 year = {2020}
}

@inproceedings{pub.1125154781,
 abstract = {Localization, or position fixing, is an important problem in robotics research. In this paper, we propose a novel approach for long-term localization in a changing environment using 3D LiDAR. We first create the map of a real environment using GPS and LiDAR. Then, we divide the map into several small parts as the targets for cloud registration, which can not only improve the robustness but also reduce the registration time. We proposed a localization method called PointLocalization. PointLocalization allows us to fuse different kinds of odometers, which can optimize the accuracy and frequency of localization results. We evaluate our algorithm on an unmanned ground vehicle (UGV) using LiDAR and a wheel encoder, and obtain the localization results at more than 20 Hz after fusion. The algorithm can also localize the UGV in a 180-degree field of view (FOV). Using an outdated map captured six months ago, this algorithm shows great robustness, and the test results show that it can achieve an accuracy of 10 cm. PointLocalization has been tested for a period of more than six months in a crowded factory and has operated successfully over a distance of more than 2000 km.},
 author = {Zhu, Yilong and Xue, Bohuan and Zheng, Linwei and Huang, Huaiyang and Liu, Ming and Fan, Rui},
 booktitle = {2019 IEEE International Conference on Imaging Systems and Techniques (IST)},
 doi = {10.1109/ist48021.2019.9010305},
 keywords = {},
 note = {http://arxiv.org/pdf/1910.12728},
 pages = {1-6},
 title = {Real-Time, Environmentally-Robust 3D LiDAR Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1125154781},
 year = {2019}
}

@inproceedings{pub.1125163405,
 abstract = {Recent years have witnessed the rapid proliferation of low-power backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such low-power backscatter tags is crucial for IoT-based smart services. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, increasing the deployment cost. To empower universal localization service, this paper presents Rover, an indoor localization system that simultaneously localizes multiple backscatter tags with zero start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing WiFi-based positioning measurements with inertial measurements to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues such as the interference among multiple tags and the real-time processing for solving the SLAM problem. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
 author = {Zhang, Shengkai and Wang, Wei and Tang, Sheyang and Jin, Shi and Jiang, Tao},
 booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
 doi = {10.1109/globecom38437.2019.9013768},
 keywords = {},
 note = {http://arxiv.org/pdf/1908.03297},
 pages = {1-6},
 title = {Localizing Backscatters by a Single Robot With Zero Start-up Cost},
 url = {https://app.dimensions.ai/details/publication/pub.1125163405},
 year = {2019}
}

@article{pub.1125168333,
 abstract = {We describe the release of reference data towards a challenging long-term
localisation and mapping benchmark based on the large-scale Oxford RobotCar
Dataset. The release includes 72 traversals of a route through Oxford, UK,
gathered in all illumination, weather and traffic conditions, and is
representative of the conditions an autonomous vehicle would be expected to
operate reliably in. Using post-processed raw GPS, IMU, and static GNSS base
station recordings, we have produced a globally-consistent centimetre-accurate
ground truth for the entire year-long duration of the dataset. Coupled with a
planned online benchmarking service, we hope to enable quantitative evaluation
and comparison of different localisation and mapping approaches focusing on
long-term autonomy for road vehicles in urban environments challenged by
changing weather.},
 author = {Maddern, Will and Pascoe, Geoffrey and Gadd, Matthew and Barnes, Dan and Yeomans, Brian and Newman, Paul},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Real-time Kinematic Ground Truth for the Oxford RobotCar Dataset},
 url = {https://app.dimensions.ai/details/publication/pub.1125168333},
 volume = {},
 year = {2020}
}

@article{pub.1125622383,
 abstract = {This article investigates long-term positioning of moving objects by monocular vision of a miniature fixed-wing unmanned aerial vehicle. It is challenging to perform a real-time onboard vision processing task, due to the strict payload capacity and power budget limitations of microflying vehicles. We propose a parallel onboard architecture that explicitly decouples the long-term positioning task into iteratively operated detection, tracking, and localization. The proposed approach is eventually called onboard detection-tracking-localization, namely oDTL. The detector automatically extracts and identifies the object from image frames captured at in-flight durations. A learning-based network is constructed to improve detection accuracy and robustness against ever-changing outdoor illumination conditions and flying viewpoints. The tracker follows the object within specified region-of-interest from frame to frame with lower computing consumption. To further reduce target-losing rate, a concept of blind zone is proposed and applied, and its boundaries in sequential images are also theoretically inferred. The position estimator maps the flying vehicle pose, the image coordinates, and calibration specifications into real-world positions of the moving target. An extended Kalman filter is developed for rough position estimation, and a smooth module is introduced for the refinement of the position. Three offline comparative experiments and three online experiments have been conducted respectively to testify the real-time capability of our approach. The collected experimental results also demonstrate the feasible accuracy and robustness of the overall solution within the specified flying onboard scenarios.},
 author = {Tang, Dengqing and Fang, Qiang and Shen, Lincheng and Hu, Tianjiang},
 doi = {10.1109/tmech.2020.2976794},
 journal = {IEEE/ASME Transactions on Mechatronics},
 keywords = {},
 number = {3},
 pages = {1555-1565},
 title = {Onboard Detection-Tracking-Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1125622383},
 volume = {25},
 year = {2020}
}

@article{pub.1125718499,
 abstract = {One of the hardest challenges to face in the development of a non GPS-based
localization system for autonomous vehicles is the changes of the environment.
LiDAR-based systems typically try to match the last measurements obtained with
a previously recorded map of the area. If the existing map is not updated along
time, there is a good chance that the measures will not match the environment
well enough, causing the vehicle to lose track of its location. In this paper,
we present and analyze experimental results regarding the robustness and
precision of a map-matching based localization system over a certain period of
time in the following three cases: (1) without any update of the initial map,
(2) updating the map as the vehicle moves and (3) with map updates that take
into account surrounding structures labeled as "fixed" which are treated
differently. The environment of the tests is a busy parking area, which ensures
drastic changes from one day to the next. The precision is obtained by
comparing the positions computed using the map with the ones provided by a
Real-Time Kinematic GPS system. The experimental results reveal a positioning
error of about 6cm which remains stable even after 23 days when using fixed
structures on the working area.},
 author = {Dominguez, Salvador and Garcia, GaÃ«tan and FrÃ©mont, Vincent and Hamon, Arnaud},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {An Experimental Evaluation of Robustness and Precision for Long-term
  LiDAR-based Localization in Highly Changing Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1125718499},
 volume = {},
 year = {2020}
}

@article{pub.1126373363,
 abstract = {Robust and efficient visual localization is essential for numerous robotic applications. However, it remains a challenging problem especially when significant environmental or perspective changes are present, as there are high percentage of outliers, i.e., incorrect feature matches between the query image and the map. In this article, we propose a novel 2-entity random sample consensus (RANSAC) framework using three-dimensionaltwo-dimensional point and line feature matches for visual localization with the aid of inertial measurements and derive minimal closed-form solutions using only 1 point 1 line or 2 point matches for both monocular and multi-camera system. The proposed 2-entity RANSAC can achieve higher robustness against outliers as multiple types of features are utilized and the number of matches needed to compute a pose is reduced. Furthermore, we propose a learning-based sampling strategy selection mechanism and a feature scoring network to be adaptive to different environmental characteristics such as structured and unstructured. Finally, both simulation and real-world experiments are performed to validate the robustness and effectiveness of the proposed method in scenarios with long-term and perspective changes.11https://youtu.be/Zqgxntz11hI. https://youtu.be/Zqgxntz11hI.},
 author = {Jiao, Yanmei and Wang, Yue and Ding, Xiaqing and Fu, Bo and Huang, Shoudong and Xiong, Rong},
 doi = {10.1109/tie.2020.2984970},
 journal = {IEEE Transactions on Industrial Electronics},
 keywords = {},
 note = {https://opus.lib.uts.edu.au/bitstream/10453/147667/3/2-Entity%20Random%20Sample%20Consensus%20for%20Robust%20Visual%20Localization.pdf},
 number = {5},
 pages = {4519-4528},
 title = {2-Entity Random Sample Consensus for Robust Visual Localization: Framework, Methods, and Verifications},
 url = {https://app.dimensions.ai/details/publication/pub.1126373363},
 volume = {68},
 year = {2021}
}

@article{pub.1127181936,
 abstract = {As one of the core technologies for autonomous mobile robots, Visual Simultaneous Localization and Mapping (VSLAM) has been widely researched in recent years. However, most state-of-the-art VSLAM adopts a strong scene rigidity assumption for analytical convenience, which limits the utility of these algorithms for real-world environments with independent dynamic objects. Hence, this paper presents a semantic and geometric constraints VSLAM (SGC-VSLAM), which is built on the RGB-D mode of ORB-SLAM2 with the addition of dynamic detection and static point cloud map construction modules. In detail, a novel improved quadtree-based method was adopted for SGC-VSLAM to enhance the performance of the feature extractor in ORB-SLAM (Oriented FAST and Rotated BRIEF-SLAM). Moreover, a new dynamic feature detection method called semantic and geometric constraints was proposed, which provided a robust and fast way to filter dynamic features. The semantic bounding box generated by YOLO v3 (You Only Look Once, v3) was used to calculate a more accurate fundamental matrix between adjacent frames, which was then used to filter all of the truly dynamic features. Finally, a static point cloud was estimated by using a new drawing key frame selection strategy. Experiments on the public TUM RGB-D (Red-Green-Blue Depth) dataset were conducted to evaluate the proposed approach. This evaluation revealed that the proposed SGC-VSLAM can effectively improve the positioning accuracy of the ORB-SLAM2 system in high-dynamic scenarios and was also able to build a map with the static parts of the real environment, which has long-term application value for autonomous mobile robots.},
 author = {Yang, Shiqiang and Fan, Guohao and Bai, Lele and Zhao, Cheng and Li, Dexin},
 doi = {10.3390/s20082432},
 journal = {Sensors},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/20/8/2432/pdf},
 number = {8},
 pages = {2432},
 title = {SGC-VSLAM: A Semantic and Geometric Constraints VSLAM for Dynamic Indoor Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1127181936},
 volume = {20},
 year = {2020}
}

@article{pub.1127419409,
 abstract = {A large mobile robot was used as a platform for research in continuous localization and path planning. Continuous localization is a technique that allows a robot to maintain an accurate estimate of its location by performing regular, small corrections to its odometry. Continuous localization utilizes an evidence grid representation, a common representation scheme that is used by other map-dependent processes, such as path planning. Although techniques exist for building evidence grid maps, most are not adaptive to changes in the environment. In this research, the continuous localization technique is extended by adding a learning component. This allows continuous localization to update the long-term map (evidence grid) with current sensor readings. Results show that the addition of the learning behavior to continuous localization allows the system to adapt to changes in its environment without a loss in its ability to remain localized. Continuous localization with the learning behavior was combined with a wavefront propagation path planner to produce a robust navigation system. This system was tested on a Nomad 200 mobile robot.},
 author = {Graves, Kevin P},
 doi = {10.21236/ada418467},
 journal = {},
 keywords = {},
 number = {},
 pages = {},
 title = {Continuous Localization and Navigation of Mobile Robots},
 url = {https://app.dimensions.ai/details/publication/pub.1127419409},
 volume = {},
 year = {1997}
}

@article{pub.1127552597,
 abstract = {Radar and lidar, provided by two different range sensors, each has pros and
cons of various perception tasks on mobile robots or autonomous driving. In
this paper, a Monte Carlo system is used to localize the robot with a rotating
radar sensor on 2D lidar maps. We first train a conditional generative
adversarial network to transfer raw radar data to lidar data, and achieve
reliable radar points from generator. Then an efficient radar odometry is
included in the Monte Carlo system. Combining the initial guess from odometry,
a measurement model is proposed to match the radar data and prior lidar maps
for final 2D positioning. We demonstrate the effectiveness of the proposed
localization framework on the public multi-session dataset. The experimental
results show that our system can achieve high accuracy for long-term
localization in outdoor scenes.},
 author = {Yin, Huan and Wang, Yue and Tang, Li and Xiong, Rong},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Radar-on-Lidar: metric radar localization on prior lidar maps},
 url = {https://app.dimensions.ai/details/publication/pub.1127552597},
 volume = {},
 year = {2020}
}

@article{pub.1127553619,
 abstract = {Autonomous harvesting and transportation is a long-term goal of the forest
industry. One of the main challenges is the accurate localization of both
vehicles and trees in a forest. Forests are unstructured environments where it
is difficult to find a group of significant landmarks for current fast
feature-based place recognition algorithms. This paper proposes a novel
approach where local observations are matched to a general tree map using the
Delaunay triangularization as the representation format. Instead of point cloud
based matching methods, we utilize a topology-based method. First, tree trunk
positions are registered at a prior run done by a forest harvester. Second, the
resulting map is Delaunay triangularized. Third, a local submap of the
autonomous robot is registered, triangularized and matched using triangular
similarity maximization to estimate the position of the robot. We test our
method on a dataset accumulated from a forestry site at Lieksa, Finland. A
total length of 2100\,m of harvester path was recorded by an industrial
harvester with a 3D laser scanner and a geolocation unit fixed to the frame.
Our experiments show a 12\,cm s.t.d. in the location accuracy and with
real-time data processing for speeds not exceeding 0.5\,m/s. The accuracy and
speed limit is realistic during forest operations.},
 author = {Li, Qingqing and Nevalainen, Paavo and Queralta, Jorge PeÃ±a and Heikkonen, Jukka and Westerlund, Tomi},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Localization in Unstructured Environments: Towards Autonomous Robots in
  Forests with Delaunay Triangulation},
 url = {https://app.dimensions.ai/details/publication/pub.1127553619},
 volume = {},
 year = {2020}
}

@article{pub.1127958781,
 abstract = {Recent years have witnessed the rapid proliferation of backscatter
technologies that realize the ubiquitous and long-term connectivity to empower
smart cities and smart homes. Localizing such backscatter tags is crucial for
IoT-based smart applications. However, current backscatter localization systems
require prior knowledge of the site, either a map or landmarks with known
positions, which is laborious for deployment. To empower universal localization
service, this paper presents Rover, an indoor localization system that
localizes multiple backscatter tags without any start-up cost using a robot
equipped with inertial sensors. Rover runs in a joint optimization framework,
fusing measurements from backscattered WiFi signals and inertial sensors to
simultaneously estimate the locations of both the robot and the connected tags.
Our design addresses practical issues including interference among multiple
tags, real-time processing, as well as the data marginalization problem in
dealing with degenerated motions. We prototype Rover using off-the-shelf WiFi
chips and customized backscatter tags. Our experiments show that Rover achieves
localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
 author = {Zhang, Shengkai and Wang, Wei and Tang, Sheyang and Jin, Shi and Jiang, Tao},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Robot-assisted Backscatter Localization for IoT Applications},
 url = {https://app.dimensions.ai/details/publication/pub.1127958781},
 volume = {},
 year = {2020}
}

@article{pub.1128202998,
 abstract = {Recent years have witnessed the rapid proliferation of backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such backscatter tags is crucial for IoT-based smart applications. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, which is laborious for deployment. To empower universal localization service, this paper presents Rover, an indoor localization system that localizes multiple backscatter tags without any start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing measurements from backscattered WiFi signals and inertial sensors to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues including interference among multiple tags, real-time processing, as well as the data marginalization problem in dealing with degenerated motions. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
 author = {Zhang, Shengkai and Wang, Wei and Tang, Sheyang and Jin, Shi and Jiang, Tao},
 doi = {10.1109/twc.2020.2997393},
 journal = {IEEE Transactions on Wireless Communications},
 keywords = {},
 note = {http://arxiv.org/pdf/2005.13534},
 number = {9},
 pages = {5807-5818},
 title = {Robot-Assisted Backscatter Localization for IoT Applications},
 url = {https://app.dimensions.ai/details/publication/pub.1128202998},
 volume = {19},
 year = {2020}
}

@article{pub.1128380526,
 abstract = {Autonomous harvesting and transportation is a long-term goal of the forest industry. One of the main challenges is the accurate localization of both vehicles and trees in a forest. Forests are unstructured environments where it is difficult to find a group of significant landmarks for current fast feature-based place recognition algorithms. This paper proposes a novel approach where local point clouds are matched to a global tree map using the Delaunay triangularization as the representation format. Instead of point cloud based matching methods, we utilize a topology-based method. First, tree trunk positions are registered at a prior run done by a forest harvester. Second, the resulting map is Delaunay triangularized. Third, a local submap of the autonomous robot is registered, triangularized and matched using triangular similarity maximization to estimate the position of the robot. We test our method on a dataset accumulated from a forestry site at Lieksa, Finland. A total length of 200 m of harvester path was recorded by an industrial harvester with a 3D laser scanner and a geolocation unit fixed to the frame. Our experiments show a 12 cm s.t.d. in the location accuracy and with real-time data processing for speeds not exceeding 0.5 m/s. The accuracy and speed limit are realistic during forest operations.},
 author = {Li, Qingqing and Nevalainen, Paavo and Queralta, Jorge PeÃ±a and Heikkonen, Jukka and Westerlund, Tomi},
 doi = {10.3390/rs12111870},
 journal = {Remote Sensing},
 keywords = {},
 note = {https://www.mdpi.com/2072-4292/12/11/1870/pdf},
 number = {11},
 pages = {1870},
 title = {Localization in Unstructured Environments: Towards Autonomous Robots in Forests with Delaunay Triangulation},
 url = {https://app.dimensions.ai/details/publication/pub.1128380526},
 volume = {12},
 year = {2020}
}

@article{pub.1128856846,
 abstract = {Loop closure detection is a fundamental problem for simultaneous localization and mapping (SLAM) in robotics. Most of the previous methods only consider one type of information, based on either visual appearances or spatial relationships of landmarks. In this paper, we introduce a novel visual-spatial information preserving multi-order graph matching approach for long-term loop closure detection. Our approach constructs a graph representation of a place from an input image to integrate visual-spatial information, including visual appearances of the landmarks and the background environment, as well as the second and third-order spatial relationships between two and three landmarks, respectively. Furthermore, we introduce a new formulation that formulates loop closure detection as a multi-order graph matching problem to compute a similarity score directly from the graph representations of the query and template images, instead of performing conventional vector-based image matching. We evaluate the proposed multi-order graph matching approach based on two public long-term loop closure detection benchmark datasets, including the St. Lucia and CMU-VL datasets. Experimental results have shown that our approach is effective for long-term loop closure detection and it outperforms the previous state-of-the-art methods.},
 author = {Gao, Peng and Zhang, Hao},
 doi = {10.1609/aaai.v34i06.6604},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 keywords = {},
 note = {https://ojs.aaai.org/index.php/AAAI/article/download/6604/6458},
 number = {06},
 pages = {10369-10376},
 title = {Long-Term Loop Closure Detection through Visual-Spatial Information Preserving Multi-Order Graph Matching},
 url = {https://app.dimensions.ai/details/publication/pub.1128856846},
 volume = {34},
 year = {2020}
}

@article{pub.1128992974,
 abstract = {Autonomous transfer vehicles (ATVs) can be considered as one of the critical components of context-aware structured smart factories in Industry 4.0 era. Conventional mapping methods such as grid maps can provide information for navigation, but they are not enough for complex environments that require interactions. On the other hand, high-definition (HD) mapping, which is mainly used in traffic networks, includes more information about an environment to perform excellent autonomous behaviour. In order to increase the efficiency of ATVs in flexible factories, an up-to-date environmental map information is required to perform successful long-term autonomous navigation. Therefore, when there exists a change in the environment, a simultaneous update of HD-map is as important as the creation of it. In this study, we propose an HD-map update methodology for ATVs that operates in smart factories. To the best of our knowledge, HD mapping has not been applied in smart factories. The proposed method includes the object detection and localisation tool to detect objects visually and determines their positions in connection with the conventional maps of the environment. Experimental results of a simulated factory environment demonstrate that the ATV can properly update the HD-map when a predefined sign is removed from or a new sign is added to the environment.},
 author = {Tas, Muhammed Oguz and Yavuz, Hasan Serhan and Yazici, Ahmet},
 doi = {10.1080/0952813x.2020.1789754},
 journal = {Journal of Experimental & Theoretical Artificial Intelligence},
 keywords = {},
 number = {5},
 pages = {1-19},
 title = {High-definition map update framework for intelligent autonomous transfer vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1128992974},
 volume = {33},
 year = {2021}
}

@article{pub.1129019867,
 abstract = {Autonomous valet parking is a specific application for autonomous vehicles.
In this task, vehicles need to navigate in narrow, crowded and GPS-denied
parking lots. Accurate localization ability is of great importance. Traditional
visual-based methods suffer from tracking lost due to texture-less regions,
repeated structures, and appearance changes. In this paper, we exploit robust
semantic features to build the map and localize vehicles in parking lots.
Semantic features contain guide signs, parking lines, speed bumps, etc, which
typically appear in parking lots. Compared with traditional features, these
semantic features are long-term stable and robust to the perspective and
illumination change. We adopt four surround-view cameras to increase the
perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel
encoders, the proposed system generates a global visual semantic map. This map
is further used to localize vehicles at the centimeter level. We analyze the
accuracy and recall of our system and compare it against other methods in real
experiments. Furthermore, we demonstrate the practicability of the proposed
system by the autonomous parking application.},
 author = {Qin, Tong and Chen, Tongqing and Chen, Yilun and Su, Qing},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous
  Vehicles in the Parking Lot},
 url = {https://app.dimensions.ai/details/publication/pub.1129019867},
 volume = {},
 year = {2020}
}

@inproceedings{pub.1129723838,
 abstract = {Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with optical flow (OF), research that combines absolute pose regression with OF is limited. We propose ViPR, a novel modular architecture for longterm 6DoF VO that leverages temporal information and synergies between absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) by combining both through recurrent layers. Experiments on known datasets and on our own Industry dataset show that our modular design outperforms state ofthe art in long-term navigation tasks.},
 author = {Ott, Felix and Feigl, Tobias and LÃ¶ffler, Christoffer and Mutschler, Christopher},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
 doi = {10.1109/cvprw50498.2020.00029},
 keywords = {},
 note = {http://arxiv.org/pdf/1912.08263},
 pages = {187-198},
 title = {ViPR: Visual-Odometry-aided Pose Regression for 6DoF Camera Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1129723838},
 year = {2020}
}

@article{pub.1129772457,
 abstract = {In many simultaneous localization and mapping (SLAM) systems, the map of the environment grows over time as the robot explores the environment. The ever-growing map prevents long-term mapping, especially in large-scale environments. In this paper, we develop a compact cognitive mapping approach inspired by neurobiological experiments. Mimicking the firing activities of neighborhood cells, neighborhood fields determined by movement information, i.e. translation and rotation, are modeled to describe one of the distinct segments of the explored environment. The vertices with low neighborhood field activities are avoided to be added into the cognitive map. The optimization of the cognitive map is formulated as a robust non-linear least squares problem constrained by the transitions between vertices, and is numerically solved efficiently. According to the cognitive decision-making of place familiarity, loop closure edges are clustered depending on time intervals, and then batch global optimization of the cognitive map is performed to satisfy the combined constraint of the whole cluster. After the loop closure process, scene integration is performed, in which revisited vertices are removed subsequently to further reduce the size of the cognitive map. The compact cognitive mapping approach is tested on a monocular visual SLAM system in a naturalistic maze for a biomimetic animated robot. Our results demonstrate that the proposed method largely restricts the growth of the size of the cognitive map over time, and meanwhile, the compact cognitive map correctly represents the overall layout of the environment. The compact cognitive mapping method is well suitable for the representation of large-scale environments to achieve long-term robot navigation.},
 author = {Zeng, Taiping and Si, Bailu},
 doi = {10.1007/s11571-020-09621-6},
 journal = {Cognitive Neurodynamics},
 keywords = {},
 note = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7947048},
 number = {1},
 pages = {91-101},
 title = {A brain-inspired compact cognitive mapping system},
 url = {https://app.dimensions.ai/details/publication/pub.1129772457},
 volume = {15},
 year = {}
}

@inproceedings{pub.1129803573,
 abstract = {In the research of unmanned vehicle technology, the integrated navigation system based on inertial measurement unit and stereo camera has gradually become a research hotspot. The inertial navigation system has the characteristics of higher short-time precision, and does not radiate information to the outside world. The stereo vision navigation system collects image information of the environment, and performs feature extraction and tracking on the feature points in the acquired images to recover the motion of the carrier. In this paper, the stereo vision navigation system is used to correct the long-term error accumulation of the inertial navigation system. On the other hand, the short-time precision of the inertial navigation system can also compensate the vision navigation system caused by the blurred image information caused by the carrier moving too fast. The integrated navigation system of IMU together with stereo vision camera can gain better comprehensive performance. In this paper, Multi-State Fusion Kalman Filter (MSF) and Multi-State Constraint Kalman Filter (MSCKF) are used to fuse the inertial navigation system and vision navigation system. When constructing the MSCKF framework, we use sparse optical flow method to achieve feature tracking, and using triangulation in computer vision to calculate the positions of feature points, and use the data set to verify the accuracy of the two algorithms. Finally, constructing an environmental point cloud map using the estimated state of feature points. Under the environment of i7-8750H cpu, the experimental results show that tight coupling is more accurate than loose coupling.},
 author = {Juan-Rou, Hou and Zhan-Qing, Wang},
 booktitle = {2020 27th Saint Petersburg International Conference on Integrated Navigation Systems (ICINS)},
 doi = {10.23919/icins43215.2020.9133980},
 keywords = {},
 pages = {1-4},
 title = {The Implementation of IMU/Stereo Vision Slam System for Mobile Robot},
 url = {https://app.dimensions.ai/details/publication/pub.1129803573},
 year = {2020}
}

@article{pub.1129834842,
 abstract = {Autonomous service mobile robots need to consistently, accurately, and
robustly localize in human environments despite changes to such environments
over time. Episodic non-Markov Localization addresses the challenge of
localization in such changing environments by classifying observations as
arising from Long-Term, Short-Term, or Dynamic Features. However, in order to
do so, EnML relies on an estimate of the Long-Term Vector Map (LTVM) that does
not change over time. In this paper, we introduce a recursive algorithm to
build and update the LTVM over time by reasoning about visibility constraints
of objects observed over multiple robot deployments. We use a signed distance
function (SDF) to filter out observations of short-term and dynamic features
from multiple deployments of the robot. The remaining long-term observations
are used to build a vector map by robust local linear regression. The
uncertainty in the resulting LTVM is computed via Monte Carlo resampling the
observations arising from long-term features. By combining occupancy-grid based
SDF filtering of observations with continuous space regression of the filtered
observations, our proposed approach builds, updates, and amends LTVMs over
time, reasoning about all observations from all robot deployments in an
environment. We present experimental results demonstrating the accuracy,
robustness, and compact nature of the extracted LTVMs from several long-term
robot datasets.},
 author = {Nashed, Samer and Biswas, Joydeep},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Curating Long-term Vector Maps},
 url = {https://app.dimensions.ai/details/publication/pub.1129834842},
 volume = {},
 year = {2020}
}

@article{pub.1130065396,
 abstract = {Abstract. The development of automated and autonomous vehicles requires highly accurate long-term maps of the environment. Urban areas contain a large number of dynamic objects which change over time. Since a permanent observation of the environment is impossible and there will always be a first time visit of an unknown or changed area, a map of an urban environment needs to model such dynamics.In this work, we use LiDAR point clouds from a large long term measurement campaign to investigate temporal changes. The data set was recorded along a 20 km route in Hannover, Germany with a Mobile Mapping System over a period of one year in bi-weekly measurements. The data set covers a variety of different urban objects and areas, weather conditions and seasons. Based on this data set, we show how scene and seasonal effects influence the measurement likelihood, and that multi-temporal maps lead to the best positioning results.},
 author = {Schachtschneider, J. and Brenner, C.},
 doi = {10.5194/isprs-archives-xliii-b2-2020-317-2020},
 journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
 keywords = {},
 note = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLIII-B2-2020/317/2020/isprs-archives-XLIII-B2-2020-317-2020.pdf},
 number = {},
 pages = {317-323},
 title = {CREATING MULTI-TEMPORAL MAPS OF URBAN ENVIRONMENTS FOR IMPROVED LOCALIZATION OF AUTONOMOUS VEHICLES},
 url = {https://app.dimensions.ai/details/publication/pub.1130065396},
 volume = {XLIII-B2-2020},
 year = {2020}
}

@inproceedings{pub.1130275058,
 abstract = {Technologies enabling long-term, wide-ranging measurement in hard-to-reach areas are a critical need for planetary science inquiry. Phenomena of interest include flows or variations in volatiles, gas composition or concentration, particulate density, or even simply temperature. Improved measurement of these processes enables understanding of exotic geologies and distributions or correlating indicators of trapped water or biological activity. However, such data is often needed in unsafe areas such as caves, lava tubes, or steep ravines not easily reached by current spacecraft and planetary robots. To address this capability gap, we have developed miniaturized, expendable sensors which can be ballistically lobbed from a robotic rover or static lander - or even dropped during a flyover. These projectiles can perform sensing during flight and after anchoring to terrain features. By augmenting exploration systems with these sensors, we can extend situational awareness, perform long-duration monitoring, and reduce utilization of primary mobility resources, all of which are crucial in surface missions. We call the integrated payload that includes a cold gas launcher, smart projectiles, planning software, network discovery, and science sensing: PHALANX. In this paper, we introduce the mission architecture for PHALANX and describe an exploration concept that pairs projectile sensors with a rover â€œmothership.â€ Science use cases explored include reconnaissance using ballistic cameras, volatiles detection, and building timelapse maps of temperature and illumination conditions. Strategies to autonomously coordinate constellations of deployed sensors to self-discover and localize with peer ranging (i.e. a â€œlocal GPSâ€) are summarized, thus providing communications infrastructure beyond-line-of-sight (BLOS) of the rover. Capabilities were demonstrated through both simulation and physical testing with a terrestrial prototype. The approach to developing a terrestrial prototype is discussed, including design of the launching mechanism, projectile optimization, micro-electronics fabrication, and sensor selection. Results from early testing and characterization of commercial-off-the-shelf (COTS) components are reported. Nodes were subjected to successful burn-in tests over 48 hours at full logging duty cycle. Integrated field tests were conducted in the Roverscape, a half-acre planetary analog environment at NASA Ames, where we tested up to 10 sensor nodes simultaneously coordinating with an exploration rover. Ranging accuracy has been demonstrated to be within +/âˆ’10cm over 20m using commodity radios when compared to high-resolution laser scanner ground truthing. Evolution of the design, including progressive miniaturization of the electronics and iterated modifications of the enclosure housing for streamlining and optimized radio performance are described. Finally, lessons learned to date, gaps toward eventual flight mission implementation, and continuing future development plans are discussed.},
 author = {Dille, Michael and Nuch, Danny and Gupta, Shiven and McCabe, Steven and Verzic, Nicholas and Fong, Terry and Wong, Uland},
 booktitle = {2020 IEEE Aerospace Conference},
 doi = {10.1109/aero47225.2020.9172595},
 keywords = {},
 note = {https://ntrs.nasa.gov/api/citations/20200002137/downloads/20200002137.pdf},
 pages = {1-12},
 title = {PHALANX: Expendable projectile sensor networks for planetary exploration},
 url = {https://app.dimensions.ai/details/publication/pub.1130275058},
 year = {2020}
}

@article{pub.1130416513,
 abstract = {Image indexing for lifelong localization is a key component for a large panel of applications, including robot navigation, autonomous driving or cultural heritage valorization. The principal difficulty in long-term localization arises from the dynamic changes that affect outdoor environments. In this work, we propose a new approach for outdoor large scale image-based localization that can deal with challenging scenarios like cross-season, cross-weather and day/night localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We show through extensive evaluation that our method can improve localization performances, especially in challenging scenarios when the visual appearance of the scene has changed. Our method is able to leverage both visual and geometric clues from monocular images to create discriminative descriptors for cross-season localization and effective matching of images acquired at different time periods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images. Finally we extended our method to reflectance modality and we compare multi-modal descriptors respectively based on geometry, material reflectance and a combination of both.},
 author = {Piasco, Nathan and SidibÃ©, DÃ©sirÃ© and Gouet-Brunet, ValÃ©rie and Demonceaux, CÃ©dric},
 doi = {10.1007/s11263-020-01363-6},
 journal = {International Journal of Computer Vision},
 keywords = {},
 note = {https://hal.archives-ouvertes.fr/hal-02912239/file/journal_paper_last.pdf},
 number = {1},
 pages = {185-202},
 title = {Improving Image Description with Auxiliary Modality for Visual Localization in Challenging Conditions},
 url = {https://app.dimensions.ai/details/publication/pub.1130416513},
 volume = {129},
 year = {}
}

@article{pub.1130460657,
 abstract = {For autonomous vehicles to operate persistently in a typical urban
environment, it is essential to have high accuracy position information. This
requires a mapping and localisation system that can adapt to changes over time.
A localisation approach based on a single-survey map will not be suitable for
long-term operation as it does not incorporate variations in the environment.
In this paper, we present new algorithms to maintain a featured-based map. A
map maintenance pipeline is proposed that can continuously update a map with
the most relevant features taking advantage of the changes in the surroundings.
Our pipeline detects and removes transient features based on their geometrical
relationships with the vehicle's pose. Newly identified features became part of
a new feature map and are assessed by the pipeline as candidates for the
localisation map. By purging out-of-date features and adding newly detected
features, we continually update the prior map to more accurately represent the
most recent environment. We have validated our approach using the USyd Campus
Dataset, which includes more than 18 months of data. The results presented
demonstrate that our maintenance pipeline produces a resilient map which can
provide sustained localisation performance over time.},
 author = {Berrio, Julie Stephany and Worrall, Stewart and Shan, Mao and Nebot, Eduardo},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Long-term map maintenance pipeline for autonomous vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1130460657},
 volume = {},
 year = {2020}
}

@article{pub.1130511687,
 abstract = {Global localization and kidnapping are two challenging problems in robot
localization. The popular method, Monte Carlo Localization (MCL) addresses the
problem by iteratively updating a set of particles with a "sampling-weighting"
loop. Sampling is decisive to the performance of MCL [1]. However, traditional
MCL can only sample from a uniform distribution over the state space. Although
variants of MCL propose different sampling models, they fail to provide an
accurate distribution or generalize across scenes. To better deal with these
problems, we present a distribution proposal model, named Deep Samplable
Observation Model (DSOM). DSOM takes a map and a 2D laser scan as inputs and
outputs a conditional multimodal probability distribution of the pose, making
the samples more focusing on the regions with higher likelihood. With such
samples, the convergence is expected to be more effective and efficient.
Considering that the learning-based sampling model may fail to capture the true
pose sometimes, we furthermore propose the Adaptive Mixture MCL (AdaM MCL),
which deploys a trusty mechanism to adaptively select updating mode for each
particle to tolerate this situation. Equipped with DSOM, AdaM MCL can achieve
more accurate estimation, faster convergence and better scalability compared to
previous methods in both synthetic and real scenes. Even in real environments
with long-term changing, AdaM MCL is able to localize the robot using DSOM
trained only by simulation observations from a SLAM map or a blueprint map.},
 author = {Chen, Runjian and Yin, Huan and Jiao, Yanmei and Dissanayake, Gamini and Wang, Yue and Xiong, Rong},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Deep Samplable Observation Model for Global Localization and Kidnapping},
 url = {https://app.dimensions.ai/details/publication/pub.1130511687},
 volume = {},
 year = {2020}
}

@inproceedings{pub.1130877108,
 abstract = {Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robotâ€™s long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.},
 author = {Shi, Xuesong and Li, Dongjiang and Zhao, Pengpeng and Tian, Qinbin and Tian, Yuxin and Long, Qiwei and Zhu, Chunhao and Song, Jingwei and Qiao, Fei and Song, Le and Guo, Yangquan and Wang, Zhigang and Zhang, Yimin and Qin, Baoxing and Yang, Wei and Wang, Fangshi and Chan, Rosa H. M. and She, Qi},
 booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra40945.2020.9196638},
 keywords = {},
 note = {http://arxiv.org/pdf/1911.05603},
 pages = {3139-3145},
 title = {Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1130877108},
 year = {2020}
}

@inproceedings{pub.1130877376,
 abstract = {Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.},
 author = {Gao, Peng and Zhang, Hao},
 booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra40945.2020.9196906},
 keywords = {},
 pages = {1070-1076},
 title = {Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships},
 url = {https://app.dimensions.ai/details/publication/pub.1130877376},
 year = {2020}
}

@inproceedings{pub.1130877695,
 abstract = {Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in the long-term, our method using only odometry and the map converges in long-term. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.},
 author = {Peng, Cheng and Weikersdorfer, David},
 booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra40945.2020.9197225},
 keywords = {},
 note = {http://arxiv.org/pdf/1910.00572},
 pages = {2317-2323},
 title = {Map As the Hidden Sensor: Fast Odometry-Based Global Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1130877695},
 year = {2020}
}

@article{pub.1130901738,
 abstract = {Visual localization is a crucial component in the application of mobile robot
and autonomous driving. Image retrieval is an efficient and effective technique
in image-based localization methods. Due to the drastic variability of
environmental conditions, e.g. illumination, seasonal and weather changes,
retrieval-based visual localization is severely affected and becomes a
challenging problem. In this work, a general architecture is first formulated
probabilistically to extract domain invariant feature through multi-domain
image translation. And then a novel gradient-weighted similarity activation
mapping loss (Grad-SAM) is incorporated for finer localization with high
accuracy. We also propose a new adaptive triplet loss to boost the contrastive
learning of the embedding in a self-supervised manner. The final coarse-to-fine
image retrieval pipeline is implemented as the sequential combination of models
without and with Grad-SAM loss. Extensive experiments have been conducted to
validate the effectiveness of the proposed approach on the CMUSeasons dataset.
The strong generalization ability of our approach is verified on RobotCar
dataset using models pre-trained on urban part of CMU-Seasons dataset. Our
performance is on par with or even outperforms the state-of-the-art image-based
localization baselines in medium or high precision, especially under the
challenging environments with illumination variance, vegetation and night-time
images. The code and pretrained models are available on
https://github.com/HanjiangHu/DISAM.},
 author = {Hu, Hanjiang and Wang, Hesheng and Liu, Zhe and Chen, Weidong},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Domain-invariant Similarity Activation Map Contrastive Learning for
  Retrieval-based Long-term Visual Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1130901738},
 volume = {},
 year = {2020}
}

@article{pub.1131970349,
 abstract = {Global localization is essential for robot navigation, of which the first
step is to retrieve a query from the map database. This problem is called place
recognition. In recent years, LiDAR scan based place recognition has drawn
attention as it is robust against the appearance change. In this paper, we
propose a LiDAR-based place recognition method, named Differentiable Scan
Context with Orientation (DiSCO), which simultaneously finds the scan at a
similar place and estimates their relative orientation. The orientation can
further be used as the initial value for the down-stream local optimal metric
pose estimation, improving the pose estimation especially when a large
orientation between the current scan and retrieved scan exists. Our key idea is
to transform the feature into the frequency domain. We utilize the magnitude of
the spectrum as the place signature, which is theoretically rotation-invariant.
In addition, based on the differentiable phase correlation, we can efficiently
estimate the global optimal relative orientation using the spectrum. With such
structural constraints, the network can be learned in an end-to-end manner, and
the backbone is fully shared by the two tasks, achieving interpretability and
light weight. Finally, DiSCO is validated on three datasets with long-term
outdoor conditions, showing better performance than the compared methods.},
 author = {Xu, Xuecheng and Yin, Huan and Chen, Zexi and Wang, Yue and Xiong, Rong},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {DiSCO: Differentiable Scan Context with Orientation},
 url = {https://app.dimensions.ai/details/publication/pub.1131970349},
 volume = {},
 year = {2020}
}

@inproceedings{pub.1131998811,
 abstract = {The humanoid robot competition is an autonomous robot with a human-like body platform with a single camera as a vision sensor and balancing sensor to support them to play soccer in the specific field. The technical challenges in this competition such following the ball, running during search the ball, dynamic walking, kicking while maintaining the balance body condition, decision making with other robot, localization and mapping as research issues investigated in the Humanoid competition. Localization and mapping still big challenges in humanoid competition, it was only single camera is used in competition rule and no others sensor to support the position and orientation during playing the game. The proposed system was developed is neighborhood probability mapping. The long-term goal of this research is to realize an ideal system to accelerate the redesign field condition and implementation process in a humanoid robot that can be monitored in real-time. The aim of this research is to take the opportunities: (a) increasing the robot's performance of vision and intelligence on the humanoid robot; (b) with this SLAM method the robot can distinguish between the balls that are in the field and outside the field; (c) able to distinguish the enemy goal from the goal itself based on goal detection and line detection; (d) the goal keeper robot capable of acting as an attacker and scanning the kick towards the enemy goal. The testing condition was implemented between simulation testing and real testing in same times. Based on the data experimental result, the robot can estimate their position and orientation during searching the ball position, goal position and obstacle coordinate with high real time accuracy. The result shows that the proposed system can be applied to the humanoid soccer robot in the real time directly and it worked with less error.},
 author = {Jiono, Mahfud and Mahandi, Yogi Dwi and Mustika, Soraya Norma and Sendari, Siti and Dzikri, Adam Maulana},
 booktitle = {2020 4th International Conference on Vocational Education and Training (ICOVET)},
 doi = {10.1109/icovet50258.2020.9230237},
 keywords = {},
 pages = {355-359},
 title = {Self Localization Based On Neighborhood Probability Mapping for Humanoid Robot},
 url = {https://app.dimensions.ai/details/publication/pub.1131998811},
 year = {2020}
}

@article{pub.1132008831,
 abstract = {This paper presents a novel two-stage system which integrates topological localisation candidates from a radar-only place recognition system with precise pose estimation using spectral landmark-based techniques. We prove that the-recently available-seminal radar place recognition (RPR) and scan matching sub-systems are complementary in a style reminiscent of the mapping and localisation systems underpinning visual teach-and-repeat (VTR) systems which have been exhibited robustly in the last decade. Offline experiments are conducted on the most extensive radar-focused urban autonomy dataset available to the community with performance comparing favourably with and even rivalling alternative state-of-the-art radar localisation systems. Specifically, we show the long-term durability of the approach and of the sensing technology itself to autonomous navigation. We suggest a range of sensible methods of tuning the system, all of which are suitable for online operation. For both tuning regimes, we achieve, over the course of a month of localisation trials against a single static map, high recalls at high precision, and much reduced variance in erroneous metric pose estimation. As such, this work is a necessary first step towards a radar teach-and-repeat (RTR) system and the enablement of autonomy across extreme changes in appearance or inclement conditions.},
 author = {De Martini, Daniele and Gadd, Matthew and Newman, Paul},
 doi = {10.3390/s20216002},
 journal = {Sensors},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/20/21/6002/pdf},
 number = {21},
 pages = {6002},
 title = {kRadar++: Coarse-to-Fine FMCW Scanning Radar Localisation},
 url = {https://app.dimensions.ai/details/publication/pub.1132008831},
 volume = {20},
 year = {2020}
}

@inbook{pub.1132269754,
 abstract = {One of the most common tasks in assistive robotics is to find some specific object in a home environment. Usually, this task is tackled by adding the objects of interest to a map of the environment as soon as the objects are detected by the vision system of the robot. However, these maps are usually static, and do not take into account the dynamic nature of a home, where anyone could move an object after the robot has seen it. In this paper, we propose a lifelong system to address this problem. The robot takes into account different possible locations for each object, and chooses the more probable one when it is required. We have designed a probability based system that stores possible locations for each object, and updates the probabilities of past locations based on newer detections.},
 author = {Romero-GonzÃ¡lez, Cristina and MartÃ­nez-GÃ³mez, Jesus and GarcÃ­a-Varea, Ismael},
 booktitle = {Advances in Physical Agents II},
 doi = {10.1007/978-3-030-62579-5_2},
 keywords = {},
 pages = {18-29},
 publisher = {},
 title = {Lifelong Object Localization in Robotic Applications},
 url = {https://app.dimensions.ai/details/publication/pub.1132269754},
 year = {2021}
}

@book{pub.1132269996,
 abstract = {The book reports on cutting-edge Artificial Intelligence (AI) theories and methods aimed at the control and coordination of agents acting and moving in a dynamic environment. It covers a wide range of topics relating to: autonomous navigation, localization and mapping; mobile and social robots; multiagent systems; human-robot interaction; perception systems; and deep-learning techniques applied to the robotics. Based on the 21st edition of the International Workshop of Physical Agents (WAF 2020), held virtually on November 19-20, 2020, from AlcalÃ¡ de Henares, Madrid, Spain, this book offers a snapshot of the state-of-the-art in the field of physical agents, with a special emphasis on novel AI techniques in perception, navigation and human robot interaction for autonomous systems.},
 author = {},
 doi = {10.1007/978-3-030-62579-5},
 editor = {Luis M. Bergasa, Manuel OcaÃ±a, Rafael Barea, Elena LÃ³pez-GuillÃ©n, Pedro Revenga},
 keywords = {},
 pages = {},
 title = {Advances in Physical Agents II, Proceedings of the 21st International Workshop of Physical Agents (WAF 2020), November 19-20, 2020, AlcalÃ¡ de Henares, Madrid, Spain},
 url = {https://app.dimensions.ai/details/publication/pub.1132269996},
 year = {2021}
}

@article{pub.1132342598,
 abstract = {Introduction. Mutations in TP53 are common (~18%) in patients with myelodysplastic syndrome (MDS) and acute myeloid leukemia and often undergo loss of heterozygosity. Our understanding of the hematopoietic consequences of expressing mutant TP53-R175H, one of the most common mutations in MDS, is incomplete. In addition, whether TP53-R175H confers a loss-of-function, gain-of-function, or dominant-negative effect in response to chemotherapy has not been fully explored.
                  Methods. We used a constitutive knock-in mouse model expressing TRP53-R172H (G-to-A substitution at nucleotide 515), corresponding to human mutant TP53-R175H. We generated Trp53R172H/+ and Trp53R172H/R172H mice and compared them to wild type (WT), Trp53+/-, and Trp53-/- mice. Peripheral blood (PB) and bone marrow (BM) was analyzed in non-BM transplant conditions, following a non-competitive BM transplant, and following a competitive BM transplant with or without exposure to N-ethyl-N-nitrosourea (ENU) and 5-fluorouracil (5FU).
                  Results. BM hematopoietic stem and progenitor cells (HSPC), including LSK-SLAM cells, were increased in Trp53+/- and Trp53-/-mice (n=4-8, 8-15 weeks old, P<0.01), but not Trp53R172H/+ and Trp53R172H/R172H mice. In order to study the hematopoietic cell-intrinsic properties of mutant TRP53, we transplanted whole BM into lethally irradiated congenic recipient mice and monitored survival. The median overall survival was dependent on the Trp53 genotype of donor cells: WT cells (100% survival at 1 year), Trp53R172H/+ (60% survival at 1 year), Trp53+/- (31 weeks), Trp53-/- (20 weeks), and Trp53R172H/R172H (18 weeks) (n=12-20, P<0.01 for all genotypes vs. WT). To test long-term HSC function of mutant cells, we performed a competitive BM transplant by injecting equal numbers of test and congenic competitor BM into lethally irradiated congenic recipient mice and monitored PB chimerism of recipient mice for 16 weeks. There was PB competitive advantage for all TRP53 mutant cells compared to WT competitor cells, with Trp53-/- cells having the most significant advantage compared to all other mutant genotypes (n=9-11, P<0.05). The results suggest that mutant TRP53-R172H has distinct properties compared to Trp53 deletions, and not consistent with loss-of-function.
                  TP53 mutant cells can clonally expand in patients following cytotoxic chemotherapy. Therefore, we investigated the response of TRP53R172H/+ mutant cells to alkylator (ENU) exposure. We created mixed BM chimeric mice by transplanting test (WT, Trp53+/-, Trp53R172H/+ and Trp53-/-) and WT competitor BM in a 1:3 ratio, respectively. Following engraftment, chimeric mice received vehicle or ENU (2 doses of 100 mg/kg, 9 days apart). ENU-exposed Trp53R172H/+ cells have a robust PB multilineage competitive advantage relative to placebo (Fig. 1A, n = 4-5, 2-fold increase at 10 weeks post-ENU, P<0.001). This expansion was greater than the rise observed for ENU-treated Trp53+/- cells relative to vehicle treatment (1.67-fold relative to vehicle), and similar to the expansion of Trp53-/- cells, regardless of ENU. BM cells from Trp53R172H/+ mice were resistant to ENU-induced p21 expression and cell cycle arrest observed in WT and Trp53+/- mice (n = 4-5, P<0.001, Fig. 1B, C). The results suggest that mutant TRP53-R172H induces a dominant-negative effect following ENU exposure, similar to prior reports following irradiation.
                  Next, we asked whether mutant TRP53-R172H has similar or different effects as Trp53 deletion following exposure to an alternative chemotherapy (5FU). We first treated mice with a single dose of 5FU (200 mg/kg) to deplete cells and monitored WBC count recovery for 4 weeks. Trp53R172H/+ mice had significantly higher recovery WBC counts compared to WT, Trp53+/-, andTrp53-/- mice (n = 5-15, P<0.05, Fig. 1D). Next, we exposed mutant mice to 4 doses of 5FU (150mg/kg x 1 dose, 90 mg/kg x 3 doses, once per week) and monitored survival. We observed that all Trp53-/- mice survived, while Trp53R172H/+ mice had a median survival of 21 days, and Trp53+/- and WT mice had the shortest median survival (13 and 14.5 days, respectively, Fig. 1E). The results suggest that mutant TRP53-R172H cells display a gain-of-function property following a single dose of 5FU.
                  Collectively, the results indicate that mutant TRP53-R172H may induce a gain-of-function or a dominant-negative effect depending on the exposure to specific hematopoietic stresses.
                  
                  
                    Disclosures
                    No relevant conflicts of interest to declare.
                  },
 author = {Ahmed, Tanzir and Liu, Tuoen and Alberti, Michael O. and Wadugu, Brian and Ndonwi, Matthew and Grieb, Sarah and Walter, Matthew J.},
 doi = {10.1182/blood-2020-141091},
 journal = {Blood},
 keywords = {},
 note = {https://doi.org/10.1182/blood-2020-141091},
 number = {Supplement 1},
 pages = {1-1},
 title = {Mutant TRP53-R172H Has Gain-of-Function or Dominant-Negative Effects in Response to Different Hematopoietic Stressors in Mice},
 url = {https://app.dimensions.ai/details/publication/pub.1132342598},
 volume = {136},
 year = {2020}
}

@article{pub.1132356582,
 abstract = {Autonomous mobile robots are becoming increasingly important in many industrial and domestic environments. Dealing with unforeseen situations is a difficult problem that must be tackled to achieve long-term robot autonomy. In vision-based localization and navigation methods, one of the major issues is the scene dynamics. The autonomous operation of the robot may become unreliable if the changes occurring in dynamic environments are not detected and managed. Moving chairs, opening and closing doors or windows, replacing objects and other changes make many conventional methods fail. To deal with these challenges, we present a novel method for change detection based on weighted local visual features. The core idea of the algorithm is to distinguish the valuable information in stable regions of the scene from the potentially misleading information in the regions that are changing. We evaluate the change detection algorithm in a visual localization framework based on feature matching by performing a series of long-term localization experiments in various real-world environments. The results show that the change detection method yields an improvement in the localization accuracy, compared to the baseline method without change detection. In addition, an experimental evaluation on a public long-term localization data set with more than 10000 images reveals that the proposed method outperforms two alternative localization methods on images recorded several months after the initial mapping.},
 author = {Derner, Erik and Gomez, Clara and Hernandez, Alejandra C. and Barber, Ramon and BabuÅ¡ka, Robert},
 doi = {10.1016/j.robot.2020.103676},
 journal = {Robotics and Autonomous Systems},
 keywords = {},
 number = {},
 pages = {103676},
 title = {Change detection using weighted features for image-based localization},
 url = {https://app.dimensions.ai/details/publication/pub.1132356582},
 volume = {135},
 year = {2021}
}

@article{pub.1132446076,
 abstract = {A robot operating in a household makes observations of multiple objects as it
moves around over the course of days or weeks. The objects may be moved by
inhabitants, but not completely at random. The robot may be called upon later
to retrieve objects and will need a long-term object-based memory in order to
know how to find them. Existing work in semantic slam does not attempt to
capture the dynamics of object movement. In this paper, we combine some aspects
of classic techniques for data-association filtering with modern
attention-based neural networks to construct object-based memory systems that
operate on high-dimensional observations and hypotheses. We perform end-to-end
learning on labeled observation trajectories to learn both the transition and
observation models. We demonstrate the system's effectiveness in maintaining
memory of dynamically changing objects in both simulated environment and real
images, and demonstrate improvements over classical structured approaches as
well as unstructured neural approaches. Additional information available at
project website: https://yilundu.github.io/obm/.},
 author = {Du, Yilun and Lozano-Perez, Tomas and Kaelbling, Leslie},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Learning Object-Based State Estimators for Household Robots},
 url = {https://app.dimensions.ai/details/publication/pub.1132446076},
 volume = {},
 year = {2020}
}

@article{pub.1132542281,
 abstract = {Mobile mapping is an efficient technology to acquire spatial data of the environment. As a supplement of vehicle-borne and air-borne methods, Backpack mobile mapping system (MMS) has a wide application prospect in indoor and underground space. High-precision positioning and attitude determination are the key to MMS. Usually, GNSS/INS integrated navigation system provides reliable pose information. However, in the GNSS-denied environments, there is no effective long-term positioning method. With the development of simultaneous localization and mapping (SLAM) algorithm, it provides a new solution for indoor mobile mapping. This paper develops a portable backpack mobile mapping system, which integrates multi-sensor such as LiDAR, IMU, GNSS and panoramic camera. The 3D laser SLAM algorithm is applied to the mobile mapping to realize the acquisition of geographic information data in various complex environments. The experimental results in typical indoor and outdoor scenes show that the system can achieve high-precision and efficient acquisition of 3D information, and the relative precision of point cloud is 2~4cm, which meets the requirements of scene mapping and reconstruction.},
 author = {Yu, Peidong and Wang, Mengke and Chen, Huanjian},
 doi = {10.1051/e3sconf/202020603014},
 journal = {E3S Web of Conferences},
 keywords = {},
 note = {https://www.e3s-conferences.org/articles/e3sconf/pdf/2020/66/e3sconf_icgec2020_03014.pdf},
 number = {},
 pages = {03014},
 title = {Integration and evaluation of SLAM-based backpack mobile mapping system},
 url = {https://app.dimensions.ai/details/publication/pub.1132542281},
 volume = {206},
 year = {2020}
}

@inproceedings{pub.1132813758,
 abstract = {Localization is an elemental requirement for autonomous navigation, simultaneous localization and mapping for mobile robots. As robots can perform long-term and large-scale tasks, finding locations in changing environment is a crucial problem. To resolve the problem, we present a robust localization system under severe appearance changes. The system consists of two stages. First, a robust feature extraction method using a deep convolutional auto-encoder is proposed. Then, global alignment of extracted feature sequences is proposed to find the actual robot's locations. Since the proposed method not only uses the condition-robust features but also considers the actual trajectory of the robot by aligning features sequences, it can show accurate localization performances in changing environments. Experiments were conducted to prove the effective of the proposed method, and the results showed that our method outperformed than existing methods.},
 author = {Oh, Jung H. and Lee, Heung-Jae},
 booktitle = {2019 3rd European Conference on Electrical Engineering and Computer Science (EECS)},
 doi = {10.1109/eecs49779.2019.00026},
 keywords = {},
 pages = {72-75},
 title = {Global Alignment of Deep Features for Robot Localization in Changing Environment},
 url = {https://app.dimensions.ai/details/publication/pub.1132813758},
 year = {2019}
}

@inproceedings{pub.1132997380,
 abstract = {Simultaneous localization and mapping (SLAM) is about consistent maps in the long run. Loop closing is the most popular way for ensure long-term consistency in presence of multiple measurements by the same or multiple robots. Loop closure can be executed using raw odometrical data, but a more sophisticated, yet still light-weight method is presented in this paper: a landmark descriptor-based relative displacement calculation method for diminishing unwanted orientation errors that otherwise often lead to map inconsistency. Landmark descriptors are created using light detection and ranging (LiDAR) scans and the relation is calculated using scan-matching. The novelty of this research is a method providing long-term orientation and position correction without additional overhead between landmark detections, thus enabling simple agents to do the SLAM in a cooperative way.},
 author = {PÃ©ter, GÃ¡bor and Kiss, BÃ¡lint},
 booktitle = {2020 23rd International Symposium on Measurement and Control in Robotics (ISMCR)},
 doi = {10.1109/ismcr51255.2020.9263722},
 keywords = {},
 pages = {1-6},
 title = {Lightweight SLAM with automatic orientation correction using 2D LiDAR scans},
 url = {https://app.dimensions.ai/details/publication/pub.1132997380},
 year = {2020}
}

@article{pub.1133101603,
 abstract = {Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation (Schaefer and BÃ¼scher, 0000). In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15Â months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy.},
 author = {Schaefer, Alexander and BÃ¼scher, Daniel and Vertens, Johan and Luft, Lukas and Burgard, Wolfram},
 doi = {10.1016/j.robot.2020.103709},
 journal = {Robotics and Autonomous Systems},
 keywords = {},
 number = {},
 pages = {103709},
 title = {Long-term vehicle localization in urban environments based on pole landmarks extracted from 3-D lidar scans},
 url = {https://app.dimensions.ai/details/publication/pub.1133101603},
 volume = {136},
 year = {2021}
}

@inproceedings{pub.1133336764,
 abstract = {At present, simultaneous localization and mapping (SLAM) has become an important method for autonomous underwater vehicles (AUVs) to realize long-term navigation. However, using only bathymetric data in unknown environment has its own disadvantages, that are low precision and large computational load. To tackle with requirements of high-precision navigation under large-scale and long-term voyage condition, a SLAM method and corresponding matching algorithm for integrating multi-geophysical field data are proposed. By dividing the feature data and location data of geophysical field obtained into various submaps and sub-segments during AUV sailing, the dominant navigation data of each segment is identified using long short-term memory network. Validity of the proposed method is done by simulation experiments. During the simulation, the loop closure detection of each submap is used, and the matching counter is set to check the correct matching rate. Finally, the matching results with single geophysics field data under the same conditions are compared with multi-geophysics field data and analyzed. The experimental results have demonstrated the feasibility and correctness of the proposed method.},
 author = {Li, ZiYuan and Yu, HuaPeng and Shen, TongSheng and Li, ZhiHui},
 booktitle = {2020 3rd International Conference on Unmanned Systems (ICUS)},
 doi = {10.1109/icus50048.2020.9274964},
 keywords = {},
 pages = {147-151},
 title = {Segmented Matching Method of Multi-Geophysics Field SLAM Data Based on LSTM},
 url = {https://app.dimensions.ai/details/publication/pub.1133336764},
 year = {2020}
}

@article{pub.1133409966,
 abstract = {We present a self-supervised learning approach for the semantic segmentation
of lidar frames. Our method is used to train a deep point cloud segmentation
architecture without any human annotation. The annotation process is automated
with the combination of simultaneous localization and mapping (SLAM) and
ray-tracing algorithms. By performing multiple navigation sessions in the same
environment, we are able to identify permanent structures, such as walls, and
disentangle short-term and long-term movable objects, such as people and
tables, respectively. New sessions can then be performed using a network
trained to predict these semantic labels. We demonstrate the ability of our
approach to improve itself over time, from one session to the next. With
semantically filtered point clouds, our robot can navigate through more complex
scenarios, which, when added to the training pool, help to improve our network
predictions. We provide insights into our network predictions and show that our
approach can also improve the performances of common localization techniques.},
 author = {Thomas, Hugues and Agro, Ben and Gridseth, Mona and Zhang, Jian and Barfoot, Timothy D.},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor
  Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1133409966},
 volume = {},
 year = {2020}
}

@article{pub.1134085133,
 abstract = {Light detection and ranging (LiDAR) sensors enable a vehicle to estimate a pose by matching their measurements with a point cloud (PCD) map. However, the PCD map structure, widely used in robot fields, has some problems to be applied for mass production in automotive fields. First, the PCD map is too big to store all map data at in-vehicle units or download the map data from a wireless network according to the vehicle location. Second, the PCD map, represented by a single origin in the Cartesian coordinates, causes coordinate conversion errors due to an inaccurate plane-orb projection, when the vehicle estimate the geodetic pose on Earth. To solve two problems, this paper presents a geodetic normal distribution (GND) map structure. The GND map structure supports a geodetic quad-tree tiling system with multiple origins to minimize the coordinate conversion errors. The map data managed by the GND map structure are compressed by using Cartesian probabilistic distributions of points as map features. The truncation errors by heterogeneous coordinates between the geodetic tiling system and Cartesian distributions are compensated by the Cartesian voxelization rule. In order to match the LiDAR measurements with the GND map structure, the paper proposes map-matching approaches based on Monte-Carlo and optimization. The paper performed some experiments to evaluate the map size compression and the long-term localization on Earth: comparison with the PCD map structure, localization in various continents, and long-term localization.},
 author = {Kim, Chansoo and Cho, Sungjin and Sunwoo, Myoungho and Resende, Paulo and BradaÃ¯, Benazouz and Jo, Kichun},
 doi = {10.1109/access.2020.3047421},
 journal = {IEEE Access},
 keywords = {},
 note = {https://ieeexplore.ieee.org/ielx7/6287639/9312710/09308903.pdf},
 number = {},
 pages = {470-484},
 title = {A Geodetic Normal Distribution Map for Long-Term LiDAR Localization on Earth},
 url = {https://app.dimensions.ai/details/publication/pub.1134085133},
 volume = {9},
 year = {2021}
}

@inproceedings{pub.1134222038,
 abstract = {Radar and lidar, provided by two different range sensors, each has pros and cons of various perception tasks on mobile robots or autonomous driving. In this paper, a Monte Carlo system is used to localize the robot with a rotating radar sensor on 2D lidar maps. We first train a conditional generative adversarial network to transfer raw radar data to lidar data, and achieve reliable radar points from generator. Then an efficient radar odometry is included in the Monte Carlo system. Combining the initial guess from odometry, a measurement model is proposed to match the radar data and prior lidar maps for final 2D positioning. We demonstrate the effectiveness of the proposed localization framework on the public multisession dataset. The experimental results show that our system can achieve high accuracy for long-term localization in outdoor scenes.},
 author = {Yin, Huan and Wang, Yue and Tang, Li and Xiong, Rong},
 booktitle = {2020 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
 doi = {10.1109/rcar49640.2020.9303291},
 keywords = {},
 note = {http://arxiv.org/pdf/2005.04644},
 pages = {1-7},
 title = {Radar-on-Lidar: metric radar localization on prior lidar maps},
 url = {https://app.dimensions.ai/details/publication/pub.1134222038},
 year = {2020}
}

@article{pub.1134363909,
 abstract = {Vehicles with prolonged autonomous missions have to maintain environment
awareness by simultaneous localization and mapping (SLAM). Closed loop
correction is substituted by interpolation in rigid body transformation space
in order to systematically reduce the accumulated error over different scales.
The computation is divided to an edge computed lightweight SLAM and iterative
corrections in the cloud environment. Tree locations in the forest environment
are sent via a potentially limited communication bandwidths. Data from a real
forest site is used in the verification of the proposed algorithm. The
algorithm adds new iterative closest point (ICP) cases to the initial SLAM and
measures the resulting map quality by the mean of the root mean squared error
(RMSE) of individual tree clusters. Adding 4% more match cases yields the mean
RMSE 0.15 m on a large site with 180 m odometric distance.},
 author = {Nevalainen, Paavo and Movahedi, Parisa and Queralta, Jorge PeÃ±a and Westerlund, Tomi and Heikkonen, Jukka},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Long-Term Autonomy in Forest Environment using Self-Corrective SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1134363909},
 volume = {},
 year = {2020}
}

@inbook{pub.1134521286,
 abstract = {While many researchers have built service robot prototypes that work perfectly under close human supervision, deploying an autonomous robot in an open environment for a long time is not always trivial. This paper presents our experience with TritonBot, a long-term autonomous receptionist and tour guide robot. We deployed TritonBot as an example to study reliability challenges in long-term autonomous service robots. During the past two years, we regularly did maintenance, fixed issues, and rolled out new features. In the process, we identified reliability engineering challenges in three aspects of long-term autonomy: scalability, resilience, and learning; we also formulated techniques to confront these challenges. Our experience shows that proper engineering practices and design principles reduce manual interventions and increase general reliability in long-term autonomous service robot deployments.},
 author = {Wang, Shengye and Liu, Xiao and Zhao, Jishen and Christensen, Henrik I.},
 booktitle = {Field and Service Robotics},
 doi = {10.1007/978-981-15-9460-1_4},
 keywords = {},
 pages = {45-58},
 publisher = {},
 title = {Robotic Reliability Engineering: Experience from Long-Term TritonBot Development},
 url = {https://app.dimensions.ai/details/publication/pub.1134521286},
 year = {2021}
}

@inbook{pub.1134523182,
 abstract = {In recent years the range of robotics platforms available for research and development has increased dramatically. Despite this, there are areas and applications which are not currently well served by the existing available platforms. Many of them are designed for indoor use; the range of outdoor and off-road robotics platforms is less diverse and few address the issue of deployment in hazardous weather conditions and integrate a suitable sensor suite. In addition almost all of the commercially available Unmanned Ground Vehicles (UGVs) are unsuitable for deployment on delicate surfaces. Given the widespread use of manicured grass within the built environment and agriculture across the Western world, this severely limits where they can be deployed and the tasks that can be accomplished. This paper introduces the design principles of a suitable autonomous vehicle. Hulk is built from a commercial zero-turn mower modified for fly-by-wire operation and equipped with a full sensor suite and computing payload. To enable remote, long-term autonomy in diverse environments, several layers of redundant safety systems were designed and installed and the entire assembly made weatherproof.},
 author = {Kyberd, Stephen and Attias, Jonathan and Get, Peter and Murcutt, Paul and Prahacs, Chris and Towlson, Matthew and Venn, Simon and Vasconcelos, Andreia and Gadd, Matthew and Martini, Daniele De and Newman, Paul},
 booktitle = {Field and Service Robotics},
 doi = {10.1007/978-981-15-9460-1_8},
 keywords = {},
 note = {https://ora.ox.ac.uk/objects/uuid:6d3804c9-623a-496d-8c2a-b60b6a627f32/download_file?safe_filename=KyberdetalAAM2021.pdf&file_format=application%2Fpdf&type_of_work=Conference+item},
 pages = {101-114},
 publisher = {},
 title = {The Hulk: Design and Development of a Weather-Proof Vehicle for Long-Term Autonomy in Outdoor Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1134523182},
 year = {2021}
}

@inbook{pub.1134525508,
 abstract = {Long-term monitoring of natural environments raises significant challenges due to the strong perceptual aliasing in trees, bushes and shrubs. This paper reports on the multi-session localization and mapping of a small lake shore using an autonomous surface vessel equipped with a 2D lidar and a camera. Our publicly available dataset includes 130 autonomous surveys of the 1Â km shoreline while recording lidar, GPS and image data. We build our globally consistent multi-session map using ICP at multiple scales. The end result is evaluated qualitatively by superimposing all the lidar maps, and quantitatively by comparing images taken from the same pose at different times. The localization and mapping results, as well as the dataset of image pairs, are made available within our public dataset.},
 author = {Pradalier, CÃ©dric and Aravecchia, StÃ©phanie and Pomerleau, FranÃ§ois},
 booktitle = {Field and Service Robotics},
 doi = {10.1007/978-981-15-9460-1_1},
 keywords = {},
 note = {https://hal.archives-ouvertes.fr/hal-01867104v2/file/root_fsr.pdf},
 pages = {1-14},
 publisher = {},
 title = {Multi-session Lake-Shore Monitoring in Visually Challenging Conditions},
 url = {https://app.dimensions.ai/details/publication/pub.1134525508},
 year = {2021}
}

@article{pub.1134942605,
 abstract = {When driving, people make decisions based on current traffic as well as their desired route. They have a mental map of known routes and are often able to navigate without needing directions. Current published self-driving models improve their performances when using additional GPS information. Here we aim to push forward self-driving research and perform route planning even in the complete absence of GPS at inference time. Our system learns to predict in real-time vehicle's current location and future trajectory, on a known map, given only the raw video stream and the final destination. Trajectories consist of instant steering commands that depend on present traffic, as well as longer-term navigation decisions towards a specific destination. Along with our novel proposed approach to localization and navigation from visual data, we also introduce a novel large dataset in an urban environment, which consists of video and GPS streams collected with a smartphone while driving. The GPS is automatically processed to obtain supervision labels and to create an analytical representation of the traversed map. In tests, our solution outperforms published state of the art methods on visual localization and steering and provides reliable navigation assistance between any two known locations. We also show that our system can adapt to short and long-term changes in weather conditions or the structure of the urban environment. We make the entire dataset and the code publicly available.},
 author = {Leordeanu, Marius and Paraicu, Iulia},
 doi = {10.3390/s21030852},
 journal = {Sensors},
 keywords = {},
 note = {https://europepmc.org/articles/pmc7865778?pdf=render},
 number = {3},
 pages = {852},
 title = {Driven by Vision: Learning Navigation by Visual Localization and Trajectory Prediction},
 url = {https://app.dimensions.ai/details/publication/pub.1134942605},
 volume = {21},
 year = {2021}
}

@article{pub.1135029698,
 abstract = {Accurate vehicle ego-localization is key for autonomous vehicles to complete high-level navigation tasks. The state-of-the-art localization methods adopt visual and light detection and ranging (LiDAR) simultaneous localization and mapping (SLAM) to estimate the position of the vehicle. However, both of them may suffer from error accumulation due to long-term running without loop optimization or prior constraints. Actually, the vehicle cannot always return to the revisited location, which will cause errors to accumulate in Global Navigation Satellite System (GNSS)-challenged environments. To solve this problem, we proposed a novel localization method with prior dense visual point cloud map constraints generated by a stereo camera. Firstly, the semi-global-block-matching (SGBM) algorithm is adopted to estimate the visual point cloud of each frame and stereo visual odometry is used to provide the initial position for the current visual point cloud. Secondly, multiple filtering and adaptive prior map segmentation are performed on the prior dense visual point cloud map for fast matching and localization. Then, the current visual point cloud is matched with the candidate sub-map by normal distribution transformation (NDT). Finally, the matching result is used to update pose prediction based on the last frame for accurate localization. Comprehensive experiments were undertaken to validate the proposed method, showing that the root mean square errors (RMSEs) of translation and rotation are less than 5.59 m and 0.08Â°, respectively.},
 author = {Lin, Xiaohu and Wang, Fuhong and Yang, Bisheng and Zhang, Wanwei},
 doi = {10.3390/rs13030506},
 journal = {Remote Sensing},
 keywords = {},
 note = {https://www.mdpi.com/2072-4292/13/3/506/pdf},
 number = {3},
 pages = {506},
 title = {Autonomous Vehicle Localization with Prior Visual Point Cloud Map Constraints in GNSS-Challenged Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1135029698},
 volume = {13},
 year = {2021}
}

@article{pub.1135071178,
 abstract = {Localization is one of the core technologies for mobile robots to achieve full autonomous movement, and is a prerequisite for other autonomous tasks. The robot working environment is dynamic in most cases, so the localization algorithm must overcome the effects of dynamic changes in the environment. The paper proposed a localization algorithm that allows the robot to perform robust and life-long localization in dynamic environment. The algorithm filter out high-dynamic objects and update semi-static object on the map at the same time, it can also use the information provided in semi-static objects to improve localization performance. In this paper, the processing of dynamic objects is divided into two parts: filtering of high-dynamic objects and updating of semi-static objects. For high dynamic object filtering, a dynamic object detection method combining a delay comparison method and a tracking method is proposed by observed the characteristics of localization system; For the update of semi-static objects, this paper uses the pose graph optimization and occupancy map to implement the dynamic update of the map. The combination of the two methods allows the robot to achieve long-term stable localization in a dynamic environment. The experimental results demonstrate that the proposed method allows the robot achieve long-term localization, overcome the effects of high-dynamic objects and keeping the map always consistent with the environment.},
 author = {Huang, Shan and Huang, Hong-Zhong and Zeng, Qi},
 doi = {10.1088/1757-899x/1043/5/052025},
 journal = {IOP Conference Series: Materials Science and Engineering},
 keywords = {},
 note = {https://doi.org/10.1088/1757-899x/1043/5/052025},
 number = {5},
 pages = {052025},
 title = {A Robust Localization Method in Indoor Dynamic Environment},
 url = {https://app.dimensions.ai/details/publication/pub.1135071178},
 volume = {1043},
 year = {2021}
}

@inproceedings{pub.1135351088,
 abstract = {Autonomous valet parking is a specific application for autonomous vehicles. In this task, vehicles need to navigate in narrow, crowded and GPS-denied parking lots. Accurate localization ability is of great importance. Traditional visual-based methods suffer from tracking lost due to texture-less regions, repeated structures, and appearance changes. In this paper, we exploit robust semantic features to build the map and localize vehicles in parking lots. Semantic features contain guide signs, parking lines, speed bumps, etc, which typically appear in parking lots. Compared with traditional features, these semantic features are long-term stable and robust to the perspective and illumination change. We adopt four surround-view cameras to increase the perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system generates a global visual semantic map. This map is further used to localize vehicles at the centimeter level. We analyze the accuracy and recall of our system and compare it against other methods in real experiments. Furthermore, we demonstrate the practicability of the proposed system by the autonomous parking application.},
 author = {Qin, Tong and Chen, Tongqing and Chen, Yilun and Su, Qing},
 booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros45743.2020.9340939},
 keywords = {},
 note = {http://arxiv.org/pdf/2007.01813},
 pages = {5939-5945},
 title = {AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot},
 url = {https://app.dimensions.ai/details/publication/pub.1135351088},
 year = {2020}
}

@inproceedings{pub.1135355524,
 abstract = {In this paper, we present a novel distributed algorithm to track a moving objectâ€™s state by utilizing a heterogenous mobile robot network in a three-dimensional (3-D) environment, wherein the robotsâ€™ poses (positions and orientations) are unknown. Each robot is equipped with a monocular camera and an inertial measurement unit (IMU), and has the ability to communicate with its neighbors. Rather than assuming a known common global frame for all the robots (which is often the case in the literature regarding multi-robot systems), we allow each robot to perform motion estimation locally. For localization, we propose a multi-robot visual-inertial navigation systems (VINS) where one robot builds a prior map and then the map is used to bound the long-term drifts of the visual-inertial odometry (VIO) running on the other robots. Moreover, a novel distributed Kalman filter is introduced and employed to cooperatively track the six degree-of-freedom (6-DoF) motion of the object which is represented as a point cloud. Further, the object can be totally invisible to some robots during the tracking period. The proposed algorithm is extensively validated in Monte-Carlo simulations.},
 author = {Zhu, Pengxiang and Ren, Wei},
 booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros45743.2020.9341393},
 keywords = {},
 pages = {11573-11580},
 title = {Multi-Robot Joint Visual-Inertial Localization and 3-D Moving Object Tracking},
 url = {https://app.dimensions.ai/details/publication/pub.1135355524},
 year = {2020}
}

@inproceedings{pub.1135359608,
 abstract = {Easy, yet robust long-term localization is still an open topic in research. Existing approaches require either dense maps, expensive sensors, specialized map features or proprietary detectors. We propose using semantic segmentation on a monocular camera to localize directly in a HD map as used for automated driving. This combines lightweight, yet powerful HD maps with the simplicity of monocular vision and the flexibility of neural networks. The major challenges arising from this combination are data association and robustness against misdetections. Association is solved efficiently by applying distance transform on binary per-class images. This provides not only a fast lookup table for a smooth gradient as needed for pose-graph optimization, but also dynamic association by default. A sliding-window pose graph optimization combines single image detections with vehicle odometry, smoothing results and helping overcome even misclassifications in consecutive frames. Evaluation against a highly accurate 6D visual localization shows that our approach can achieve accuracy levels as required for automated driving, being one of the most lightweight and flexible methods to do so.},
 author = {Pauls, Jan-Hendrik and Petek, KÃ¼rsat and Poggenhans, Fabian and Stiller, Christoph},
 booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros45743.2020.9341003},
 keywords = {},
 pages = {4595-4601},
 title = {Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform},
 url = {https://app.dimensions.ai/details/publication/pub.1135359608},
 year = {2020}
}

@article{pub.1135388936,
 abstract = {Loop closure detection is of vital importance in the process of simultaneous localization and mapping (SLAM), as it helps to reduce the cumulative error of the robot's estimated pose and generate a consistent global map. Many variations of this problem have been considered in the past and the existing methods differ in the acquisition approach of query and reference views, the choice of scene representation, and associated matching strategy. Contributions of this survey are many-fold. It provides a thorough study of existing literature on loop closure detection algorithms for visual and Lidar SLAM and discusses their insight along with their limitations. It presents a taxonomy of state-of-the-art deep learning-based loop detection algorithms with detailed comparison metrics. Also, the major challenges of conventional approaches are identified. Based on those challenges, deep learning-based methods were reviewed where the identified challenges are tackled focusing on the methods providing long-term autonomy in various conditions such as changing weather, light, seasons, viewpoint, and occlusion due to the presence of mobile objects. Furthermore, open challenges and future directions were also discussed.},
 author = {Arshad, Saba and Kim, Gon-Woo},
 doi = {10.3390/s21041243},
 journal = {Sensors},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/21/4/1243/pdf?version=1613620831},
 number = {4},
 pages = {1243},
 title = {Role of Deep Learning in Loop Closure Detection for Visual and Lidar SLAM: A Survey},
 url = {https://app.dimensions.ai/details/publication/pub.1135388936},
 volume = {21},
 year = {2021}
}

@article{pub.1135419756,
 abstract = {To cope with the growing demand for transportation on the railway system,
accurate, robust, and high-frequency positioning is required to enable a safe
and efficient utilization of the existing railway infrastructure. As a basis
for a localization system we propose a complete on-board mapping pipeline able
to map robust meaningful landmarks, such as poles from power lines, in the
vicinity of the vehicle. Such poles are good candidates for reliable and long
term landmarks even through difficult weather conditions or seasonal changes.
To address the challenges of motion blur and illumination changes in railway
scenarios we employ a Dynamic Vision Sensor, a novel event-based camera. Using
a sideways oriented on-board camera, poles appear as vertical lines. To map
such lines in a real-time event stream, we introduce Hough2Map, a novel
consecutive iterative event-based Hough transform framework capable of
detecting, tracking, and triangulating close-by structures. We demonstrate the
mapping reliability and accuracy of Hough2Map on real-world data in typical
usage scenarios and evaluate using surveyed infrastructure ground truth maps.
Hough2Map achieves a detection reliability of up to 92% and a mapping root mean
square error accuracy of 1.1518m.},
 author = {Tschopp, Florian and von Einem, Cornelius and Cramariuc, Andrei and Hug, David and Palmer, Andrew William and Siegwart, Roland and Chli, Margarita and Nieto, Juan},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Hough2Map -- Iterative Event-based Hough Transform for High-Speed
  Railway Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1135419756},
 volume = {},
 year = {2021}
}

@article{pub.1135633780,
 abstract = {Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.},
 author = {Hu, Hanjiang and Wang, Hesheng and Liu, Zhe and Chen, Weidong},
 doi = {10.1109/jas.2021.1003907},
 journal = {IEEE/CAA Journal of Automatica Sinica},
 keywords = {},
 note = {http://arxiv.org/pdf/2009.07719},
 number = {2},
 pages = {313-328},
 title = {Domain-Invariant Similarity Activation Map Contrastive Learning for Retrieval-Based Long-Term Visual Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1135633780},
 volume = {9},
 year = {2022}
}

@article{pub.1135633982,
 abstract = {LiDAR-based odometry and mapping is used in many robotic applications to retrieve the robot's position in an unknown environment and allows for autonomous operation in GPS-denied (e.g., indoor) environments. With a 3D LiDAR sensor, highly accurate localization becomes possible, which enables high quality 3D reconstruction of the environment. In this letter we extend the well-known LOAM framework by leveraging prior knowledge about a reference object in the environment to further improve the localization accuracy. This requires a known 3D model of the reference object and its known position in a global coordinate frame. Instead of only relying on the point features in the mapping module of LOAM, we also include mesh features extracted from the 3D triangular mesh of the reference object in the optimization problem. For fast correspondence computation of mesh features, we use the Axis-Aligned-Bounding-Box-Tree (AABB) structure. Essentially, our approach not only makes use of the previously built map for absolute localization in the environment, but also takes the relative position to the reference object into account, effectively reducing long-term drift. To validate the proposed concept, we generated datasets using the Gazebo simulation environment in exemplary visual inspection scenarios of an airplane inside a hangar and the Eiffel Tower. An actuated 3D LiDAR sensor is mounted via a 1-DoF gimbal on a UAV capturing 360$^\circ$ scans. We benchmark our approach against the state-of-the-art open-source LOAM framework. The results show that the proposed joint optimization using both point and mesh features yields a significant reduction in Absolute Pose Error (APE) and therefore improves the map and 3D reconstruction quality during long-term operations.},
 author = {Oelsch, Martin and Karimi, Mojtaba and Steinbach, Eckehard},
 doi = {10.1109/lra.2021.3060413},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 number = {2},
 pages = {2068-2075},
 title = {R-LOAM: Improving LiDAR Odometry and Mapping With Point-to-Mesh Features of a Known 3D Reference Object},
 url = {https://app.dimensions.ai/details/publication/pub.1135633982},
 volume = {6},
 year = {2021}
}

@article{pub.1135634013,
 abstract = {Global localization is essential for robot navigation, of which the first step is to retrieve a query from the map database. This problem is called place recognition. In recent years, LiDAR scan based place recognition has drawn attention as it is robust against the appearance change. In this letter, we propose a LiDAR-based place recognition method, named Differentiable Scan Context with Orientation (DiSCO), which simultaneously finds the scan at a similar place and estimates their relative orientation. The orientation can further be used as the initial value for the down-stream local optimal metric pose estimation, improving the pose estimation especially when a large orientation between the current scan and retrieved scan exists. Our key idea is to transform the feature into the frequency domain. We utilize the magnitude of the spectrum as the place descriptor, which is theoretically rotation-invariant. In addition, based on the differentiable phase correlation, we can efficiently estimate the global optimal relative orientation using the spectrum. With such structural constraints, the network can be learned in an end-to-end manner, and the backbone is fully shared by the two tasks, achieving better interpretability and lightweight. Finally, DiSCO is validated on three datasets with long-term outdoor conditions, showing better performance than the compared methods. Codes are released at https://github.com/MaverickPeter/DiSCO-pytorch.},
 author = {Xu, Xuecheng and Yin, Huan and Chen, Zexi and Li, Yuehua and Wang, Yue and Xiong, Rong},
 doi = {10.1109/lra.2021.3060741},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 note = {http://arxiv.org/pdf/2010.10949},
 number = {2},
 pages = {2791-2798},
 title = {DiSCO: Differentiable Scan Context With Orientation},
 url = {https://app.dimensions.ai/details/publication/pub.1135634013},
 volume = {6},
 year = {2021}
}

@article{pub.1135690841,
 abstract = {Global localization and kidnapping are two challenging problems in robot localization. The popular method, Monte Carlo Localization (MCL) addresses the problem by iteratively updating a set of particles with a sampling-weighting loop. Sampling is decisive to the performance of MCL [1]. However, traditional MCL can only sample from a uniform distribution over the state space. Although variants of MCL propose different sampling models, they fail to provide an accurate distribution or generalize across scenes. To better deal with these problems, we present a distribution proposal model named Deep Samplable Observation Model (DSOM). DSOM takes a map and a 2D laser scan as inputs and outputs a conditional multimodal probability distribution of the pose, making the samples more focusing on the regions with higher likelihood. With such samples, the convergence is expected to be more effective and efficient. Considering that the learning-based sampling model may fail to capture the accurate pose sometimes, we furthermore propose the Adaptive Mixture MCL (AdaM MCL), which deploys a trusty mechanism to adaptively select updating mode for each particle to tolerate this situation. Equipped with DSOM, AdaM MCL can achieve more accurate estimation, faster convergence and better scalability than previous methods in both synthetic and real scenes. Even in real environments with long-term changes, AdaM MCL is able to localize the robot using DSOM trained only by simulation observations from a SLAM map or a blueprint map. Source code for this paper is available here: https://github.com/Runjian-Chen/AdaM_MCL.},
 author = {Chen, Runjian and Yin, Huan and Jiao, Yanmei and Dissanayake, Gamini and Wang, Yue and Xiong, Rong},
 doi = {10.1109/lra.2021.3061339},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 note = {http://arxiv.org/pdf/2009.00211},
 number = {2},
 pages = {2296-2303},
 title = {Deep Samplable Observation Model for Global Localization and Kidnapping},
 url = {https://app.dimensions.ai/details/publication/pub.1135690841},
 volume = {6},
 year = {2021}
}

@article{pub.1135690878,
 abstract = {To cope with the growing demand for transportation on the railway system, accurate, robust, and high-frequency positioning is required to enable a safe and efficient utilization of the existing railway infrastructure. As a basis for a localization system we propose a complete on-board mapping pipeline able to map robust meaningful landmarks, such as poles from power lines, in the vicinity of the vehicle. Such poles are good candidates for reliable and long term landmarks even through difficult weather conditions or seasonal changes. To address the challenges of motion blur and illumination changes in railway scenarios we employ a Dynamic Vision Sensor, a novel event-based camera. Using a sideways oriented on-board camera, poles appear as vertical lines. To map such lines in a real-time event stream, we introduce Hough$^\mathbf {2}$Map, a novel consecutive iterative event-based Hough transform framework capable of detecting, tracking, and triangulating close-by structures. We demonstrate the mapping reliability and accuracy of Hough$^\mathbf {2}$Map on real-world data in typical usage scenarios and evaluate using surveyed infrastructure ground truth maps. Hough$^\mathbf {2}$Map achieves a detection reliability of up to $92\,\%$ and a mapping root mean square error accuracy of 1.1518 m.11The code is available at https://github.com/ethz-asl/Hough2Map. The code is available at https://github.com/ethz-asl/Hough2Map.},
 author = {Tschopp, Florian and von Einem, Cornelius and Cramariuc, Andrei and Hug, David and Palmer, Andrew William and Siegwart, Roland and Chli, Margarita and Nieto, Juan},
 doi = {10.1109/lra.2021.3061404},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 note = {http://arxiv.org/pdf/2102.08145},
 number = {2},
 pages = {2745-2752},
 title = {Hough$^2$Map Iterative Event-Based Hough Transform for High-Speed Railway Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1135690878},
 volume = {6},
 year = {2021}
}

@article{pub.1135854615,
 abstract = {Precise positioning is the basic condition for intelligent vehicles to complete perception, decision making and control tasks. In response to this challenge, in this article, lidar simultaneous localization and mapping (SLAM) is taken as the research object, and a SLAM system is designed that integrates motion compensation and ground information removal functions, and can construct a real-time environment map and determine its own position on the map while the vehicle is driving. A loop-closure detection method with a multiresolution point cloud histogram mode is proposed, which can effectively detect whether the vehicle passes through the same position and perform optimization to obtain globally consistent pose and map information in the urban conditions with more driving loops. We conduct experiments on the well-known KITTI dataset and compare the results with those of state-of-the-art systems. The experiments confirm that the lidar SLAM system designed in this article can provide accurate and effective positioning information for intelligent vehicles. The proposed loop-closure detection algorithm has an excellent real-time performance and accuracy, which can guarantee the long-term driving operation of these vehicles.},
 author = {Meng, Qingyu and Guo, Hongyan and Zhao, Xiaoming and Cao, Dongpu and Chen, Hong},
 doi = {10.1109/tmech.2021.3062647},
 journal = {IEEE/ASME Transactions on Mechatronics},
 keywords = {},
 number = {3},
 pages = {1307-1317},
 title = {Loop-Closure Detection With a Multiresolution Point Cloud Histogram Mode in Lidar Odometry and Mapping for Intelligent Vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1135854615},
 volume = {26},
 year = {2021}
}

@article{pub.1136331679,
 abstract = {Visual simultaneous localization and mapping (vSLAM), one of the most important applications in autonomous vehicles and robots to estimate the position and pose using inexpensive visual sensors, suffers from error accumulation for longâ€term navigation without loop closure detection. Recently, deep neural networks (DNNs) are leveraged to achieve high accuracy for loop closure detection, however the execution time is much slower than those employing handcrafted visual features. In this paper, a parallel loop searching and verifying method for loop closure detection with both high accuracy and high speed, which combines two parallel tasks using handcrafted and DNN features, respectively, is proposed. A fast loop searching is proposed to link the bagâ€ofâ€words features and histogram for higher accuracy, and it splits the images into multiple grids for high parallelism; meanwhile, a DNN feature extractor is utilized for further verification. A loop state control method based on a finite state machine to control these tasks is designed, wherein the loop closure detection is described as a contextâ€related procedure. The framework is implemented on a real machine, and the topâ€2 best accuracy and fastest execution time of 80â€543 frames per second (min: 1.84ms, and max: 12.45ms) are achieved on several public benchmarks compared with some existing algorithms.},
 author = {Yang, Zhe and Pan, Yun and Deng, Lei and Xie, Yuan and Huan, Ruohong},
 doi = {10.1049/itr2.12054},
 journal = {IET Intelligent Transport Systems},
 keywords = {},
 note = {https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/itr2.12054},
 number = {5},
 pages = {683-698},
 title = {PLSAV: Parallel loop searching and verifying for loop closure detection},
 url = {https://app.dimensions.ai/details/publication/pub.1136331679},
 volume = {15},
 year = {2021}
}

@inproceedings{pub.1136595010,
 abstract = {Appearance changes are a challenge for visual localization in outdoor environments. Revisiting familiar places but retrieving keyframes that were taken under different environmental condition can result in inaccurate localization. To overcome this difficulty, we propose a localization approach able to take advantage of a visual landmark map composed of $N$ sequences gathered at different times and conditions. During this localization process, we exploit information collected in the beginning of the trajectory to compute a ranking function which will be used in the rest of the trajectory to retrieve from the map the keyframes that maximise the number of matched points. The retrieval depends on the geometric distance between the pose of the keyframe and the current pose of the vehicle, and the similarity of this keyframe with the current environmental condition. The results demonstrate that our approach has significantly improved localization performance in challenging conditions (snow, rain, change of season â€¦).},
 author = {Bouaziz, Youssef and Royer, Eric and Bresson, Guillaume and Dhome, Michel},
 booktitle = {2021 IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI)},
 doi = {10.1109/sami50585.2021.9378614},
 keywords = {},
 pages = {000093-000100},
 title = {Keyframes retrieval for robust long-term visual localization in changing conditions},
 url = {https://app.dimensions.ai/details/publication/pub.1136595010},
 year = {2021}
}

@article{pub.1136691969,
 abstract = {Localization on 3D data is a challenging task for unmanned vehicles,
especially in long-term dynamic urban scenarios. Due to the generality and
long-term stability, the pole-like objects are very suitable as landmarks for
unmanned vehicle localization in time-varing scenarios. In this paper, a
long-term LiDAR-only localization algorithm based on semantic cluster map is
proposed. At first, the Convolutional Neural Network(CNN) is used to infer the
semantics of LiDAR point clouds. Combined with the point cloud segmentation,
the long-term static objects pole/trunk in the scene are extracted and
registered into a semantic cluster map. When the unmanned vehicle re-enters the
environment again, the relocalization is completed by matching the clusters of
the local map with the clusters of the global map. Furthermore, the continuous
matching between the local and global maps stably outputs the global pose at
2Hz to correct the drift of the 3D LiDAR odometry. The proposed approach
realizes localization in the long-term scenarios without maintaining the
high-precision point cloud map. The experimental results on our campus dataset
demonstrate that the proposed approach performs better in localization accuracy
compared with the current state-of-the-art methods. The source of this paper is
available at: http://www.github.com/HITSZ-NRSL/long-term-localization.},
 author = {Wang, Zhihao and Li, Silin and Cao, Ming and Chen, Haoyao and Liu, Yunhui},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Pole-like Objects Mapping and Long-Term Robot Localization in Dynamic
  Urban Scenarios},
 url = {https://app.dimensions.ai/details/publication/pub.1136691969},
 volume = {},
 year = {2021}
}

@inbook{pub.1136721905,
 abstract = {We present a novel method for outdoor monocular visual mapping and localization, by detecting and probabilistically parameterizing road lanes. To present road lane information, we use cascaded deep models for detection on keyframes, and rely on piecewise cubic Catmull-Rom splines for parameterization in complicated environments. Additionally, we propose a maximum-a-posteriori estimation framework to conduct offline mapping, by jointly optimizing the lanesâ€™ geometric appearance and vehicle poses. The computed maps can be used to perform semantic localization, without relying on traditional visual point features. The underlying key design motivation is that, compared to visual point features, road semantic information is naturally highly compact and of long-term consistency (in terms of existence, appearance, and so on). Results from both simulated and real-world experiments show that our method is able to enhance estimation accuracy with cost-effective maps by wide margins.},
 author = {Yang, Sheng and Chen, Yiming and Li, Mingyang},
 booktitle = {Experimental Robotics},
 doi = {10.1007/978-3-030-71151-1_33},
 keywords = {},
 pages = {369-379},
 publisher = {},
 title = {Visual Semantic Mapping and Localization Using Parameterized Road Lanes},
 url = {https://app.dimensions.ai/details/publication/pub.1136721905},
 year = {2021}
}

@article{pub.1136783392,
 abstract = {Accurate localization is fundamental to a variety of applications, such as
navigation, robotics, autonomous driving, and Augmented Reality (AR). Different
from incremental localization, global localization has no drift caused by error
accumulation, which is desired in many application scenarios. In addition to
GPS used in the open air, 3D maps are also widely used as alternative global
localization references. In this paper, we propose a compact 3D map-based
global localization system using a low-cost monocular camera and an IMU
(Inertial Measurement Unit). The proposed compact map consists of two types of
simplified elements with multiple semantic labels, which is well adaptive to
various man-made environments like urban environments. Also, semantic edge
features are used for the key image-map registration, which is robust against
occlusion and long-term appearance changes in the environments. To further
improve the localization performance, the key semantic edge alignment is
formulated as an optimization problem based on initial poses predicted by an
independent VIO (Visual-Inertial Odometry) module. The localization system is
realized with modular design in real time. We evaluate the localization
accuracy through real-world experimental results compared with ground truth,
long-term localization performance is also demonstrated.},
 author = {Qiu, Kejie and Chen, Shenzhou and Zhang, Jiahui and Huang, Rui and Cui, Le and Zhu, Siyu and Tan, Ping},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Compact 3D Map-Based Monocular Localization Using Semantic Edge
  Alignment},
 url = {https://app.dimensions.ai/details/publication/pub.1136783392},
 volume = {},
 year = {2021}
}

@article{pub.1137162325,
 abstract = {A Simultaneous Localization and Mapping (SLAM) system must be robust to
support long-term mobile vehicle and robot applications. However, camera and
LiDAR based SLAM systems can be fragile when facing challenging illumination or
weather conditions which degrade their imagery and point cloud data. Radar,
whose operating electromagnetic spectrum is less affected by environmental
changes, is promising although its distinct sensing geometry and noise
characteristics bring open challenges when being exploited for SLAM. % However,
there are still open challenges since most existing visual and LiDAR SLAM
systems do not operate in bad weathers. This paper studies the use of a
Frequency Modulated Continuous Wave radar for SLAM in large-scale outdoor
environments. We propose a full radar SLAM system, including a novel radar
motion tracking algorithm that leverages radar geometry for reliable feature
tracking. It also optimally compensates motion distortion and estimates pose by
joint optimization. Its loop closure component is designed to be simple yet
efficient for radar imagery by capturing and exploiting structural information
of the surrounding environment. % while a scheme to reject ambiguous loop
closure candidates is also designed specifically for radar. Extensive
experiments on three public radar datasets, ranging from city streets and
residential areas to countryside and highways, show competitive accuracy and
reliability performance of the proposed radar SLAM system compared to the
state-of-the-art LiDAR, vision and radar methods. The results show that our
system is technically viable in achieving reliable SLAM in extreme weather
conditions, e.g. heavy snow and dense fog, demonstrating the promising
potential of using radar for all-weather localization and mapping.},
 author = {Hong, Ziyang and Petillot, Yvan and Wallace, Andrew and Wang, Sen},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Radar SLAM: A Robust SLAM System for All Weather Conditions},
 url = {https://app.dimensions.ai/details/publication/pub.1137162325},
 volume = {},
 year = {2021}
}

@article{pub.1137251168,
 abstract = {A key feature in the context of simultaneous localization and mapping is loop-closure detection, a process determining whether the current robotâ€™s environment perception coincides with previous observation. However, in long-term operations, both computational efficiency and memory requirements involved in an autonomous robot operation in uncontrolled environments, are of particular importance. The majority of approaches scale linearly with the environmentâ€™s size in terms of storage and query time. The article at hand presents an efficient appearance-based loop-closure detection pipeline, which encodes the traversed trajectory by a low amount of unique visual words generated on-line through feature tracking. The incrementally constructed visual vocabulary is referred to as the â€œBag of Tracked Words.â€ A nearest-neighbor voting scheme is utilized to query the database and assign probabilistic scores to all visited locations. Exploiting the inherent temporal coherency in the loop-closure task, the produced scores are processed through a Bayesian filter to estimate the belief state about the robotâ€™s location on the map. Also, a geometrical verification step ensures consistency between image matches. Management is also applied to the resulting vocabulary to reduce its growth rate and constraint the systemâ€™s computational complexity while improving its voting distinctiveness. The proposed approachâ€™s performance is experimentally evaluated on several publicly available and challenging datasets, including hand-held, car-mounted, aerial, and ground trajectories. Results demonstrate the methodâ€™s adaptability, which retains high operational frequency in environments of up to 13 km and high recall rates for perfect precision, outperforming other state-of-the-art techniques. The systemâ€™s effectiveness is owed to the reduced vocabulary size, which is at least one order of magnitude smaller than other contemporary approaches. An open research-oriented source code has been made publicly available, which is dubbed as â€œBoTW-LCD.â€},
 author = {Tsintotas, Konstantinos A. and Bampis, Loukas and Gasteratos, Antonios},
 doi = {10.1016/j.robot.2021.103782},
 journal = {Robotics and Autonomous Systems},
 keywords = {},
 number = {},
 pages = {103782},
 title = {Modest-vocabulary loop-closure detection with incremental bag of tracked words},
 url = {https://app.dimensions.ai/details/publication/pub.1137251168},
 volume = {141},
 year = {2021}
}

@article{pub.1137307389,
 abstract = {Many applications require accurate indoor localization. Fingerprint-based
localization methods propose a solution to this problem, but rely on a radio
map that is effort-intensive to acquire. We automate the radio map acquisition
phase using a software-defined radio (SDR) and a wheeled robot. Furthermore, we
open-source a radio map acquired with our automated tool for a 3GPP Long-Term
Evolution (LTE) wireless link. To the best of our knowledge, this is the first
publicly available radio map containing channel state information (CSI).
Finally, we describe first localization experiments on this radio map using a
convolutional neural network to regress for location coordinates.},
 author = {Gassner, Arthur and Musat, Claudiu and Rusu, Alexandru and Burg, Andreas},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {OpenCSI: An Open-Source Dataset for Indoor Localization Using CSI-Based
  Fingerprinting},
 url = {https://app.dimensions.ai/details/publication/pub.1137307389},
 volume = {},
 year = {2021}
}

@inproceedings{pub.1137774827,
 abstract = {The contemporary SLAM mapping systems assume a static environment and build a map that is then used for mobile robot navigation disregarding the dynamic changes in this environment. The paper at hand presents a novel solution for the problem of life-long mapping that continually updates a metric map represented as a 2D occupancy grid in large scale indoor environments with movable objects such as people, robots, objects etc. suitable for industrial applications. We formalize each cell's occupancy as a failure analysis problem and contribute temporal persistence modeling (TPM), an algorithm for probabilistic prediction of the time that a cell in an observed location is expected to be â€œoccupiedâ€ or â€œemptyâ€ given sparse prior observations from a task specific mobile robot. Our work is evaluated in Gazebo simulation environment against the nominal occupancy of cells and the estimated obstacles persistence. We also show that robot navigation with life-long mapping demands less replans and leads to more efficient navigation in highly dynamic environments.},
 author = {Tsamis, Georgios and Kostavelis, Ioannis and Giakoumis, Dimitrios and Tzovaras, Dimitrios},
 booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
 doi = {10.1109/icpr48806.2021.9413161},
 keywords = {},
 pages = {10480-10485},
 title = {Towards life-long mapping of dynamic environments using temporal persistence modeling},
 url = {https://app.dimensions.ai/details/publication/pub.1137774827},
 year = {2021}
}

@inproceedings{pub.1137857256,
 abstract = {Simultaneous localization and mapping, SLAM can product a Map for autonomous robots and self-driving vehicle in navigation. In the actual environment, the scene changes frequently, which makes the old map no long reliable. Therefore, it is necessary to update such a map by an efficient and safely way. In this paper, we review the existing map updating, long-term localization methods and discuss about the challenges in this situation. We present a Map updating method in mathematical way which can update accurately. Our proposed method are tested in five indoor dataset and demonstrated feasibility.},
 author = {Chen, Hongjian and Wang, Zhiqiang and Zhu, Qing},
 booktitle = {2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)},
 doi = {10.1109/ipec51340.2021.9421197},
 keywords = {},
 pages = {505-508},
 title = {Map Updating Revisited for Navigation Map},
 url = {https://app.dimensions.ai/details/publication/pub.1137857256},
 year = {2021}
}

@article{pub.1138107246,
 abstract = {Place recognition is critical for both offline mapping and online localization. However, current single-sensor based place recognition still remains challenging in adverse conditions. In this paper, a heterogeneous measurement based framework is proposed for long-term place recognition, which retrieves the query radar scans from the existing lidar (Light Detection and Ranging) maps. To achieve this, a deep neural network is built with joint training in the learning stage, and then in the testing stage, shared embeddings of radar and lidar are extracted for heterogeneous place recognition. To validate the effectiveness of the proposed method, we conducted tests and generalization experiments on the multi-session public datasets and compared them to other competitive methods. The experimental results indicate that our model is able to perform multiple place recognitions: lidar-to-lidar (L2L), radar-to-radar (R2R), and radar-to-lidar (R2L), while the learned model is trained only once. We also release the source code publicly: https://github.com/ZJUYH/radar-to-lidar-place-recognition.},
 author = {Yin, Huan and Xu, Xuecheng and Wang, Yue and Xiong, Rong},
 doi = {10.3389/frobt.2021.661199},
 journal = {Frontiers in Robotics and AI},
 keywords = {},
 note = {https://www.frontiersin.org/articles/10.3389/frobt.2021.661199/pdf},
 number = {},
 pages = {661199},
 title = {Radar-to-Lidar: Heterogeneous Place Recognition via Joint Learning},
 url = {https://app.dimensions.ai/details/publication/pub.1138107246},
 volume = {8},
 year = {2021}
}

@article{pub.1138191980,
 abstract = {Although image change detection (ICD) methods provide good detection accuracy for many scenarios, most existing methods rely on place-specific background modeling. The time/space cost for such place-specific models is prohibitive for large-scale scenarios, such as long-term robotic visual simultaneous localization and mapping (SLAM). Therefore, we propose a novel ICD framework that is specifically customized for long-term SLAM. This study is inspired by the multi-map-based SLAM framework, where multiple maps can perform mutual diagnosis and hence do not require any explicit background modeling/model. We extend this multi-map-based diagnosis approach to a more generic single-map-based object-level diagnosis framework (i.e., ICD), where the self-localization module of SLAM, which is the change object indicator, can be used in its original form. Furthermore, we consider map diagnosis on a state-of-the-art deep convolutional neural network (DCN)-based SLAM system (instead of on conventional bag-of-words or landmark-based systems), in which the blackbox nature of the DCN complicates the diagnosis problem. Additionally, we consider a three-dimensional point cloud (PC)-based (instead of typical monocular color image-based) SLAM and adopt a state-of-the-art scan context PC descriptor for map diagnosis for the first time.},
 author = {Tanaka, Kanji},
 doi = {10.20965/jaciii.2021.p0356},
 journal = {Journal of Advanced Computational Intelligence and Intelligent Informatics},
 keywords = {},
 note = {https://doi.org/10.20965/jaciii.2021.p0356},
 number = {3},
 pages = {356-364},
 title = {Fault-Diagnosing Deep-Visual-SLAM for 3D Change Object Detection},
 url = {https://app.dimensions.ai/details/publication/pub.1138191980},
 volume = {25},
 year = {2021}
}

@article{pub.1138352862,
 abstract = {<div>Underground water pipes are important to any countryâ€™s infrastructure. Overtime, the metallic pipes are prone to corrosion, which can lead to water leakage and pipe bursts. In order to prolong the service life of those assets, water utilities in Australia apply protective pipe linings. Long-term monitoring and timely intervention are crucial for maintaining those lining assets. However, the water utilities do not possess the comprehensive technology to achieve it. The main reasons for lacking such technology are the unavailability of sensors and accurate robot localization technologies. Feature based localization methods such as SLAM has limited use as the application of liners alters the features and the environment. Encoder based localization is not accurate enough to observe the evolution of defects over a long period of time requiring unique defect correspondence. This motivates us to explore accurate contact-less and wireless based localization methods. We propose a cost-effective localization method using UHFRFID signals for robot localization inside pipelines based on Gaussian process combined particle filter. Experiments carried out in field extracted pipe samples from the Sydney water pipe network show that using the RSSI and Phase data together in the measurement model with particle filter algorithm improves the localization accuracy up to 15 centimeters precision.</div>},
 author = {Gunatilake, Amal and Thiyagarajan, Karthick and kodagoda, sarath and Piyathilaka, Lasitha and Darji, Poojaben},
 doi = {10.36227/techrxiv.14658795},
 journal = {TechRxiv},
 keywords = {},
 note = {https://www.techrxiv.org/articles/preprint/Using_UHF-RFID_Signals_for_Robot_Localization_Inside_Pipelines/14658795/files/28140882.pdf},
 number = {},
 pages = {},
 title = {Using UHF-RFID Signals for Robot Localization Inside Pipelines},
 url = {https://app.dimensions.ai/details/publication/pub.1138352862},
 volume = {},
 year = {2021}
}

@article{pub.1138352863,
 abstract = {<div>Underground water pipes are important to any countryâ€™s infrastructure. Overtime, the metallic pipes are prone to corrosion, which can lead to water leakage and pipe bursts. In order to prolong the service life of those assets, water utilities in Australia apply protective pipe linings. Long-term monitoring and timely intervention are crucial for maintaining those lining assets. However, the water utilities do not possess the comprehensive technology to achieve it. The main reasons for lacking such technology are the unavailability of sensors and accurate robot localization technologies. Feature based localization methods such as SLAM has limited use as the application of liners alters the features and the environment. Encoder based localization is not accurate enough to observe the evolution of defects over a long period of time requiring unique defect correspondence. This motivates us to explore accurate contact-less and wireless based localization methods. We propose a cost-effective localization method using UHFRFID signals for robot localization inside pipelines based on Gaussian process combined particle filter. Experiments carried out in field extracted pipe samples from the Sydney water pipe network show that using the RSSI and Phase data together in the measurement model with particle filter algorithm improves the localization accuracy up to 15 centimeters precision.</div>},
 author = {Gunatilake, Amal and Thiyagarajan, Karthick and kodagoda, sarath and Piyathilaka, Lasitha and Darji, Poojaben},
 doi = {10.36227/techrxiv.14658795.v1},
 journal = {TechRxiv},
 keywords = {},
 note = {https://www.techrxiv.org/articles/preprint/Using_UHF-RFID_Signals_for_Robot_Localization_Inside_Pipelines/14658795/files/28140882.pdf},
 number = {},
 pages = {},
 title = {Using UHF-RFID Signals for Robot Localization Inside Pipelines},
 url = {https://app.dimensions.ai/details/publication/pub.1138352863},
 volume = {},
 year = {2021}
}

@article{pub.1138662391,
 abstract = {Large-scale, high-accuracy, and adaptive three-dimensional (3D) perception are the basic technical requirements for constructing a practical and stable fruit-picking robot. The latest vision-based fruit-picking robots have been able to adapt to the complex background, uneven lighting and low color contrast of the orchard environment. However, most of them have, until now, been limited to a small field of view or rigid sampling manners. Although the simultaneous localization and mapping (SLAM) methods have the potential to realize large scale sensing, it was critically revealed in this study that the classic SLAM pipeline is not completely adapted to orchard picking tasks. In this study, the eye-in-hand stereo vision and SLAM system were integrated to provide detailed global map supporting long-term, flexible and large-scale orchard picking. To be specific, a mobile robot based on eye-in-hand vision was built and an effective hand-eye calibration method was proposed; a state-of-the-art object detection network was trained and used to establish a dynamic stereo matching method adapted well to complex orchard environments; a SLAM system was deployed and combined with the above eye-in-hand stereo vision system to obtain a detailed, wide 3D orchard map. The main contribution of this work is to build a new global mapping framework compatible to the nature of orchard picking tasks. Compared with the existing studies, this work pays more attention to the structural details of the orchard. Experimental results indicated that the constructed global map achieved both large-scale and high-resolution. This is an exploratory work providing theoretical and technical references for the future research on more stable, accurate and practical mobile fruit picking robots.},
 author = {Chen, Mingyou and Tang, Yunchao and Zou, Xiangjun and Huang, Zhaofeng and Zhou, Hao and Chen, Siyu},
 doi = {10.1016/j.compag.2021.106237},
 journal = {Computers and Electronics in Agriculture},
 keywords = {},
 number = {},
 pages = {106237},
 title = {3D global mapping of large-scale unstructured orchard integrating eye-in-hand stereo vision and SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1138662391},
 volume = {187},
 year = {2021}
}

@inbook{pub.1138777764,
 abstract = {In this paper, we propose a real-time and low-drift localization method for lidar-equipped robot in indoor environments. State-of-the-art lidar localization research mostly uses a scan-to-scan method, which produces high drifts during the localization of the robot. It is not suitable for robots to operate indoors (such as factory environment) for a long term. Besides, the mapping and localization of this method are susceptible to the dynamic objects (such as pedestrians). To solve above problems, we propose the scan-to-submap matching method for real-time localization. Currently, this method has been used for building maps, and there are few studies to use it for localization, especially for real-time localization. In our research, we build the hardware and software platform for the scan-to-submap matching method. We extensively evaluate our approach with simulations and real-world tests. Compared with the scan-to-scan method, the results demonstrate that our approach can cope with the mapping and localization problem with high localization accuracy and low drift.},
 author = {Li, Qipeng and Huai, Jianzhu and Chen, Dong and Zhuang, Yuan},
 booktitle = {China Satellite Navigation Conference (CSNC 2021) Proceedings},
 doi = {10.1007/978-981-16-3142-9_39},
 keywords = {},
 pages = {414-423},
 publisher = {},
 title = {Real-Time Robot Localization Based on 2D Lidar Scan-to-Submap Matching},
 url = {https://app.dimensions.ai/details/publication/pub.1138777764},
 year = {2021}
}

@inproceedings{pub.1138871406,
 abstract = {Visual camera localization using offline maps is widespread in robotics and mobile applications. Most state-of-the-art localization approaches assume static scenes, so maps are often reconstructed once and then kept constant. However, many scenes are dynamic and as changes in the scene happen, future localization attempts may struggle or fail entirely. Therefore, it is important for successful long-term localization to update and maintain maps as new observations of the scene, and changes in it, arrive. We propose a novel method for automatically discovering which points in a map remain stable over time, and which are due to transient changes. To this end, we calculate a stability store for each point based on its visibility over time, weighted by an exponential decay over time. This allows us to consider the impact of time when scoring points, and to distinguish which points are useful for long-term localization. We evaluate our method on the CMU Extended Seasons dataset (outdoors) and a new indoor dataset of a retail shop, and show the benefit of maintaining a â€˜live mapâ€™ that integrates updates over time using our exponential decay based method over a static â€˜base mapâ€™.},
 author = {Rotsidis, Alexandros and Lutteroth, Christof and Hall, Peter and Richardt, Christian},
 booktitle = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
 doi = {10.1109/wacv48630.2021.00291},
 keywords = {},
 pages = {2866-2875},
 title = {ExMaps: Long-Term Localization in Dynamic Scenes using Exponential Decay},
 url = {https://app.dimensions.ai/details/publication/pub.1138871406},
 year = {2021}
}

@article{pub.1138906754,
 abstract = {Localization is one of the essential process in robotics, as it plays an important role in autonomous navigation, simultaneous localization, and mapping for mobile robots. As robots perform large-scale and long-term operations, identifying the same locations in a changing environment has become an important problem. In this paper, we describe a robust visual localization system under severe appearance changes. First, a robust feature extraction method based on a deep variational autoencoder is described to calculate the similarity between images. Then, a global sequence alignment is proposed to find the actual trajectory of the robot. To align sequences, local fragments are detected from the similarity matrix and connected using a rectangle chaining algorithm considering the robot's motion constraint. Since the chained fragments provide reliable clues to find the global path, false matches on featureless structures or partial failures during the alignment could be recovered and perform accurate robot localization in changing environments. The presented experimental results demonstrated the benefits of the proposed method, which outperformed existing algorithms in long-term conditions.},
 author = {Oh, Junghyun and Han, Changwan and Lee, Seunghwan},
 doi = {10.3390/s21124103},
 journal = {Sensors},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/21/12/4103/pdf},
 number = {12},
 pages = {4103},
 title = {Condition-Invariant Robot Localization Using Global Sequence Alignment of Deep Features},
 url = {https://app.dimensions.ai/details/publication/pub.1138906754},
 volume = {21},
 year = {2021}
}

@article{pub.1138922376,
 abstract = {Loop closure detection (LCD) is of significant importance in simultaneous localization and mapping. It represents the robotâ€™s ability to recognize whether the current surrounding corresponds to a previously observed one. In this paper, we conduct this task in a two-step strategy: candidate frame selection and loop closure verification. The first step aims to search semantically similar images for the query one using features obtained by Key.Net with HardNet. Instead of adopting the traditional Bag-of-Words strategy, we utilize the aggregated selective match kernel to calculate the similarity between images. Subsequently, based on the potential property of motion field in the LCD scene, we propose a novel feature matching method, i.e., exploiting the smoothness prior and learning the motion field for an image pair in a reproducing kernel Hilbert space (RKHS), to implement loop closure verification. Concretely, we formulate the learning problem into a Bayesian framework with latent variables indicating the true/false correspondences and a mixture model accounting for the distribution of data. Furthermore, we propose a locality-driven mechanism to enhance the local relevance of motion vectors and term the algorithm as locality-driven accurate motion field learning (LAL). To satisfy the requirement of efficiency in the LCD task, we use a sparse approximation and search a suboptimal solution for the motion field in the RKHS, termed as LAL*. Extensive experiments are conducted on public datasets for feature matching and LCD tasks. The quantitative results demonstrate the effectiveness of our method over the current state-of-the-art, meanwhile showing its potential for long-term visual localization. The codes of LAL and LAL* are publicly available at https://github.com/KN-Zhang/LAL.},
 author = {Zhang, Kaining and Jiang, Xingyu and Ma, Jiayi},
 doi = {10.1109/tits.2021.3086822},
 journal = {IEEE Transactions on Intelligent Transportation Systems},
 keywords = {},
 number = {3},
 pages = {2350-2365},
 title = {Appearance-Based Loop Closure Detection via Locality-Driven Accurate Motion Field Learning},
 url = {https://app.dimensions.ai/details/publication/pub.1138922376},
 volume = {23},
 year = {2022}
}

@article{pub.1139317792,
 abstract = {This paper proposes and implements a lightweight, "real-time" localization system (SORLA) with artificial landmarks (reflectors), which only uses LiDAR data for the laser odometer compensation in the case of high-speed or sharp-turning. Theoretically, due to the feature-matching mechanism of the LiDAR, locations of multiple reflectors and the reflector layout are not limited by geometrical relation. A series of algorithms is implemented to find and track the features of the environment, such as the reflector localization method, the motion compensation technique, and the reflector matching optimization algorithm. The reflector extraction algorithm is used to identify the reflector candidates and estimates the precise center locations of the reflectors from 2D LiDAR data. The motion compensation algorithm predicts the potential velocity, location, and angle of the robot without odometer errors. Finally, the matching optimization algorithm searches the reflector combinations for the best matching score, which ensures that the correct reflector combination could be found during the high-speed movement and fast turning. All those mechanisms guarantee the algorithm's precision and robustness in the high speed and noisy background. Our experimental results show that the SORLA algorithm has an average localization error of 6.45 mm at a speed of 0.4 m/s, and 9.87 mm at 4.2 m/s, and still works well with the angular velocity of 1.4 rad/s at a sharp turn. The recovery mechanism in the algorithm could handle the failure cases of reflector occlusion, and the long-term stability test of 72 h firmly proves the algorithm's robustness. This work shows that the strategy used in the SORLA algorithm is feasible for industry-level navigation with high precision and a promising alternative solution for SLAM.},
 author = {Wang, Sen and Chen, Xiaohe and Ding, Guanyu and Li, Yongyao and Xu, Wenchang and Zhao, Qinglei and Gong, Yan and Song, Qi},
 doi = {10.3390/s21134479},
 journal = {Sensors},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/21/13/4479/pdf},
 number = {13},
 pages = {4479},
 title = {A Lightweight Localization Strategy for LiDAR-Guided Autonomous Robots with Artificial Landmarks},
 url = {https://app.dimensions.ai/details/publication/pub.1139317792},
 volume = {21},
 year = {2021}
}

@inproceedings{pub.1139477425,
 abstract = {In many applications of mobile robot, the environment is constantly changing. How to use historical information to analysis environmental changes and generate a map corresponding with current environment is important to achieve high-precision localization. Inspired by predictive mechanism of brain, this paper presents a long-term localization approach named ArmMPU (ARMA-based Map Prediction and Update) based on time series modeling and prediction. Autoregressive moving average model (ARMA), a kind of time series modeling method, is employed for environmental map modeling and prediction, then predicted map and filtered observation are fused to fix the prediction error. The simulation and experiment results show that the proposed method improves long-term localization performance in dynamic environments.},
 author = {Wang, Lisai and Chen, Weidong and Wang, Jingchuan},
 booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros45743.2020.9468884},
 keywords = {},
 pages = {1-7},
 title = {Long-Term Localization With Time Series Map Prediction for Mobile Robots in Dynamic Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1139477425},
 year = {2020}
}

@article{pub.1139698323,
 abstract = {For autonomous vehicles to operate persistently in a typical urban environment, it is essential to have high accuracy position information. This requires a mapping and localisation system that can adapt to changes over time. A localisation approach based on a single-survey map will not be suitable for long-term operation as it does not incorporate variations in the environment. In this paper, we present new algorithms to maintain a featured-based map. A map maintenance pipeline is proposed that can continuously update a map with the most relevant features taking advantage of the changes in the surroundings. Our pipeline detects and removes transient features based on their geometrical relationships with the vehicle's pose. Newly identified features became part of a new feature map and are assessed by the pipeline as candidates for the localisation map. By purging out-of-date features and adding newly detected features, we continually update the prior map to more accurately represent the most recent environment. We have validated our approach using the USyd Campus Dataset, which includes more than 18 months of data. The results presented demonstrate that our maintenance pipeline produces a resilient map which can provide sustained localisation performance over time.},
 author = {Berrio, Julie Stephany and Worrall, Stewart and Shan, Mao and Nebot, Eduardo},
 doi = {10.1109/tits.2021.3094485},
 journal = {IEEE Transactions on Intelligent Transportation Systems},
 keywords = {},
 note = {http://arxiv.org/pdf/2008.12449},
 number = {99},
 pages = {1-14},
 title = {Long-Term Map Maintenance Pipeline for Autonomous Vehicles},
 url = {https://app.dimensions.ai/details/publication/pub.1139698323},
 volume = {PP},
 year = {2021}
}

@article{pub.1139748384,
 abstract = {Most real-time autonomous robot applications require a robot to traverse
through a dynamic space for a long time. In some cases, a robot needs to work
in the same environment. Such applications give rise to the problem of a
life-long SLAM system. Life-long SLAM presents two main challenges i.e. the
tracking should not fail in a dynamic environment and the need for a robust and
efficient mapping strategy. The system should update maps with new information;
while also keeping track of older observations. But, mapping for a long time
can require higher computational requirements. In this paper, we propose a
solution to the problem of life-long SLAM. We represent the global map as a set
of rasterized images of local maps along with a map management system
responsible for updating local maps and keeping track of older values. We also
present an efficient approach of using the bag of visual words method for loop
closure detection and relocalization. We evaluate the performance of our system
on the KITTI dataset and an indoor dataset. Our loop closure system reported
recall and precision of above 90 percent. The computational cost of our system
is much lower as compared to state-of-the-art methods. Our method reports lower
computational requirements even for long-term operation.},
 author = {Ali, Waqas and Liu, Peilin and Ying, Rendong and Gong, Zheng},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {A life-long SLAM approach using adaptable local maps based on rasterized
  LIDAR images},
 url = {https://app.dimensions.ai/details/publication/pub.1139748384},
 volume = {},
 year = {2021}
}

@article{pub.1139777894,
 abstract = {Simultaneous Localization and Mapping is a key technique for mobile robot applications and has received much research effort over the last three decades. A precondition for a robust and life-long landmark-based SLAM algorithm is the stable and reliable landmark detector. However, traditional methods are based on laserbased data which are believed very unstable, especially in dynamic-changing environments. In this work, we introduce a new landmark detection approach using vision-based data. Based on this approach, we exploit a deep neural network for processing images from a stereo camera system installed on mobile robots. Two deep neural network models named YOLOv3 and PSMNet were re-trained and used to perform the landmark detection and landmark localization, respectively. The landmarkâ€™s information is associated with the landmark data through tracking and filtering algorithm. The obtained results show that our method can detect and localize landmarks with high stability and accuracy, which are validated by laser-based measurement data. This approach has opened a new research direction toward a robust and life-long SLAM algorithm.},
 author = {Nguyen, Xuan-Ha and Nguyen, Van-Huy and Ngo, Thanh-Tung},
 doi = {10.51316/30.7.6},
 journal = {Journal of Science and Technology - Technical Universities},
 keywords = {},
 note = {https://storage.googleapis.com/jst-journals/articles/146/06-20125.pdf},
 number = {146},
 pages = {31-36},
 title = {A New Landmark Detection Approach for Slam Algorithm Applied in Mobile Robot},
 url = {https://app.dimensions.ai/details/publication/pub.1139777894},
 volume = {30.7},
 year = {2020}
}

@article{pub.1139787641,
 abstract = {There has been exciting recent progress in using radar as a sensor for robot
navigation due to its increased robustness to varying environmental conditions.
However, within these different radar perception systems, ground penetrating
radar (GPR) remains under-explored. By measuring structures beneath the ground,
GPR can provide stable features that are less variant to ambient weather,
scene, and lighting changes, making it a compelling choice for long-term
spatio-temporal mapping. In this work, we present the CMU-GPR dataset--an
open-source ground penetrating radar dataset for research in subsurface-aided
perception for robot navigation. In total, the dataset contains 15 distinct
trajectory sequences in 3 GPS-denied, indoor environments. Measurements from a
GPR, wheel encoder, RGB camera, and inertial measurement unit were collected
with ground truth positions from a robotic total station. In addition to the
dataset, we also provide utility code to convert raw GPR data into processed
images. This paper describes our recording platform, the data format, utility
scripts, and proposed methods for using this data.},
 author = {Baikovitz, Alexander and Sodhi, Paloma and Dille, Michael and Kaess, Michael},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {CMU-GPR Dataset: Ground Penetrating Radar Dataset for Robot Localization
  and Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1139787641},
 volume = {},
 year = {2021}
}

@article{pub.1139787747,
 abstract = {Long-term 3D map management is a fundamental capability required by a robot
to reliably navigate in the non-stationary real-world. This paper develops
open-source, modular, and readily available LiDAR-based lifelong mapping for
urban sites. This is achieved by dividing the problem into successive
subproblems: multi-session SLAM (MSS), high/low dynamic change detection, and
positive/negative change management. The proposed method leverages MSS and
handles potential trajectory error; thus, good initial alignment is not
required for change detection. Our change management scheme preserves efficacy
in both memory and computation costs, providing automatic object segregation
from a large-scale point cloud map. We verify the framework's reliability and
applicability even under permanent year-level variation, through extensive
real-world experiments with multiple temporal gaps (from day to year).},
 author = {Kim, Giseop and Kim, Ayoung},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {LT-mapper: A Modular Framework for LiDAR-based Lifelong Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1139787747},
 volume = {},
 year = {2021}
}

@article{pub.1139991231,
 abstract = {Most real-time autonomous robot applications require a robot to traverse through a dynamic space for a long time. In some cases, a robot needs to work in the same environment. Such applications give rise to the problem of a life-long SLAM system. Life-long SLAM presents two main challenges i.e. the tracking should not fail in a dynamic environment and the need for a robust and efficient mapping strategy. The system should update maps with new information; while also keeping track of older observations. But, mapping for a long time can require higher computational requirements. In this paper, we propose a solution to the problem of life-long SLAM. We represent the global map as a set of rasterized images of local maps along with a map management system responsible for updating local maps and keeping track of older values. We also present an efficient approach of using the bag of visual words method for loop closure detection and relocalization. We evaluate the performance of our system on the KITTI dataset and an indoor dataset. Our loop closure system reported recall and precision of above 90 percent. The computational cost of our system is much lower as compared to state-of-the-art methods. Our method reports lower computational requirements even for long-term operation.},
 author = {Ali, Waqas and Liu, Peilin and Ying, Rendong and Gong, Zheng},
 doi = {10.1109/jsen.2021.3100882},
 journal = {IEEE Sensors Journal},
 keywords = {},
 note = {http://arxiv.org/pdf/2107.07133},
 number = {19},
 pages = {21740-21749},
 title = {A Life-Long SLAM Approach Using Adaptable Local Maps Based on Rasterized LIDAR Images},
 url = {https://app.dimensions.ai/details/publication/pub.1139991231},
 volume = {21},
 year = {2021}
}

@article{pub.1140154296,
 abstract = {Tracking the 6D pose of objects in video sequences is important for robot
manipulation. Most prior efforts, however, often assume that the target
object's CAD model, at least at a category-level, is available for offline
training or during online template matching. This work proposes BundleTrack, a
general framework for 6D pose tracking of novel objects, which does not depend
upon 3D models, either at the instance or category-level. It leverages the
complementary attributes of recent advances in deep learning for segmentation
and robust feature extraction, as well as memory-augmented pose graph
optimization for spatiotemporal consistency. This enables long-term, low-drift
tracking under various challenging scenarios, including significant occlusions
and object motions. Comprehensive experiments given two public benchmarks
demonstrate that the proposed approach significantly outperforms state-of-art,
category-level 6D tracking or dynamic SLAM methods. When compared against
state-of-art methods that rely on an object instance CAD model, comparable
performance is achieved, despite the proposed method's reduced information
requirements. An efficient implementation in CUDA provides a real-time
performance of 10Hz for the entire framework. Code is available at:
https://github.com/wenbowen123/BundleTrack},
 author = {Wen, Bowen and Bekris, Kostas},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {BundleTrack: 6D Pose Tracking for Novel Objects without Instance or
  Category-Level 3D Models},
 url = {https://app.dimensions.ai/details/publication/pub.1140154296},
 volume = {},
 year = {2021}
}

@article{pub.1140647278,
 abstract = {<p>Dynamic locomotion is realized through a simultaneous integration of adaptability and optimality. This article proposes a neuro-cognitive model for multi-legged locomotion robot that can seamlessly integrate multi-modal sensing, ecological perception, and cognition through the coordination of interoceptive and exteroceptive sensory information. Importantly, cognitive models can be discussed as micro-, meso-, and macro-scopic; these concepts correspond to sensing, perception, and cognition; and short-, medium-, and long-term adaptation (in terms of ecological psychology). The proposed neuro-cognitive model integrates these intelligent functions from a multi-scopic point of view. Macroscopic-level presents an attention mechanism with short-term adaptive locomotion control conducted by lower-level sensorimotor coordination-based model. Macrosopic-level serves environmental cognitive map featuring higher-level behavior planning. Mesoscopic level shows integration between the microscopic and macroscopic approaches, enabling the model to reconstruct a map and conduct localization using bottom-up facial environmental information and top-down map information, generating intention towards the ultimate goal at the macroscopic level. The experiments demonstrated that adaptability and optimality of multi-legged locomotion could be achieved using the proposed multi-scale neuro-cognitive model, from short to long-term adaptation, with efficient computational usage. Future research directions can be implemented not only in robotics contexts but also in the context of interdisciplinary studies incorporating cognitive science and ecological psychology.</p>},
 author = {Saputra, Azhar Aulia and Wada, Kazuyoshi and Masuda, Shiro and Kubota, Naoyuki},
 doi = {10.21203/rs.3.rs-798472/v1},
 journal = {Research Square},
 keywords = {},
 note = {https://www.researchsquare.com/article/rs-798472/v1.pdf?c=1631891341000},
 number = {},
 pages = {},
 title = {Multi-Scopic Neuro-Cognitive Adaptation for Legged Locomotion Robots},
 url = {https://app.dimensions.ai/details/publication/pub.1140647278},
 volume = {},
 year = {2021}
}

@inproceedings{pub.1140748757,
 abstract = {Underground water pipes are important to any country's infrastructure. Overtime, the metallic pipes are prone to corrosion, which can lead to water leakage and pipe bursts. In order to prolong the service life of those assets, water utilities in Australia apply protective pipe linings. Long-term monitoring and timely intervention are crucial for maintaining those lining assets. However, the water utilities do not possess the comprehensive technology to achieve it. The main reasons for lacking such technology are the unavailability of sensors and accurate robot localization technologies. Feature based localization methods such as SLAM has limited use as the application of liners alters the features and the environment. Encoder based localization is not accurate enough to observe the evolution of defects over a long period of time requiring unique defect correspondence. This motivates us to explore accurate contact-less and wireless based localization methods. We propose a cost-effective localization method using UHF-RFID signals for robot localization inside pipelines based on Gaussian process combined particle filter. Experiments carried out in field extracted pipe samples from the Sydney water pipe network show that using the RSSI and Phase data together in the measurement model with particle filter algorithm improves the localization accuracy up to 15 centimeters precision.},
 author = {Gunatilake, Amal and Galea, Mitchell and Thiyagarajan, Karthick and Kodagoda, Sarath and Piyathilaka, Lasitha and Darji, Poojaben},
 booktitle = {2021 IEEE 16th Conference on Industrial Electronics and Applications (ICIEA)},
 doi = {10.1109/iciea51954.2021.9516284},
 keywords = {},
 pages = {1109-1114},
 title = {Using UHF-RFID Signals for Robot Localization Inside Pipelines},
 url = {https://app.dimensions.ai/details/publication/pub.1140748757},
 year = {2021}
}

@article{pub.1141028162,
 abstract = {The Government â€œDigital Economy of the Russian Federationâ€ Program describes future economic development of Russia, its regions and industries. However, this Program has not been scrutinized for the modern economic theories and inner conceptual inconsistencies. Therefore, the purpose of the study is to identify the Programâ€™s conceptual uncertainties which could give rise to the alternative scenarios of the economic development of Russia and to inconsistent managerial decisions resulted in the dramatic differences in unfolding the digital economy in different regions and industries. It has been found that the Program contains three definitions for the digital economy â€“ declared (with the focus on the digital data), latent (with the focus on the digital platforms), and promising (with the focus on the artificial intelligence) ones. The Programâ€™s content was compared with the modern economic theories, which revealed five levels of strategic uncertainty: 1)Â cluster or platform economy; 2)Â dispersed or agglomeration economy; 3)Â linear or circular economy; 4)Â homogeneous or heterogeneous economic landscape; 5)Â smart cities, smart agglomerations or smart regions. The reviewed Program lacks a clearly defined priority in the development of the cluster or post-cluster (platform) economy in Russia, which creates the theoretical (conceptual) and practical (connected with the development of the social and economic strategy) uncertainties for the industries and regions. With a stronger focus on the latent definition of the Program, there is a risk of extreme monopolization of the digital markets in Russia by the platform leading companies which are mainly located in Moscow and established with the public support under this Program implementation. The article offers to expand the concept of platform economy with a new type of digital ecosystem â€“ a territorial digital platform to construct regional digital platforms and to develop business-ecosystems around them. It has been found that the digital economy in Russia is developed together with the implementation of the previously adopted spatial development strategy aimed to diminish the inter-regional differences, therefore, the dispersed digital economy should be seen as a promising approach. At the same time, there is a risk to activate the process of extreme territorial concentration of digital economic activities as the reviewed Program contains no measures aimed to disperse the agents of digital economy with any financial and other public mechanisms. One more strategic uncertainty induced by the fact that the Program lacks any priorities in preserving or expanding value chain is connected with linear or circular economy dichotomy. This uncertainty misinforms the domestic economic agents and could lead to the technological retardation of Russia from the leading countries in the circular digital economy. The advent of the artificial intelligence gives rise to the debate about the alternative routes of development with the homogeneous (human only) types of agents or heterogeneous (humans, intelligent machines and human-machine systems) economic landscape. It has been shown that the Program does not regulate the interaction of the different economic agents and the development of the economic landscapes, which could result in the unwanted transformation of the Russian economic space. In connection with the smart territorial units, the conclusion is that the Program does not contain any instructions concerning a preferable theory for the territorial organization of digital economy (smart city, smart region, smart agglomeration). This could lead to the situation when different regions observe different theories, with their implementation violating the equilibrium in the digital economic space of Russia. Interception of all alternative theories gives 48 possible development scenarios for digital economy in Russia. It has been established that Russia is likely to follow the platform agglomeration linear digital economy in homogeneous landscapes and smart cities. It is noted that the development of platform dispersed circular digital economy in heterogeneous landscapes and smart regions is seen to be the most favorable scenario in terms of modern studies and digital economy practices in other countries. The practical value of the study is determined by the adjustment of the existing Program or a development of a new document. Further studies are seen to be performed in the field of finding new uncertainty levels, one of them being â€œ4G-, 5G- or 6G-determined digital economyâ€. Keywords national program, digital economy, strategic uncertainty, artificial intelligence, platform economy, agglomeration economy, circular economy, economic landscape, smart city, smart region, digital agglomeration. Acknowledgements The research was funded under the government assignment (theme registration number ÐÐÐÐ-Ð17-117041910166-3). References 1. Polozhikhina M.A. Natsional'nye modeli tsifrovoi ekonomiki [The national models of the digital economy]. Ekonomicheskie i sotsial'nye problemy Rossii [Economic and Social Problems of Russia], 2018, no. 1, pp. 111â€“154. (In Russian).2. Yakutin Yu.V. Rossiiskaya ekonomika: strategiya tsifrovoi transfor-matsii (k konstruktivnoi kritike pravitel'stvennoi programmy Â«Tsifrovaya ekonomika Rossiiskoi FederatsiiÂ») [The Russian economy: A strategy for digital transformation (constructive criticism of the government programme â€œDigital economy of the Russian Federationâ€). Menedzhment i biznes-administrirovanie [Management and Business Administration], 2017, no. 4, pp. 27â€“52. (In Russian).3. Makogonova N.V. Riski realizatsii gosudarstvennoi programmy Â«Tsifrovaya ekonomika Rossiiskoi FederatsiiÂ» [Risk of implementation of the government program â€œDigital economy of the Russian Federation]. Upravlencheskie nauki v sovremennom mire [Managerial Sciences in the Modern World], 2018, vol. 1, no. 1, pp. 569â€“576. (In Russian).4. Lenchuk E.B., Vlaskin G.A. Formirovanie tsifrovoi ekonomiki v Rossii: Problemy, riski, perspektivy [Formation of the digital economy in Russia: Problems, risks, prospects]. Vestnik Instituta ekonomiki RAN [Bulletin of the IE RAS], 2018, no. 5, pp. 9â€“21. (In Russian).5. Dukhovnykh D.A., Agafonova M.S. Problemy i riski formirovaniya i razvitiya tsifrovoi ekonomiki v Rossii [Problems and risks of the formation and development of the digital economy in Russia]. European Journal of Natural History, 2020, no. 1, pp. 110â€“114. (In Russian).6. Dean T.J., Meyer G.D. Industry environments and new venture formations in US manufacturing: A conceptual and empirical analysis of demand determinants. Journal of Business Venturing, 1996, vol. 11 (2), pp. 107â€“132. doi: 10.1016/0883-9026(95)00109-3.7. Musole M. Property rights, transaction costs and institutional change: Conceptual framework and literature review. Progress in Planning, 2009, vol. 71 (2), pp. 43â€“85. doi: 10.1016/j.progress.2008.09.002.8. Barra G.M.J., Ladeira M.B. Theories institutional applied to agro industrial systems studies in the context of coffee agribusiness: A conceptual analysis. REGE â€“ Revista de Gestao, 2016, vol. 23 (2), pp. 159â€“171. doi: 10.1016/j.rege.2015.12.005. 9. Yeboah-Assiamah E., Muller K., Domfeh K.A. Institutional assessment in natural resource governance: A conceptual overview. Forest Policy and Economic, 2017, vol. 74, pp. 1â€“12. doi: 10.1016/j.forpol.2016.10.006.10. Adanu K. Institutional change and economic development: A conceptual analysis of the African case. International Journal of Social Economics, 2017, vol. 44 (4), pp. 547â€“559. doi: 10.1108/IJSE-02-2014-0022.11. Granstrand O., Holgersson M. Innovation ecosystems: A conceptual review and a new definition. Technovation, 2020, vol. 90-91, pp. 1â€“12. doi: 10.1016/j.technovation.2019.102098. 12. Di Tommaso M.R., Tassinari M., Bonnini S., Marozzi M. Industrial policy and manufacturing targeting in the US: New methodological tools for strategic policy-making. International Review of Applied Economics, 2017, vol. 31 (5), pp. 681â€“703. doi: 10.1080/02692171.2017.1303036.13. Mirza S.S., Ahsan T. Corporatesâ€™ strategic responses to economic policy uncertainty in China. Business Strategy and the Environment, 2019, vol. 29 (2), pp. 375â€“389. doi: 10.1002/bse.2370.14. Tapscott D. The Digital economy: Promise and peril in the age of networked intelligence. New York, McGraw-Hill, 1994. 368 p.15. D'yachenko O.V. Definitsiya kategorii Â«tsifrovaya ekonomikaÂ» v zaru-bezhnoi i otechestvennoi ekonomicheskoi nauke [Categorical definition of digital economy in foreign and Russian economic theory]. Ekonomicheskoe vozrozhdenie Rossii [Economic Revival of Russia], 2019, no. 1, pp. 86â€“98. (In Russian).16. Brynjolfsson E. The productivity paradox of information technology. Communication of the ACM, 1993, vol. 36 (12), pp. 66â€“77. doi: 10.1145/163298.163309. 17. Chen S., Xie Z. Is Chinaâ€™s e-governance sustainable? Testing Solow IT productivity paradox in Chinaâ€™s context. Technological Forecasting and Social Change, 2015, vol. 96, pp. 51â€“61. doi: 10.1016/j.techfore.2014.10.014.18. Polak P. The productivity paradox: A meta-analysis. Information Economics and Policy, 2017, vol. 38, pp. 38â€“54. doi: 10.1016/j.infoecopol.2016.11.003.19. Kaurova O.V., Maloletko A.N., Matraeva L.V., Korol'kova N.A. Opredelenie sostava pokazatelei otsenki urovnya razvitiya tsifrovoi ekonomiki v regione (regional'noi tsifrovoi sredy) [Identifying the indicators of digital economy development for a region (regional digital environment). Fundamental'nye i prikladnye issledovaniya kooperativnogo sek-tora ekonomiki [Fundamental and Applied Research Studies of the Economic Cooperative Sector], 2020, no. 1, pp. 138â€“149. (In Russian).20. Stepanova V.V., Ukhanova A.V., Grigorishchin A.V., Yakhyaev D.B. Otsenka tsifrovykh ekosistem regionov Rossii [Evaluating digital ecosystems in Russiaâ€™s regions. Ekonomicheskie i sotsial'nye peremeny: fakty, tendentsii, prognoz [Economic and Social Changes: Facts, Trends, Forecasts], 2019, vol. 12, no. 2, pp. 73â€“90. doi: 10.15838/esc.2019.2.62.4.21. Schwab K. The fourth industrial revolution. New York, Crown Business, 2017. 192 p.22. Liao Y., Deschamps S., Loures E.F.R., Ramos L.F.R. Past, present and future of Industry 4.0 â€“ A systematic literature review and research agenda proposal. International Journal of Production Research, 2017, vol. 55 (1), pp. 3609â€“3629. doi: 10.1080/00207543.2017.1308576. 23. Dalenogare L.S., Benitez G.B., Ayala N.F., Frank A.G. The expected contribution of Industry 4.0 technologies for industrial performance. International Journal of Production Economics, 2018, vol. 204, pp. 383â€“394. doi: 10.1016/j.ijpe.2018.08.019.24. Muhuri P.K., Shukla A.K., Abraham A. Industry 4.0: A bibliometric analysis and detailed overview. Engineering Applications of Artificial Intelligence, 2019, vol. 78, pp. 218â€“235. doi: 10.1016/j.engappai.2018.11.007.25. Castelo-Branco I., Cruz-Jesus F., Oliveira T. Assessing Industry 4.0 readiness in manufacturing: Evidence for the European Union. Computers in Industry, 2019, vol. 107, pp. 22â€“32. doi: 10.1016/j.compind.2019.01.007. 26. Kuo C.-C., Shyu J.Z., Ding K. Industrial revitalization via industry 4.0 â€“ A comparative policy analysis among China, Germany and the USA. Global Transition, 2019, vol. 1, pp. 3â€“14. doi: 10.1016/j.glt.2018.12.001. 27. Tien J.M. The next industrial revolution: Integrated service and good. Journal of System Science and System Engineering, 2012, vol. 21, pp. 257â€“296. doi: 10.1007/s11518-012-5194-1. 28. Dirican C. The impacts of robotics, artificial intelligence on business and economics. Procedia â€“ Social and Behavioral Sciences, 2015, vol. 195, pp. 564â€“573. doi: 10.1016/j.sbspro.2015.06.134.29. Wagner D.N. Economics patterns in a world with artificial intelligence. Evolutionary and Institutional Economics Review, 2020, vol. 17 (1), pp. 111â€“131. doi: 10.1007/s4084-019-00157-x.30. Soni N., Sharma E.K., Singh N., Kapoor A. Artificial intelligence in business: From research and innovation to market deployment. Procedia Computer Science, 2020, vol. 167, pp. 2200â€“2210. doi: 10.1016/j.procs.2020.03.272.31. Johansson B., Karlsson C., Stough R. (Eds.). Emerging digital economy: Entrepreneurship, clusters, and policy. Berlin, Springer-Verlag, 2006. 352 p. doi: 10.1007/3-540-34488-8.32. Halbert L. Collaborative and collective: Reflexive co-ordination and the dynamics of open innovation in the digital industry clusters of the Paris Region. Urban Studies, 2012, vol. 49 (11), pp. 2357â€“2376. doi: 10.1177/0042098011427186.33. Gotz M., Jankowska B. Clusters and Industry 4.0 â€“ Do they fit together? European Planning Studies, 2017, vol. 25 (9), pp. 1633â€“1653. doi: 10.1080/09654313.2017.1327037.34. Nathan M., Vandore E., Voss G. Spatial imaginaries and tech cities: Place-branding East Londonâ€™s digital economy. Journal of Economic Geography, 2019, vol. 19 (2), pp. 409â€“432. doi: 10.1093/jeg/lby018.35. Parker G.G., van Alstyne M.W., Choudary S.P. Platform revolution: How networked markets are transforming the economy and how to make them work for you. New York, W.W. Norton & Company, 2016. 352 p.36. Spulber D.F. The economics of markets and platforms. Journal of Economics and Management Strategy, 2019, vol. 28 (1), pp. 159â€“172. doi: 10.1111/jems.12290.37. Cusumano M.A., Gawer A., Yoffie D.B. The Business of platforms: Strategy in the age of digital competition, innovation, and power. New York, HarperCollins, 2019. 304 p. 38. Baronian L. Digital platforms and the nature of the firm. Journal of Economic Issues, 2020, vol. 54 (1), pp. 214â€“232. doi: 10.1080/00213624.2020.1720588.39. Nuccio M., Guerzoni M. Big data: Hell or heaven? Digital platforms and market power in the data-driven economy. Competition and Change, 2019, vol. 23 (3), pp. 312â€“328. doi: 10.1177/1024529418816525.40. Condorelli D., Padilla J. Harnessing platform envelopment in the digital world. Journal of Competition Law and Economics, 2020, vol. 16(2), pp. 143â€“187. doi: 10.1093/joclec/nhaa006.41. Eferin Y., Hohlov Y., Rossotto C. Digital platforms in Russia: Competition between national and foreign multi-sided platforms stimulates growth and innovation. Digital Policy Regulation and Governance, 2019, vol. 21 (2), pp. 129â€“145. doi: 10.1108/DPRG-11-2018-0065.42. Lima V. Towards an understanding of the regional impact of Airbnb in Ireland. Regional Studies, Regional Science, 2019, vol. 6 (1), pp. 78â€“91. doi: 10.1080/21681376.2018.1562366.43. Boutsioukis G., Fasianos A., Petrohilos-Andrianos Y. The spatial distribution of short-term rental listings in Greece: A regional graphic. Regional Studies, Regional Science, 2019, vol. 6 (1), pp. 455â€“459. doi: 10.1080/21681376.2019.1660210.44. Porter M.E. Location, competition, and economic development: Local clusters in a global economy. Economic Development Quarterly, 2000, vol. 14 (1), pp. 15â€“34. doi: 10.1177/089124240001400105.45. Delgado M., Porter M.E., Stern S. Defining clusters of related industries. Journal of Economic Geography, 2016, vol. 16 (1), pp. 1â€“38. doi: 10.1093/jeg/lbv017.46. Slaper T.F., Harmon K.M., Rubin B.M. Industry clusters and regional economic performance: A study across US metropolitan statistical areas. Economic Development Quarterly, 2018, vol. 32 (1), pp. 44â€“59. doi: 10.1177/0891242417752248.47. Poell T., Nieborg D., Dijck J. van. Platformisatio. Internet Policy Review, 2019, vol. 8 (4), pp. 1â€“13. doi: 10.14763/2019.4.1425.48. Valdez-De-Leon O. How to develop a digital ecosystem: A practical framework. Technology Innovation Management Review, 2019, vol. 9 (8), pp. 43â€“54. doi: 10.22215/timreview/1260.49. Hein A., Schreieck M., Riasanow T., Setzke D.S., Wiesche M., Bohm M., Krcmar H. Digital platform ecosystems. Electronic Markets, 2020, vol. 30 (1), pp. 87â€“98. doi: 10.1007/s12525-019-00377-4.50. Alaimo C., Kallinikos J., Valderrama E. Platforms as service ecosystem: Lessons from social media. Journal of Information Technology, 2020, vol. 35 (1), pp. 25â€“48. doi: 10.1177/0268396219881462.51. Moore J.F. The Death of competition: Leadership and strategy in the age of business ecosystems. New York, HarperCollins, 1996. 288 p.52. Song A.K. The digital entrepreneurial ecosystem â€“ A critique and reconfiguration. Small Business Economics, 2019, vol. 53 (3), pp. 569â€“590. doi: 10.1007/s11187-019-00232-y. 53. Selander L., Henfridsson O., Svahn F. Capability search and redeem across digital ecosystems. Journal of Information Technology, 2013, vol. 28 (3), pp. 183â€“197. doi: 10.1057/jit.2013.14.54. Blanutsa V.I. Ekonomicheskaya svyaznost' rossiiskikh regionov v pro-stranstve Internet [Economic connectivity of Russian regions in the Internet space]. Kreativnaya ekonomika [Creative Economy], 2018, vol. 12, no. 5, pp. 701â€“716. (In Russian). doi: 10.18334/ce.12.5.39144.55. Blanutsa V.I. Tsifrovaya ekonomika Sibiri: territorial'nye platformy dlya klasterov [Digital economy of Siberia: Territorial platforms for clusters]. Aktual'nye problemy ekonomiki i prava [Actual problems of economics and law], 2019, vol. 13, no. 3, pp. 1343â€“1355. (In Russian). doi: 10.21202/1993-047X.13.2019.3.1343-1355.56. Perroux F. Economic space: Theory and application. Quarterly Journal of Economics, 1950, vol. 64 (1), pp. 89â€“104.57. Darwent D.F. Growth poles and growth centers in regional planning â€“ A review. Environment and Planning A: Economy and Space, 1969, vol. 1 (1), pp. 5â€“32. doi: 10.1068/a010005.58. Parr J.B. Growth-pole strategies in regional economic planning: A retrospective view. Part 1. Origins and advocacy. Urban Studies, 1999, vol. 36 (7), pp. 1195â€“1215. doi: 10.1080/0042098993187.59. Friedman J. Regional development policy: A Case study of Venezuela. Boston, MIT Press, 1966. 279 p.60. Krugman P. Increasing returns and economic geography. Journal of Political Economy, 1991, vol. 99 (3), pp. 483â€“499. doi: 10.1086/261763.61. Krugman P. Whatâ€™s new about the new economic geography? Oxford Review of Economic Policy, 1998, vol. 14 (2), pp. 7â€“17.62. Fujita M., Krugman P. The new economic geography: Past, present and the future. Papers in regional science, 2003, vol. 83 (1), pp. 139â€“164. doi: 10.1007/s10110-003-0180-0.63. Proost S., Thisse J.-F. What can be learned from spatial economics? Journal of Economic Literature, 2019, vol. 57 (3), pp. 575â€“643. doi: 10.1257/jel.20181414.64. Di Comite F., Kancs dâ€™A., Lecca P. Modeling agglomeration and dispersion in space: The role of labor migration, capital mobility and vertical linkages. Review of International Economics, 2017, vol. 26 (3), pp. 555â€“577. doi: 10.1111/roie.12313.65. Akamatsu T., Mori T., Osawa M., Takayama Y. Spatial scale of agglomeration and dispersion: Theoretical foundation and empirical implications. RIETI Discussion Paper Series 17-E-125. Tokyo, The Research Institute of Economy, Trade and Industry, 2017. 92 p.66. Fujita M., Thisse J.-F. Economics of agglomeration: Cities, industrial location, and regional growth. Cambridge, UK, Cambridge University Press, 2002. 466 p.67. Viladecans-Marsal E. Agglomeration economies and industrial location: City-level evidence. Journal of Economic Geography, 2004, vol. 4 (5), pp. 565â€“582. doi: 10.1093/jnlecg/lbh040.68. Brulhart M., Sbergami F. Agglomeration and growth: Cross-country evidence. Journal of Urban Economics, 2009, vol. 65 (1), pp. 48â€“63. doi: 10.1016/j.jue.2008.08.003.69. Puga D. The magnitude and causes of agglomeration economies. Journal of Regional Science, 2010, vol. 50 (1), pp. 203â€“219. doi: 10.1111/j.1467-9787.2009.00657.x.70. Combes P.-P., Duranton G., Gobillon L. The identification of agglomeration economies. Journal of Economic Geography, 2011, vol. 11 (2), pp. 253â€“266. doi: 10.1093/jeg/lbq038.71. Beandry C., Schiffauerova A. Whoâ€™s right, Marshall or Jacobs? The localization versus urbanization debate. Research Policy, 2009, vol. 38 (2), pp. 318â€“337. doi: 10.1016/j.respol.2008.11.010. 72. Picard P.M., Tabuchi T. Self-organized agglomerations and transport costs. Economic Theory, 2010, vol. 42 (3), pp. 565â€“589. doi: 10.1007/s00199-008-0410-4.73. Gaspar J.M., Castro S.B.S.D., Correia-da-Silva J. Agglomeration patterns in a multi-regional economy without income effects. Economic Theory, 2018, vol. 66 (4), pp. 863â€“899. doi: 10.1007/s00199-017-1065-9.74. Mel'nikova L.V. Teoreticheskie argumenty i empiricheskoe znanie v strategicheskom planirovanii [Theoretical arguments and empirical evidence in strategic planning]. Region: ekonomika i sotsiologiya [Region: Economics and Sociology], 2018, no. 2, pp. 52â€“80. (In Russian). doi: 10.15372/REG20180203.75. Barbero J., Zofio J.L. The multiregional core-periphery model: The role of the spatial topology. Networks and Spatial Economics, 2016, vol. 16 (2), pp. 469â€“496. doi: 10.1007/s11067-015-9285-7.76. Davelaar E.J., Nijkamp P. Spatial dispersion of technological innovation. A review. In: Innovation behaviour in space and time. Bertuglia C.S., Lombardo S., Nijkamp P. (Eds.). Berlin, Springer-Verlag, 1997, pp. 17â€“40.77. Myint S. An exploration of spatial dispersion, pattern, and association of socio-economic functional units in an urban system. Applied Geography, 2008, vol. 28 (3), pp. 168â€“188. doi: 10.1016/j.apgeog.2008.02.005.78. MacFeely S. Opportunism over strategy: A history of regional policy and spatial planning in Ireland. International Planning Studies, 2016, vol. 21 (4), pp. 377â€“402. doi: 10.1080/13563475.2016.1162403.79. Marot N., Golobic M. Delivering a national spatial development strategy: A success story? European Planning Studies, 2018, vol. 26 (6), pp. 1202â€“1221. doi: 10.1080/09654313.2018.1459502.80. Nosek S. Territorial cohesion storylines in 2014â€“2020 Cohesion Policy. European Planning Studies, 2017, vol. 25 (12), pp. 2157â€“2174. doi: 10.1080/09654313.2017.1349079.81. Rivera P.P., Vazquez F.J.C. Rethinking the territorial cohesion in the EU: Institutional and functional elements of the concept. Eastern Journal of European Studies, 2019, vol. 10 (2), pp. 41â€“62. 82. Barro R.J. Economic growth in a cross section of countries. The Quarterly Journal of Economics, 1991, vol. 106 (2), pp. 407â€“443. doi: 10.2307/2937943.83. Barro R.J., Sala-i-Martin X. Convergence. Journal of Political Economy, 1992, vol. 100 (2), pp. 223â€“251. doi: 10.1086/261816.84. Sala-i-Martin X. Regional cohesion: Evidence and theories of regional growth and convergence. European Economic Review, 1996, vol. 40 (6), pp. 1325â€“1352. doi: 10.1016/0014-2921(95)00029-1.85. Bartkowska M., Riedl A. Regional convergence clubs in Europe: Identification and conditioning factors. Economic Modelling, 2012, vol. 29 (1), pp. 22â€“31. doi: 10.1016/j.econmod.2011.01.013.86. Von Lyncker K., Thoennessen R. Regional club convergence in the EU: Evidence from a panel data analysis. Empirical Economics, 2017, vol. 52 (2), pp. 525â€“553. doi: 10.1007/s00181-016-1096-2.87. Marelli E.P., Parisi M.L., Signorelli M. Economic convergence in the EU and Eurozone. Journal of Economic Studies, 2019, vol. 46 (7), pp. 1332â€“1344. doi: 10.1108/jes-03-2019-0139.88. Blanutsa V.I. Perspektivnye ekonomicheskie spetsializatsii dlya rossiiskikh regionov v Strategii prostranstvennogo razvitiya: kluby konvergentsii [Perspective economic specializations for the Russian regions in the strategy of spatial development: Convergence clubs]. Ekonomika. Informatika [Economics. Information Technologies], 2020, vol. 47, no. 2, pp. 233â€“243. (In Russian). doi: 10.18413/2687-0932-2020-47-2-233-243. 89. Cainelli G., Ganau R. Distance-based agglomeration externalities and neighboring firmsâ€™ characteristics. Regional Studies, 2018, Vol. 52 (7), pp. 922â€“933. doi: 10.1080/00343404.2017.1360482. 90. Kinossian N. Planning strategies and practices in non-core regions: A critical response. European Planning Studies, 2018, vol. 26 (2), pp. 365â€“375. doi: 10.1080/09654313.2017.1361606.91. Humer A. Linking polycentricity concepts to periphery: Implications for an integrative Austrian strategic spatial planning practice. European Planning Studies, 2018, vol. 26 (4), pp. 635â€“652. doi: 10.1080/09654313.2017.1403570.92. Geissinger A., Laurell C., Sandstrom C., Eriksson K., Nykvist R. Digital entrepreneurship and field condition for institutional change â€“ Investigation the enabling role of cities. Technological Forecasting and Social Change, 2019, vol. 146, pp. 877â€“886. doi: 10.1016/j.techfore.2018.06.019.93. Lu Y., Cao K. Spatial analysis of big data industrial agglomeration and development in China. Sustainability, 2019, vol. 11 (6), pp. 1â€“22. doi: 10.3390/SU11061783.94. De Groot H.L.F., Poot J., Smit M.J. Which agglomeration externalities matter most and why? Journal of Economic Surveys, 2016, vol. 30 (4), pp. 756â€“782. doi: 10.1111/joes.12112.95. Freret S., Maguain D. The effects of agglomeration on the tax competition: Evidence from a two-regime spatial panel model on French data. International Tax and Public Finance, 2017, vol. 24 (6), pp. 1100â€“1140. doi: 10.1007/s10797-016-9429-9.96. Wang B., Sun Y., Wang Z. Agglomeration effect of CO2 emissions and emissions reduction effect of technology: A spatial econometric perspective based on Chinaâ€™s province-level data. Journal of Cleaner Production, 2018, vol. 204, pp. 96â€“106. doi: 10.1016/j.jclepro.2018.08.243.97. Otsuka A. Dynamics of agglomeration, accessibility, and total factor productivity: Evidence from Japanese region. Economics of Innovation and New Technology, 2018, vol. 27 (7), pp. 611â€“627. doi: 10.1080/10438599.2017.1384110.98. Liang J., Goetz S.J. Technology intensity and agglomeration economies. Research Policy, 2018, vol. 47 (10), pp. 1990â€“1995. doi: 10.1016/j.respol.2018.07.006.99. Widya A.H.B., Hartono D., Indraswari K.D., Setyonugroho L.D. Population concentration and productivity in the metropolitan area: Evidence from Indonesia. International Journal of Economics and Management, 2019, vol. 13 (2), pp. 453â€“466.100. Tao J., Ho C.-Y., Luo S., Sheng Y. Agglomeration economies in creative industries. Regional Science and Urban Economics, 2019, vol. 77, pp. 141â€“154. doi: 10.1016/j.regsciurbeco.2019.04.002.101. Gokan T., Kuroiwa I., Nakajima K. Agglomeration economies in Vietnam: A firm-level analysis. Journal of Asian Economics, 2019, vol. 62, pp. 52â€“64. doi: 10.1016/j.asieco.2019.03.002.102. Bergeaud A., Cette G., Lecat R. Long-term growth and productivity trends: Secular stagnation or temporary slowdown? Revue de Iâ€™OFCE, 2018, vol. 157 (3), pp. 37â€“54. doi: 10.3917/reof.157.0037. 103. Polyan P.M. Metodika vydeleniya i analiza opornogo karkasa rasseleniya [Identification and analysis methodology for the basic resettlement framework]. Ðœosocw, Izd-vo In-ta geografii AN SSSR Publ., 1988. 283 p. (In Russian). 104. Blanutsa V.I. Territorial'naya struktura tsifrovoi ekonomiki Rossii: predvaritel'naya delimitatsiya Â«umnykhÂ» gorodskikh aglomeratsii i regionov [Territorial structure of digital economy of Russia: Preliminary delimitation of â€˜smartâ€™ urban agglomerations and regions]. Prostranstvennaya ekonomika [Spatial Economy], 2018, no. 2, pp. 17â€“35. (In Russian). doi: 10.14530/se.2018.2.017-035.105. Suarez-Eiroa B., Fernandez E., Mendez-Martinez G., Soto-Onate D. Operational principles of circular economy for sustainable development: Linking theory and practice. Journal of Cleaner Production, 2019, vol. 214, pp. 952â€“961. doi: 10.1016/j.jclepro.2018.12.271.106. Sassanelli C., Rosa P., Rocca R., Terzi S. Circular economy performance assessment methods: A systematic literature review. Journal of Cleaner Production, 2019, vol. 229, pp. 440â€“453. doi: 10.1016/j.jclepro.2019.05.019.107. Winans K., Kendall A., Deng H. The history and current applications of the circular economy concept. Renewable and Sustainable Energy Reviews, 2017, vol. 68, pp. 825â€“833. doi: 10.1016/j.rser.2016.09.123.108. McDowall W., Geng Y., Huang B., Bartekova E., Bleischwitz R., Turkeli S., Kemp R., Domenech T. Circular economy policies in China and Europe. Journal of Industrial Ecology, 2017, vol. 21 (3), pp. 651â€“661. doi: Kirchherr J., Reike D., Hekkert M. Conceptualizing the circular economy: An analysis of 114 definitions. Resources, Conservation and Recycling, 2017, vol. 127, pp. 221â€“232. doi: 10.1016/j.resconrec.2017.09.005. 109. Kirchherr J., Reike D., Hekkert M. Conceptualizing the circular economy: An analysis of 114 definitions. Resources, Conservation and Recycling, 2017, vol. 127, pp. 221â€“232. doi: 10.1016/j.resconrec.2017.09.005. 110. Skene K.R. Circles, spirals, pyramids and cubes: Why the circular economy cannot work. Sustainability Science, 2018, vol. 13, pp. 479â€“492. doi: 10.1007/s11625-017-0443-3.111. Zotti J., Bigano A. Write circular economy, read economyâ€™s circularity. How to avoid going in circles. Economia Politica, 2019, vol. 36, pp. 629â€“652. doi: 10.1007/s40888-019-00145-9.112. Lieder M., Rashid A. Towards circular economy implementation: A comprehensive review in context of manufacturing industry. Journal of Cleaner Production, 2016, vol. 115, pp. 36â€“51. doi: 10.1016/j.jclepro.2015.12.042. 113. Kalmykova Y., Sadagopan M., Rosado L. Circular economy â€“ From review of theories and practices to development of implementation tools. Resources, Conservation and Recycling, 2018, vol. 135, pp. 190â€“201. doi: 10.1016/j.resconrec.2017.10.034.114. Merli R., Preziosi M., Acampora A. How do scholars approach the circular economy? A systematic literature review. Journal of Cleaner Production, 2018, vol. 178, pp. 703â€“722. doi: 10.1016/j.jclepro.2017.12.112.115. Bressanelli G., Saccani N., Pigosso D.C.A., Perona M. Circular economy in the WEEE industry: A systematic literature review and a research agenda. Sustainable Production and Consumption, 2020, vol. 23, pp. 174â€“188. doi: 10.1016/j.spc.2020.05.007.116. Wu H.Q., Shi Y., Xia Q., Zhu W.D. Effectiveness of the policy of circular economy in China: A DEA-based analysis for the period of 11th five-year-plan. Resources, Conservation and Recycling, 2014, vol. 83, pp. 163â€“175. doi: 10.1016/j.resconrec.2013.10.003.117. Jawahir I.S., Bradley R. Technological elements of circular economy and the principles of 6R-based closed-loop material flow in sustainable manufacturing. Procedia CIRP, 2016, vol. 40, pp. 103â€“108. doi: 10.1016/j.procir.2016.01.067.118. Gbededo M.A., Liyanage K., Garza-Reyes J.A. Towards a life cycle sustainability analysis: A systematic review of approaches to sustainable manufacturing. Journal of Cleaner Production, 2018, vol. 184, pp. 1002â€“1015. doi: 10.1016/j.jclepro.2018.02.310.119. Tukker A., Tischner U. Product-services as a research field: Past, present and future. Reflections from a decade of research. Journal of Cleaner Production, 2006, vol. 14, pp. 1552â€“1556. doi: 10.1016/j.jclepro.2006.01.022.120. Sposato P., Preka R., Cappellaro F., Cutaia L. Sharing economy and circular economy. How technology and collaborative consumption innovations boost closing the loop strategies. Environmental Engineering and Management Journal, 2017, vol. 16 (8), pp. 1797â€“1806. doi: 10.30638/EEMJ.2017.196.121. Sari R., Meyliana, Hidayanto A.N., Prabowo H. Sharing economy in people, process and technology perspective: A systematic literature review. International Journal of Management, 2019, vol. 10 (2), pp. 100â€“116. doi: 10.34218/IJM.10.2.2019.009.122. Schlagwein D., Schoder D., Spindeldreher K. Consolidated, systemic conceptualization, and definition of the â€œsharing economyâ€. JASIST, 2020, vol. 71 (7), pp. 817â€“838. doi: 10.1002/asi.24300.123. Pieroni M.P.P., McAloone T.C., Pigosso D.C.A. Business model innovation for circular economy and sustainability: A review of approaches. Journal of Cleaner Production, 2019, vol. 215, pp. 198â€“216. doi: 10.1016/j.jclepro.2019.01.036.124. Centobelli P., Cerchione R., Chiaroni D., Del Vecchio P., Urbinati A. Designing business models in circular economy: A systematic literature review and research agenda. Business Strategy and the Environment, 2020, vol. 29 (4), pp. 1734â€“1749. doi: 10.1002/bse.2466. 125. Turcu C., Gillie H. Governing the circular economy in the city: Local planning practice in London. Planning Practice and Research, 2020, vol. 35 (1), pp. 62â€“85. doi: 10.1080/02697459.2019.1703335.126. KÄ™bÅ‚owski W., Lambert D., Bassens D. Circular economy and the city: An urban political economy agenda. Culture and Organization, 2020, vol. 26 (2), pp. 142â€“158. doi: 10.1080/14759551.2020.1718148.127. Aranda-Uson A., Moneva J.M., Portillo-Tarragona P., Llena-Macarulla F. Measurement of the circular economy in business: Impact and implications for regional policies. Economics and Policy of Energy and the Environment, 2018, vol. 2018 (2), pp. 187â€“205. doi: 10.3280/EFE2018-002010.128. Cramer J.M. The function of transition brokers in the regional governance of implementing circular economy â€“ A comparative case study of six Dutch regions. Sustainability, 2020, vol. 12 (12), pp. 1â€“21. doi: 10.3390/SU12125015. 129. Jabbour A.B.L.S.,Jabbour C.J.C., Filho M.G., Roubaud D. Industry 4.0 and the circular economy: A proposed research agenda and original roadmap for sustainable operations. Annals of Operations Research, 2018, vol. 270, pp. 273â€“286. doi: 10.1007/s10479-018-2772-8.130. Acerbi F., Sassanelli C., Terzi S., Taisch M. Towards a data-based circular economy: Exploring opportunities from digital knowledge management. Lecture Notes in Networks and Systems, 2020, vol. 122, pp. 331â€“339. doi: 10.1007/978-3-030-41429-0_33.131. Rosa P., Sassanelli C., Urbinati A., Chiaroni D., Terzi S. Assessing relations between Circular Economy and Industry 4.0: A systematic literature review. International Journal of Production Research, 2020, vol. 58 (6), pp. 1662â€“1687. doi: 10.1080/00207543.2019.1680896.132. Pagoropoulos A., Pigosso D.C.A., McAloone T.C. The emergent role of digital technologies in the circular economy: A review. Procedia CIRP, 2017, vol. 64, pp. 19â€“24. doi: 10.1016/j.procir.2017.02.047.133. Hatzivasilis G., Fysarakis K., Soultatos O., Askoxylakis I., Papaefstathiou I., Demetriou G. The industrial internet of things as an enabler for a circular economy Hy-LP: A novel IIoT protocol, evaluated on a wind parkâ€™s SDN/NFV-enabled 5G industrial network. Computer Communications, 2018, vol. 119, pp. 127â€“137. doi: 10.1016/j.comcom.2018.02.007. 134. Unruh G. Circular economy, 3D printing, and the biosphere rules. California Management Review, 2018, vol. 60 (3), pp. 95â€“111. doi: 10.1177/0008125618759684.135. Casado-Vara R., Prieto J., De La Prieta F., Corchado J.M. How blockchain improves the supply chain: Case study alimentary supply chain. Procedia Computer Science, 2018, vol. 134, pp. 393â€“398. doi: 10.1016/j.procs.2018.07.193.136. Berg H., Wilts H. Digital platforms as market places for the circular economy â€“ requirements and challenges. Sustainability Management Forum, 2019, vol. 27, pp. 1â€“9. doi: 10.1007/s00550-018-0468-9.137. Olugu E., Wong K.Y. An expert fuzzy rule-based system for closed-loop chain performance assessment in the automotive industry. Expert Systems with Applications, 2012, vol. 39, pp. 375â€“384. doi: 10.1016/j.eswa.2011.07.026.138. Huysman S., De Schaepmeester J., Ragaert K., Dewulf J., De Meester S. Performance indicators for a circular economy: A case study on post-industrial plastic waste. Resources, Conservation and Recycling, 2017, vol. 120, pp. 46â€“54. doi: 10.1016/j.resconrec.2017.01.013.139. Xu J., Li X., Wu D.D. Optimizing circular economy planning and risk analysis using system dynamics. Human and Ecological Risk Assessment, 2009, vol. 15 (2), pp. 316â€“331. doi: 10.1080/10807030902761361.140. Ranta V., Aarikka-Stenroos L., Ritala P., MÃ¤kinen S.J. Exploring institutional drivers and barriers of the circular economy: A cross-regional comparison of China, the US, and Europe. Resources, Conservation and Recycling, 2018, vol. 135, pp. 70â€“82. doi: 10.1016/j.resconrec.2017.08.017.141. Losch Ð. Geograficheskoe razmeshchenie khozyaistva. Per. s angl [Geographical location of a household. Transl. from Eng.]. Moscow, Izd-vo inostrannoi literatury Publ., 1959. 455 p. (In Russian). 142. Sonis M., Hewings G.J.D. Economic landscapes: Multiplier product matrix analysis for multiregional input-output system. Hitotsubashi Journal of Economics, 1999, vol. 40 (1), pp. 59â€“74. doi: 10.15057/7722.143. Plummer P. Modelling economic landscapes: A geographical perspective. Regional Studies, 2003, vol. 37 (6-7), pp. 687â€“695. doi: 10.1080/0034340032000108778.144. Rafiqui P.S. Evolving economic landscapes: Why new institutional economics matters for economic geography. Journal of Economic Geography, 2009, vol. 9 (3), pp. 329â€“353. doi: 10.1093/jeg/lbn050.145. Hachem K. Shadow banking in China. Annual Review of Financial Economics, 2018, vol. 10, pp. 287â€“308. doi: 10.1146/annurev-financial-110217-023025.146. Jean S. How the COVID-19 pandemic is reshaping the trade landscape and what to do about it. Intereconomics: Review of European Economic Policy, 2020, vol. 55 (3), pp. 135â€“139. doi: 10.1007/s10272-020-0890-4.147. Ghossoub E.A., Reed R.R. Banking competition, production externalities, and the effects of monetary policy. Economic Theory, 2019, vol. 67 (1), pp. 91â€“154. doi: 10.1007/s00199-017-1086-4.148. Eichengreen B., Park D., Shin K. The landscape economic growth: Do middle-income countries differ? Emerging Markets Finance and Trade, 2018, vol. 54 (4), pp. 836â€“858. doi: 10.1080/1540496X.2017.1419427.149. Filculescu A. The heterogeneous landscape of innovation in female led-businesses â€“ Cross-country comparisons. Management and Marketing. Challenges for the Knowledge Society, 2016, vol. 11 (4), pp. 610â€“623. doi: 10.1515/mmcks-2016-0019.150. Wentrup R., StrÃ¶m P., Nakamura H.R. Digital oases and digital deserts in Sub-Saharan Africa. Journal of Science and Technology Policy Management, 2016, vol. 7 (1), pp. 77â€“100. doi: 10.1108/JSTPM-03-2015-0013.151. Batten D.F. Complex landscapes of spatial interaction. Annals of Regional Science, 2001, vol. 35, pp. 81â€“111. doi: 10.1007/s001680000032.152. Fagiolo G., Marengo L., Valente M. Population learning in a model with random payoff landscapes and endogenous networks. Computational Economics, 2005, vol. 24, pp. 383â€“408. doi: 10.1007/s10614-005-6160-5.153. Filatova T., Veen A. van der, Parker D.C. Land market interaction between heterogeneous agents in a heterogeneous landscape â€“ Tracing the macro-scale effects of individual trade-offs between environmental amenities and disamenities. Canadian Journal of Agricultural Economics, 2009, vol. 57 (4), pp. 431â€“457. doi: 10.1111/j.1744-7976.2009.01164.x.154. Lipovska H., Coufalova L., Zidek L. Homo economicus in the shortage economy. DANUBE: Law, economics and social issues review, 2018, vol. 9 (4), pp. 207â€“226. doi: 10.2478/danb-2018-0013.155. Simon H.A. A behavioral model of rational choice. Quarterly Journal of Economics, 1955, vol. 69 (1), pp. 99â€“118. doi: 10.2307/1884852.156. Dopfer K. The economic agent as rule maker and rule user: Homo Sapiens Oeconomicus. Journal of Evolutionary Economics, 2004, vol. 14 (2), pp. 177â€“195. doi: 10.1007/s00191-004-0189-9.157. Miljkovic D. Rational choice and irrational individuals or simply an irrational theory: A critical review of the hypothesis of perfect rationality. Journal of Socio-Economics, 2005, vol. 34 (5), pp. 621â€“634.158. Urbina D.A., Ruiz-Villaverde A. A critical review of Homo Economicus from five approaches. American Journal of Economics and Sociology, 2019, vol. 78 (1), pp. 63â€“93. doi: 10.1111/ajes.12258.159. Parkes D.C., Wellmann M.P. Economic reasoning and artificial intelligence. Science, 2015, vol. 349 (6245), pp. 267â€“272. doi: 10.1126/science.aaa8403.160. Batty M., Axhausen K., Giannotti F., Pozdnoukhov A., Bazzani A., Wachowicz M., Ouzounis G., Portugali Y. Smart cities of the future. The European Physical Journal Special Topics, 2012, vol. 214, pp. 481â€“518. doi: 10.1140/epjst/e2012-01703-3.161. Albino V., Berardi U., Dangelico R.M. Smart cities: Definitions, dimensions, performance and initiatives. Journal of Urban Technology, 2015, vol. 22 (1), pp. 3â€“21. doi: 10.1080/10630732.2014.942092. 162. Mora L., Bolici R., Deakin M. The first two decades of smart-city research: A bibliometric analysis. Journal of Urban Technology, 2017, vol. 24 (1), pp. 3â€“27. doi: 10.1080/10630732.2017.1285123.163. Silva B.N., Khan M., Han K. Towards sustainable smart cities: A review of trends, architectures, components, and open challenges in smart cities. Sustainable Cities and Society, 2018, vol. 38, pp. 697â€“713. doi: 10.1016/j.scs.2018.01.053.164. Coletta C., Evans L., Heaphy L., Kitchin R. (eds.) Creating Smart Cities. London, Routledge, 2018. 254 p. doi: 10.4324/9781351182409.165. Winkowska J., Szpilko D., PejÃ­Ä‡ S. Smart city concept in the light of the literature review. Engineering Management in Production and Services, 2019, vol. 11 (2), pp. 70â€“86. doi: 10.2478/emj-2019-0012.166. Caragliu A., Bo C. del, Nijkamp P. Smart cities in Europe. Journal of Urban Technology, 2011, vol. 18 (2), pp. 65â€“82. doi: 10.1080/10630732.2011.601117.167. Joss S., Sengers F., Schraven D., Caprotti F., Dayot Y. The smart city as global discourse: Storylines and critical junctures across 27 cities. Journal of Urban Technology, 2019, vol. 26 (1), pp. 3â€“34. doi: 10.1080/10630732.2018.1558387. 168. Kourtit K., Nijkamp P. Smart cities in smart space: A regional science perspective. Scienze Regionali. Italian Journal of Regional Science. 2018, vol. 17 (1), pp. 105â€“114. doi: 10.14650/88819.169. Greco I., Cresta A. From smart cities to smart city-regions: Reflections and proposals. Proceeding of the International Conference on Computational Science and Its Applications (ICCSA 2017), 2017, pp. 282â€“295. doi: 10.1007/978-3-319-62398-6_20.170. De Falco S., Angelidou M., Addie J.-P.D. From the â€œsmart cityâ€ to the â€œsmart metropolisâ€? Building resilience in the urban periphery. European Urban and Regional Studies, 2018, vol. 26 (2), pp. 205â€“223. doi: 10.1177/0969776418783813.171. Komninos N., Tsarchopoulos P. Toward intelligent Thessaloniki: From an agglomeration of apps to smart districts. Journal of the Knowledge Economy, 2013, vol. 4 (2), pp. 149â€“168. doi: 10.1007/s13132-012-0085-8.172. Morandi C., Rolando A., Di Vita S. From smart city to smart region: Digital services for an internet of places. Milan, Springer-Verlag, 2016. 103 p. doi: 10.1007/978-3-319-17338-2.173. Vanolo A. Smartmentality: The smart city as disciplinary strategy. Urban Studies, 2014, vol. 51 (5), pp. 883â€“898. doi: 10.1177/0042098013494427.174. Luque-Ayala A., Marvin S. Developing a critical understanding of smart urbanism? Urban Studies, 2015, vol. 52(12), pp. 2105â€“2116. doi: 10.1177/0042098015577319. 175. Cugurullo F. Exposing smart cities and eco-cities: Frankenstein urbanism and the sustainability challenges of the experimental city. Environment and Planning A: Economy and Space, 2018, vol. 50 (1), pp. 73â€“92. doi: 10.1177/0308518X17738535. 176. Colantonio E., Cialfi D. Smart regions in Italy: A comparative study through self-organizing maps. European Journal of Business and Social Sciences, 2016, vol. 5 (9), pp. 84â€“99.177. Mikki L., Markkula M., Schaffers H. (Eds.) Helsinki smart region: Pioneering for Europe 2020. Helsinki, Aalto University, 2014. 45 p.178. Ma S., Zhao Y., Tan X. Exploring smart growth boundaries of urban agglomeration with land use spatial optimization: A case study of Changsha-Zhuzhou-Xiangtan city group, China. Chinese Geographical Science, 2020, vol. 30 (4), pp. 665â€“676. doi: 10.1007/s11769-020-1140-1.179. Blanutsa V.I., Cherepanov K.A. Regional information flows: Existing and new approaches to geographical study. Regional Research of Russia, 2019, vol. 9 (1), pp. 97â€“106. doi: 10.1134/S2079970519010039.180. Schwenker B., Wulf T. (Eds.) Scenario-based strategic planning: Developing strategies in an uncertain world. Berlin, Springer, 2013. 214 p.181. Alam K., Erdiaw-Kwasie M.O., Shahiduzzaman M., Ryan B. Assessing regional digital competence: Digital futures and strategic planning implications. Journal of Rural Studies, 2018, vol. 60, pp. 60â€“69. doi: 10.1016/j.jrurstud.2018.02.009. 182. Tikhvinskii V.O., Bochechka G.S. Perspektivy setei 5G i trebovaniya k kachestvu ikh obsluzhivaniya [5G network future and their maintenance requirements]. Elektrosvyaz' [Electric Connection], 2014, no. 11, pp. 40â€“43. (In Russian)183. Patwary M.N., Nawaz S.J., Rahman A., Sharma S.K., Rashid M. The potential short- and long-term disruptions and transformative impacts of 5G and beyond wireless networks: Lessons learnt from the development of a 5G testbed environment. IEEE Access, 2020, vol. 8, pp. 11352â€“11379. doi: 10.1109/ACCESS.2020.2964673.184. Letaief K.B., Chen W., Shi Y., Zhang J., Zhang Y.-J.A. The roadmap to 6G: AI empowered wireless networks. IEEE Communication Magazine, 2019, vol. 57 (8), pp. 84â€“90. doi: 10.1109/MCOM.2019.1900271.},
 author = {Blanutza, Viktor Ivanovich},
 doi = {10.17072/1994-9960-2020-4-463-493},
 journal = {Ð’ÐµÑÑ‚Ð½Ð¸Ðº ÐŸÐµÑ€Ð¼ÑÐºÐ¾Ð³Ð¾ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ¸Ñ‚ÐµÑ‚Ð°. Ð¡ÐµÑ€Ð¸Ñ Â«Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ°Â» = Perm University Herald. ECONOMY},
 keywords = {},
 number = {4},
 pages = {463-493},
 title = {Ð¦Ð¸Ñ„Ñ€Ð¾Ð²Ð°Ñ ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸ÐºÐ° Ð Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¹ Ð¤ÐµÐ´ÐµÑ€Ð°Ñ†Ð¸Ð¸: ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð½Ð°Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹},
 url = {https://app.dimensions.ai/details/publication/pub.1141028162},
 volume = {15},
 year = {2020}
}

@article{pub.1141035297,
 abstract = {In this paper, we learn visual features that we use to first build a map and
then localize a robot driving autonomously across a full day of lighting
change, including in the dark. We train a neural network to predict sparse
keypoints with associated descriptors and scores that can be used together with
a classical pose estimator for localization. Our training pipeline includes a
differentiable pose estimator such that training can be supervised with ground
truth poses from data collected earlier, in our case from 2016 and 2017
gathered with multi-experience Visual Teach and Repeat (VT&R). We insert the
learned features into the existing VT&R pipeline to perform closed-loop path
following in unstructured outdoor environments. We show successful path
following across all lighting conditions despite the robot's map being
constructed using daylight conditions. Moreover, we explore generalizability of
the features by driving the robot across all lighting conditions in new areas
not present in the feature training dataset. In all, we validated our approach
with 35.5 km of autonomous path following experiments in challenging
conditions.},
 author = {Gridseth, Mona and Barfoot, Timothy D.},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Keeping an Eye on Things: Deep Learned Features for Long-Term Visual
  Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1141035297},
 volume = {},
 year = {2021}
}

@article{pub.1141124226,
 abstract = {Simultaneous Localization and Mapping (SLAM) techniques play a key role
towards long-term autonomy of mobile robots due to the ability to correct
localization errors and produce consistent maps of an environment over time.
Contrarily to urban or man-made environments, where the presence of unique
objects and structures offer unique cues for localization, the appearance of
unstructured natural environments is often ambiguous and self-similar,
hindering the performances of loop closure detection. In this paper, we present
an approach to improve the robustness of place recognition in the context of a
submap-based stereo SLAM based on Gaussian Process Gradient Maps (GPGMaps).
GPGMaps embed a continuous representation of the gradients of the local terrain
elevation by means of Gaussian Process regression and Structured Kernel
Interpolation, given solely noisy elevation measurements. We leverage the
image-like structure of GPGMaps to detect loop closures using traditional
visual features and Bag of Words. GPGMap matching is performed as an SE(2)
alignment to establish loop closure constraints within a pose graph. We
evaluate the proposed pipeline on a variety of datasets recorded on Mt. Etna,
Sicily and in the Morocco desert, respectively Moon- and Mars-like
environments, and we compare the localization performances with
state-of-the-art approaches for visual SLAM and visual loop closure detection.},
 author = {Giubilato, Riccardo and Gentil, Cedric Le and Vayugundla, Mallikarjuna and Schuster, Martin J. and Vidal-Calleja, Teresa and Triebel, Rudolph},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {GPGM-SLAM: a Robust SLAM System for Unstructured Planetary Environments
  with Gaussian Process Gradient Maps},
 url = {https://app.dimensions.ai/details/publication/pub.1141124226},
 volume = {},
 year = {2021}
}

@article{pub.1141271491,
 abstract = {Plans for establishing a long-term human presence on the Moon will require
substantial increases in robot autonomy and multi-robot coordination to support
establishing a lunar outpost. To achieve these objectives, algorithm design
choices for the software developments need to be tested and validated for
expected scenarios such as autonomous in-situ resource utilization (ISRU),
localization in challenging environments, and multi-robot coordination.
However, real-world experiments are extremely challenging and limited for
extraterrestrial environment. Also, realistic simulation demonstrations in
these environments are still rare and demanded for initial algorithm testing
capabilities. To help some of these needs, the NASA Centennial Challenges
program established the Space Robotics Challenge Phase 2 (SRC2) which consist
of virtual robotic systems in a realistic lunar simulation environment, where a
group of mobile robots were tasked with reporting volatile locations within a
global map, excavating and transporting these resources, and detecting and
localizing a target of interest. The main goal of this article is to share our
team's experiences on the design trade-offs to perform autonomous robotic
operations in a virtual lunar environment and to share strategies to complete
the mission requirements posed by NASA SRC2 competition during the
qualification round. Of the 114 teams that registered for participation in the
NASA SRC2, team Mountaineers finished as one of only six teams to receive the
top qualification round prize.},
 author = {Kilic, Cagri and R., Bernardo Martinez and Tatsch, Christopher A. and Beard, Jared and Strader, Jared and Das, Shounak and Ross, Derek and Gu, Yu and Pereira, Guilherme A. S. and Gross, Jason N.},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {NASA Space Robotics Challenge 2 Qualification Round: An Approach to
  Autonomous Lunar Rover Operations},
 url = {https://app.dimensions.ai/details/publication/pub.1141271491},
 volume = {},
 year = {2021}
}

@article{pub.1141424262,
 abstract = {At the present time, water and sewer pipe networks are predominantly inspected manually. In the near future, smart cities will perform intelligent autonomous monitoring of buried pipe networks, using teams of small robots. These robots, equipped with all necessary computational facilities and sensors (optical, acoustic, inertial, thermal, pressure and others) will be able to inspect pipes whilst navigating, self-localising and communicating information about the pipe condition and faults such as leaks or blockages to human operators for monitoring and decision support. The predominantly manual inspection of pipe networks will be replaced with teams of autonomous inspection robots that can operate for long periods of time over a large spatial scale. Reliable autonomous navigation and reporting of faults at this scale requires effective localization and mapping, which is the estimation of the robotâ€™s position and its surrounding environment. This survey presents an overview of state-of-the-art works on robot simultaneous localization and mapping (SLAM) with a focus on water and sewer pipe networks. It considers various aspects of the SLAM problem in pipes, from the motivation, to the water industry requirements, modern SLAM methods, map-types and sensors suited to pipes. Future challenges such as robustness for long term robot operation in pipes are discussed, including how making use of prior knowledge, e.g. geographic information systems (GIS) can be used to build map estimates, and improve multi-robot SLAM in the pipe environment.},
 author = {Aitken, Jonathan M. and Evans, Mathew H. and Worley, Rob and Edwards, S. and Zhang, Rui and Dodd, Tony and Mihaylova, Lyudmila and Anderson, Sean R.},
 doi = {10.1109/access.2021.3115981},
 journal = {IEEE Access},
 keywords = {},
 note = {https://ieeexplore.ieee.org/ielx7/6287639/6514899/09548901.pdf},
 number = {},
 pages = {140173-140198},
 title = {Simultaneous Localization and Mapping for Inspection Robots in Water and Sewer Pipe Networks: A Review},
 url = {https://app.dimensions.ai/details/publication/pub.1141424262},
 volume = {9},
 year = {2021}
}

@article{pub.1141451660,
 abstract = {Progress in the last decade has brought about significant improvements in the
accuracy and speed of SLAM systems, broadening their mapping capabilities.
Despite these advancements, long-term operation remains a major challenge,
primarily due to the wide spectrum of perturbations robotic systems may
encounter.
  Increasing the robustness of SLAM algorithms is an ongoing effort, however it
usually addresses a specific perturbation. Generalisation of robustness across
a large variety of challenging scenarios is not well-studied nor understood.
This paper presents a systematic evaluation of the robustness of open-source
state-of-the-art SLAM algorithms with respect to challenging conditions such as
fast motion, non-uniform illumination, and dynamic scenes. The experiments are
performed with perturbations present both independently of each other, as well
as in combination in long-term deployment settings in unconstrained
environments (lifelong operation).},
 author = {Bujanca, Mihai and Shi, Xuesong and Spear, Matthew and Zhao, Pengpeng and Lennox, Barry and Lujan, Mikel},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Robust SLAM Systems: Are We There Yet?},
 url = {https://app.dimensions.ai/details/publication/pub.1141451660},
 volume = {},
 year = {2021}
}

@article{pub.1141523598,
 abstract = { Purpose This paper aims to study the localization problem for autonomous industrial vehicles in the complex industrial environments. Aiming for practical applications, the pursuit is to build a map-less localization system which can be used in the presence of dynamic obstacles, short-term and long-term environment changes.   Design/methodology/approach The proposed system contains four main modules, including long-term place graph updating, global localization and re-localization, location tracking and pose registration. The first two modules fully exploit the deep-learning based three-dimensional point cloud learning techniques to achieve the map-less global localization task in large-scale environment. The location tracking module implements the particle filter framework with a newly designed perception model to track the vehicle location during movements. Finally, the pose registration module uses visual information to exclude the influence of dynamic obstacles and short-term changes and further introduces point cloud registration network to estimate the accurate vehicle pose.   Findings Comprehensive experiments in real industrial environments demonstrate the effectiveness, robustness and practical applicability of the map-less localization approach.   Practical implications This paper provides comprehensive experiments in real industrial environments.   Originality/value The system can be used in the practical automated industrial vehicles for long-term localization tasks. The dynamic objects, short-/long-term environment changes and hardware limitations of industrial vehicles are all considered in the system design. Thus, this work moves a big step toward achieving real implementations of the autonomous localization in practical industrial scenarios. },
 author = {Liu, Zhe and Qiao, Zhijian and Suo, Chuanzhe and Liu, Yingtian and Jin, Kefan},
 doi = {10.1108/aa-06-2021-0088},
 journal = {Assembly Automation},
 keywords = {},
 number = {6},
 pages = {714-724},
 title = {Map-less long-term localization in complex industrial environments},
 url = {https://app.dimensions.ai/details/publication/pub.1141523598},
 volume = {41},
 year = {2021}
}

@article{pub.1141615196,
 abstract = {Natural hazards such as landslides impose very high risks to the community and infrastructures. To mitigate such risk, proper long-term engineering solutions are must. Several studies conducted in the past proposed countermeasures on many occasions. Despite such interventions, the Rinchending Goenpa area still suffers high vulnerability to landslide hazard which continues every monsoon. To assess landslide characteristics in detail, we performed landslide susceptibility mapping using the unmanned aerial vehicle (UAV) technique and conducted a site investigation. High-resolution digital elevation model (DEM) was generated with the site-specific aerial photographs and processed in Agisoft PhotoScan to developed thematic layers in geographical information system (GIS) software. We implemented a multi-influencing factor (MIF) for deriving the influence of landslide conditioning factors and developed a landslide susceptibility map (LSM) using the weighted overlay method (WOM). The landslide conditioning factors include slope, elevation, aspect, topographical wetness index (TWI), and normalized difference vegetation index (NDVI). According to LSM, the areal coverage of landslide susceptibility of study area reveals 2.41% in very low susceptibility, 37.14% in low susceptibility, with highest in moderate susceptibility zone of 49.55% followed by 10.60% in high susceptibility zone, and 0.30% in the very high susceptible zone. Based on the zonal distribution of LSM, and considering findings from several other site investigation such as geophysical survey, the performance of existing countermeasures, soil characteristics, and footprint of last landslide occurrences, we proposed mitigation measures under each zone to provide long-term solutions.},
 author = {Tempa, Karma and Peljor, Kinley and Wangdi, Sangay and Ghalley, Rupesh and Jamtsho, Kelzang and Ghalley, Samir and Pradhan, Pratima},
 doi = {10.1016/j.nhres.2021.09.001},
 journal = {Natural Hazards Research},
 keywords = {},
 note = {https://doi.org/10.1016/j.nhres.2021.09.001},
 number = {4},
 pages = {171-186},
 title = {UAV technique to localize landslide susceptibility and mitigation proposal: A case of Rinchending Goenpa landslide in Bhutan},
 url = {https://app.dimensions.ai/details/publication/pub.1141615196},
 volume = {1},
 year = {2021}
}

@article{pub.1141618156,
 abstract = {Robots deployed in settings such as warehouses and parking lots must cope
with frequent and substantial changes when localizing in their environments.
While many previous localization and mapping algorithms have explored methods
of identifying and focusing on long-term features to handle change in such
environments, we propose a different approach -- can a robot understand the
distribution of movable objects and relate it to observations of such objects
to reason about global localization? In this paper, we present probabilistic
object maps (POMs), which represent the distributions of movable objects using
pose-likelihood sample pairs derived from prior trajectories through the
environment and use a Gaussian process classifier to generate the likelihood of
an object at a query pose. We also introduce POM-Localization, which uses an
observation model based on POMs to perform inference on a factor graph for
globally consistent long-term localization. We present empirical results
showing that POM-Localization is indeed effective at producing globally
consistent localization estimates in challenging real-world environments, and
that POM-Localization improves trajectory estimates even when the POM is formed
from partially incorrect data.},
 author = {Adkins, Amanda and Chen, Taijing and Biswas, Joydeep},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Probabilistic Object Maps for Long-Term Robot Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1141618156},
 volume = {},
 year = {2021}
}

@article{pub.1141619295,
 abstract = {Lifelong SLAM considers long-term operation of a robot where already mapped
locations are revisited many times in changing environments. As a result,
traditional graph-based SLAM approaches eventually become extremely slow due to
the continuous growth of the graph and the loss of sparsity. Both problems can
be addressed by a graph pruning algorithm. It carefully removes vertices and
edges to keep the graph size reasonable while preserving the information needed
to provide good SLAM results. We propose a novel method that considers
geometric criteria for choosing the vertices to be pruned. It is efficient,
easy to implement, and leads to a graph with evenly spread vertices that remain
part of the robot trajectory. Furthermore, we present a novel approach of
marginalization that is more robust to wrong loop closures than existing
methods. The proposed algorithm is evaluated on two publicly available
real-world long-term datasets and compared to the unpruned case as well as
ground truth. We show that even on a long dataset (25h), our approach manages
to keep the graph sparse and the speed high while still providing good accuracy
(40 times speed up, 6cm map error compared to unpruned case).},
 author = {Kurz, Gerhard and Holoch, Matthias and Biber, Peter},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Geometry-based Graph Pruning for Lifelong SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1141619295},
 volume = {},
 year = {2021}
}

@article{pub.1142014968,
 abstract = {Visual place recognition (VPR) is critical in not only localization and
mapping for autonomous driving vehicles, but also assistive navigation for the
visually impaired population. To enable a long-term VPR system on a large
scale, several challenges need to be addressed. First, different applications
could require different image view directions, such as front views for
self-driving cars while side views for the low vision people. Second, VPR in
metropolitan scenes can often cause privacy concerns due to the imaging of
pedestrian and vehicle identity information, calling for the need for data
anonymization before VPR queries and database construction. Both factors could
lead to VPR performance variations that are not well understood yet. To study
their influences, we present the NYU-VPR dataset that contains more than
200,000 images over a 2km by 2km area near the New York University campus,
taken within the whole year of 2016. We present benchmark results on several
popular VPR algorithms showing that side views are significantly more
challenging for current VPR methods while the influence of data anonymization
is almost negligible, together with our hypothetical explanations and in-depth
analysis.},
 author = {Sheng, Diwei and Chai, Yuxiang and Li, Xinru and Feng, Chen and Lin, Jianzhe and Silva, Claudio and Rizzo, John-Ross},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {NYU-VPR: Long-Term Visual Place Recognition Benchmark with View
  Direction and Data Anonymization Influences},
 url = {https://app.dimensions.ai/details/publication/pub.1142014968},
 volume = {},
 year = {2021}
}

@inproceedings{pub.1142027517,
 abstract = {We present a self-supervised learning approach for the semantic segmentation of lidar frames. Our method is used to train a deep point cloud segmentation architecture without any human annotation. The annotation process is automated with the combination of simultaneous localization and mapping (SLAM) and ray-tracing algorithms. By performing multiple navigation sessions in the same environment, we are able to identify permanent structures, such as walls, and disentangle short-term and long-term movable objects, such as people and tables, respectively. New sessions can then be performed using a network trained to predict these semantic labels. We demonstrate the ability of our approach to improve itself over time, from one session to the next. With semantically filtered point clouds, our robot can navigate through more complex scenarios, which, when added to the training pool, help to improve our network predictions. We provide insights into our network predictions and show that our approach can also improve the performances of common localization techniques.},
 author = {Thomas, Hugues and Agro, Ben and Gridseth, Mona and Zhang, Jian and Barfoot, Timothy D.},
 booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra48506.2021.9561701},
 keywords = {},
 note = {http://arxiv.org/pdf/2012.05897},
 pages = {14047-14053},
 title = {Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1142027517},
 year = {2021}
}

@inproceedings{pub.1142027550,
 abstract = {Mapping and localization in non-static environments are fundamental problems in robotics. Most of previous methods mainly focus on static and highly dynamic objects in the environment, which may suffer from localization failure in semi-dynamic scenarios without considering objects with lower dynamics, such as parked cars and stopped pedestrians. In this paper, we introduce semantic mapping and lifelong localization approaches to recognize semi-dynamic objects in non-static environments. We also propose a generic framework that can integrate mainstream object detection algorithms with mapping and localization algorithms. The mapping method combines an object detection algorithm and a SLAM algorithm to detect semi-dynamic objects and constructs a semantic map that only contains semi-dynamic objects in the environment. During navigation, the localization method can classify observation corresponding to static and non-static objects respectively and evaluate whether those semi-dynamic objects have moved, to reduce the weight of invalid observation and localization fluctuation. Real-world experiments show that the proposed method can improve the localization accuracy of mobile robots in non-static scenarios.},
 author = {Zhu, Shifan and Zhang, Xinyu and Guo, Shichun and Li, Jun and Liu, Huaping},
 booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra48506.2021.9561584},
 keywords = {},
 pages = {14389-14395},
 title = {Lifelong Localization in Semi-Dynamic Environment},
 url = {https://app.dimensions.ai/details/publication/pub.1142027550},
 year = {2021}
}

@article{pub.1142131264,
 abstract = {This paper presents a panoramic visual-inertial simultaneous localization and mapping (SLAM) system that is tightly coupled with a wheel encoder, which can be used in a mobile mapping system (MMS), robot, or driverless car. The whole SLAM system is made up of four modules: 1) measurement preprocessing; 2) system initialization; 3) tracking; and 4) local mapping. The core of the system is the combined adjustment we propose for the observations of the panoramic camera, inertial measurement unit (IMU), and wheel encoder, to optimize the system state, which includes the rotation, translation, velocity, IMU bias, and local map point coordinates in the initialization, tracking, and local mapping modules. In the preprocessing, the most important contribution is that we derive the wheel preintegration based on a two-wheel differential model, which combines the many wheel measurements between two frames into a single relative motion constraint. A novel initialization algorithm is also introduced, which can be divided into two steps: 1) the first step is to initialize the visual scale and parameters of the IMU; and 2) the second step is to perform the combined adjustment for the camera, IMU, and preintegrated wheel encoder data, to refine the initial parameters. The proposed panoramic camera-IMU-wheel SLAM (PIW-SLAM) system can achieve high-precision and robust localization in challenging scenes through multi-sensor fusion. We validated the performance of the proposed system on seven different challenging data sequences from the University of Michigan North Campus Long-Term Vision and LiDAR Dataset (NCLT) dataset. The results showed that the PIW-SLAM system has a higher localization accuracy than the other state-of-the-art SLAM systems, and it also showed a superior robustness in various complex environments. We also verified the accuracy of the scale obtained by the initialization algorithm. Furthermore, we confirmed the performance of the proposed PIW-SLAM system in a wide-baseline scene of the ground motion measurement system.},
 author = {Jiang, Fan and Chen, Jiagang and Ji, Shunping},
 doi = {10.1016/j.isprsjprs.2021.10.006},
 journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
 keywords = {},
 note = {https://doi.org/10.1016/j.isprsjprs.2021.10.006},
 number = {},
 pages = {96-111},
 title = {Panoramic Visual-Inertial SLAM Tightly Coupled with a Wheel Encoder},
 url = {https://app.dimensions.ai/details/publication/pub.1142131264},
 volume = {182},
 year = {2021}
}

@inproceedings{pub.1142216592,
 abstract = {A Graph SLAM system is only as good as the edges in its pose graph. Critical mistakes in the generation of these edges can instantly render a map inconsistent, misleading, and ultimately unusable. For a lifelong mapping system, where the map is updated continuously, avoiding these errors altogether is infeasible. Instead, we propose a system for detection of and recovery from severe errors in edge generation. Our system remedies both edges created by view observations and edges created by an odometry motion model. For observation edges, we pair a novel method for monitoring ambiguous views with an intelligent graph-merging algorithm capable of rejecting a relocalization in progress. For motion edges, we propose a qualitative geometric approach for detecting structural aberrations characteristic of odometry failures. We conclude with an analysis of our results based on an empirical study of thousands of robot runs.},
 author = {Banerjee, Nandan and Lisin, Dimitri and Albanese, Victoria and Zhu, Zhongjian and Lenser, Scott R. and Shriver, Justin and Ramaswamy, Tyagaraja and Briggs, Jimmy and Fong, Phil},
 booktitle = {2021 European Conference on Mobile Robots (ECMR)},
 doi = {10.1109/ecmr50962.2021.9568826},
 keywords = {},
 pages = {1-8},
 title = {Preventing and Correcting Mistakes in Lifelong Mapping},
 url = {https://app.dimensions.ai/details/publication/pub.1142216592},
 year = {2021}
}

@inbook{pub.1142255881,
 abstract = {Vehicles with prolonged autonomous missions have to maintain environment awareness by simultaneous localization and mapping (SLAM). Closed-loop correction used for SLAM consistency maintenance is proposed to be substituted by interpolation in rigid body transformation space in order to systematically reduce the accumulated error over different scales. The computation is divided into an edge-computed lightweight SLAM and iterative corrections in the cloud environment. Tree locations in the forest environment are sent via a potentially limited communication bandwidth. Data from a real forest site is used in the verification of the proposed algorithm. The algorithm adds new iterative closest point (ICP) cases to the initial SLAM and measures the resulting map quality by the mean of the root mean squared error (RMSE) of individual tree clusters. Adding 4% more match cases yields the mean RMSE of 0.15 m on a large site with 180 m odometric distance.},
 author = {Nevalainen, Paavo and Movahedi, Parisa and Queralta, Jorge PeÃ±a and Westerlund, Tomi and Heikkonen, Jukka},
 booktitle = {New Developments and Environmental Applications of Drones},
 doi = {10.1007/978-3-030-77860-6_5},
 keywords = {},
 note = {http://arxiv.org/pdf/2101.00043},
 pages = {83-107},
 publisher = {},
 title = {Long-Term Autonomy in Forest Environment Using Self-Corrective SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1142255881},
 year = {2022}
}

@article{pub.1142288495,
 abstract = {High-precision and robust localization is the key issue for long-term and autonomous navigation of mobile robots in industrial scenes. In this article, we propose a high-precision and robust localization system based on laser and artificial landmarks. The proposed localization system is mainly composed of three modules, namely scoring mechanism-based global localization module, laser and artificial landmark-based localization module, and relocalization trigger module. Global localization module processes the global map to obtain the map pyramid, thus improve the global localization speed and accuracy when robots are powered on or kidnapped. Laser and artificial landmark-based localization module is employed to achieve robust localization in highly dynamic scenes and high-precision localization in target areas. The relocalization trigger module is used to monitor the current localization quality in real time by matching the current laser scan with the global map and feeds it back to the global localization module to improve the robustness of the system. Experimental results show that our method can achieve robust robot localization and real-time detection of the current localization quality in indoor scenes and industrial environment. In the target area, the position error is less than 0.004 m and the angle error is less than 0.01 rad.},
 author = {Wang, Jibo and Li, Chengpeng and Li, Bangyu and Pang, Chenglin and Fang, Zheng},
 doi = {10.1177/17298814211047690},
 journal = {International Journal of Advanced Robotic Systems},
 keywords = {},
 number = {5},
 pages = {17298814211047690},
 title = {High-precision and robust localization system for mobile robots in complex and large-scale indoor scenes},
 url = {https://app.dimensions.ai/details/publication/pub.1142288495},
 volume = {18},
 year = {2021}
}

@article{pub.1142738609,
 abstract = {The environment of most real-world scenarios such as malls and supermarkets
changes at all times. A pre-built map that does not account for these changes
becomes out-of-date easily. Therefore, it is necessary to have an up-to-date
model of the environment to facilitate long-term operation of a robot. To this
end, this paper presents a general lifelong simultaneous localization and
mapping (SLAM) framework. Our framework uses a multiple session map
representation, and exploits an efficient map updating strategy that includes
map building, pose graph refinement and sparsification. To mitigate the
unbounded increase of memory usage, we propose a map-trimming method based on
the Chow-Liu maximum-mutual-information spanning tree. The proposed SLAM
framework has been comprehensively validated by over a month of robot
deployment in real supermarket environment. Furthermore, we release the dataset
collected from the indoor and outdoor changing environment with the hope to
accelerate lifelong SLAM research in the community. Our dataset is available at
https://github.com/sanduan168/lifelong-SLAM-dataset.},
 author = {Zhao, Min and Guo, Xin and Song, Le and Qin, Baoxing and Shi, Xuesong and Lee, Gim Hee and Sun, Guanghui},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {A General Framework for Lifelong Localization and Mapping in Changing
  Environment},
 url = {https://app.dimensions.ai/details/publication/pub.1142738609},
 volume = {},
 year = {2021}
}

@article{pub.1143204860,
 abstract = {Accurate localization and mapping in a large-scale environment is an essential system of an autonomous vehicle. The difficulty of the previous LiDAR or LiDAR-inertial simultaneous localization and mapping (SLAM) methods is correcting long-term drift error in a large-scale environment. This paper proposes a novel approach of a large-scale, graph-based SLAM with traffic sign data involved in a high-definition (HD) map. The graph of the system is structured with the inertial measurement unit (IMU) factor, LiDAR-inertial odometry factor, map-matching factor, and loop closure factor. The local sliding window-based optimization method is employed for real-time processing. As a result, the proposed method improves the accuracy of the localization and mapping compared with the state-of-the-art LiDAR or LiDAR-inertial SLAM methods. In addition, the proposed method can localize accurately without revisit, required for conventional graph-based SLAM for graph optimization, unlike previous studies. The proposed method is intensively validated with a data set collected in a city where the Global Navigation Satellite System (GNSS) signal is unreliable and on a university campus.},
 author = {Sung, Changki and Jeon, Seulgi and Lim, Hyungtae and Myung, Hyun},
 doi = {10.1007/s11370-021-00395-2},
 journal = {Intelligent Service Robotics},
 keywords = {},
 number = {},
 pages = {1-10},
 title = {What if there was no revisit? Large-scale graph-based SLAM with traffic sign detection in an HD map using LiDAR inertial odometry},
 url = {https://app.dimensions.ai/details/publication/pub.1143204860},
 volume = {},
 year = {}
}

@inproceedings{pub.1143317975,
 abstract = {Nowadays the question of Moon exploration is one of the key priorities. Many Lunar robotics missions are planned in near future by different space agencies around the world. Moon has considered to be the best place for a research station with long-term human presence for finding answers on fundamental questions about the universe. Automatic navigation of starship during a landing phase on Lunar surface is already solved with a help of inertial reference system aided visual algorithms. However, questions of automatic navigation of moving and flying vehicles on the Lunar surface are still open. Inertial navigation is limited by time, self-localization and mapping algorithms require multiple unique features of relief to guarantee required accuracy for successful automatic mission complication. In the current study, we propose the deployment of a network of visual navigational aids on the Lunar surface to support ground automatic missions. A weak atmosphere of the Moon makes effective visual beacons navigation system for long areas. A network of navigational aids includes primary and secondary ground stations which are blinking synchronously. Synchronization is supported by radio waves from the primary ground station. We consider the nature of crater relief to increase operational area of the system. The Time Difference of Arrival method is used to detect vehicle position by blinking network of visual navigational aids. In the numerical application, we consider different scenarios of network configuration to support automatic vehicle navigation inside of Tycho crater. Also, deployment of visual navigational aids network will increase the number of optical features which improve performance of already used positioning methods.},
 author = {Ostroumov, Ivan and Kuzmenko, Nataliia},
 booktitle = {2021 IEEE 6th International Conference on Actual Problems of Unmanned Aerial Vehicles Development (APUAVD)},
 doi = {10.1109/apuavd53804.2021.9615417},
 keywords = {},
 pages = {71-75},
 title = {Vehicle Navigation by Visual Navigational Aids for Automatic Lunar Mission},
 url = {https://app.dimensions.ai/details/publication/pub.1143317975},
 year = {2021}
}

@article{pub.1143322236,
 abstract = {With the recent discovery of water-ice and lava tubes on the Moon and Mars along with the development of in-situ resource utilization (ISRU) technology, the recent planetary exploration has focused on rover (or lander)-based surface missions toward the base construction for long-term human exploration and habitation. However, a 3D terrain map, mostly based on orbiters' terrain images, has insufficient resolutions for construction purposes. In this regard, this paper introduces the visual simultaneous localization and mapping (SLAM)-based robotic mapping method employing a stereo camera system on a rover. In the method, S-PTAM is utilized as a base framework, with which the disparity map from the self-supervised deep learning is combined to enhance the mapping capabilities under homogeneous and unstructured environments of planetary terrains. The overall performance of the proposed method was evaluated in the emulated planetary terrain and validated with potential results.},
 author = {Hong, Sungchul and Bangunharcana, Antyanta and Park, Jae-Min and Choi, Minseong and Shin, Hyu-Soung},
 doi = {10.3390/s21227715},
 journal = {Sensors (Basel, Switzerland)},
 keywords = {},
 note = {https://www.mdpi.com/1424-8220/21/22/7715/pdf},
 number = {22},
 pages = {7715},
 title = {Visual SLAM-Based Robotic Mapping Method for Planetary Construction},
 url = {https://app.dimensions.ai/details/publication/pub.1143322236},
 volume = {21},
 year = {2021}
}

@inbook{pub.1143323749,
 abstract = {Abstract
As changes in external environments are inevitable, a lifelong mapping system is desirable for autonomous robots that aim at long-term operation. Capturing external environment changes into internal representations (for example, maps) is crucial for proper behavior and safety, especially in the case of autonomous vehicles. In this work, we propose a new large-scale mapping system for our autonomous vehicle or any other. The new mapping system is based on the Graph SLAM algorithm, with extensions to deal with the calibration of odometry directly in the optimization of the graph and to address map merging for long-term map maintenance. The mapping system can use sensor data from one or more robots to build and merge different types of occupancy grid maps. The systemâ€™s performance is evaluated in a series of experiments carried out with data captured in complex real-world scenarios. The experimental results indicate that the new large-scale mapping system can provide high-quality occupancy grid maps for later navigation and localization of autonomous vehicles that use occupancy grid maps.},
 author = {Oliveira, Josias and Mutz, Filipe and Forechi, Avelino and Azevedo, Pedro and Oliveira-Santos, Thiago and De Souza, Alberto F. and Badue, Claudine},
 booktitle = {Intelligent Systems},
 doi = {10.1007/978-3-030-91699-2_11},
 keywords = {},
 pages = {146-161},
 publisher = {},
 title = {Long-Term Map Maintenance in Complex Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1143323749},
 year = {2021}
}

@article{pub.1143552627,
 abstract = {The object of this paper is to create a system that can control any vehicle in any gaming environment to simulate, study, experiment and improve how self-driving vehicles operate. It is to be taken as the bases for future work on autonomous vehicles with real hardware devices. The long-term goal is to eliminate human error. Perception, localisation, planning and control subsystems were developed. LiDAR and RADAR sensors were used in addition to a normal web Camera. After getting information from the perception module, the system will be able to localise where the vehicle is, then the planning module is used to plan to which location the vehicle will move, using localisation module data to draw up the best path to use. After knowing the best path, the system will control the vehicle to move autonomously without human help. As a controller a Proportional Integral Derivative PID controller was used. Python programming language, computer vision, and machine learning were used in developing the system, where the only hardware required is a computer with a GPU and powerful graphical card that can run a game which has a vehicle, roads with lane lines and a map of the road. The developed system is intended to be a good tool in conducting experiments for achieving reliable autonomous vehicle navigation.},
 author = {Rutendo, M. and Al Akkad, M. A.},
 doi = {10.22213/2410-9304-2021-3-95-104},
 journal = {Intellekt. Sist. Proizv.},
 keywords = {},
 note = {http://izdat.istu.ru/index.php/ISM/article/download/4739/3051},
 number = {3},
 pages = {95-104},
 title = {Exploiting Machine Learning for Vision and Motion Planning of Autonomous Vehicles Navigation},
 url = {https://app.dimensions.ai/details/publication/pub.1143552627},
 volume = {19},
 year = {2021}
}

@article{pub.1143724954,
 abstract = {Plans for establishing a long-term human presence on the Moon will require substantial increases in robot autonomy and multirobot coordination to support establishing a lunar outpost. To achieve these objectives, algorithm design choices for the software developments need to be tested and validated for expected scenarios such as autonomous in situ resource utilization, localization in challenging environments, and multirobot coordination. However, real-world experiments are extremely challenging and limited for extraterrestrial environment. Also, realistic simulation demonstrations in these environments are still rare and demanded for initial algorithm testing capabilities. To help some of these needs, the NASA Centennial Challenges program established the Space Robotics Challenge Phase 2 (SRC2), which consist of virtual robotic systems in a realistic lunar simulation environment, where a group of mobile robots were tasked with reporting volatile locations within a global map, excavating and transporting these resources, and detecting and localizing a target of interest. The main goal of this article is to share our team's experiences on the design tradeoffs to perform autonomous robotic operations in a virtual lunar environment and to share strategies to complete the mission requirements posed by NASA SRC2 competition during the qualification round. Of the 114 teams that registered for participation in the NASA SRC2, team Mountaineers finished as one of only six teams to receive the top qualification round prize.},
 author = {Kilic, Cagri and R., Bernardo Martinez and Tatsch, Christopher A. and Beard, Jared and Strader, Jared and Das, Shounak and Ross, Derek and Gu, Yu and Pereira, Guilherme A. S. and Gross, Jason N.},
 doi = {10.1109/maes.2021.3115897},
 journal = {IEEE Aerospace and Electronic Systems Magazine},
 keywords = {},
 note = {http://arxiv.org/pdf/2109.09620},
 number = {12},
 pages = {24-41},
 title = {NASA Space Robotics Challenge 2 Qualification Round: An Approach to Autonomous Lunar Rover Operations},
 url = {https://app.dimensions.ai/details/publication/pub.1143724954},
 volume = {36},
 year = {2021}
}

@article{pub.1143758533,
 abstract = {In the long-term deployment of mobile robots, changing appearance brings challenges for localization. When a robot travels to the same place or restarts from an existing map, global localization is needed, where place recognition provides coarse position information. For visual sensors, changing appearances such as the transition from day to night and seasonal variation can reduce the performance of a visual place recognition system. To address this problem, we propose to learn domain-unrelated features across extreme changing appearance, where a domain denotes a specific appearance condition, such as a season or a kind of weather. We use an adversarial network with two discriminators to disentangle domain-related features and domain-unrelated features from images, and the domain-unrelated features are used as descriptors in place recognition. Provided images from different domains, our network is trained in a self-supervised manner which does not require correspondences between these domains. Besides, our feature extractors are shared among all domains, making it possible to contain more appearance without increasing model complexity. Qualitative and quantitative results on two toy cases are presented to show that our network can disentangle domain-related and domain-unrelated features from given data. Experiments on three public datasets and one proposed dataset for visual place recognition are conducted to illustrate the performance of our method compared with several typical algorithms. Besides, an ablation study is designed to validate the effectiveness of the introduced discriminators in our network. Additionally, we use a four-domain dataset to verify that the network can extend to multiple domains with one model while achieving similar performance.},
 author = {Tang, Li and Wang, Yue and Tan, Qimeng and Xiong, Rong},
 doi = {10.1177/17298814211037497},
 journal = {International Journal of Advanced Robotic Systems},
 keywords = {},
 note = {https://journals.sagepub.com/doi/pdf/10.1177/17298814211037497},
 number = {6},
 pages = {17298814211037497},
 title = {Explicit feature disentanglement for visual place recognition across appearance changes},
 url = {https://app.dimensions.ai/details/publication/pub.1143758533},
 volume = {18},
 year = {2021}
}

@article{pub.1143803532,
 abstract = {The article deals with certain medico-legal aspects of trauma in the salon of bus as one of the types of road traffic accidents with a large number of dead and injured. Are shown the typical causes of such incidents and the nature of the victims injury. Was developed and proposed a modern approach to optimization of expert research in case of appearance a large number of victims in the bus. Circumstances of injury in case of personal injury people in the bus are very diverse:âˆ’ rollover of the bus when transporting a large number of people while driving;âˆ’ bus falling from height;âˆ’ a massive collision with a fixed roadside objects; âˆ’ collision with other vehicles; among the latter is the most fatal bus collision with a moving train.Naturally, in these cases, the massive injuries have affected depends on the intensity of injury to passengers in the bus, and the mechanism of damage is determined by the specific form of an accident involving a bus. In such cases, the experts faced, usually with mechanical trauma inside the cabin, and mixed types of injuries passengers (e.g. in case of fire). For in-car trauma characterized by formation damage from the following mechanisms:âˆ’ shock bodies on the inner part of the interior (interior);âˆ’ injuries from the shards of broken glass.Basically, the nature of injury is determined by the structural features of the bus, the presence of foreign objects, the location of the victims. If the vehicle rolls over, the occupants people are numerous additional impact. Formed characteristic for the driver damage to the hands, fractures of the sternum fractures of the hips, legs and feet. For passengers is characteristic fractures of the lower limbs, bruised head wounds, fractures and dislocations of the cervical spine when using the seat belts âˆ’ stripe-like bruises and abrasions ofthe chest and abdomen, broken ribs, collarbone, sternum, in the projection of the belts. Shards of broken glass caused by the multiple linear abrasions and (or) surface or deep cut wounds mainly in the face and upper extremities. In the case of deformation of the bus body can be compression of the bodies are formed by damage to several areas, primarily the chest, abdomen, extremities, accompanied by multiple bilateral rib fractures, ruptures of internal organs. If in the future there is a fire or explosion of the vehicle, the nature of thedamage detected on the bodies will correspond to the combined injury.In cases of injuries in the bus to work with the bodies of the victims begins at the scene. Thus, the Protocol of inspection of the scene and of the corpse in the first place should reflect the data about the mutual position of bodies and (or) their fragments relative to the vehicle and other parts, the distance between them; the condition of clothing, odors from it, the presence of different overlays, damage; contamination of the skin; localization and nature of the injuries on the bodies, the presence of deformations of its individualparts; the presence of traces of biological origin on the vehicle in comparison with the nature of the deformation (damage) of the body.Be sure to note the results of the inspection of the road where there was a traffic accident, a bus traces of blood, and fragments of various things, etc. Despite the small percentage of bus injured in world General statistics of fatal injuries, it presents certain difficulties in planning, organization, execution and coordination of forensic work on multi-step liquidation of medical consequences of the accident, usually associated with a large number of victims, gross impact of factors affecting on the bodies of the victims, the need to quickly address some specific issues: establishing at autopsy pathological symptoms that indicate the status of the health of drivers in the period priorto the tragic event; the existence of facts pointing to the use of intoxicating and medicinal substances that depress the nervous system and many others), early identification of all victims. According to the results of the analysis made it impossible to offer modern, optimal, evidence-informed, and until only itremains to be reliable in practice the system approach to the organizational model of forensic activities, while ensuring the interests of the investigation of an accident involving a bus and a large number of victims:1. The preliminary stage of organization expert services. It can conditionally enough be divided into 2 phases:âˆ’ advance (pre-) phase;âˆ’ the immediate phase.To the basic questions of the early phases include: early development, coordination and approval of the optimal legislative and other regulatory framework; preliminary methodological, administrative and organizational, theoretical-practical, logistical, software and applied training; exercise reasonable estimates of projected short and long term needs and costs with regard to the peculiarities specified by the tragic events; creation, storage, use and replenishment of the trust reserves, logistical and financial resources areinviolable, is intended solely for use in such emergencies. It also includes the creation, maintenance and continuous improvement of a Single centralized situation center on a temporary or permanent basis, with a good system of departmental and interdepartmental cooperation, primarily containing a - operational information-Supervisory and analytical center for the collection, processing, storage, information exchange and joint action with the threat, occurrence and prevention of emergencies with a large number of victims.Immediately with the receipt of the news of the accident involving a bus and a large number of victims for forensic services begin immediate phase, the main elements of which include:âˆ’ prompt notification and collection of employees and expert institutions;âˆ’ an emergency conference call to discuss the organizational, theoretical and practical questions and short specialized trainingon occupational safety, including use of personal protective equipment depending on the nature of the accident and actions are potentially dangerous to health and life of employees and expert institutions factors.All plans of measures are necessarily coordinated and agreed with appropriate representatives of structures of fast reaction, especially when conducting urgent investigative actions in the emergency areas, primarily the inspection of the scene. 2. The inspection of the crime scene it is advisable to start with a preliminary review (Â«intelligenceÂ»), which finally determined the necessity of application of those or other technical means, and the number of specialists who will participate in the inspection.The static phase of scene examination with the participation of forensic doctors is accompanied by clear mapping; mapping, photo - and video fixing of vehicle, various objects; it is noted the exact relative positions of the bus (its parts) and discovered the corpses, fragments of human remains and other biological material. During dynamic examination of the scene produce a detailed external examination of the human remains, their fragments, biological material, perform primary medical sorting, their careful packaging,clear detailed marking. Then performed the proper loading, transportation and unloading. In case of need in a temporary Deposit of biological material, can be used in railway wagons refrigerators, refrigerated trailers, mobile camera with a refrigeration unit, and in the absence or lack of volume for the total number of remains and the biomaterial deploys heat-resistant boxes, fit the space with the use of outdoor mobile air conditioning systems, large amounts of ice obtained from specialized industrial ice makers, etc., which is especially important for braking processes of rotting corpses, their fragments and biomaterial in the warm season.3. After the initial registration and a secondary sort examine corpses, their fragments and biological material collection for postmortem identification of significant information, determine the cause of death, nature, mechanism and prescription of formation damage and address other special issues. At this stage also produce the identification of fragmented body parts and (or) tissues that or another body. In expert identification work on the fragments of human remains or biological material, preference is given to genetic research providing highly accurate results. Depending on the extent of influence of damaging factors on the bodies of the victims and their degree of preservation, only after the completion of the necessary is judicial-medical research with a full range of fence material for additional research, producing restoration of the exterior, embalming, sanitary and cosmetic processing of human remains and give them to relatives (relatives, authorized representatives, etc.) for burial. 4. Issued the final results of examinations; establishes data that may be useful for later investigative and judicial actions aimed at gathering and verification of evidence in a criminal case.5. The penultimate stage consists of conducting sanitary-and-hygienic, treatment-and-prophylactic, rehabilitation (including a full psychological) of interventions for physical and mental health of employees and expert institutions involved in this work.6. After the conclusion of the criminal proceedings in general, with the official opening of data access, it is advisable to analyze the material, and publish the relevant data in the scientific literature, with the goal of widespread study and use of gained experience.CONCLUSIONS.1. Research platform forensic activities in cases of accidents involving buses and a large number of victims to date have not been developed.2. The effectiveness of forensic medical groups in this situation is in direct proportion to the degree of readiness for quick response and timely quality completion of tasks.3. Based on this, very urgent is the development of modern optimal evidence-based systemic approach to the organizational model of forensic activities in the presence of a large number of injured persons in the bus; the solution to this problem and sent the above recommendations.4. The recommendations, in principle, can be applied not only in cases of injuries in the bus, but also to similar situations in which there is a massive injury and loss of life.5. It is necessary to continue scientific and practical research aimed at improving this algorithm works experts.},
 author = {Ð’Ð°Ñ€ÑÐ°Ð½, Ð•Ð²Ð³ÐµÐ½},
 doi = {10.24061/2707-8728.1.2017.7},
 journal = {Ð¡ÑƒÐ´Ð¾Ð²Ð¾-Ð¼ÐµÐ´Ð¸Ñ‡Ð½Ð° ÐµÐºÑÐ¿ÐµÑ€Ñ‚Ð¸Ð·Ð°},
 keywords = {},
 note = {http://forensic.bsmu.edu.ua/article/download/243985/241982},
 number = {1},
 pages = {31-37},
 title = {ÐžÑÐ¾Ð±ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÑÑƒÐ´ÐµÐ±Ð½Ð¾-Ð¼ÐµÐ´Ð¸Ñ†Ð¸Ð½ÑÐºÐ¸Ñ… ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð½Ñ‹Ñ… Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹ Ð² ÑÐ»ÑƒÑ‡Ð°ÑÑ… Ð¼Ð°ÑÑÐ¾Ð²Ð¾Ð³Ð¾ Ñ‚Ñ€Ð°Ð²Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð¾ÑÑ‚Ñ€Ð°Ð´Ð°Ð²ÑˆÐ¸Ñ… Ð² ÑÐ°Ð»Ð¾Ð½Ðµ Ð°Ð²Ñ‚Ð¾Ð±ÑƒÑÐ°},
 url = {https://app.dimensions.ai/details/publication/pub.1143803532},
 volume = {},
 year = {2017}
}

@inproceedings{pub.1143940659,
 abstract = {The environment of most real-world scenarios such as malls and supermarkets changes at all times. A pre-built map that does not account for these changes becomes out-of-date easily. Therefore, it is necessary to have an up-to-date model of the environment to facilitate long-term operation of a robot. To this end, this paper presents a general lifelong simultaneous localization and mapping (SLAM) framework. Our framework uses a multiple session map representation, and exploits an efficient map updating strategy that includes map building, pose graph refinement and sparsification. To mitigate the unbounded increase of memory usage, we propose a map-trimming method based on the Chow-Liu maximum-mutual-information spanning tree. The proposed SLAM framework has been comprehensively validated by over a month of robot deployment in real supermarket environment. Furthermore, we release the dataset collected from the indoor and outdoor changing environment with the hope to accelerate lifelong SLAM research in the community. Our dataset is available at https://github.com/sanduan168/lifelong-SLAM-dataset.},
 author = {Zhao, Min and Guo, Xin and Song, Le and Qin, Baoxing and Shi, Xuesong and Lee, Gim Hee and Sun, Guanghui},
 booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros51168.2021.9635985},
 keywords = {},
 note = {http://arxiv.org/pdf/2111.10946},
 pages = {3305-3312},
 title = {A General Framework for Lifelong Localization and Mapping in Changing Environment},
 url = {https://app.dimensions.ai/details/publication/pub.1143940659},
 year = {2021}
}

@inproceedings{pub.1143940665,
 abstract = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed methodâ€™s reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack},
 author = {Wen, Bowen and Bekris, Kostas},
 booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros51168.2021.9635991},
 keywords = {},
 note = {http://arxiv.org/pdf/2108.00516},
 pages = {8067-8074},
 title = {BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models},
 url = {https://app.dimensions.ai/details/publication/pub.1143940665},
 year = {2021}
}

@inproceedings{pub.1143941203,
 abstract = {Lifelong SLAM considers long-term operation of a robot where already mapped locations are revisited many times in changing environments. As a result, traditional graph-based SLAM approaches eventually become extremely slow due to the continuous growth of the graph and the loss of sparsity. Both problems can be addressed by a graph pruning algorithm. It carefully removes vertices and edges to keep the graph size reasonable while preserving the information needed to provide good SLAM results. We propose a novel method that considers geometric criteria for choosing the vertices to be pruned. It is efficient, easy to implement, and leads to a graph with evenly spread vertices that remain part of the robot trajectory. Furthermore, we present a novel approach of marginalization that is more robust to wrong loop closures than existing methods. The proposed algorithm is evaluated on two publicly available real-world long-term datasets and compared to the unpruned case as well as ground truth. We show that even on a long dataset (25h), our approach manages to keep the graph sparse and the speed high while still providing good accuracy (40 times speed up, 6cm map error compared to unpruned case).},
 author = {Kurz, Gerhard and Holoch, Matthias and Biber, Peter},
 booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros51168.2021.9636530},
 keywords = {},
 note = {http://arxiv.org/pdf/2110.01286},
 pages = {3313-3320},
 title = {Geometry-based Graph Pruning for Lifelong SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1143941203},
 year = {2021}
}

@inproceedings{pub.1143941313,
 abstract = {Visual place recognition (VPR) is critical in not only localization and mapping for autonomous driving vehicles, but also assistive navigation for the visually impaired population. To enable a long-term VPR system on a large scale, several challenges need to be addressed. First, different applications could require different image view directions, such as front views for self-driving cars while side views for the low vision people. Second, VPR in metropolitan scenes can often cause privacy concerns due to the imaging of pedestrian and vehicle identity information, calling for the need for data anonymization before VPR queries and database construction. Both factors could lead to VPR performance variations that are not well understood yet. To study their influences, we present the NYU-VPR dataset that contains more than 200,000 images over a 2kmÃ—2km area near the New York University campus, taken within the whole year of 2016. We present benchmark results on several popular VPR algorithms showing that side views are significantly more challenging for current VPR methods while the influence of data anonymization is almost negligible, together with our hypothetical explanations and in-depth analysis.},
 author = {Sheng, Diwei and Chai, Yuxiang and Li, Xinru and Feng, Chen and Lin, Jianzhe and Silva, Claudio and Rizzo, John-Ross},
 booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros51168.2021.9636640},
 keywords = {},
 note = {http://arxiv.org/pdf/2110.09004},
 pages = {9773-9779},
 title = {NYU-VPR: Long-Term Visual Place Recognition Benchmark with View Direction and Data Anonymization Influences},
 url = {https://app.dimensions.ai/details/publication/pub.1143941313},
 year = {2021}
}

@inproceedings{pub.1143941487,
 abstract = {Progress in the last decade has brought about significant improvements in the accuracy and speed of SLAM systems, broadening their mapping capabilities. Despite these advancements, long-term operation remains a major challenge, primarily due to the wide spectrum of perturbations robotic systems may encounter. Increasing the robustness of SLAM algorithms is an ongoing effort, however it usually addresses a specific perturbation. Generalisation of robustness across a large variety of challenging scenarios is not well-studied nor understood. This paper presents a systematic evaluation of the robustness of open-source state-of-the-art SLAM algorithms with respect to challenging conditions such as fast motion, non-uniform illumination, and dynamic scenes. The experiments are performed with perturbations present both independently of each other, as well as in combination in long-term deployment settings in unconstrained environments (lifelong operation). The detailed results (approx. 20,000 experiments) along with comprehensive documentation of the benchmarking tool for integrating new datasets and evaluating SLAM algorithms not studied in this work are available at https://robustslam.github.io/evaluation.},
 author = {Bujanca, Mihai and Shi, Xuesong and Spear, Matthew and Zhao, Pengpeng and Lennox, Barry and LujÃ¡n, Mikel},
 booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 doi = {10.1109/iros51168.2021.9636814},
 keywords = {},
 note = {http://arxiv.org/pdf/2109.13160},
 pages = {5320-5327},
 title = {Robust SLAM Systems: Are We There Yet?},
 url = {https://app.dimensions.ai/details/publication/pub.1143941487},
 year = {2021}
}

@article{pub.1144088657,
 abstract = {In this letter, we learn visual features that we use to first build a map and then localize a robot driving autonomously across a full day of lighting change, including in the dark. We train a neural network to predict sparse keypoints with associated descriptors and scores that can be used together with a classical pose estimator for localization. Our training pipeline includes a differentiable pose estimator such that training can be supervised with ground truth poses from data collected earlier, in our case from 2016 and 2017 gathered with multi-experience Visual Teach and Repeat (VT&R). We insert the learned features into the existing VT&R pipeline to perform closed-loop path following in unstructured outdoor environments. We show successful path following across all lighting conditions despite the robots map being constructed using daylight conditions. Moreover, we explore generalizability of the features by driving the robot across all lighting conditions in new areas not present in the feature training dataset. In all, we validated our approach with 35.5 km of autonomous path following experiments in challenging conditions.},
 author = {Gridseth, Mona and Barfoot, Timothy D.},
 doi = {10.1109/lra.2021.3136867},
 journal = {IEEE Robotics and Automation Letters},
 keywords = {},
 note = {http://arxiv.org/pdf/2109.04041},
 number = {2},
 pages = {1016-1023},
 title = {Keeping an Eye on Things: Deep Learned Features for Long-Term Visual Localization},
 url = {https://app.dimensions.ai/details/publication/pub.1144088657},
 volume = {7},
 year = {2022}
}

@inproceedings{pub.1144172180,
 abstract = {For long-term planning, localization and mapping, the robot must constantly update the map by the changing environment and new areas that the robot is exploring. At the same time, this map should not take up too much of the robotâ€™s memory, since the robotâ€™s performance is limited due to the small size of the robot and increased performance requirements. The robot must interact with the map on time, updating its location to build a further route to explore areas that have not been visited. In addition to compiling a map, when solving the problem of exploration rooms, the following steps are also important: forming a plan for bypassing an unknown room, calculating the trajectory, resolving collisions with obstacles, and following the trajectory. In the course of this work, an autonomous robotic system was developed, the task of which is to map previously unknown premises. For this, SPLAM algorithms, algorithms for building map and working with graphs, algorithms for following a trajectory were used.},
 author = {Kozlov, Daniil and Myasnikov, Vladislav},
 booktitle = {2021 International Conference on Information Technology and Nanotechnology (ITNT)},
 doi = {10.1109/itnt52450.2021.9649028},
 keywords = {},
 pages = {1-5},
 title = {Development of an Autonomous Robotic System Using the Graph-based SPLAM Algorithm},
 url = {https://app.dimensions.ai/details/publication/pub.1144172180},
 year = {2021}
}

@inproceedings{pub.1144211410,
 abstract = {During simultaneous localization and mapping, the robot should build a map of its surroundings and simultaneously estimate its pose in the generated map. However, a fundamental task is to detect loops, i.e., previously visited areas, allowing consistent map generation. Moreover, within long-term mapping, every autonomous system needs to address its scalability in terms of storage requirements and database search. In this paper, we present a low-complexity sequence-based visual loop-closure detection pipeline. Our system dynamically segments the traversed route through a feature matching technique in order to define sub-maps. In addition, visual words are generated incrementally for the corresponding sub-maps representation. Comparisons among these sequences-of-images are performed thanks to probabilistic scores originated from a voting scheme. When a candidate sub-map is indicated, global descriptors are utilized for image-to-image pairing. Our evaluation took place on several publicly-available datasets exhibiting the systemâ€™s low complexity and high recall compared to other state-of-the-art approaches.},
 author = {Tsintotas, Konstantinos A. and Bampis, Loukas and An, Shan and Fragulis, George F. and Mouroutsos, Spyridon G. and Gasteratos, Antonios},
 booktitle = {2021 IEEE International Conference on Imaging Systems and Techniques (IST)},
 doi = {10.1109/ist50367.2021.9651458},
 keywords = {},
 pages = {1-6},
 title = {Sequence-based mapping for probabilistic visual loop-closure detection},
 url = {https://app.dimensions.ai/details/publication/pub.1144211410},
 year = {2021}
}

@article{pub.1144543458,
 abstract = {In recent years, We are aware of many environmental problems, such as environmental destruction, reduction of wild animals. It is required to make use of autonomous robots for environmental monitoring to predict disasters and grasp situations. In this research, we consider image processing technology that contributes to environmental monitoring by WAMOT (Waseda Animal Monitoring Robot), an environmental monitoring robot developed for the purpose of creating environment and animal ecology maps.WAMOT uses a charging station to withstand long-term autonomous observation. When the battery of the robot runs short, it automatically returns to the charging station. As soon as charging is completed, repeat the operation of starting the observation again to grasp the surrounding environment. In this research, we aim to make this autonomous driving possible by using the visual information.We propose a method that enables stable self position estimation in irregular areas like forests by estimating the robot's state from the camera pose obtained by Large-Scale Direct Monocular SLAM (LSD-SLAM) and combining wheel odometry . As a result of the experiment, self - location estimation with higher reliability than wheel odometry alone could be performed.},
 author = {è£•ä¹‹, è—¤æœ¬ and è£•ä¹‹, çŸ³äº• and æ·³, å¤§è°· and æ·³å¸, å¤§å’Œ and æ·³å¤«, é«˜è¥¿},
 doi = {10.11371/aiieej.45.0_20},
 journal = {},
 keywords = {},
 number = {},
 pages = {20-20},
 title = {ç’°å¢ƒãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ­ãƒœãƒƒãƒˆã®è¦–è¦šæƒ…å ±å‡¦ç†ã‚’ç”¨ã„ãŸæ£®æž—ã«ãŠã‘ã‚‹èµ·ç‚¹ã¸ã®å¸°é‚„æ³•ã®æ¤œè¨Ž},
 url = {https://app.dimensions.ai/details/publication/pub.1144543458},
 volume = {},
 year = {2022}
}

@article{pub.1144544784,
 abstract = {In recent years, we are aware of many environmental problems, such as environmental destruction,reduction of wild animals. It is required to make use of autonomous robots for environmental monitoring topredict disasters and grasp situations.In this research, we consider image processing technology that contributes to environmental monitoring byWAMOT (Waseda Animal Monitoring Robot), an environmental monitoring robot developed for creatingenvironment and animal ecology maps. WAMOT uses a charging station to withstand long-term autonomousobservation. When the battery of the robot runs short, it automatically returns to the charging station. Assoon as charging is completed, repeat the operation of starting the observation again to grasp thesurrounding environment. In this research, we aim to make the Path Planning Based on SLAM andSemantic Segmentation by Deep Learning.We propose a method the path planning that based on SLAM and cost map reflecting the result of SemanticSegmentation. SLAM in irregular areas like forests is estimating the robot's state from the stable selfposition estimation and combining wheel odometry. Cost map based on the environment map (self positionestimation result) and the estimation result by the deep learning from RGB image. As a result of theexperiment, path planning from cost map generation reflecting the result of environment estimation couldbe performed.},
 author = {ç´”çŸ¢, æ£®æœ¬ and æ‹“å“‰, æž— and è£•ä¹‹, è—¤æœ¬ and è£•ä¹‹, çŸ³äº• and æ·³, å¤§è°· and æ·³å¸, å¤§å’Œ and æ·³å¤«, é«˜è¥¿},
 doi = {10.11371/aiieej.46.0_3},
 journal = {},
 keywords = {},
 number = {},
 pages = {3-3},
 title = {æ£®æž—ç’°å¢ƒãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ­ãƒœãƒƒãƒˆã«ãŠã‘ã‚‹ SLAM ã¨ æ·±å±¤å­¦ç¿’ã‚’ç”¨ã„ãŸé ˜åŸŸæŽ¨å®šã«åŸºã¥ãçµŒè·¯è¨ˆç”»æ³•ã®æ¤œè¨Ž},
 url = {https://app.dimensions.ai/details/publication/pub.1144544784},
 volume = {},
 year = {2022}
}

@article{pub.1144782292,
 abstract = {Mapping and localization are two essential tasks for mobile robots in
real-world applications. However, largescale and dynamic scenes challenge the
accuracy and robustness of most current mature solutions. This situation
becomes even worse when computational resources are limited. In this paper, we
present a novel lightweight object-level mapping and localization method with
high accuracy and robustness. Different from previous methods, our method does
not need a prior constructed precise geometric map, which greatly releases the
storage burden, especially for large-scale navigation. We use object-level
features with both semantic and geometric information to model landmarks in the
environment. Particularly, a learning topological primitive is first proposed
to efficiently obtain and organize the object-level landmarks. On the basis of
this, we use a robot-centric mapping framework to represent the environment as
a semantic topology graph and relax the burden of maintaining global
consistency at the same time. Besides, a hierarchical memory management
mechanism is introduced to improve the efficiency of online mapping with
limited computational resources. Based on the proposed map, the robust
localization is achieved by constructing a novel local semantic scene graph
descriptor, and performing multi-constraint graph matching to compare scene
similarity. Finally, we test our method on a low-cost embedded platform to
demonstrate its advantages. Experimental results on a large scale and
multi-session real-world environment show that the proposed method outperforms
the state of arts in terms of lightweight and robustness.},
 author = {Wang, Fan and Zhang, Chaofan and Tang, Fulin and Jiang, Hongkui and Wu, Yihong and Liu, Yong},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Lightweight Object-level Topological Semantic Mapping and Long-term
  Global Localization based on Graph Matching},
 url = {https://app.dimensions.ai/details/publication/pub.1144782292},
 volume = {},
 year = {2022}
}

@article{pub.1144947689,
 abstract = {In a Global Navigation Satellite System (GNSS)-restricted area, a mobile robot navigation system exploits surrounding environment information. For an aerial or underwater vehicle, undulating terrain of a land or seabed surface is a valuable information resource that leads to the development of terrain-referenced navigation (TRN) algorithms. However, due to the vast amount of a vehicleâ€™s activity area, surveying all the regions to obtain a high-resolution terrain map is impractical and requires simultaneous localization and mapping (SLAM) as a highly desirable capability. This paper presents a topographic SLAM algorithm using only a single terrain altimeter, which is low-cost, computationally efficient, and sufficiently stable for long-term operation. The proposed rectangular panel map structure and update method enable robust and efficient SLAM. As terrain elevation changes are inherently nonlinear, an extended Kalman filter (EKF)-based SLAM filter is adopted. The feasibility and validity of the proposed algorithm are demonstrated through simulations using terrain elevation data from a real-world undersea environment.},
 author = {Jang, Junwoo and Kim, Jinwhan},
 doi = {10.1109/access.2022.3145978},
 journal = {IEEE Access},
 keywords = {},
 note = {https://ieeexplore.ieee.org/ielx7/6287639/9668973/09690856.pdf},
 number = {},
 pages = {10806-10815},
 title = {Topographic SLAM Using a Single Terrain Altimeter in GNSS-Restricted Environment},
 url = {https://app.dimensions.ai/details/publication/pub.1144947689},
 volume = {10},
 year = {2022}
}

@article{pub.1145072481,
 abstract = {Smart living is an emerging technology that has attracted a lot of attention all around the world. As a key technology of smart space, which is the principal part of smart living, the SLAM system has effectively expanded the ability of space intelligent robots to explore unknown environments. Loop closure detection is an important part of SLAM system and plays a very important role in eliminating cumulative errors. The SLAM system without loop closure detection is degraded to an odometer. The state estimation solely relying on an odometer will be seriously deviated in the long-term and large-scale navigation and positioning. This paper proposes a metric learning method that uses deep neural networks for loop closure detection based on triplet loss. The map points obtained by metric learning are fused with all map points in the current keyframe, and the map points that do not meet the filtering conditions are eliminated. Based on the Batch Hard Triplet loss, the weighted triplet loss function avoids suboptimal convergence in the learning process by applying weighted value constraints. At the same time, considering that fixed boundary parameters cannot be well adapted to the diversity of scales between different samples, we use the semantic similarity of anchor samples and negative samples to redefine boundary parameters. Finally, a SLAM system based on metric learning is constructed, and the SLAM dataset TUM and KITTI are used to evaluate the proposed modelâ€™s accuracy rate and recall rate. The scene features in this method are extracted automatically through neural networks instead of being artificially set. Finally, a high-precision closed-loop detection method based on weight adaptive triple loss is effectively realised through the closed-loop detection experiment. The minimum relative pose error is 0.00048Â m, which is 15.8% less than that of the closed-loop detection algorithm based on the word bag model.},
 author = {Dong, Na and Qin, Minghui and Chang, Jianfang and Wu, C.H. and Ip, W.H. and Yung, K.L.},
 doi = {10.1016/j.comcom.2022.01.013},
 journal = {Computer Communications},
 keywords = {},
 number = {},
 pages = {153-165},
 title = {Weighted triplet loss based on deep neural networks for loop closure detection in VSLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1145072481},
 volume = {186},
 year = {2022}
}

@article{pub.1145304056,
 abstract = {A fundamental task for mobile robots is simultaneous localization and mapping (SLAM). Moreover, long-term robustness is an important property for SLAM. When vehicles or robots steer fast or steer in certain scenarios, such as low-texture environments, long corridors, tunnels, or other duplicated structural environments, most SLAM systems might fail. In this paper, we propose a novel robust visual inertial light detection and ranging (LiDaR) navigation (VILN) SLAM system, including stereo visual-inertial LiDaR odometry and visual-LiDaR loop closure. The proposed VILN SLAM system can perform well with low drift after long-term experiments, even when the LiDaR or visual measurements are degraded occasionally in complex scenes. Extensive experimental results show that the robustness has been greatly improved in various scenarios compared to state-of-the-art SLAM systems.},
 author = {Wei, Wei and Zhu, Xiaorui and Wang, Yi},
 doi = {10.1631/fitee.2000358},
 journal = {Frontiers of Information Technology & Electronic Engineering},
 keywords = {},
 number = {2},
 pages = {234-245},
 title = {Novel robust simultaneous localization and mapping for long-term autonomous robots},
 url = {https://app.dimensions.ai/details/publication/pub.1145304056},
 volume = {23},
 year = {}
}

@inbook{pub.1145467516,
 abstract = {Long-term autonomy is of great importance in various real-world applications of aerial robotics, including, but not limited to, search and rescue missions in underground mines, detection, and monitoring of victims trapped under collapsed buildings, aerial radiation detection in nuclear power plants after an accident, or extraterrestrial explorations. Due to the limited energy storage capacity and inefficiency of rechargeable batteries, drones must execute their missions as fast as possible with the most efficient machine learning algorithms for perception, planning, and control tasks. Motivated by the aforementioned need, agile aerial robots have gained increasing interest in the robotics community. Autonomous drone racing (ADR), which is one of the most challenging robotics problems, is an appropriate testbed for benchmarking efficiency of developed machine learning algorithms as well as their required sensors. ADR problem is chosen to demonstrate the value of the developed deep neural network-based algorithms as drones require low-latency of processing, since they have to maneuver and react to the changes in the environment rapidly throughout the race. In this chapter, we introduce deep learning methods for perception and planning of aerial robots as applied to ADR problem. We present two distinct approaches: system decomposition and end-to-end planning. In the former, we present how convolutional neural networks are used for gate localization and gate pose estimation in perception, either by predicting a gate's center or building a 3D global gate map. In the latter, we show how to bypass the perceive-plan-act subtasks and utilize deep learning methods, such as transfer learning and reinforcement learning, to learn directly from raw images and calculate desired robot actions. Furthermore, a few useful simulation tools and data sets are introduced for ADR to develop, validate, and evaluate novel algorithms. We believe developed algorithms for ADR can be transferred to other domains in which drones must navigate in a cluttered environment without a need for external sensing.},
 author = {Pham, Huy Xuan and Ugurlu, Halil Ibrahim and Le Fevre, Jonas and Bardakci, Deniz and Kayacan, Erdal},
 booktitle = {Deep Learning for Robot Perception and Cognition},
 doi = {10.1016/b978-0-32-385787-1.00020-8},
 keywords = {},
 pages = {371-406},
 publisher = {},
 title = {Chapter 15 Deep learning for vision-based navigation in autonomous drone racing},
 url = {https://app.dimensions.ai/details/publication/pub.1145467516},
 year = {2022}
}

@article{pub.1145523295,
 abstract = {Simultaneous localization and mapping (SLAM) is crucial for autonomous mobile robots. Most of the current SLAM systems are based on an assumption: the environment is static. However, the real environment is full of dynamic elements, such as pedestrians or vehicles, as well as changes in illumination and appearance over time. In this paper, DEâ€SLAM, a visual SLAM system that can deal with shortâ€term and longâ€term dynamic elements at the same time is proposed. A novel dynamic detection and tracking module that utilizes both semantic and metric information is proposed, and the localization accuracy is highly improved by eliminating features falling on the dynamic objects. A unified loop detection, loop check and global optimization module is used to perform loop closure. Experimental results on datasets and real environments show that DEâ€SLAM outperforms other stateâ€ofâ€theâ€art SLAM systems in dynamic environments.},
 author = {Xing, Zhiwei and Zhu, Xiaorui and Dong, Dingcheng},
 doi = {10.1002/rob.22062},
 journal = {Journal of Field Robotics},
 keywords = {},
 number = {},
 pages = {},
 title = {DEâ€SLAM: SLAM for highly dynamic environment},
 url = {https://app.dimensions.ai/details/publication/pub.1145523295},
 volume = {},
 year = {2022}
}

@inproceedings{pub.1145646880,
 abstract = {Many scientific mapping surveys that deploy robotic platforms in underwater and polar environments perform Visual Simultaneous Localization and Mapping (VSLAM), Structure for Motion (SfM), and Image Mosaicking. These techniques heavily rely on robust and reliable feature-based vision front ends. The job of a vision front end is to provide correspondence information between different camera views which is then directly fed into a bundle adjustment step. Although the atomic steps involved in constructing a vision front end are well-known, many of the popular choices of the features and their parameters do not perform reliably in a variety of visually degraded underwater and polar environments that are characterized by low texture and contrast, and by unevenly lit and low-overlap imagery. In this paper, we develop novel metrics and quantitative analysis methods which can measure the impact of image pre-processing steps such as Contrast Limited Adaptive Histogram Equalization (CLAHE) on the improvement of vision front-end outputs. Our metrics and quantitative analysis can guide the selection between different feature detectors and descriptors to develop a reliable and robust vision front end that can operate in a wider range of underwater and polar environments. We showcase how CLAHE improves the saliency of features and feature track length on a visually degraded dataset underwater, resulting in a substantial increase in correspondence information for the SfM solution. Finally, we perform an end-to-end SfM analysis that shows reduced accumulated drift over the long term and improved accuracy.},
 author = {Shah, Vikrant and Nir, Jagatpreet and Kaveti, Pushyami and Singh, Hanumant},
 booktitle = {OCEANS 2021: San Diego â€“ Porto},
 doi = {10.23919/oceans44145.2021.9705975},
 keywords = {},
 pages = {1-7},
 title = {Performance Analysis of Feature Detectors and Descriptors in Underwater and Polar Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1145646880},
 year = {2021}
}

@inproceedings{pub.1145733477,
 abstract = {In recent years, robots have been widely used in the service industry to improve work efficiency. An indoor service robot should have long-term autonomous adaptability and this is achieved by performing lifelong simultaneous localization and mapping (SLAM). However, when a robot wakes up to SLAM, it needs to relocate itself first. In real-world applications, the visual ambiguous environment which contains multiple locations with similar appearances or features is a challenging scenario for localization. Considering the insufficient expression ability of a single sensor, this paper proposes a bio-inspired relocalization method to deal with this problem. The local view cells model maintains multi-hypotheses to provide visual-based coarse relocalization. Then, an obstacle cell model converts the self-centered lidar information into allocentric representation. And it serves as the fine relocalization. A continuous attractor neural network (CANN) is applied to integrate the candidates obtained through the above two stages. Finally, the result of relocalization is selected by a double-checking mechanism. In the experiments, the success rate of the proposed method reaches 95 % in total and 75% in the most challenging scene. The translation error in all scenes is less than 0.15m.},
 author = {Long, Yixuan and Jiang, Rui and Ye, Fang},
 booktitle = {2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE)},
 doi = {10.1109/iccece54139.2022.9712794},
 keywords = {},
 pages = {685-689},
 title = {Bio-inspired Relocalization for Indoor Robots in Visual Ambiguous Scenarios},
 url = {https://app.dimensions.ai/details/publication/pub.1145733477},
 year = {2022}
}

@article{pub.1146046628,
 abstract = {While lifelong SLAM addresses the capability of a robot to adapt to changes
within a single environment over time, in this paper we introduce the task of
continual SLAM. Here, a robot is deployed sequentially in a variety of
different environments and has to transfer its knowledge of previously
experienced environments to thus far unseen environments, while avoiding
catastrophic forgetting. This is particularly relevant in the context of
vision-based approaches, where the relevant features vary widely between
different environments. We propose a novel approach for solving the continual
SLAM problem by introducing CL-SLAM. Our approach consists of a dual-network
architecture that handles both short-term adaptation and long-term memory
retention by incorporating a replay buffer. Extensive evaluations of CL-SLAM in
three different environments demonstrate that it outperforms several baselines
inspired by existing continual learning-based visual odometry methods. The code
of our work is publicly available at http://continual-slam.cs.uni-freiburg.de.},
 author = {VÃ¶disch, Niclas and Cattaneo, Daniele and Burgard, Wolfram and Valada, Abhinav},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Continual SLAM: Beyond Lifelong Simultaneous Localization and Mapping
  through Continual Learning},
 url = {https://app.dimensions.ai/details/publication/pub.1146046628},
 volume = {},
 year = {2022}
}

@article{pub.1146120646,
 abstract = {Place recognition is an important capability for autonomously navigating
vehicles operating in complex environments and under changing conditions. It is
a key component for tasks such as loop closing in SLAM or global localization.
In this paper, we address the problem of place recognition based on 3D LiDAR
scans recorded by an autonomous vehicle. We propose a novel lightweight neural
network exploiting the range image representation of LiDAR sensors to achieve
fast execution with less than 4 ms per frame. Based on that, we design a
yaw-rotation-invariant architecture exploiting a transformer network, which
boosts the place recognition performance of our method. We evaluate our
approach on the KITTI and Ford Campus datasets. The experimental results show
that our method can effectively detect loop closures compared to the
state-of-the-art methods and generalizes well across different environments. To
further evaluate long-term place recognition performance, we provide a novel
challenging Haomo dataset, which contains LiDAR sequences recorded by a mobile
robot in repetitive places across seasons. Both the implementation of our
method and our new Haomo dataset are released here:
https://github.com/haomo-ai/OverlapTransformer},
 author = {Ma, Junyi and Zhang, Jun and Xu, Jintao and Ai, Rui and Gu, Weihao and Stachniss, Cyrill and Chen, Xieyuanli},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {OverlapTransformer: An Efficient and Rotation-Invariant Transformer
  Network for LiDAR-Based Place Recognition},
 url = {https://app.dimensions.ai/details/publication/pub.1146120646},
 volume = {},
 year = {2022}
}

@article{pub.1146188448,
 abstract = {In environments where GPS signals are denied, signals of opportunity (SOPs) could serve as an alternative positioning, navigation, and timing (PNT) source to GPS, and more generally, to global navigation satellite systems (GNSS). This paper presents a radio simultaneous localization and mapping (radio SLAM) approach that enables the exploitation of SOPs for resilient and accurate PNT. Radio SLAM estimates the states of the navigator-mounted receiver simultaneously with the SOPs states. This paper presents the first published experimental results evaluating the efficacy of radio SLAM in a real GPS-denied environment. These experiments took place at Edwards Air Force Base, California, USA, during which GPS was intentionally jammed with jamming-to-signal (J/S) ratio as high as 90 dB. The paper evaluates the timing of two cellular long-term evolution (LTE) SOPs located in the jammed environment. Moreover, the paper presents navigation results showcasing a ground vehicle traversing a trajectory of about 5 km in 180 seconds in the GPS-jammed environment. The vehicles GPS-IMU system drifted from the vehicles ground truth trajectory, resulting in a position root mean-squared error (RMSE) of 238 m. In contrast, the radio SLAM approach with a single cellular LTE SOP whose position was poorly known achieved a position RMSE of 32 m.},
 author = {Kassas, Zaher Zak and Khalife, Joe and Abdallah, Ali and Lee, Chiawei},
 doi = {10.1109/maes.2022.3154110},
 journal = {IEEE Aerospace and Electronic Systems Magazine},
 keywords = {},
 number = {99},
 pages = {1-1},
 title = {I am Not Afraid of the GPS Jammer: Resilient Navigation via Signals of Opportunity in GPS-Denied Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1146188448},
 volume = {PP},
 year = {2022}
}

@article{pub.1146378129,
 abstract = {We propose Human-centered 4D Scene Capture (HSC4D) to accurately and
efficiently create a dynamic digital world, containing large-scale
indoor-outdoor scenes, diverse human motions, and rich interactions between
humans and environments. Using only body-mounted IMUs and LiDAR, HSC4D is
space-free without any external devices' constraints and map-free without
pre-built maps. Considering that IMUs can capture human poses but always drift
for long-period use, while LiDAR is stable for global localization but rough
for local positions and orientations, HSC4D makes both sensors complement each
other by a joint optimization and achieves promising results for long-term
capture. Relationships between humans and environments are also explored to
make their interaction more realistic. To facilitate many down-stream tasks,
like AR, VR, robots, autonomous driving, etc., we propose a dataset containing
three large scenes (1k-5k $m^2$) with accurate dynamic human motions and
locations. Diverse scenarios (climbing gym, multi-story building, slope, etc.)
and challenging human activities (exercising, walking up/down stairs, climbing,
etc.) demonstrate the effectiveness and the generalization ability of HSC4D.
The dataset and code are available at http://www.lidarhumanmotion.net/hsc4d/.},
 author = {Dai, Yudi and Lin, Yitai and Wen, Chenglu and Shen, Siqi and Xu, Lan and Yu, Jingyi and Ma, Yuexin and Wang, Cheng},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {HSC4D: Human-centered 4D Scene Capture in Large-scale Indoor-outdoor
  Space Using Wearable IMUs and LiDAR},
 url = {https://app.dimensions.ai/details/publication/pub.1146378129},
 volume = {},
 year = {2022}
}

@inbook{pub.1146452946,
 abstract = {Visual simultaneous localization and mapping (V-SLAM) technique plays a key role in perception of autonomous mobile robots, augmented/mixed/virtual reality, as well as spatial AI applications. This paper gives a very concise survey about the front-end module of a V-SLAM system, which is charge of feature extraction, short/long-term data association with outlier rejection, as well as variable initialization. Visual features are mainly salient and repeatable points, called keypoints, their traditional extractors and matchers are hand-engineered and not robust to viewpoint/illumination/seasonal change, which is crucial for long-term autonomy of mobile robots. Therefore, new trend about deep-learning-based keypoint extractors and matchers are introduced to enhance the robustness of V-SLAM systems even under challenging conditions.},
 author = {Wang, Yue and Fu, Yu and Zheng, Ruixue and Wang, Le and Qi, Jianzhong},
 booktitle = {Artificial Intelligence in China},
 doi = {10.1007/978-981-16-9423-3_38},
 keywords = {},
 pages = {298-307},
 publisher = {},
 title = {New Trend in Front-End Techniques of Visual SLAM: From Hand-Engineered Features to Deep-Learned Features},
 url = {https://app.dimensions.ai/details/publication/pub.1146452946},
 year = {2022}
}

@inproceedings{pub.1146629302,
 abstract = {Robust pose estimation and map reconstruction are the basic requirements of the robotics autonomous. In this paper, a static ground feature enhanced SLAM system is proposed for dynamic environments with RGB-D sensors. Compared with the typical point-based SLAM, our designed system extra introduce the ground and other plane constraints to solve the dynamic SLAM. In the front-end, the ground as a special plane feature is detected and tracked, which can provide realiable constraint for the pose estimation in dynamic environments. In the back-end, a point-ground based factor graph is constructed and optimized for more accurate map. Moreover, plane structure is exploited to repair the keyframe dynamic regions, new synthesized keyframes are used to reconstruct the static map for long-term applications. Real world dataset tests demonstrate the effectiveness of our proposed system.},
 author = {Guo, Ruibin and Liu, Xinghua},
 booktitle = {2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
 doi = {10.1109/robio54168.2021.9739362},
 keywords = {},
 pages = {1171-1177},
 title = {Ground Enhanced RGB-D SLAM for Dynamic Environments},
 url = {https://app.dimensions.ai/details/publication/pub.1146629302},
 year = {2021}
}

@inproceedings{pub.1146629486,
 abstract = {Localization on 3D data is a challenging task for unmanned vehicles, especially in long-term dynamic urban scenarios. Due to the generality and long-term stability, the pole-like objects are very suitable as landmarks for unmanned vehicle localization in time-varying scenarios. In this paper, a long-term LiDAR-only localization algorithm based on semantic cluster map is proposed. At first, the Convolutional Neural Network(CNN) is used to infer the semantics of LiDAR point clouds. Combined with the point cloud segmentation, the static objects pole/trunk are extracted and registered into global semantic cluster map. When the unmanned vehicle re-enters the environment again, the relocalization is completed by matching the clusters of current scan with the clusters of the global map. Furthermore, the matching between the local and global maps stably outputs the global pose at 2Hz to correct the drift of the 3D LiDAR odometry. The experimental results on our campus dataset demonstrate that the proposed approach performs better in localization accuracy compared with the current state-of-the-art methods. The source of this paper is available at: http://www.github.com/HITSZ-NRSL/long-term-localization.},
 author = {Wang, Zhihao and Li, Silin and Cao, Ming and Chen, Haoyao and Liu, Yunhui},
 booktitle = {2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
 doi = {10.1109/robio54168.2021.9739599},
 keywords = {},
 note = {http://arxiv.org/pdf/2103.13224},
 pages = {998-1003},
 title = {Pole-like Objects Mapping and Long-Term Robot Localization in Dynamic Urban Scenarios},
 url = {https://app.dimensions.ai/details/publication/pub.1146629486},
 year = {2021}
}

@article{pub.1146680153,
 abstract = {Simultaneous localization and mapping (SLAM) is a critical capability in
autonomous navigation, but in order to scale SLAM to the setting of "lifelong"
SLAM, particularly under memory or computation constraints, a robot must be
able to determine what information should be retained and what can safely be
forgotten. In graph-based SLAM, the number of edges (measurements) in a pose
graph determines both the memory requirements of storing a robot's observations
and the computational expense of algorithms deployed for performing state
estimation using those observations; both of which can grow unbounded during
long-term navigation. To address this, we propose a spectral approach for pose
graph sparsification which maximizes the algebraic connectivity of the
sparsified measurement graphs, a key quantity which has been shown to control
the estimation error of pose graph SLAM solutions. Our algorithm, MAC (for
"maximizing algebraic connectivity"), which is based on convex relaxation, is
simple and computationally inexpensive, and admits formal post hoc performance
guarantees on the quality of the solutions it provides. In experiments on
benchmark pose-graph SLAM datasets, we show that our approach quickly produces
high-quality sparsification results which retain the connectivity of the graph
and, in turn, the quality of corresponding SLAM solutions, as compared to a
baseline approach which does not consider graph connectivity.},
 author = {Doherty, Kevin J. and Rosen, David M. and Leonard, John J.},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Spectral Measurement Sparsification for Pose-Graph SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1146680153},
 volume = {},
 year = {2022}
}

@article{pub.1146726270,
 abstract = {Traditional simultaneous localization and mapping (SLAM) methods focus on
improvement in the robot's localization under environment and sensor
uncertainty. This paper, however, focuses on mitigating the need for exact
localization of a mobile robot to pursue autonomous navigation using a sparse
set of images. The proposed method consists of a model architecture - RoomNet,
for unsupervised learning resulting in a coarse identification of the
environment and a separate local navigation policy for local identification and
navigation. The former learns and predicts the scene based on the short term
image sequences seen by the robot along with the transition image scenarios
using long term image sequences. The latter uses sparse image matching to
characterise the similarity of frames achieved vis-a-vis the frames viewed by
the robot during the mapping and training stage. A sparse graph of the image
sequence is created which is then used to carry out robust navigation purely on
the basis of visual goals. The proposed approach is evaluated on two robots in
a test environment and demonstrates the ability to navigate in dynamic
environments where landmarks are obscured and classical localization methods
fail.},
 author = {Mathur, Pranay and Kumar, Rajesh and Upadhyay, Sarthak},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Sparse Image based Navigation Architecture to Mitigate the need of
  precise Localization in Mobile Robots},
 url = {https://app.dimensions.ai/details/publication/pub.1146726270},
 volume = {},
 year = {2022}
}

@article{pub.1146888775,
 abstract = {A key functional block of visual navigation system for intelligent autonomous
vehicles is Loop Closure detection and subsequent relocalisation.
State-of-the-Art methods still approach the problem as uni-directional along
the direction of the previous motion. As a result, most of the methods fail in
the absence of a significantly similar overlap of perspectives. In this study,
we propose an approach for bi-directional loop closure. This will, for the
first time, provide us with the capability to relocalize to a location even
when traveling in the opposite direction, thus significantly reducing long-term
odometry drift in the absence of a direct loop. We present a technique to
select training data from large datasets in order to make them usable for the
bi-directional problem. The data is used to train and validate two different
CNN architectures for loop closure detection and subsequent regression of 6-DOF
camera pose between the views in an end-to-end manner. The outcome packs a
considerable impact and aids significantly to real-world scenarios that do not
offer direct loop closure opportunities. We provide a rigorous empirical
comparison against other established approaches and evaluate our method on both
outdoor and indoor data from the FinnForest dataset and PennCOSYVIO dataset.},
 author = {Ali, Ihtisham and Peltonen, Sari and Gotchev, Atanas},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Bi-directional Loop Closure for Visual SLAM},
 url = {https://app.dimensions.ai/details/publication/pub.1146888775},
 volume = {},
 year = {2022}
}

@article{pub.1147023391,
 abstract = {Changes in appearance present a tremendous problem for the visual localization of an autonomous vehicle in outdoor environments. Data association between the current image and the landmarks in the map can be challenging in cases where the map was built with different environmental conditions. This paper introduces a solution to build and use multi-session maps incorporating sequences recorded in different conditions (day, night, fog, snow, rain, change of season, etc.). During visual localization, we exploit a ranking function to extract the most relevant keyframes from the map. This ranking function is designed to take into account the pose of the vehicle as well as the current environmental condition. In the mapping phase, covering all conditions by constantly adding data to the map leads to a continuous growth in the map size which in turn deteriorates the localization speed and performance. Our map management strategy is an incremental approach that aims to limit the size of the map while keeping it as diverse as possible. Our experiments were performed on real data collected with our autonomous shuttle as well as on a widely used public dataset. The results demonstrate that our keyframe-based ranking function is suitable for long-term scenarios. Our map management algorithm aims to build a map with as much diversity as possible whereas some state of the art approaches tend to filter out the less observed landmarks. This strategy shows a reduction of localization failures while maintaining real-time performance.},
 author = {Bouaziz, Youssef and Royer, Eric and Bresson, Guillaume and Dhome, Michel},
 doi = {10.1007/s11042-021-11870-4},
 journal = {Multimedia Tools and Applications},
 keywords = {},
 number = {},
 pages = {1-32},
 title = {Map management for robust long-term visual localization of an autonomous shuttle in changing conditions},
 url = {https://app.dimensions.ai/details/publication/pub.1147023391},
 volume = {},
 year = {}
}

@article{pub.1147351435,
 abstract = {A Simultaneous Localization and Mapping (SLAM) system must be robust to support long-term mobile vehicle and robot applications. However, camera and LiDAR based SLAM systems can be fragile when facing challenging illumination or weather conditions which degrade the utility of imagery and point cloud data. Radar, whose operating electromagnetic spectrum is less affected by environmental changes, is promising although its distinct sensor model and noise characteristics bring open challenges when being exploited for SLAM. This paper studies the use of a Frequency Modulated Continuous Wave radar for SLAM in large-scale outdoor environments. We propose a full radar SLAM system, including a novel radar motion estimation algorithm that leverages radar geometry for reliable feature tracking. It also optimally compensates motion distortion and estimates pose by joint optimization. Its loop closure component is designed to be simple yet efficient for radar imagery by capturing and exploiting structural information of the surrounding environment. Extensive experiments on three public radar datasets, ranging from city streets and residential areas to countryside and highways, show competitive accuracy and reliability performance of the proposed radar SLAM system compared to the state-of-the-art LiDAR, vision and radar methods. The results show that our system is technically viable in achieving reliable SLAM in extreme weather conditions on the RADIATE Dataset, for example, heavy snow and dense fog, demonstrating the promising potential of using radar for all-weather localization and mapping.},
 author = {Hong, Ziyang and Petillot, Yvan and Wallace, Andrew and Wang, Sen},
 doi = {10.1177/02783649221080483},
 journal = {The International Journal of Robotics Research},
 keywords = {},
 note = {https://journals.sagepub.com/doi/pdf/10.1177/02783649221080483},
 number = {},
 pages = {027836492210804},
 title = {RadarSLAM: A robust simultaneous localization and mapping system for all weather conditions},
 url = {https://app.dimensions.ai/details/publication/pub.1147351435},
 volume = {},
 year = {2022}
}

@article{pub.1147408104,
 abstract = {Drift-free localization is essential for autonomous vehicles. In this paper,
we address the problem by proposing a filter-based framework, which integrates
the visual-inertial odometry and the measurements of the features in the
pre-built map. In this framework, the transformation between the odometry frame
and the map frame is augmented into the state and estimated on the fly.
Besides, we maintain only the keyframe poses in the map and employ Schmidt
extended Kalman filter to update the state partially, so that the uncertainty
of the map information can be consistently considered with low computational
cost. Moreover, we theoretically demonstrate that the ever-changing
linearization points of the estimated state can introduce spurious information
to the augmented system and make the original four-dimensional unobservable
subspace vanish, leading to inconsistent estimation in practice. To relieve
this problem, we employ first-estimate Jacobian (FEJ) to maintain the correct
observability properties of the augmented system. Furthermore, we introduce an
observability-constrained updating method to compensate for the significant
accumulated error after the long-term absence (can be 3 minutes and 1 km) of
map-based measurements. Through simulations, the consistent estimation of our
proposed algorithm is validated. Through real-world experiments, we demonstrate
that our proposed algorithm runs successfully on four kinds of datasets with
the lower computational cost (20% time-saving) and the better estimation
accuracy (45% trajectory error reduction) compared with the baseline algorithm
VINS-Fusion, whereas VINS-Fusion fails to give bounded localization performance
on three of four datasets because of its inconsistent estimation.},
 author = {Zhang, Zhuqing and Jiao, Yanmei and Huang, Shoudong and Wang, Yue and Xiong, Rong},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {Map-based Visual-Inertial Localization: Consistency and Complexity},
 url = {https://app.dimensions.ai/details/publication/pub.1147408104},
 volume = {},
 year = {2022}
}

@article{pub.1147408792,
 abstract = {Where am I? This is one of the most critical questions that any intelligent
system should answer to decide whether it navigates to a previously visited
area. This problem has long been acknowledged for its challenging nature in
simultaneous localization and mapping (SLAM), wherein the robot needs to
correctly associate the incoming sensory data to the database allowing
consistent map generation. The significant advances in computer vision achieved
over the last 20 years, the increased computational power, and the growing
demand for long-term exploration contributed to efficiently performing such a
complex task with inexpensive perception sensors. In this article, visual loop
closure detection, which formulates a solution based solely on appearance input
data, is surveyed. We start by briefly introducing place recognition and SLAM
concepts in robotics. Then, we describe a loop closure detection system's
structure, covering an extensive collection of topics, including the feature
extraction, the environment representation, the decision-making step, and the
evaluation process. We conclude by discussing open and new research challenges,
particularly concerning the robustness in dynamic environments, the
computational complexity, and scalability in long-term operations. The article
aims to serve as a tutorial and a position paper for newcomers to visual loop
closure detection.},
 author = {Tsintotas, Konstantinos A. and Bampis, Loukas and Gasteratos, Antonios},
 doi = {},
 journal = {arXiv},
 keywords = {},
 number = {},
 pages = {},
 title = {The Revisiting Problem in Simultaneous Localization and Mapping: A
  Survey on Visual Loop Closure Detection},
 url = {https://app.dimensions.ai/details/publication/pub.1147408792},
 volume = {},
 year = {2022}
}
