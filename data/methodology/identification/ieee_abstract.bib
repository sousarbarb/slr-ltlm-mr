@INPROCEEDINGS{5753494,
author={Lutz, Matthias and Hochdorfer, Siegfried and Schlegel, Christian},
booktitle={2011 IEEE Conference on Technologies for Practical Robot Applications}, title={Global localization using multiple hypothesis tracking: A real-world approach},
year={2011},
volume={},
number={},
pages={127-132},
abstract={Life-long and robust operation are important challenges to be solved towards everyday usability of service robots. Global localization is of particular interest for real-world applications. If a robot would not be able to relocalize itself within a known map, all positions stored by the robot (rooms, objects, etc.) would become obsolete. Although Simultaneous Localization and Mapping (SLAM) allows to initially map new and unknown environments and to keep track of environmental changes, it does not solve the global localization problem. Each time SLAM is restarted at different locations, it introduces a new map and a new frame of reference. In this paper, we propose a solution to the global localization problem which uses a SLAM generated feature map. The approach is demonstrated with an omnicam and bearing-only features. A new way to weight hypotheses and to sort out false hypotheses results in fast convergence even with arbitrary relocalization paths. The combined approach is a further step towards life-long operation of service robots and covers every part of a robot lifecycle, ranging from a setup via SLAM to efficient global localization for reuse of maps and object poses after restart.},
keywords={Simultaneous localization and mapping;Uncertainty;Robot kinematics;Kalman filters;Estimation},
doi={10.1109/TEPRA.2011.5753494},
ISSN={2325-0534},
month={April},}
@INPROCEEDINGS{9196638,
author={Shi, Xuesong and Li, Dongjiang and Zhao, Pengpeng and Tian, Qinbin and Tian, Yuxin and Long, Qiwei and Zhu, Chunhao and Song, Jingwei and Qiao, Fei and Song, Le and Guo, Yangquan and Wang, Zhigang and Zhang, Yimin and Qin, Baoxing and Yang, Wei and Wang, Fangshi and Chan, Rosa H. M. and She, Qi},
booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, title={Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM},
year={2020},
volume={},
number={},
pages={3139-3145},
abstract={Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot’s long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.},
keywords={Simultaneous localization and mapping;Robot kinematics;Cameras;Synchronization;Trajectory},
doi={10.1109/ICRA40945.2020.9196638},
ISSN={2577-087X},
month={May},}
@INPROCEEDINGS{9635985,
author={Zhao, Min and Guo, Xin and Song, Le and Qin, Baoxing and Shi, Xuesong and Lee, Gim Hee and Sun, Guanghui},
booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={A General Framework for Lifelong Localization and Mapping in Changing Environment},
year={2021},
volume={},
number={},
pages={3305-3312},
abstract={The environment of most real-world scenarios such as malls and supermarkets changes at all times. A pre-built map that does not account for these changes becomes out-of-date easily. Therefore, it is necessary to have an up-to-date model of the environment to facilitate long-term operation of a robot. To this end, this paper presents a general lifelong simultaneous localization and mapping (SLAM) framework. Our framework uses a multiple session map representation, and exploits an efficient map updating strategy that includes map building, pose graph refinement and sparsification. To mitigate the unbounded increase of memory usage, we propose a map-trimming method based on the Chow-Liu maximum-mutual-information spanning tree. The proposed SLAM framework has been comprehensively validated by over a month of robot deployment in real supermarket environment. Furthermore, we release the dataset collected from the indoor and outdoor changing environment with the hope to accelerate lifelong SLAM research in the community. Our dataset is available at https://github.com/sanduan168/lifelong-SLAM-dataset.},
keywords={Location awareness;Simultaneous localization and mapping;Buildings;Intelligent robots},
doi={10.1109/IROS51168.2021.9635985},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{7354094,
author={Schuster, Martin J. and Brand, Christoph and Hirschmüller, Heiko and Suppa, Michael and Beetz, Michael},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Multi-robot 6D graph SLAM connecting decoupled local reference filters},
year={2015},
volume={},
number={},
pages={5093-5100},
abstract={Teams of mobile robots can be deployed in search and rescue missions to explore previously unknown environments. Methods for joint localization and mapping constitute the basis for (semi-)autonomous cooperative action, in particular when navigating in GPS-denied areas. As communication losses may occur, a decentralized solution is required. With these challenges in mind, we designed a submap-based SLAM system that relies on inertial measurements and stereo-vision to create multi-robot dense 3D maps. For online pose and map estimation, we integrate the results of keyframe-based local reference filters through incremental graph SLAM. To the best of our knowledge, we are the first to combine these two methods to benefit from their particular advantages for 6D multi-robot localization and mapping: Local reference filters on each robot provide real-time, long-term stable state estimates that are required for stabilization, control and fast obstacle avoidance, whereas online graph optimization provides global multi-robot pose and map estimates needed for cooperative planning. We propose a novel graph topology for a decoupled integration of local filter estimates from multiple robots into a SLAM graph according to the filters' uncertainty estimates and independence assumptions and evaluated its benefits on two different robots in indoor, outdoor and mixed scenarios. Further, we performed two extended experiments in a multi-robot setup to evaluate the full SLAM system, including visual robot detections and submap matches as inter-robot loop closure constraints.},
keywords={Simultaneous localization and mapping;Robot kinematics;Optimization;Visualization;Real-time systems},
doi={10.1109/IROS.2015.7354094},
ISSN={},
month={Sep.},}
@ARTICLE{9548901,
author={Aitken, Jonathan M. and Evans, Mathew H. and Worley, Rob and Edwards, S. and Zhang, Rui and Dodd, Tony and Mihaylova, Lyudmila and Anderson, Sean R.},
journal={IEEE Access}, title={Simultaneous Localization and Mapping for Inspection Robots in Water and Sewer Pipe Networks: A Review},
year={2021},
volume={9},
number={},
pages={140173-140198},
abstract={At the present time, water and sewer pipe networks are predominantly inspected manually. In the near future, smart cities will perform intelligent autonomous monitoring of buried pipe networks, using teams of small robots. These robots, equipped with all necessary computational facilities and sensors (optical, acoustic, inertial, thermal, pressure and others) will be able to inspect pipes whilst navigating, self-localising and communicating information about the pipe condition and faults such as leaks or blockages to human operators for monitoring and decision support. The predominantly manual inspection of pipe networks will be replaced with teams of autonomous inspection robots that can operate for long periods of time over a large spatial scale. Reliable autonomous navigation and reporting of faults at this scale requires effective localization and mapping, which is the estimation of the robot’s position and its surrounding environment. This survey presents an overview of state-of-the-art works on robot simultaneous localization and mapping (SLAM) with a focus on water and sewer pipe networks. It considers various aspects of the SLAM problem in pipes, from the motivation, to the water industry requirements, modern SLAM methods, map-types and sensors suited to pipes. Future challenges such as robustness for long term robot operation in pipes are discussed, including how making use of prior knowledge, e.g. geographic information systems (GIS) can be used to build map estimates, and improve multi-robot SLAM in the pipe environment.},
keywords={Simultaneous localization and mapping;Robots;Sensors;Inspection;Service robots;Water resources;Location awareness;Water;sewer;network;pipe networks;robots;SLAM;data fusion;Bayesian estimation;visual odometry;laser and lidar scanning},
doi={10.1109/ACCESS.2021.3115981},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8968017,
author={Song, Bowen and Chen, Weidong and Wang, Jingchuan and Wang, Hesheng},
booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Long-Term Visual Inertial SLAM based on Time Series Map Prediction},
year={2019},
volume={},
number={},
pages={5364-5369},
abstract={With the advance in the field of mobile robots, autonomous robots are required for long-term deployment in dynamic and complex environments. However, the performance of Visual Inertial SLAM systems in long-term operation is not satisfactory, and most long-term SLAM systems assumes periodic changes in the environment. This paper presents a novel solution for long-term monocular VI SLAM system in dynamic environment based on autoregression(AR) modeling and map prediction. Map points are first classified into static and semi-static map points according to a memory model. Modeling and prediction of the different states of semi-static map points are performed that are derived from time series models. The predicted map is then fused with the current map to achieve a better forecast for the next frame if the prediction is not satisfactory enough. Experiments are carried out on an embedded system. The results indicate that the map prediction is reliable and the proposed approach improves the performance of long-term localization and mapping in dynamic environments.},
keywords={},
doi={10.1109/IROS40897.2019.8968017},
ISSN={2153-0866},
month={Nov},}
@INPROCEEDINGS{6689619,
author={Slimane, Noureddine and Khireddine, Mohamed Salah and Chafaa, Kheireddine},
booktitle={2013 International Conference on Control, Decision and Information Technologies (CoDIT)}, title={A metric approach for environments mapping},
year={2013},
volume={},
number={},
pages={647-652},
abstract={One of the main issues in mobile robotics is the autonomous navigation of a mobile robot in an unknown environment. Concurrent mapping and localisation or simultaneous localisation and mapping is a stochastic map building method which permits consistent robot navigation without requiring an a priori map. The governing idea which guides autonomous robotics consists in saying that the vehicle builds its chart progressively during exploration enabling it to evolve in the long term in unknown places in advance. When the robot environment chart is not known a priori, a generation module of incremental chart must obligatorily be integrated into the navigation system. The map is built incrementally as the robot observes the environment with its on-board sensors and, at the same time, is used to localise the robot. Unfortunately, the inaccuracy of the odometric sensors does not allow a sufficiently correct positioning of the robot. In this paper, simultaneous localisation and map building is performed with a metric approach which permits both precision and robustness. The most important innovation of the approach is the way how errors in the robot localisation control are handled by map building using the landmarks localisation information. The method uses data from a laser scanner to extract distances and orientations of landmarks and combines control localisation and metric paradigm. The metric approach, based on the Kalman filter, uses a new concept to avoid the problem of the drift in odometry. The simulation section will validate the maps representation approach and presents different aspect of environments.},
keywords={Robot sensing systems;Covariance matrices;Measurement;Vehicles;Vectors;Buildings;Map building;metric chart;real-time control;mobile robot localisation;extended Kalman filter;landmarks detection},
doi={10.1109/CoDIT.2013.6689619},
ISSN={},
month={May},}
@INPROCEEDINGS{1655801,
author={Bryson, M. and Salah Sukkarieh},
booktitle={2006 IEEE Aerospace Conference}, title={Active airborne localisation and exploration in unknown environments using inertial SLAM},
year={2006},
volume={},
number={},
pages={13 pp.-},
abstract={Future unmanned aerial vehicle (UAV) applications will require high-accuracy localisation in environments in which navigation infrastructure such as the Global Positioning System (GPS) and prior terrain maps may be unavailable or unreliable. In these applications, long-term operation requires the vehicle to build up a spatial map of the environment while simultaneously localising itself within the map, a task known as simultaneous localisation and mapping (SLAM). In the first part of this paper we present an architecture for performing inertial-sensor based SLAM on an aerial vehicle. We demonstrate an on-line path planning scheme that intelligently plans the vehicle's trajectory while exploring unknown terrain in order to maximise the quality of both the resulting SLAM map and localisation estimates necessary for the autonomous control of the UAV. Two important performance properties and their relationship to the dynamic motion and path planning systems on-board the UAV are analysed. Firstly we analyse information-based measures such as entropy. Secondly we perform an observability analysis of inertial SLAM by recasting the algorithms into an indirect error model form. Qualitative knowledge gained from the observability analysis is used to assist in the design of an information-based trajectory planner for the UAV. Results of the online path planning algorithm are presented using a high-fidelity 6-DoF simulation of a UAV during a simulated navigation and mapping task.},
keywords={Simultaneous localization and mapping;Unmanned aerial vehicles;Path planning;Navigation;Global Positioning System;Remotely operated vehicles;Performance analysis;Information analysis;Observability;Terrain mapping;Navigation;Mapping;SLAM;Autonomous Vehicles;Observability},
doi={10.1109/AERO.2006.1655801},
ISSN={1095-323X},
month={March},}
@ARTICLE{7839213,
author={Han, Fei and Yang, Xue and Deng, Yiming and Rentschler, Mark and Yang, Dejun and Zhang, Hao},
journal={IEEE Robotics and Automation Letters}, title={SRAL: Shared Representative Appearance Learning for Long-Term Visual Place Recognition},
year={2017},
volume={2},
number={2},
pages={1172-1179},
abstract={Place recognition, or loop closure detection, is an essential component to address the problem of visual simultaneous localization and mapping (SLAM). Long-term navigation of robots in outdoor environments introduces new challenges to enable life-long SLAM, including the strong appearance change resulting from vegetation, weather, and illumination variations across various times of the day, different days, months, or even seasons. In this paper, we propose a new shared representative appearance learning (SRAL) approach to address long-term visual place recognition. Different from previous methods using a single feature modality or a concatenation of multiple features, our SRAL method autonomously learns representative features that are shared in all scene scenarios, and then fuses the features together to represent the long-term appearance of environments observed by a robot during life-long navigation. By formulating SRAL as a regularized optimization problem, we use structured sparsity-inducing norms to model interrelationships of feature modalities. In addition, an optimization algorithm is developed to efficiently solve the formulated optimization problem, which holds a theoretical convergence guarantee. Extensive empirical study was performed to evaluate the SRAL method using large-scale benchmark datasets, including St Lucia, CMU-VL, and Nordland datasets. Experimental results have shown that our SRAL method obtains superior performance for life-long place recognition using individual images, outperforms previous single image-based methods, and is capable of estimating the importance of feature modalities.},
keywords={Visualization;Image recognition;Feature extraction;Simultaneous localization and mapping;Optimization;Navigation;Loop closure detection;long-term place recognition;simultaneous localization and mapping (SLAM);visual learning},
doi={10.1109/LRA.2017.2662061},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{8500432,
author={Bürki, Mathias and Dymczyk, Marcin and Gilitschenski, Igor and Cadena, Cesar and Siegwart, Roland and Nieto, Juan},
booktitle={2018 IEEE Intelligent Vehicles Symposium (IV)}, title={Map Management for Efficient Long-Term Visual Localization in Outdoor Environments},
year={2018},
volume={},
number={},
pages={682-688},
abstract={We present a complete map management process for a visual localization system designed for multi-vehicle long-term operations in resource constrained outdoor environments. Outdoor visual localization generates large amounts of data that need to be incorporated into a lifelong visual map in order to allow localization at all times and under all appearance conditions. Processing these large quantities of data is non-trivial, as it is subject to limited computational and storage capabilities both on the vehicle and on the mapping backend. We address this problem with a two-fold map update paradigm capable of, either, adding new visual cues to the map, or updating co-observation statistics. The former, in combination with offline map summarization techniques, allows enhancing the appearance coverage of the lifelong map while keeping the map size limited. On the other hand, the latter is able to significantly boost the appearance-based landmark selection for efficient online localization without incurring any additional computational or storage burden. Our evaluation in challenging outdoor conditions shows that our proposed map management process allows building and maintaining maps for precise visual localization over long time spans in a tractable and scalable fashion.},
keywords={Visualization;Buildings;Robot sensing systems;Autonomous automobiles;Maintenance engineering;Bandwidth;Intelligent vehicles},
doi={10.1109/IVS.2018.8500432},
ISSN={1931-0587},
month={June},}
@INPROCEEDINGS{4141823,
author={Adams, Martin E.},
booktitle={2006 IEEE International Conference on Robotics and Biomimetics}, title={Autonomous Navigation: Achievements in Complex Environments},
year={2006},
volume={},
number={},
pages={nil19-nil19},
abstract={Over the past decade, challenging applications for autonomous robots have been identified, in the areas of servicing crowded, built-up areas, mining, search and rescue operations, underwater exploration and airborne surveillance. Autonomous navigation arguably remains the key enabling issue behind any realistic commercial success in these areas. Consequently, autonomous robotic research has focused on large scale and long term navigation algorithms, sensing technologies, robust sensor data interpretation and map building. The recent breakthroughs which contribute to the success of outdoor field robotics, and remaining fundamental research issues involved, will be the theme of this presentation. The most successful robot navigation algorithms to-date, have been derived from a probabilistic perspective, which takes into account vehicle motion and terrain uncertainty and sensor noise. Over the past decade, an explosion of interest in the estimation of an autonomous robot's location state, and that of its surroundings, known as simultaneous localisation and map building (SLAM), is evident. New algorithms which represent uncertain information based on particle filters and Gaussian mixture models, as well as the more classical Kalman filter based techniques, are advancing the progress of a robot's long term navigation abilities. This has been significantly aided by recently affordable sensor technologies, including GPS and inertial measurement units (IMUs) as well as fast and reliable laser range finders.},
keywords={Navigation;Robot sensing systems;Surveillance;Large-scale systems;Robustness;Remotely operated vehicles;Uncertainty;Explosions;State estimation;Simultaneous localization and mapping},
doi={10.1109/ROBIO.2006.340225},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8248292,
author={Shen, Qihui and Sun, Hanxu and Ye, Ping},
booktitle={2017 4th International Conference on Systems and Informatics (ICSAI)}, title={Research of large-scale offline map management in visual SLAM},
year={2017},
volume={},
number={},
pages={215-219},
abstract={This paper presents a novel method of visual simultaneous localization and mapping (SLAM), which is a method of real-time localization and mapping. It is important for a mobile robot to build a map while autonomously navigation. Due to the complexity of the robot work scene, the SLAM method proposed in this paper optimizes map management. It will cost a lot of time and space when a robot long-term works in a same large scene. Therefore, we propose a method in this paper to save a detail map as an offline map in advance. At the same time in order to facilitate the follow-up optimization, the offline map can be divided into several sub-graphs according to the similarity of the scene. Since the segmented offline map has been saved to local system, it can be loaded at any time to localization and obtain the pose of current frame.},
keywords={Simultaneous localization and mapping;Image segmentation;Cameras;Real-time systems;Optimization;Symmetric matrices;SLAM;offline map;segment graph;normalized-cut},
doi={10.1109/ICSAI.2017.8248292},
ISSN={},
month={Nov},}
@ARTICLE{9308903,
author={Kim, Chansoo and Cho, Sungjin and Sunwoo, Myoungho and Resende, Paulo and Bradaï, Benazouz and Jo, Kichun},
journal={IEEE Access}, title={A Geodetic Normal Distribution Map for Long-Term LiDAR Localization on Earth},
year={2021},
volume={9},
number={},
pages={470-484},
abstract={Light detection and ranging (LiDAR) sensors enable a vehicle to estimate a pose by matching their measurements with a point cloud (PCD) map. However, the PCD map structure, widely used in robot fields, has some problems to be applied for mass production in automotive fields. First, the PCD map is too big to store all map data at in-vehicle units or download the map data from a wireless network according to the vehicle location. Second, the PCD map, represented by a single origin in the Cartesian coordinates, causes coordinate conversion errors due to an inaccurate plane-orb projection, when the vehicle estimate the geodetic pose on Earth. To solve two problems, this paper presents a geodetic normal distribution (GND) map structure. The GND map structure supports a geodetic quad-tree tiling system with multiple origins to minimize the coordinate conversion errors. The map data managed by the GND map structure are compressed by using Cartesian probabilistic distributions of points as map features. The truncation errors by heterogeneous coordinates between the geodetic tiling system and Cartesian distributions are compensated by the Cartesian voxelization rule. In order to match the LiDAR measurements with the GND map structure, the paper proposes map-matching approaches based on Monte-Carlo and optimization. The paper performed some experiments to evaluate the map size compression and the long-term localization on Earth: comparison with the PCD map structure, localization in various continents, and long-term localization.},
keywords={Laser radar;Sensors;Three-dimensional displays;Gaussian distribution;Semantics;Cameras;Roads;World-scale map management;map compression;normal distribution map;registration},
doi={10.1109/ACCESS.2020.3047421},
ISSN={2169-3536},
month={},}
@ARTICLE{7747236,
author={Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, José and Reid, Ian and Leonard, John J.},
journal={IEEE Transactions on Robotics}, title={Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age},
year={2016},
volume={32},
number={6},
pages={1309-1332},
abstract={Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
keywords={Graph theory;Simultaneous location and mapping;Service robots;Robustness;Localization;Factor graphs;localization;mapping;maximum a posteriori estimation;perception;robots;sensing;simultaneous localization and mapping (SLAM)},
doi={10.1109/TRO.2016.2624754},
ISSN={1941-0468},
month={Dec},}
@INPROCEEDINGS{6385561,
author={Walcott-Bryant, Aisha and Kaess, Michael and Johannsson, Hordur and Leonard, John J.},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={Dynamic pose graph SLAM: Long-term mapping in low dynamic environments},
year={2012},
volume={},
number={},
pages={1871-1878},
abstract={Maintaining a map of an environment that changes over time is a critical challenge in the development of persistently autonomous mobile robots. Many previous approaches to mapping assume a static world. In this work we incorporate the time dimension into the mapping process to enable a robot to maintain an accurate map while operating in dynamical environments. This paper presents Dynamic Pose Graph SLAM (DPG-SLAM), an algorithm designed to enable a robot to remain localized in an environment that changes substantially over time. Using incremental smoothing and mapping (iSAM) as the underlying SLAM state estimation engine, the Dynamic Pose Graph evolves over time as the robot explores new places and revisits previously mapped areas. The approach has been implemented for planar indoor environments, using laser scan matching to derive constraints for SLAM state estimation. Laser scans for the same portion of the environment at different times are compared to perform change detection; when sufficient change has occurred in a location, the dynamic pose graph is edited to remove old poses and scans that no longer match the current state of the world. Experimental results are shown for two real-world dynamic indoor laser data sets, demonstrating the ability to maintain an up-to-date map despite long-term environmental changes.},
keywords={Simultaneous localization and mapping;Measurement by laser beam;Mobile robots;Heuristic algorithms;Lasers},
doi={10.1109/IROS.2012.6385561},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{7759671,
author={Krajník, Tomáš and Pulido Fentanes, Jaime and Hanheide, Marc and Duckett, Tom},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Persistent localization and life-long mapping in changing environments using the Frequency Map Enhancement},
year={2016},
volume={},
number={},
pages={4558-4563},
abstract={We present a lifelong mapping and localisation system for long-term autonomous operation of mobile robots in changing environments. The core of the system is a spatio-temporal occupancy grid that explicitly represents the persistence and periodicity of the individual cells and can predict the probability of their occupancy in the future. During navigation, our robot builds temporally local maps and integrates then into the global spatio-temporal grid. Through re-observation of the same locations, the spatio-temporal grid learns the long-term environment dynamics and gains the ability to predict the future environment states. This predictive ability allows to generate time-specific 2d maps used by the robot's localisation and planning modules. By analysing data from a long-term deployment of the robot in a human-populated environment, we show that the proposed representation improves localisation accuracy and the efficiency of path planning. We also show how to integrate the method into the ROS navigation stack for use by other roboticists.},
keywords={Navigation;Two dimensional displays;Robot sensing systems;Planning;Predictive models;Robot kinematics;mobile robotics;long-term autonomy},
doi={10.1109/IROS.2016.7759671},
ISSN={2153-0866},
month={Oct},}
@ARTICLE{9732682,
author={Kassas, Zaher Zak and Khalife, Joe and Abdallah, Ali and Lee, Chiawei},
journal={IEEE Aerospace and Electronic Systems Magazine}, title={I am Not Afraid of the GPS Jammer: Resilient Navigation via Signals of Opportunity in GPS-Denied Environments},
year={2022},
volume={},
number={},
pages={1-1},
abstract={In environments where GPS signals are denied, signals of opportunity (SOPs) could serve as an alternative positioning, navigation, and timing (PNT) source to GPS, and more generally, to global navigation satellite systems (GNSS). This paper presents a radio simultaneous localization and mapping (radio SLAM) approach that enables the exploitation of SOPs for resilient and accurate PNT. Radio SLAM estimates the states of the navigator-mounted receiver simultaneously with the SOPs states. This paper presents the first published experimental results evaluating the efficacy of radio SLAM in a real GPS-denied environment. These experiments took place at Edwards Air Force Base, California, USA, during which GPS was intentionally jammed with jamming-to-signal (J/S) ratio as high as 90 dB. The paper evaluates the timing of two cellular long-term evolution (LTE) SOPs located in the jammed environment. Moreover, the paper presents navigation results showcasing a ground vehicle traversing a trajectory of about 5 km in 180 seconds in the GPS-jammed environment. The vehicles GPS-IMU system drifted from the vehicles ground truth trajectory, resulting in a position root mean-squared error (RMSE) of 238 m. In contrast, the radio SLAM approach with a single cellular LTE SOP whose position was poorly known achieved a position RMSE of 32 m.},
keywords={Radio navigation;Global navigation satellite system;Simultaneous localization and mapping;Jamming;Navigation;Global Positioning System;Sensors},
doi={10.1109/MAES.2022.3154110},
ISSN={1557-959X},
month={},}
@INPROCEEDINGS{9561584,
author={Zhu, Shifan and Zhang, Xinyu and Guo, Shichun and Li, Jun and Liu, Huaping},
booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, title={Lifelong Localization in Semi-Dynamic Environment},
year={2021},
volume={},
number={},
pages={14389-14395},
abstract={Mapping and localization in non-static environments are fundamental problems in robotics. Most of previous methods mainly focus on static and highly dynamic objects in the environment, which may suffer from localization failure in semi-dynamic scenarios without considering objects with lower dynamics, such as parked cars and stopped pedestrians. In this paper, we introduce semantic mapping and lifelong localization approaches to recognize semi-dynamic objects in non-static environments. We also propose a generic framework that can integrate mainstream object detection algorithms with mapping and localization algorithms. The mapping method combines an object detection algorithm and a SLAM algorithm to detect semi-dynamic objects and constructs a semantic map that only contains semi-dynamic objects in the environment. During navigation, the localization method can classify observation corresponding to static and non-static objects respectively and evaluate whether those semi-dynamic objects have moved, to reduce the weight of invalid observation and localization fluctuation. Real-world experiments show that the proposed method can improve the localization accuracy of mobile robots in non-static scenarios.},
keywords={Location awareness;Simultaneous localization and mapping;Fluctuations;Navigation;Heuristic algorithms;Conferences;Semantics},
doi={10.1109/ICRA48506.2021.9561584},
ISSN={2577-087X},
month={May},}
@INPROCEEDINGS{7759843,
author={Gadd, Matthew and Newman, Paul},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Checkout my map: Version control for fleetwide visual localisation},
year={2016},
volume={},
number={},
pages={5729-5736},
abstract={This paper is about underpinning long-term operations of fleets of vehicles using visual localisation. In particular it examines ways in which vehicles, considered as independent agents, can share, update and leverage each others' visual experiences in a mutually beneficial way. We draw on our previous work in Experience-based Navigation (EBN) [1], in which a visual map supporting multiple representations of the same place is built, yielding real-time localisation capability for a solitary vehicle. We now consider how any number of such agents might operate in concert via data sharing policies that are germane to the shared task of lifelong localisation. We rapidly construct considerable maps by the conjoining of work distributed to asynchronous processes, and share expertise amongst the team by the selective dispensing of mission-specific map contents. We demonstrate and evaluate our system against 100km of data collected in North Oxford over a period of a month featuring diverse deviation in appearance due to atmospheric, lighting, and structural dynamics. We show that our framework is capable of creating maps in a fraction of the time required by single-agent EBN, with no significant loss in localisation robustness, and is able to furnish robots on real-world forays with maps which require much less storage.},
keywords={Robot sensing systems;Servers;Databases;Visualization;Robustness;Vehicles},
doi={10.1109/IROS.2016.7759843},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{9421197,
author={Chen, Hongjian and Wang, Zhiqiang and Zhu, Qing},
booktitle={2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)}, title={Map Updating Revisited for Navigation Map : A mathematical way to perform map updating for autonomous mobile robot},
year={2021},
volume={},
number={},
pages={505-508},
abstract={Simultaneous localization and mapping, SLAM can product a Map for autonomous robots and self-driving vehicle in navigation. In the actual environment, the scene changes frequently, which makes the old map no long reliable. Therefore, it is necessary to update such a map by an efficient and safely way. In this paper, we review the existing map updating, long-term localization methods and discuss about the challenges in this situation. We present a Map updating method in mathematical way which can update accurately. Our proposed method are tested in five indoor dataset and demonstrated feasibility.},
keywords={Location awareness;Computers;Simultaneous localization and mapping;Navigation;Image processing;Conferences;Reliability;map;updating;visual;point cloud},
doi={10.1109/IPEC51340.2021.9421197},
ISSN={},
month={April},}
@INPROCEEDINGS{8662397,
author={Li, Ziyun and Chen, Yu and Gong, Luyao and Liu, Lu and Sylvester, Dennis and Blaauw, David and Kim, Hun-Seok},
booktitle={2019 IEEE International Solid- State Circuits Conference - (ISSCC)}, title={An 879GOPS 243mW 80fps VGA Fully Visual CNN-SLAM Processor for Wide-Range Autonomous Exploration},
year={2019},
volume={},
number={},
pages={134-136},
abstract={Simultaneous localization and mapping (SLAM) estimates an agent's trajectory for all six degrees of freedom (6 DoF) and constructs a 3D map of an unknown surrounding. It is a fundamental kernel that enables head-mounted augmented/virtual reality devices and autonomous navigation of micro aerial vehicles. A noticeable recent trend in visual SLAM is to apply computationand memory-intensive convolutional neural networks (CNNs) that outperform traditional hand-designed feature-based methods [1]. For each video frame, CNN-extracted features are matched with stored keypoints to estimate the agent's 6-DoF pose by solving a perspective-n-points (PnP) non-linear optimization problem (Fig. 7.3.1, left). The agent's long-term trajectory over multiple frames is refined by a bundle adjustment process (BA, Fig. 7.3.1 right), which involves a large-scale (~120 variables) non-linear optimization. Visual SLAM requires massive computation (>250GOP/s) in the CNN-based feature extraction and matching, as well as datadependent dynamic memory access and control flow with high-precision operations, creating significant low-power design challenges. Software implementations are impractical, resulting in 0.2s runtime with a ~3GHz CPU+ GPU system with >100MB memory footprint and >100W power consumption. Prior ASICs have implemented either an incomplete SLAM system [2,3] that lacks estimation of ego-motion or employed a simplified (non-CNN) feature extraction and tracking [2,4,5] that limits SLAM quality and range. A recent ASIC [5] augments visual SLAM with an off-chip high-precision inertial measurement unit (IMU), simplifying the computational complexity, but incurring additional power and cost overhead.},
keywords={Simultaneous localization and mapping;Engines;Three-dimensional displays;Two dimensional displays;Feature extraction;Visualization;Trajectory},
doi={10.1109/ISSCC.2019.8662397},
ISSN={2376-8606},
month={Feb},}
@ARTICLE{9484742,
author={Berrio, Julie Stephany and Worrall, Stewart and Shan, Mao and Nebot, Eduardo},
journal={IEEE Transactions on Intelligent Transportation Systems}, title={Long-Term Map Maintenance Pipeline for Autonomous Vehicles},
year={2021},
volume={},
number={},
pages={1-14},
abstract={For autonomous vehicles to operate persistently in a typical urban environment, it is essential to have high accuracy position information. This requires a mapping and localisation system that can adapt to changes over time. A localisation approach based on a single-survey map will not be suitable for long-term operation as it does not incorporate variations in the environment. In this paper, we present new algorithms to maintain a featured-based map. A map maintenance pipeline is proposed that can continuously update a map with the most relevant features taking advantage of the changes in the surroundings. Our pipeline detects and removes transient features based on their geometrical relationships with the vehicle's pose. Newly identified features became part of a new feature map and are assessed by the pipeline as candidates for the localisation map. By purging out-of-date features and adding newly detected features, we continually update the prior map to more accurately represent the most recent environment. We have validated our approach using the USyd Campus Dataset, which includes more than 18 months of data. The results presented demonstrate that our maintenance pipeline produces a resilient map which can provide sustained localisation performance over time.},
keywords={Feature extraction;Pipelines;Maintenance engineering;Transient analysis;Visualization;Autonomous vehicles;Task analysis;Long-term localisation;feature-based map;map update.},
doi={10.1109/TITS.2021.3094485},
ISSN={1558-0016},
month={},}
@INPROCEEDINGS{9739599,
author={Wang, Zhihao and Li, Silin and Cao, Ming and Chen, Haoyao and Liu, Yunhui},
booktitle={2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)}, title={Pole-like Objects Mapping and Long-Term Robot Localization in Dynamic Urban Scenarios},
year={2021},
volume={},
number={},
pages={998-1003},
abstract={Localization on 3D data is a challenging task for unmanned vehicles, especially in long-term dynamic urban scenarios. Due to the generality and long-term stability, the pole-like objects are very suitable as landmarks for unmanned vehicle localization in time-varying scenarios. In this paper, a long-term LiDAR-only localization algorithm based on semantic cluster map is proposed. At first, the Convolutional Neural Network(CNN) is used to infer the semantics of LiDAR point clouds. Combined with the point cloud segmentation, the static objects pole/trunk are extracted and registered into global semantic cluster map. When the unmanned vehicle re-enters the environment again, the relocalization is completed by matching the clusters of current scan with the clusters of the global map. Furthermore, the matching between the local and global maps stably outputs the global pose at 2Hz to correct the drift of the 3D LiDAR odometry. The experimental results on our campus dataset demonstrate that the proposed approach performs better in localization accuracy compared with the current state-of-the-art methods. The source of this paper is available at: http://www.github.com/HITSZ-NRSL/long-term-localization.},
keywords={Location awareness;Point cloud compression;Laser radar;Three-dimensional displays;Heuristic algorithms;Semantics;Clustering algorithms},
doi={10.1109/ROBIO54168.2021.9739599},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7274622,
author={Rao, Akshay and Han, Wang},
booktitle={2015 IEEE 7th International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)}, title={An Adaptive Gaussian Particle Filter based Simultaneous Localization and Mapping with dynamic process model noise bias compensation},
year={2015},
volume={},
number={},
pages={210-215},
abstract={Simultaneous Localization and Mapping (SLAM) is a fundamental component of all autonomous robotics systems, which probabilisticaly fuses information from an exteroceptive sensor and a proprioceptive sensor to simultaneously estimate the robot's trajectory and the map. Inputs from the pro-prioceptive sensor are fed into the estimation algorithm via a process model corresponding with the vehicle kinematics, while a measurement model is used to process inputs from the exteroceptive sensor. Most SLAM algorithms assume known, fixed model estimate bias. This assumption does not hold true for systems with wrongly modeled estimate bias, or those affected by component fatigue due to applications requiring long term autonomy. This paper will display the adverse effects of mismodeled process model bias using a simulation. An adaptive algorithm employing Adaptive Gaussian Particle Filter based process model bias compensation will be deployed in tandem with a particle filter based FastSLAM algorithm. The algorithm will be compared favourably with existing state of the art SLAM algorithms in controlled simulations. Experimental data from a marine environment will be used to validate the efficacy of the algorithm.},
keywords={Simultaneous localization and mapping;Adaptation models;Noise;Trajectory;Estimation;Kalman filters},
doi={10.1109/ICCIS.2015.7274622},
ISSN={2326-8239},
month={July},}
@INPROCEEDINGS{8569323,
author={Luthardt, Stefan and Willert, Volker and Adamy, Jürgen},
booktitle={2018 21st International Conference on Intelligent Transportation Systems (ITSC)}, title={LLama-SLAM: Learning High-Quality Visual Landmarks for Long-Term Mapping and Localization},
year={2018},
volume={},
number={},
pages={2645-2652},
abstract={The precise localization of vehicles is an important requirement for autonomous driving or advanced driver assistance systems. Using common GNSS the ego position can be measured but not with the reliability and precision necessary. An alternative approach to achieve precise localization is the usage of visual landmarks observed by a camera mounted in the vehicle. However, this raises the necessity of reliable visual landmarks that are easily recognizable and persistent. We propose a novel SLAM algorithm that focuses on learning and mapping such visual long-term landmarks (LLamas). The algorithm therefore processes stereo image streams from several recording sessions in the same spatial area. The key part within LLama-SLAM is the assessment of the landmarks with quality values that are inferred as viewpoint dependent probabilities from observation statistics. By adding solely landmarks of high quality to the final LLama Map, it can be kept compact while still allowing reliable localization. Due to the long-term evaluation of the GNSS measurement during the sessions, the landmarks can be positioned precisely in a global referenced coordinate system. For a first assessment of the algorithm's capabilities, we present some experimental results from the mapping process combining three sessions recorded over two months on the same route.},
keywords={Cameras;Visualization;Simultaneous localization and mapping;Global navigation satellite system;Reliability;Probability;Optimization},
doi={10.1109/ITSC.2018.8569323},
ISSN={2153-0017},
month={Nov},}
@INPROCEEDINGS{5756810,
author={Abrate, Fabrizio and Bona, Basilio and Indri, Marina and Rosa, Stefano and Tibaldi, Federico},
booktitle={ISR 2010 (41st International Symposium on Robotics) and ROBOTIK 2010 (6th German Conference on Robotics)}, title={Map updating in dynamic environments},
year={2010},
volume={},
number={},
pages={1-8},
abstract={While building maps when robot poses are known is a tractable problem requiring limited computational complexity, the simultaneous estimation of the trajectory and the map of the environment (known as SLAM) is much more complex and requires many computational resources. Moreover, SLAM is generally peformed in environments that do not vary over time (called static environments), whereas real applications commonly require navigation services in changing environments (called dynamic environments). Many real robotic applications require updated maps of the environment that vary over time, starting from a given known initial condition. In this context classical SLAM approaches are generally not directly applicable: such approaches only apply in static environments or in dynamic environments where it is possible to model the environment dynamics. We are interested here in long-term mapping operativity in presence of variations in the map, as in the case of robotic applications in logistic spaces, where rovers have to track the presence of goods in given areas. In this paper we propose a methodology that is able to detect variations in the environment, generate a local map containing only the persistent variations and finally merge the local map with the global one used for localization.},
keywords={Simultaneous localization and mapping;Correlation;Robot kinematics;Turning;Pixel;Merging},
doi={},
url={https://ieeexplore.ieee.org/document/5756810},
ISSN={},
month={June},}
@ARTICLE{9500238,
author={Ali, Waqas and Liu, Peilin and Ying, Rendong and Gong, Zheng},
journal={IEEE Sensors Journal}, title={A Life-Long SLAM Approach Using Adaptable Local Maps Based on Rasterized LIDAR Images},
year={2021},
volume={21},
number={19},
pages={21740-21749},
abstract={Most real-time autonomous robot applications require a robot to traverse through a dynamic space for a long time. In some cases, a robot needs to work in the same environment. Such applications give rise to the problem of a life-long SLAM system. Life-long SLAM presents two main challenges i.e. the tracking should not fail in a dynamic environment and the need for a robust and efficient mapping strategy. The system should update maps with new information; while also keeping track of older observations. But, mapping for a long time can require higher computational requirements. In this paper, we propose a solution to the problem of life-long SLAM. We represent the global map as a set of rasterized images of local maps along with a map management system responsible for updating local maps and keeping track of older values. We also present an efficient approach of using the bag of visual words method for loop closure detection and relocalization. We evaluate the performance of our system on the KITTI dataset and an indoor dataset. Our loop closure system reported recall and precision of above 90 percent. The computational cost of our system is much lower as compared to state-of-the-art methods. Our method reports lower computational requirements even for long-term operation.},
keywords={Simultaneous localization and mapping;Three-dimensional displays;Feature extraction;Robots;Databases;Laser radar;Sensors;Laser scanning;place recognition;bag of words;rasterization;mapping;simultaneous localization;mapping},
doi={10.1109/JSEN.2021.3100882},
ISSN={1558-1748},
month={Oct},}
@INPROCEEDINGS{7759683,
author={Nashed, Samer and Biswas, Joydeep},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Curating Long-Term Vector Maps},
year={2016},
volume={},
number={},
pages={4643-4648},
abstract={Autonomous service mobile robots need to consistently, accurately, and robustly localize in human environments despite changes to such environments over time. Episodic non-Markov Localization addresses the challenge of localization in such changing environments by classifying observations as arising from Long-Term, Short-Term, or Dynamic Features. However, in order to do so, EnML relies on an estimate of the Long-Term Vector Map (LTVM) that does not change over time. In this paper, we introduce a recursive algorithm to build and update the LTVM over time by reasoning about visibility constraints of objects observed over multiple robot deployments. We use a signed distance function (SDF) to filter out observations of short-term and dynamic features from multiple deployments of the robot. The remaining long-term observations are used to build a vector map by robust local linear regression. The uncertainty in the resulting LTVM is computed via Monte Carlo resampling the observations arising from long-term features. By combining occupancy-grid based SDF filtering of observations with continuous space regression of the filtered observations, our proposed approach builds, updates, and amends LTVMs over time, reasoning about all observations from all robot deployments in an environment. We present experimental results demonstrating the accuracy, robustness, and compact nature of the extracted LTVMs from several long-term robot datasets.},
keywords={Robots;Uncertainty;Robustness;Feature extraction;Heuristic algorithms;Monte Carlo methods;Laser noise},
doi={10.1109/IROS.2016.7759683},
ISSN={2153-0866},
month={Oct},}
@ARTICLE{9690856,
author={Jang, Junwoo and Kim, Jinwhan},
journal={IEEE Access}, title={Topographic SLAM Using a Single Terrain Altimeter in GNSS-Restricted Environment},
year={2022},
volume={10},
number={},
pages={10806-10815},
abstract={In a Global Navigation Satellite System (GNSS)-restricted area, a mobile robot navigation system exploits surrounding environment information. For an aerial or underwater vehicle, undulating terrain of a land or seabed surface is a valuable information resource that leads to the development of terrain-referenced navigation (TRN) algorithms. However, due to the vast amount of a vehicle’s activity area, surveying all the regions to obtain a high-resolution terrain map is impractical and requires simultaneous localization and mapping (SLAM) as a highly desirable capability. This paper presents a topographic SLAM algorithm using only a single terrain altimeter, which is low-cost, computationally efficient, and sufficiently stable for long-term operation. The proposed rectangular panel map structure and update method enable robust and efficient SLAM. As terrain elevation changes are inherently nonlinear, an extended Kalman filter (EKF)-based SLAM filter is adopted. The feasibility and validity of the proposed algorithm are demonstrated through simulations using terrain elevation data from a real-world undersea environment.},
keywords={Simultaneous localization and mapping;Navigation;Global navigation satellite system;Sensors;Surface topography;Kalman filters;Sea measurements;Terrain-referenced navigation;simultaneous localization and mapping;extended Kalman filter;topography;bathymetry;autonomous underwater vehicle},
doi={10.1109/ACCESS.2022.3145978},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{6907397,
author={Pomerleau, Francois and Krüsi, Philipp and Colas, Francis and Furgale, Paul and Siegwart, Roland},
booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, title={Long-term 3D map maintenance in dynamic environments},
year={2014},
volume={},
number={},
pages={3712-3719},
abstract={New applications of mobile robotics in dynamic urban areas require more than the single-session geometric maps that have dominated simultaneous localization and mapping (SLAM) research to date; maps must be updated as the environment changes and include a semantic layer (such as road network information) to aid motion planning in dynamic environments. We present an algorithm for long-term localization and mapping in real time using a three-dimensional (3D) laser scanner. The system infers the static or dynamic state of each 3D point in the environment based on repeated observations. The velocity of each dynamic point is estimated without requiring object models or explicit clustering of the points. At any time, the system is able to produce a most-likely representation of underlying static scene geometry. By storing the time history of velocities, we can infer the dominant motion patterns within the map. The result is an online mapping and localization system specifically designed to enable long-term autonomy within highly dynamic environments. We validate the approach using data collected around the campus of ETH Zurich over seven months and several kilometers of navigation. To the best of our knowledge, this is the first work to unify long-term map update with tracking of dynamic objects.},
keywords={Three-dimensional displays;Dynamics;Simultaneous localization and mapping;Heuristic algorithms;Laser modes;Long-term mapping;dynamic obstacles;ICP;kd-tree;registration;scan matching;robot;SLAM},
doi={10.1109/ICRA.2014.6907397},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{8205992,
author={Qiu, Kejie and Shen, Shaojie},
booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Model-aided monocular visual-inertial state estimation and dense mapping},
year={2017},
volume={},
number={},
pages={1783-1789},
abstract={Robust state estimation and real-time dense mapping are two core capabilities for autonomous navigation of mobile robots. Global Navigation Satellite System (GNSS) and visual odometry/SLAM are popular methods for state estimation. However, when working between tall buildings or in indoor environments, GNSS fails due to limited sky view or obstruction from buildings. Visual odometry/SLAM are prone to long-term drifting in the absence of reliable loop closure detection. A state estimation method with global-consistent guarantee is desirable for navigation applications. As for real-time mapping, SLAM methods usually get a sparse map that is not good enough for obstacle avoidance and path-planning, and high-quality dense mapping is often computationally too demanding for mobile devices. Realizing the availability of city-scale 3D models, in this work, we improve our previous work on model-based global localization, and propose a model-aided monocular visual-inertial state estimation and dense mapping solution. We first develop a global-consistent state estimator by fusing visual-inertial odometry with the model-based localization results. Utilizing depth prior from the model, we perform motion stereo with semi-global disparity smoothing. Our dense mapping pipeline is capable of online detection of obstacles that are originally not included in the offline 3D model. Our method runs onboard an embedded computer in real-time. We validate both the state estimation and mapping accuracy in real-world experiments.},
keywords={Solid modeling;Cameras;Three-dimensional displays;Computational modeling;Image edge detection;State estimation;Real-time systems},
doi={10.1109/IROS.2017.8205992},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{8593854,
author={Egger, Philipp and Borges, Paulo V K and Catt, Gavin and Pfrunder, Andreas and Siegwart, Roland and Dubé, Renaud},
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization},
year={2018},
volume={},
number={},
pages={3430-3437},
abstract={Reliable long-term localization is key for robotic systems in dynamic environments. In this paper, we propose a novel approach for long-term localization using 3D LiDARs, coined PoseMap. In essence, we extract distinctive features from range measurements and bundle these into local views along with observation poses. The sensor's trajectory is then estimated in a sliding window fashion by matching current and old features and minimizing the distances in-between. The map representation facilitates finding a suitable set of old features, by selecting the closest local map(s) for matching. Similarly to a visibility analysis, this procedure provides a suitable set of features for localization but at a fraction of the computational cost. PoseMap also allows for updates and extensions of the map at any time by replacing and adding local maps when necessary. We evaluate our approach using two platforms both equipped with a 3D LiDAR and an IMU, demonstrating localization at 8 Hz and robustness to changes in the environment such as moving vehicles and changing vegetation. PoseMap was implemented on an autonomous vehicle allowing it to drive autonomously over a period of 18 months through a mix of industrial and unstructured off-road environments, covering more than 100 kms without a single localization failure.},
keywords={Simultaneous localization and mapping;Three-dimensional displays;Laser radar;Optimization;Feature extraction},
doi={10.1109/IROS.2018.8593854},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{8781616,
author={Ganti, Pranav and Waslander, Steven L.},
booktitle={2019 16th Conference on Computer and Robot Vision (CRV)}, title={Network Uncertainty Informed Semantic Feature Selection for Visual SLAM},
year={2019},
volume={},
number={},
pages={121-128},
abstract={In order to facilitate long-term localization using a visual simultaneous localization and mapping (SLAM) algorithm, careful feature selection can help ensure that reference points persist over long durations and the runtime and storage complexity of the algorithm remain consistent. We present SIVO (Semantically Informed Visual Odometry and Mapping), a novel information-theoretic feature selection method for visual SLAM which incorporates semantic segmentation and neural network uncertainty into the feature selection pipeline. Our algorithm selects points which provide the highest reduction in Shannon entropy between the entropy of the current state and the joint entropy of the state, given the addition of the new feature with the classification entropy of the feature from a Bayesian neural network. Each selected feature significantly reduces the uncertainty of the vehicle state and has been detected to be a static object (building, traffic sign, etc.) repeatedly with a high confidence. This selection strategy generates a sparse map which can facilitate long-term localization. The KITTI odometry dataset is used to evaluate our method, and we also compare our results against ORB_SLAM2. Overall, SIVO performs comparably to the baseline method while reducing the map size by almost 70%.},
keywords={Simultaneous localization and mapping;Feature extraction;Uncertainty;Artificial neural networks;Semantics;Entropy;Visualization;Localization;Mapping;SLAM;Deep Learning;Information Theory;Semantic Segmentation},
doi={10.1109/CRV.2019.00024},
ISSN={},
month={May},}
@ARTICLE{9366410,
author={Meng, Qingyu and Guo, Hongyan and Zhao, Xiaoming and Cao, Dongpu and Chen, Hong},
journal={IEEE/ASME Transactions on Mechatronics}, title={Loop-Closure Detection With a Multiresolution Point Cloud Histogram Mode in Lidar Odometry and Mapping for Intelligent Vehicles},
year={2021},
volume={26},
number={3},
pages={1307-1317},
abstract={Precise positioning is the basic condition for intelligent vehicles to complete perception, decision making and control tasks. In response to this challenge, in this article, lidar simultaneous localization and mapping (SLAM) is taken as the research object, and a SLAM system is designed that integrates motion compensation and ground information removal functions, and can construct a real-time environment map and determine its own position on the map while the vehicle is driving. A loop-closure detection method with a multiresolution point cloud histogram mode is proposed, which can effectively detect whether the vehicle passes through the same position and perform optimization to obtain globally consistent pose and map information in the urban conditions with more driving loops. We conduct experiments on the well-known KITTI dataset and compare the results with those of state-of-the-art systems. The experiments confirm that the lidar SLAM system designed in this article can provide accurate and effective positioning information for intelligent vehicles. The proposed loop-closure detection algorithm has an excellent real-time performance and accuracy, which can guarantee the long-term driving operation of these vehicles.},
keywords={Three-dimensional displays;Laser radar;Intelligent vehicles;Simultaneous localization and mapping;Histograms;Trajectory;Feature extraction;KITTI dataset;lidar simultaneous localization and mapping (SLAM);loop-closure detection;multiresolution histogram;pose estimation},
doi={10.1109/TMECH.2021.3062647},
ISSN={1941-014X},
month={June},}
@INPROCEEDINGS{6907415,
author={Ozog, Paul and Eustice, Ryan M.},
booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, title={Toward long-term, automated ship hull inspection with visual SLAM, explicit surface optimization, and generic graph-sparsification},
year={2014},
volume={},
number={},
pages={3832-3839},
abstract={This paper reports on a method for an autonomous underwater vehicle to perform real-time visual simultaneous localization and mapping (SLAM) on large ship hulls over multiple sessions. Along with a monocular camera, our method uses a piecewise-planar model to explicitly optimize the ship hull surface in our factor-graph framework, and anchor nodes to co-register multiple surveys. To enable realtime performance for long-term SLAM, we use the recent Generic Linear Constraints (GLC) framework to sparsify our factor-graph. This paper analyzes how our single-session SLAM techniques can be used in the GLC framework, and describes a particle filter reacquisition algorithm so that an underwater session can be automatically re-localized to a previously built SLAM graph. We provide real-world experimental results involving automated ship hull inspection, and show that our localization filter out-performs Fast Appearance-Based Mapping (FAB-MAP), a popular place-recognition system. Using our approach, we can automatically align surveys that were taken days, months, and even years apart.},
keywords={Simultaneous localization and mapping;Marine vehicles;Visualization;Cameras;Sonar navigation},
doi={10.1109/ICRA.2014.6907415},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{9230237,
author={Jiono, Mahfud and Mahandi, Yogi Dwi and Norma Mustika, Soraya and Sendari, Siti and Dzikri, Adam Maulana},
booktitle={2020 4th International Conference on Vocational Education and Training (ICOVET)}, title={Self Localization Based On Neighborhood Probability Mapping for Humanoid Robot},
year={2020},
volume={},
number={},
pages={355-359},
abstract={The humanoid robot competition is an autonomous robot with a human-like body platform with a single camera as a vision sensor and balancing sensor to support them to play soccer in the specific field. The technical challenges in this competition such following the ball, running during search the ball, dynamic walking, kicking while maintaining the balance body condition, decision making with other robot, localization and mapping as research issues investigated in the Humanoid competition. Localization and mapping still big challenges in humanoid competition, it was only single camera is used in competition rule and no others sensor to support the position and orientation during playing the game. The proposed system was developed is neighborhood probability mapping. The long-term goal of this research is to realize an ideal system to accelerate the redesign field condition and implementation process in a humanoid robot that can be monitored in real-time. The aim of this research is to take the opportunities: (a) increasing the robot's performance of vision and intelligence on the humanoid robot; (b) with this SLAM method the robot can distinguish between the balls that are in the field and outside the field; (c) able to distinguish the enemy goal from the goal itself based on goal detection and line detection; (d) the goal keeper robot capable of acting as an attacker and scanning the kick towards the enemy goal. The testing condition was implemented between simulation testing and real testing in same times. Based on the data experimental result, the robot can estimate their position and orientation during searching the ball position, goal position and obstacle coordinate with high real time accuracy. The result shows that the proposed system can be applied to the humanoid soccer robot in the real time directly and it worked with less error.},
keywords={Robots;Robot kinematics;Image color analysis;Humanoid robots;Sports;Robot sensing systems;Feature extraction;humanoid robot;localization and mapping;neighborhood propability;single camera based},
doi={10.1109/ICOVET50258.2020.9230237},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8968550,
author={Ding, Xiaqing and Wang, Yue and Tang, Li and Yin, Huan and Xiong, Rong},
booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Communication constrained cloud-based long-term visual localization in real time},
year={2019},
volume={},
number={},
pages={2159-2166},
abstract={Visual localization is one of the primary capabilities for mobile robots. Long-term visual localization in real time is particularly challenging, in which the robot is required to efficiently localize itself using visual data where appearance may change significantly over time. In this paper, we propose a cloud-based visual localization system targeting at long-term localization in real time. On the robot, we employ two estimators to achieve accurate and real-time performance. One is a sliding-window based visual inertial odometry, which integrates constraints from consecutive observations and self-motion measurements, as well as the constraints induced by localization results from the cloud. This estimator builds a local visual submap as the virtual observation which is then sent to the cloud as new localization constraints. The other one is a delayed state Extended Kalman Filter to fuse the pose of the robot localized from the cloud, the local odometry and the high-frequency inertial measurements. On the cloud, we propose a longer sliding-window based localization method to aggregate the virtual observations for larger field of view, leading to more robust alignment between virtual observations and the map. Under this architecture, the robot can achieve drift-free and real-time localization using onboard resources even in a network with limited bandwidth, high latency and existence of package loss, which enables the autonomous navigation in real-world environment. We evaluate the effectiveness of our system on a dataset with challenging seasonal and illuminative variations. We further validate the robustness of the system under challenging network conditions.},
keywords={},
doi={10.1109/IROS40897.2019.8968550},
ISSN={2153-0866},
month={Nov},}
@INPROCEEDINGS{7799127,
author={Mu, Beipeng and Giamou, Matthew and Paull, Liam and Agha-mohammadi, Ali-akbar and Leonard, John and How, Jonathan},
booktitle={2016 IEEE 55th Conference on Decision and Control (CDC)}, title={Information-based Active SLAM via topological feature graphs},
year={2016},
volume={},
number={},
pages={5583-5590},
abstract={Exploring an unknown space and building maps is a fundamental capability for mobile robots. For fully autonomous systems, the robot would further need to actively plan its paths during exploration. The problem of designing robot trajectories to actively explore an unknown environment and minimize the map error is referred to as active simultaneous localization and mapping (active SLAM). Existing work has focused on planning paths with occupancy grid maps, which do not scale well and suffer from long term drift. This work proposes a Topological Feature Graph (TFG) representation that scales well and develops an active SLAM algorithm with it. The TFG uses graphical models, which utilize independences between variables, and enables a unified quantification of exploration and exploitation gains with a single entropy metric. Hence, it facilitates a natural and principled balance between map exploration and refinement. A probabilistic roadmap path-planner is used to generate robot paths in real time. Experimental results demonstrate that the proposed approach achieves better accuracy than a standard grid-map based approach while requiring orders of magnitude less computation and memory resources.},
keywords={Simultaneous localization and mapping;Trajectory;Uncertainty;Planning;Entropy;Measurement},
doi={10.1109/CDC.2016.7799127},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9712794,
author={Long, Yixuan and Jiang, Rui and Ye, Fang},
booktitle={2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE)}, title={Bio-inspired Relocalization for Indoor Robots in Visual Ambiguous Scenarios},
year={2022},
volume={},
number={},
pages={685-689},
abstract={In recent years, robots have been widely used in the service industry to improve work efficiency. An indoor service robot should have long-term autonomous adaptability and this is achieved by performing lifelong simultaneous localization and mapping (SLAM). However, when a robot wakes up to SLAM, it needs to relocate itself first. In real-world applications, the visual ambiguous environment which contains multiple locations with similar appearances or features is a challenging scenario for localization. Considering the insufficient expression ability of a single sensor, this paper proposes a bio-inspired relocalization method to deal with this problem. The local view cells model maintains multi-hypotheses to provide visual-based coarse relocalization. Then, an obstacle cell model converts the self-centered lidar information into allocentric representation. And it serves as the fine relocalization. A continuous attractor neural network (CANN) is applied to integrate the candidates obtained through the above two stages. Finally, the result of relocalization is selected by a double-checking mechanism. In the experiments, the success rate of the proposed method reaches 95 % in total and 75% in the most challenging scene. The translation error in all scenes is less than 0.15m.},
keywords={Location awareness;Visualization;Simultaneous localization and mapping;Uncertainty;Laser radar;Service robots;Biological system modeling;relocalization;bio-inspired;indoor robots;visual ambiguous scene;RatSLAM},
doi={10.1109/ICCECE54139.2022.9712794},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8885807,
author={Schmuck, Patrik and Chli, Margarita},
booktitle={2019 International Conference on 3D Vision (3DV)}, title={On the Redundancy Detection in Keyframe-Based SLAM},
year={2019},
volume={},
number={},
pages={594-603},
abstract={Egomotion and scene estimation is a key component in automating robot navigation, as well as in virtual reality applications for mobile phones or head-mounted displays. It is well known, however, that with long exploratory trajectories and multi-session mapping for long-term autonomy or collaborative applications, the maintenance of the ever-increasing size of these maps quickly becomes a bottleneck. With the explosion of data resulting in increasing runtime of the optimization algorithms ensuring the accuracy of the Simultaneous Localization And Mapping (SLAM) estimates, the large quantity of collected experiences is imposing hard limits on the scalability of such techniques. Considering the keyframe-based paradigm of SLAM techniques, this paper investigates the redundancy inherent in SLAM maps, by quantifying the information of different experiences of the scene as encoded in keyframes. Here we propose and evaluate different information-theoretic and heuristic metrics to remove dispensable scene measurements with minimal impact on the accuracy of the SLAM estimates. Evaluating the proposed metrics in two state-of-the-art centralized collaborative SLAM systems, we provide our key insights into how to identify redundancy in keyframe-based SLAM.},
keywords={Simultaneous localization and mapping;Redundancy;Collaboration;Visualization;Three-dimensional displays;Optimization;SLAM;Collaborative SLAM;Redundancy Detection;Keyframe Selection;Multi Robot Systems;Graph Compression},
doi={10.1109/3DV.2019.00071},
ISSN={2475-7888},
month={Sep.},}
@INPROCEEDINGS{6942926,
author={Labbé, Mathieu and Michaud, François},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={Online global loop closure detection for large-scale multi-session graph-based SLAM},
year={2014},
volume={},
number={},
pages={2661-2666},
abstract={For large-scale and long-term simultaneous localization and mapping (SLAM), a robot has to deal with unknown initial positioning caused by either the kidnapped robot problem or multi-session mapping. This paper addresses these problems by tying the SLAM system with a global loop closure detection approach, which intrinsically handles these situations. However, online processing for global loop closure detection approaches is generally influenced by the size of the environment. The proposed graph-based SLAM system uses a memory management approach that only consider portions of the map to satisfy online processing requirements. The approach is tested and demonstrated using five indoor mapping sessions of a building using a robot equipped with a laser rangefinder and a Kinect.},
keywords={Optimization;Simultaneous localization and mapping;Memory management;Visualization;Lasers;Three-dimensional displays},
doi={10.1109/IROS.2014.6942926},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{6840104,
author={Caballero, F. and Perez, J. and Merino, L.},
booktitle={ISR/Robotik 2014; 41st International Symposium on Robotics}, title={Long-term Ground Robot Localization Architecture for Mixed Indoor-Outdoor Scenarios},
year={2014},
volume={},
number={},
pages={1-8},
abstract={This paper summarizes the validation and experimental results of an architecture for six degree-of-freedom robot localization developed in the framework of the EC funded project FROG (FP7-ICT-2011.2.1). Two main localization issues are considered; one is accuracy, required by the Augmented Reality application, and the second is robustness, in order to achieve long-term autonomy of the robot. The experiments were carried out mainly at the Lisbon Zoo (Portugal), a low GPS visibility area with more than 40,000 square meters and non-planar routes as long as 1 kilometer. The approach considers an offline SLAM and multi-sensor data fusion for map building, and a Rao-Blackwellized filter for online robot localization based on previously computed map. The approach also considers localization failures and provides a method for robot re-localization based on visual place recognition.},
keywords={},
doi={},
url={https://ieeexplore.ieee.org/document/6840104},
ISSN={},
month={June},}
@ARTICLE{8734150,
author={Yin, Huan and Wang, Yue and Ding, Xiaqing and Tang, Li and Huang, Shoudong and Xiong, Rong},
journal={IEEE Transactions on Intelligent Transportation Systems}, title={3D LiDAR-Based Global Localization Using Siamese Neural Network},
year={2020},
volume={21},
number={4},
pages={1380-1392},
abstract={Global localization in 3D point clouds is a challenging task for mobile vehicles in outdoor scenarios, which requires the vehicle to localize itself correctly in a given map without prior knowledge of its pose. This is a critical component of autonomous vehicles or robots on the road for handling localization failures. In this paper, based on reduced dimension scan representations learned from neural networks, a solution to global localization is proposed by achieving place recognition first and then metric pose estimation in the global prior map. Specifically, we present a semi-handcrafted feature learning method for 3D Light detection and ranging (LiDAR) point clouds using artificial statistics and siamese network, which transforms the place recognition problem into a similarity modeling problem. Additionally, the sensor data using dimension reduced representations require less storage space and make the searching easier. With the learned representations by networks and the global poses, a prior map is built and used in the localization framework. In the localization step, position only observations obtained by place recognition are used in a particle filter algorithm to achieve precise pose estimation. To demonstrate the effectiveness of our place recognition and localization approach, KITTI benchmark and our multi-session datasets are employed for comparison with other geometric-based algorithms. The results show that our system can achieve both high accuracy and efficiency for long-term autonomy.},
keywords={Three-dimensional displays;Laser radar;Pose estimation;Neural networks;Task analysis;Robot sensing systems;Measurement;Mobile vehicles;place recognition;siamese network;global localization},
doi={10.1109/TITS.2019.2905046},
ISSN={1558-0016},
month={April},}
@INPROCEEDINGS{7138985,
author={Linegar, Chris and Churchill, Winston and Newman, Paul},
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, title={Work smart, not hard: Recalling relevant experiences for vast-scale but time-constrained localisation},
year={2015},
volume={},
number={},
pages={90-97},
abstract={This paper is about life-long vast-scale localisation in spite of changes in weather, lighting and scene structure. Building upon our previous work in Experience-based Navigation [1], we continually grow and curate a visual map of the world that explicitly supports multiple representations of the same place. We refer to these representations as experiences, where a single experience captures the appearance of an environment under certain conditions. Pedagogically, an experience can be thought of as a visual memory. By accumulating experiences we are able to handle cyclic appearance change (diurnal lighting, seasonal changes, and extreme weather conditions) and also adapt to slow structural change. This strategy, although elegant and effective, poses a new challenge: In a region with many stored representations - which one(s) should we try to localise against given finite computational resources? By learning from our previous use of the experience-map, we can make predictions about which memories we should consider next, conditioned on how the robot is currently localised in the experience-map. During localisation, we prioritise the loading of past experiences in order to minimise the expected computation required. We do this in a probabilistic way and show that this memory policy significantly improves localisation efficiency, enabling long-term autonomy on robots with limited computational resources. We demonstrate and evaluate our system over three challenging datasets, totalling 206km of outdoor travel. We demonstrate the system in a diverse range of lighting and weather conditions, scene clutter, camera occlusions, and permanent structural change in the environment.},
keywords={Robots;Visualization;Cameras;Trajectory;Meteorology;Lighting;Navigation},
doi={10.1109/ICRA.2015.7138985},
ISSN={1050-4729},
month={May},}
@ARTICLE{8853391,
author={Ding, Xiaqing and Wang, Yue and Xiong, Rong and Li, Dongxuan and Tang, Li and Yin, Huan and Zhao, Liang},
journal={IEEE Transactions on Intelligent Transportation Systems}, title={Persistent Stereo Visual Localization on Cross-Modal Invariant Map},
year={2020},
volume={21},
number={11},
pages={4646-4658},
abstract={Autonomous mobile vehicles are expected to perform persistent and accurate localization with low-cost equipment. To achieve this goal, we propose a stereo camera based visual localization method using a modified laser map, which takes the advantage of both the low cost of camera, and high geometric precision of laser data to achieve long-term performance. Considering that LiDAR and camera give measurements of the same environment in different modalities, the cross-modal invariance is investigated to modify the laser map for visual localization. Specifically, a map learning algorithm is introduced to sample the robust subsets in laser maps that are useful for visual localization using multi-session visual and laser data. Further, a generative map model is derived to describe this cross-modal invariance, based on which two types of measurements are defined to model the laser map points as appropriate visual observations. Tightly coupling these measurements within the local bundle adjustment during online sliding-window based visual odometry, the vehicle can achieve robust localization even one year after the map was built. The effectiveness of the proposed method is evaluated on both the public KITTI datasets and self-collected datasets in our campus, which include seasonal, illumination and object variations. On all experimental localization sessions, our method provides satisfactory results, even when the direction is opposite to that in the mapping session, verifying the superior performance of the laser map based visual localization method.},
keywords={Visualization;Cameras;Measurement by laser beam;Laser modes;Maintenance engineering;Laser stability;Visual localization;persistent autonomy;map maintenance;map incorporated bundle adjustment},
doi={10.1109/TITS.2019.2942760},
ISSN={1558-0016},
month={Nov},}
@INPROCEEDINGS{9649028,
author={Kozlov, Daniil and Myasnikov, Vladislav},
booktitle={2021 International Conference on Information Technology and Nanotechnology (ITNT)}, title={Development of an Autonomous Robotic System Using the Graph-based SPLAM Algorithm},
year={2021},
volume={},
number={},
pages={1-5},
abstract={For long-term planning, localization and mapping, the robot must constantly update the map by the changing environment and new areas that the robot is exploring. At the same time, this map should not take up too much of the robot’s memory, since the robot’s performance is limited due to the small size of the robot and increased performance requirements. The robot must interact with the map on time, updating its location to build a further route to explore areas that have not been visited. In addition to compiling a map, when solving the problem of exploration rooms, the following steps are also important: forming a plan for bypassing an unknown room, calculating the trajectory, resolving collisions with obstacles, and following the trajectory. In the course of this work, an autonomous robotic system was developed, the task of which is to map previously unknown premises. For this, SPLAM algorithms, algorithms for building map and working with graphs, algorithms for following a trajectory were used.},
keywords={Measurement;Space vehicles;Memory management;Robot vision systems;Production;Real-time systems;Trajectory;SPLAM;SLAM;robot;ROS;RTABMap;Voronoi diagram;bang-bang controller;Jetson;Zed;point cloud;odometry;Dijkstra algorithm},
doi={10.1109/ITNT52450.2021.9649028},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9197225,
author={Peng, Cheng and Weikersdorfer, David},
booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, title={Map As the Hidden Sensor: Fast Odometry-Based Global Localization},
year={2020},
volume={},
number={},
pages={2317-2323},
abstract={Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in the long-term, our method using only odometry and the map converges in long-term. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.},
keywords={Robot sensing systems;Tensile stress;Trajectory;Robustness;Uncertainty;Real-time systems},
doi={10.1109/ICRA40945.2020.9197225},
ISSN={2577-087X},
month={May},}
@INPROCEEDINGS{1438024,
author={Spero, D.J. and Jarvis, R.A.},
booktitle={IEEE Conference on Robotics, Automation and Mechatronics, 2004.}, title={Towards exteroceptive based localisation},
year={2004},
volume={2},
number={},
pages={822-827 vol.2},
abstract={The intelligent application of a mobile robot, outside the experimental laboratory, requires a robust locomotive strategy that is rarely conducive to stringent kinematic modeling. Localisation methods that rely upon such modeling often fail, as model boundaries succumb to unpredictable events. This paper presents the development of a self-contained localisation system that purposely obviates the need for odometric information, and an associated kinematic model, to provide robot anonymity. Without odometry, the system is oblivious to the non-systematic vagaries of the robotic platform interacting with a natural domain. The proposed system hypothesises about the robot's absolute pose by algorithmically solving the kidnapped robot problem using exteroceptive based perception. Since no a priori information is assumed, long-term pose fixes are derived within a simultaneous localisation and mapping (SLAM) framework. Preliminary results were gathered using a skid steering mobile robot, equipped with a scanning laser rangefinder, in an outdoor environment. This novel localisation approach was found to be efficient and robust, while exhibiting the capacity for widespread applicability.},
keywords={Mobile robots;Robustness;Intelligent robots;Simultaneous localization and mapping;Wheels;Kinematics;Steering systems;Australia;Laboratories;Fires},
doi={10.1109/RAMECH.2004.1438024},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5940504,
author={Badino, H. and Huber, D. and Kanade, T.},
booktitle={2011 IEEE Intelligent Vehicles Symposium (IV)}, title={Visual topometric localization},
year={2011},
volume={},
number={},
pages={794-799},
abstract={One of the fundamental requirements of an autonomous vehicle is the ability to determine its location on a map. Frequently, solutions to this localization problem rely on GPS information or use expensive three dimensional (3D) sensors. In this paper, we describe a method for long-term vehicle localization based on visual features alone. Our approach utilizes a combination of topological and metric mapping, which we call topometric localization, to encode the coarse topology of the route as well as detailed metric information required for accurate localization. A topometric map is created by driving the route once and recording a database of visual features. The vehicle then localizes by matching features to this database at runtime. Since individual feature matches are unreliable, we employ a discrete Bayes filter to estimate the most likely vehicle position using evidence from a sequence of images along the route. We illustrate the approach using an 8.8 km route through an urban and suburban environment. The method achieves an average localization error of 2.7 m over this route, with isolated worst case errors on the order of 10 m.},
keywords={Vehicles;Visualization;Measurement;Databases;Feature extraction;Global Positioning System;Probability density function},
doi={10.1109/IVS.2011.5940504},
ISSN={1931-0587},
month={June},}
@INPROCEEDINGS{7081165,
author={Hidalgo, Franco and Bräunl, Thomas},
booktitle={2015 6th International Conference on Automation, Robotics and Applications (ICARA)}, title={Review of underwater SLAM techniques},
year={2015},
volume={},
number={},
pages={306-311},
abstract={SLAM (Simultaneous Localization and Mapping) for underwater vehicles is a challenging research topic due to the limitations of underwater localization sensors and error accumulation over long-term operations. Furthermore, acoustic sensors for mapping often provide noisy and distorted images or low-resolution ranging, while video images provide highly detailed images but are often limited due to turbidity and lighting. This paper presents a review of the approaches used in state-of-the-art SLAM techniques: Extended Kalman Filter SLAM (EKF-SLAM), FastSLAM, GraphSLAM and its application in underwater environments.},
keywords={Simultaneous localization and mapping;Feature extraction;Estimation;Vehicles;Simultaneous Localization and Mapping (SLAM);Extended Kalman Filter (EKF);Particle Filter (PF);FastSLAM;GraphSLAM;Underwater Vehicle;AUV},
doi={10.1109/ICARA.2015.7081165},
ISSN={},
month={Feb},}
@INPROCEEDINGS{174711,
author={Leonard, J.J. and Durrant-Whyte, H.F.},
booktitle={Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91}, title={Simultaneous map building and localization for an autonomous mobile robot},
year={1991},
volume={},
number={},
pages={1442-1447 vol.3},
abstract={Discusses a significant open problem in mobile robotics: simultaneous map building and localization, which the authors define as long-term globally referenced position estimation without a priori information. This problem is difficult because of the following paradox: to move precisely, a mobile robot must have an accurate environment map; however, to build an accurate map, the mobile robot's sensing locations must be known precisely. In this way, simultaneous map building and localization can be seen to present a question of 'which came first, the chicken or the egg?' (The map or the motion?) When using ultrasonic sensing, to overcome this issue the authors equip the vehicle with multiple servo-mounted sonar sensors, to provide a means in which a subset of environment features can be precisely learned from the robot's initial location and subsequently tracked to provide precise positioning.<>},
keywords={Mobile robots;Vehicles;Sonar navigation;Robot sensing systems;Stochastic resonance;Sensor phenomena and characterization;Target tracking;Testing;National electric code;Humans},
doi={10.1109/IROS.1991.174711},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8870928,
author={Schaefer, Alexander and Büscher, Daniel and Vertens, Johan and Luft, Lukas and Burgard, Wolfram},
booktitle={2019 European Conference on Mobile Robots (ECMR)}, title={Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from 3-D Lidar Scans},
year={2019},
volume={},
number={},
pages={1-7},
abstract={Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation [1]. In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15 months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy.},
keywords={Laser radar;Detectors;Reliability;Feature extraction;Urban areas;Trajectory;Roads},
doi={10.1109/ECMR.2019.8870928},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9423280,
author={Rotsidis, Alexandros and Lutteroth, Christof and Hall, Peter and Richardt, Christian},
booktitle={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)}, title={ExMaps: Long-Term Localization in Dynamic Scenes using Exponential Decay},
year={2021},
volume={},
number={},
pages={2866-2875},
abstract={Visual camera localization using offline maps is widespread in robotics and mobile applications. Most state-of-the-art localization approaches assume static scenes, so maps are often reconstructed once and then kept constant. However, many scenes are dynamic and as changes in the scene happen, future localization attempts may struggle or fail entirely. Therefore, it is important for successful long-term localization to update and maintain maps as new observations of the scene, and changes in it, arrive. We propose a novel method for automatically discovering which points in a map remain stable over time, and which are due to transient changes. To this end, we calculate a stability store for each point based on its visibility over time, weighted by an exponential decay over time. This allows us to consider the impact of time when scoring points, and to distinguish which points are useful for long-term localization. We evaluate our method on the CMU Extended Seasons dataset (outdoors) and a new indoor dataset of a retail shop, and show the benefit of maintaining a `live map' that integrates updates over time using our exponential decay based method over a static `base map'.},
keywords={Location awareness;Visualization;Computer vision;Conferences;Robot vision systems;Cameras;Mobile applications},
doi={10.1109/WACV48630.2021.00291},
ISSN={2642-9381},
month={Jan},}
@INPROCEEDINGS{613834,
author={Graves, K. and Adams, W. and Schultz, A.},
booktitle={Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation'}, title={Continuous localization in changing environments},
year={1997},
volume={},
number={},
pages={28-33},
abstract={Continuous localization is a technique that allows a robot to maintain an accurate estimate of its location by performing regular small corrections to its odometry. Continuous localization uses an evidence grid representation, a common representation scheme that is used by other map-dependent processes, such as path planning. Although techniques exist for building evidence grid maps, most are not adaptive to changes in the environment. In this research, we extend the continuous localization technique by adding a learning component. This allows continuous localization to update the long-term map (evidence grid) with current sensor readings. Results show that the addition of the learning behavior to continuous localization allows the system to adapt to changes in its environment without a loss in its ability to remain localized. This system was tested on a Nomad 200 mobile robot.},
keywords={Path planning;Error correction;Mobile robots;Navigation;Computer science;Artificial intelligence;Laboratories;Intelligent robots;State estimation;System testing},
doi={10.1109/CIRA.1997.613834},
ISSN={},
month={July},}
@INPROCEEDINGS{8463150,
author={Stenborg, Erik and Toft, Carl and Hammarstrand, Lars},
booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, title={Long-Term Visual Localization Using Semantically Segmented Images},
year={2018},
volume={},
number={},
pages={6484-6490},
abstract={Robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles. In this paper, we exploit recent advances in semantic segmentation of images, i.e., where each pixel is assigned a label related to the type of object it represents, to attack the problem of long-term visual localization. We show that semantically labeled 3D point maps of the environment, together with semantically segmented images, can be efficiently used for vehicle localization without the need for detailed feature descriptors (SIFT, SURF, etc.), Thus, instead of depending on hand-crafted feature descriptors, we rely on the training of an image segmenter. The resulting map takes up much less storage space compared to a traditional descriptor based map. A particle filter based semantic localization solution is compared to one based on SIFT-features, and even with large seasonal variations over the year we perform on par with the larger and more descriptive SIFT-features, and are able to localize with an error below 1 m most of the time.},
keywords={Semantics;Cameras;Roads;Image segmentation;Visualization;Robustness;Feature extraction},
doi={10.1109/ICRA.2018.8463150},
ISSN={2577-087X},
month={May},}
@ARTICLE{5751236,
author={Taylor, Clark N. and Veth, Michael J. and Raquet, John F. and Miller, Mikel M.},
journal={IEEE Transactions on Aerospace and Electronic Systems}, title={Comparison of Two Image and Inertial Sensor Fusion Techniques for Navigation in Unmapped Environments},
year={2011},
volume={47},
number={2},
pages={946-958},
abstract={To enable navigation of miniature aerial vehicles (MAVs) with a low-quality inertial measurement unit (IMU), external sensors are typically fused with the information generated by the low-quality IMU. Most commercial systems for MAVs currently fuse GPS measurements with IMU information to navigate the MAV. However there are many scenarios in which an MAV might prove useful, but GPS is not available (e.g., indoors, urban terrain, etc.). Therefore several approaches have recently been introduced that couple information from an IMU with visual information (usually captured by an electro-optical camera). In general the methods for fusing visual information with an IMU utilizes one of two techniques: 1) applying rigid body constraints on where landmarks should appear in a set of two images (constraint-based fusion) or 2) simultaneously estimating the location of features that are observed by the camera (mapping) and the location of the camera (simultaneous localization and mapping-SLAM-based fusion). While each technique has some nuances associated with its implementation in a true MAV environment (i.e., computational requirements, real-time implementation, feature tracking, etc.), this paper focuses solely on answering the question "Which fusion technique (constraint- or SLAM-based) enables more accurate long-term MAV navigation?" To answer this question, specific implementations of a constraint- and SLAM-based fusion technique, with novel modifications for improved results on MAVs, are described. A basic simulation environment is used to perform a comparison of the constraint- and SLAM-based fusion methods. We demonstrate the superiority of SLAM-based techniques in specific MAV flight scenarios and discuss the relative weaknesses and strengths of each fusion approach.},
keywords={Cameras;Visualization;Force;Global Positioning System;Kalman filters;Velocity measurement},
doi={10.1109/TAES.2011.5751236},
ISSN={1557-9603},
month={April},}
@INPROCEEDINGS{1570189,
author={Newman, P. and Kin Ho},
booktitle={Proceedings of the 2005 IEEE International Conference on Robotics and Automation}, title={SLAM-Loop Closing with Visually Salient Features},
year={2005},
volume={},
number={},
pages={635-642},
abstract={Within the context of Simultaneous Localisation and Mapping (SLAM), “loop closing” is the task of deciding whether or not a vehicle has, after an excursion of arbitrary length, returned to a previously visited area. Reliable loop closing is both essential and hard. It is without doubt one of the greatest impediments to long term, robust SLAM. This paper illustrates how visual features, used in conjunction with scanning laser data, can be used to a great advantage. We use the notion of visual saliency to focus the selection of suitable (affine invariant) image-feature descriptors for storage in a database. When queried with a recently taken image the database returns the capture time of matching images. This time information is used to discover loop closing events. Crucially this is achieved independently of estimated map and vehicle location. We integrate the above technique into a SLAM algorithm using delayed vehicle states and scan matching to form interpose geometric constraints. We present initial results using this system to close loops (around 100m) in an indoor environment.},
keywords={Simultaneous localization and mapping;Vehicles;Image storage;Image databases;Spatial databases;Visual databases;Impedance;Robustness;Focusing;Delay;Mobile Robotics;SLAM;Loop Closing;Saliency;Visual Features},
doi={10.1109/ROBOT.2005.1570189},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{6907435,
author={Biswas, Joydeep and Veloso, Manuela},
booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, title={Episodic non-Markov localization: Reasoning about short-term and long-term features},
year={2014},
volume={},
number={},
pages={3969-3974},
abstract={Markov localization and its variants are widely used for localization of mobile robots. These methods assume Markov independence of observations, implying that observations made by a robot correspond to a static map. However, in real human environments, observations include occlusions due to unmapped objects like chairs and tables, and dynamic objects like humans. We introduce an episodic non-Markov localization algorithm that maintains estimates of the belief over the trajectory of the robot while explicitly reasoning about observations and their correlations arising from unmapped static objects, moving objects, as well as objects from the static map. Observations are classified as arising from long-term features, short-term features, or dynamic features, which correspond to mapped objects, unmapped static objects, and unmapped dynamic objects respectively. By detecting time steps along the robot's trajectory where unmapped observations prior to such time steps are unrelated to those afterwards, non-Markov localization limits the history of observations and pose estimates to “episodes” over which the belief is computed. We demonstrate non-Markov localization in challenging real world indoor and outdoor environments over multiple datasets, comparing it with alternative state-of-the-art approaches, showing it to be robust as well as accurate.},
keywords={Markov processes;Cost function;Correlation;Maximum likelihood estimation;Robot kinematics;History},
doi={10.1109/ICRA.2014.6907435},
ISSN={1050-4729},
month={May},}
@ARTICLE{9361285,
author={Chen, Runjian and Yin, Huan and Jiao, Yanmei and Dissanayake, Gamini and Wang, Yue and Xiong, Rong},
journal={IEEE Robotics and Automation Letters}, title={Deep Samplable Observation Model for Global Localization and Kidnapping},
year={2021},
volume={6},
number={2},
pages={2296-2303},
abstract={Global localization and kidnapping are two challenging problems in robot localization. The popular method, Monte Carlo Localization (MCL) addresses the problem by iteratively updating a set of particles with a “sampling-weighting” loop. Sampling is decisive to the performance of MCL [1]. However, traditional MCL can only sample from a uniform distribution over the state space. Although variants of MCL propose different sampling models, they fail to provide an accurate distribution or generalize across scenes. To better deal with these problems, we present a distribution proposal model named Deep Samplable Observation Model (DSOM). DSOM takes a map and a 2D laser scan as inputs and outputs a conditional multimodal probability distribution of the pose, making the samples more focusing on the regions with higher likelihood. With such samples, the convergence is expected to be more effective and efficient. Considering that the learning-based sampling model may fail to capture the accurate pose sometimes, we furthermore propose the Adaptive Mixture MCL (AdaM MCL), which deploys a trusty mechanism to adaptively select updating mode for each particle to tolerate this situation. Equipped with DSOM, AdaM MCL can achieve more accurate estimation, faster convergence and better scalability than previous methods in both synthetic and real scenes. Even in real environments with long-term changes, AdaM MCL is able to localize the robot using DSOM trained only by simulation observations from a SLAM map or a blueprint map. Source code for this paper is available here: https://github.com/Runjian-Chen/AdaM_MCL.},
keywords={Robots;Location awareness;Proposals;Probability distribution;Adaptation models;Feature extraction;Two dimensional displays;Global localization;multimodal;samplable observation model},
doi={10.1109/LRA.2021.3061339},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{6512139,
author={Anwar, Shahzad and Qingjie Zhao and Qadeer, Nouman and Khan, Saqib Ishaq},
booktitle={Proceedings of 2013 10th International Bhurban Conference on Applied Sciences Technology (IBCAST)}, title={A framework for RF-Visual SLAM},
year={2013},
volume={},
number={},
pages={103-108},
abstract={Simultaneous Localization and Mapping, SLAM, is an important topic in the field of robotics and autonomous navigation. The metric SLAM suffers from sensor inaccuracies and thus cannot be used for long-term navigation. In such case, Visual SLAM or a Hybrid SLAM based on both metric and visual approach is a good alternative. In this paper, in order to speed up a Visual SLAM, we propose a novel concept of dynamic dictionary generated on the results of triangulation done on RF, radio frequency, signals from nearest cell towers of a cellular network. This dynamic dictionary efficiently manages the scalability of a Visual SLAM and make it possible to work in a large-scale environment. A framework is proposed along with triangulation data of a city and with simulations to support the concept.},
keywords={Dictionaries;Simultaneous localization and mapping;Poles and towers;Navigation;Hybrid power systems},
doi={10.1109/IBCAST.2013.6512139},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8675637,
author={Wilbers, Daniel and Rumberg, Lars and Stachniss, Cyrill},
booktitle={2019 Third IEEE International Conference on Robotic Computing (IRC)}, title={Approximating Marginalization with Sparse Global Priors for Sliding Window SLAM-Graphs},
year={2019},
volume={},
number={},
pages={25-31},
abstract={Most autonomous vehicles rely on some kind of map for localization or navigation. Outdated maps however are a risk to the performance of any map-based localization system applied in autonomous vehicles. It is necessary to update the used maps to ensure stable and long-term operation. We address the problem of computing landmark updates live in the vehicle, which requires efficient use of the computational resources. In particular, we employ a graph-based sliding window approach for simultaneous localization and incremental map refinement. We propose a novel method that approximates sliding window marginalization without inducing fill-in. Our method maintains the exact same sparsity pattern as without performing marginalization, but simultaneously improves the landmark estimates. The main novelty of this work is the derivation of sparse global priors that approximate dense marginalization. In comparison to state-of-the-art work, our approach utilizes global instead of local linearization points, but still minimizes linearization errors. We first approximate marginalization via Kullback-Leibler divergence and then recalculate the mean to compensate linearization errors. We evaluate our approach on simulated and real data from a prototype vehicle and compare our approach to state-of-the-art sliding window marginalization.},
keywords={Microsoft Windows;Optimization;Autonomous vehicles;Jacobian matrices;Robots;Navigation;Trajectory;SLAM;Sensor Fusion;Incremental Mapping;Localization;Automated Driving},
doi={10.1109/IRC.2019.00013},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8968082,
author={Pang, Su and Kent, Daniel and Morris, Daniel and Radha, Hayder},
booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={FLAME: Feature-Likelihood Based Mapping and Localization for Autonomous Vehicles},
year={2019},
volume={},
number={},
pages={5312-5319},
abstract={Accurate vehicle localization is arguably the most critical and fundamental task for autonomous vehicle navigation. While dense 3D point-cloud-based maps enable precise localization, they impose significant storage and transmission burdens when used in city-scale environments. In this paper, we propose a highly compressed representation for LiDAR maps, along with an efficient and robust real-time alignment algorithm for on-vehicle LiDAR scans. The proposed mapping framework, which we refer to as Feature Likelihood Acquisition Map Emulation (FLAME), requires less than 0.1% of the storage space of the original 3D point cloud map. In essence, FLAME emulates an original map through feature likelihood functions. In particular, FLAME models planar, pole and curb features. These three feature classes are long-term stable, distinct and common among vehicular roadways. Multiclass feature points are extracted from LiDAR scans through feature detection. A new multiclass-based point-to-distribution alignment method is proposed to find the association and alignment between the multiclass feature points and the FLAME map. The experimental results show that the proposed framework can achieve the same level of accuracy (less than 10cm) as the 3D point cloud based localization.},
keywords={},
doi={10.1109/IROS40897.2019.8968082},
ISSN={2153-0866},
month={Nov},}
@INPROCEEDINGS{8867531,
author={Jang, Junwoo and Kim, Jinwhan},
booktitle={OCEANS 2019 - Marseille}, title={Weighted Grid Partitioning for Panel-Based Bathymetric SLAM},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Bathymetric navigation enables the long-term operation of autonomous underwater vehicles by reducing navigation drift errors with no need for GPS position fixes. In the case that a bathymetric map is not available, the simultaneous localization and mapping (SLAM) algorithm is required, but this increases computational complexity and memory requirement. Panel-based bathymetric SLAM could considerably reduce the computational burden. However, it may suffers from incorrect update when the vehicle does not belong to the updated panel. This study proposes a new update method, called weighted grid partitioning, which considers the probability distribution of a vehicle's location, and is more effective in terms of the map accuracy, computational burden, and memory usage compared to standard update methods. The feasibility of the proposed algorithm is verified through simulations.},
keywords={Simultaneous localization and mapping;Navigation;Probability distribution;Signal processing algorithms;Measurement uncertainty;Uncertainty;Predictive models},
doi={10.1109/OCEANSE.2019.8867531},
ISSN={},
month={June},}
@INPROCEEDINGS{7404369,
author={Li, Jie and Eustice, Ryan M. and Johnson-Roberson, Matthew},
booktitle={OCEANS 2015 - MTS/IEEE Washington}, title={Underwater robot visual place recognition in the presence of dramatic appearance change},
year={2015},
volume={},
number={},
pages={1-6},
abstract={This paper reports on an algorithm for underwater visual place recognition in the presence of dramatic appearance change. Long-term visual place recognition is challenging underwater due to biofouling, corrosion, and other effects that lead to dramatic visual appearance change, which often causes traditional point-based feature methods to perform poorly. Building upon the authors' earlier work, this paper presents an algorithm for underwater vehicle place recognition and relocalization that enables an autonomous underwater vehicle (AUV) to relocalize itself to a previously-built simultaneous localization and mapping (SLAM) graph. High-level structural features are learned using a supervised learning framework that retains features that have a high potential to persist in the underwater environment. Combined with a particle filtering framework, these features are used to provide a probabilistic representation of localization confidence. The algorithm is evaluated on real data, from multiple years, collected by a Hovering Autonomous Underwater Vehicle (HAUV) for ship hull inspection.},
keywords={Visualization;Feature extraction;Atmospheric measurements;Particle measurements;Support vector machines;Vehicles;Image segmentation},
doi={10.23919/OCEANS.2015.7404369},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1574594,
author={Newman, P.},
booktitle={2005 The IEE Forum on Autonomous Systems (Ref. No. 2005/11271)}, title={Automating answers to "where am i?"},
year={2005},
volume={},
number={},
pages={7 pp.-},
abstract={In many situations, large-scale, long term deployment of an autonomous vehicle requires an ability to navigate in arbitrary workspaces and must be able to establish "where am I what surrounds me?". This paper describe simultaneous localisation and mapping (SLAM)techniques and implementations in which an autonomous vehicle explores its workspace using onboard sensors and inextricably binds together the tasks of mapping and localisation.},
keywords={},
doi={10.1049/ic:20050473},
ISSN={0537-9989},
month={Nov},}
@INPROCEEDINGS{5651229,
author={Hochdorfer, Siegfried and Schlegel, Christian},
booktitle={2010 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={6 DoF SLAM using a ToF camera: The challenge of a continuously growing number of landmarks},
year={2010},
volume={},
number={},
pages={3981-3986},
abstract={Localization and mapping are fundamental problems in service robotics since representations of the environment and knowledge about the own pose significantly simplify the implementation of a series of high-level applications. ToF (time-of-flight) cameras are a relatively new kind of sensors in robotics. They enable the real-time capture of the distance and the grayscale information of a scene. Due to the increase of the image resolution of ToF cameras, now highlevel computer vision algorithms for visual feature extraction (e.g. SIFT or SURF) can be applied to the captured images. These visual features combined with the corresponding distance information give a full measurement of 3D landmarks. An obvious problem to be solved is the continuously growing number of landmarks. So far, all ever seen landmarks are just accumulated irrespective of their utility and the then required resources. Rather, one should keep only really useful landmarks, e.g. such that localization quality in the whole operational area is kept above a given threshold. In fact a lifelong running SLAM approach is dependent on means to select and discard landmarks. That is even more acute in case of feature-rich sensor data as provided with high update rates by sensors like a ToF camera. We run our SLAM approach in a real-world experiment within an indoor environment. The experiment was performed on a P3DX-platform equipped with a PMD CamCube 2.0 and a Xsens IMU.},
keywords={Cameras;Three dimensional displays;Robot kinematics;Simultaneous localization and mapping;Robot vision systems},
doi={10.1109/IROS.2010.5651229},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{9013768,
author={Zhang, Shengkai and Wang, Wei and Tang, Sheyang and Jin, Shi and Jiang, Tao},
booktitle={2019 IEEE Global Communications Conference (GLOBECOM)}, title={Localizing Backscatters by a Single Robot with Zero Start-Up Cost},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Recent years have witnessed the rapid proliferation of low- power backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such low-power backscatter tags is crucial for IoT-based smart services. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, increasing the deployment cost. To empower universal localization service, this paper presents Rover, an indoor localization system that simultaneously localizes multiple backscatter tags with zero start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing WiFi-based positioning measurements with inertial measurements to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues such as the interference among multiple tags and the real- time processing for solving the SLAM problem. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords={Backscatter;Wireless fidelity;Antenna arrays;Interference;Simultaneous localization and mapping},
doi={10.1109/GLOBECOM38437.2019.9013768},
ISSN={2576-6813},
month={Dec},}
@INPROCEEDINGS{9636530,
author={Kurz, Gerhard and Holoch, Matthias and Biber, Peter},
booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Geometry-based Graph Pruning for Lifelong SLAM},
year={2021},
volume={},
number={},
pages={3313-3320},
abstract={Lifelong SLAM considers long-term operation of a robot where already mapped locations are revisited many times in changing environments. As a result, traditional graph-based SLAM approaches eventually become extremely slow due to the continuous growth of the graph and the loss of sparsity. Both problems can be addressed by a graph pruning algorithm. It carefully removes vertices and edges to keep the graph size reasonable while preserving the information needed to provide good SLAM results. We propose a novel method that considers geometric criteria for choosing the vertices to be pruned. It is efficient, easy to implement, and leads to a graph with evenly spread vertices that remain part of the robot trajectory. Furthermore, we present a novel approach of marginalization that is more robust to wrong loop closures than existing methods. The proposed algorithm is evaluated on two publicly available real-world long-term datasets and compared to the unpruned case as well as ground truth. We show that even on a long dataset (25h), our approach manages to keep the graph sparse and the speed high while still providing good accuracy (40 times speed up, 6cm map error compared to unpruned case).},
keywords={Simultaneous localization and mapping;Three-dimensional displays;Costs;Density functional theory;Trajectory;Standards;Intelligent robots},
doi={10.1109/IROS51168.2021.9636530},
ISSN={2153-0866},
month={Sep.},}
@ARTICLE{9357902,
author={Oelsch, Martin and Karimi, Mojtaba and Steinbach, Eckehard},
journal={IEEE Robotics and Automation Letters}, title={R-LOAM: Improving LiDAR Odometry and Mapping With Point-to-Mesh Features of a Known 3D Reference Object},
year={2021},
volume={6},
number={2},
pages={2068-2075},
abstract={LiDAR-based odometry and mapping is used in many robotic applications to retrieve the robot's position in an unknown environment and allows for autonomous operation in GPS-denied (e.g., indoor) environments. With a 3D LiDAR sensor, highly accurate localization becomes possible, which enables high quality 3D reconstruction of the environment. In this letter we extend the well-known LOAM framework by leveraging prior knowledge about a reference object in the environment to further improve the localization accuracy. This requires a known 3D model of the reference object and its known position in a global coordinate frame. Instead of only relying on the point features in the mapping module of LOAM, we also include mesh features extracted from the 3D triangular mesh of the reference object in the optimization problem. For fast correspondence computation of mesh features, we use the Axis-Aligned-Bounding-Box-Tree (AABB) structure. Essentially, our approach not only makes use of the previously built map for absolute localization in the environment, but also takes the relative position to the reference object into account, effectively reducing long-term drift. To validate the proposed concept, we generated datasets using the Gazebo simulation environment in exemplary visual inspection scenarios of an airplane inside a hangar and the Eiffel Tower. An actuated 3D LiDAR sensor is mounted via a 1-DoF gimbal on a UAV capturing 360° scans. We benchmark our approach against the state-of-the-art open-source LOAM framework. The results show that the proposed joint optimization using both point and mesh features yields a significant reduction in Absolute Pose Error (APE) and therefore improves the map and 3D reconstruction quality during long-term operations.},
keywords={Three-dimensional displays;Location awareness;Laser radar;Robot sensing systems;Optimization;Simultaneous localization and mapping;Feature extraction;SLAM;localization;mapping;range sensing},
doi={10.1109/LRA.2021.3060413},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{7152387,
author={Pastor-Moreno, Daniel and Shin, Hyo-Sang and Waldock, Antony},
booktitle={2015 International Conference on Unmanned Aircraft Systems (ICUAS)}, title={Optical flow localisation and appearance mapping (OFLAAM) for long-term navigation},
year={2015},
volume={},
number={},
pages={980-988},
abstract={This paper presents a novel method to use optical flow navigation for long term navigation. Unlike standard SLAM approaches for augmented reality, OFLAAM is designed for Micro Air Vehicles (MAV). It uses a optical flow camera pointing downwards, a IMU and a monocular camera pointing frontwards. That configuration avoids the computational expensive mapping and tracking of the 3D features. It only maps these features in a vocabulary list by a localization module to tackle the optical flow drift and the lose of the navigation estimation. That module, based on the well established algorithm DBoW2, will be also used to close the loop and allow long-term navigation in previously visited areas. The combination of high speed optical flow navigation with a low rate localization algorithm allows fully autonomous navigation for MAV, at the same time it reduces the overall computational load. This framework is implemented in ROS (Robot Operating System) and tested attached to a laptop. A representative scenario is used to validate and analyze the performance of the system.},
keywords={Cameras;Optical sensors;Optical imaging;Computers;Vehicles;Adaptive optics;High-speed optical techniques},
doi={10.1109/ICUAS.2015.7152387},
ISSN={},
month={June},}
@INPROCEEDINGS{9010305,
author={Zhu, Yilong and Xue, Bohuan and Zheng, Linwei and Huang, Huaiyang and Liu, Ming and Fan, Rui},
booktitle={2019 IEEE International Conference on Imaging Systems and Techniques (IST)}, title={Real-Time, Environmentally-Robust 3D LiDAR Localization},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Localization, or position fixing, is an important problem in robotics research. In this paper, we propose a novel approach for long-term localization in a changing environment using 3D LiDAR. We first create the map of a real environment using GPS and LiDAR. Then, we divide the map into several small parts as the targets for cloud registration, which can not only improve the robustness but also reduce the registration time. We proposed a localization method called PointLocalization. PointLocalization allows us to fuse different kinds of odometers, which can optimize the accuracy and frequency of localization results. We evaluate our algorithm on an unmanned ground vehicle (UGV) using LiDAR and a wheel encoder, and obtain the localization results at more than 20 Hz after fusion. The algorithm can also localize the UGV in a 180-degree field of view (FOV). Using an outdated map captured six months ago, this algorithm shows great robustness, and the test results show that it can achieve an accuracy of 10 cm. PointLocalization has been tested for a period of more than six months in a crowded factory and has operated successfully over a distance of more than 2000 km.},
keywords={Laser radar;Global Positioning System;Simultaneous localization and mapping;Wheels;Cameras;Three-dimensional displays},
doi={10.1109/IST48021.2019.9010305},
ISSN={1558-2809},
month={Dec},}
@INPROCEEDINGS{8814347,
author={Zhang, Mingming and Chen, Yiming and Li, Mingyang},
booktitle={2019 American Control Conference (ACC)}, title={SDF-Loc: Signed Distance Field based 2D Relocalization and Map Update in Dynamic Environments},
year={2019},
volume={},
number={},
pages={1997-2004},
abstract={To empower an autonomous robot to perform long-term navigation in a given area, a concurrent localization and map update algorithm is required. In this paper, we tackle this problem by providing both theoretical analysis and algorithm design for robotic systems equipped with 2D laser range finders. The first key contribution of this paper is that we propose a hybrid signed distance field (SDF) framework for laser based localization. The proposed hybrid SDF integrates two methods with complementary characteristics, namely Euclidean SDF (ESDF) and Truncated SDF (TSDF). With our framework, accurate pose estimation and fast map update can be performed simultaneously. Moreover, we introduce a novel sliding window estimator which attains better accuracy by consistently utilizing sensor and map information with both scan-to-scan and scan-to-map data association. Real-world experimental results demonstrate that the proposed algorithm can be used for commercial robots in various environments with long-term usage. Experiments also show that our approach outperforms competing approaches by a wide margin.},
keywords={Optimization;Robot sensing systems;Lasers;Measurement by laser beam;Two dimensional displays;Pose estimation},
doi={10.23919/ACC.2019.8814347},
ISSN={2378-5861},
month={July},}
@INPROCEEDINGS{9263722,
author={Péter, Gábor and Kiss, Bálint},
booktitle={2020 23rd International Symposium on Measurement and Control in Robotics (ISMCR)}, title={Lightweight SLAM with automatic orientation correction using 2D LiDAR scans},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Simultaneous localization and mapping (SLAM) is about consistent maps in the long run. Loop closing is the most popular way for ensure long-term consistency in presence of multiple measurements by the same or multiple robots. Loop closure can be executed using raw odometrical data, but a more sophisticated, yet still light-weight method is presented in this paper: a landmark descriptor-based relative displacement calculation method for diminishing unwanted orientation errors that otherwise often lead to map inconsistency. Landmark descriptors are created using light detection and ranging (LiDAR) scans and the relation is calculated using scan-matching. The novelty of this research is a method providing long-term orientation and position correction without additional overhead between landmark detections, thus enabling simple agents to do the SLAM in a cooperative way.},
keywords={Manganese;SLAM;LiDAR;mapping;orientation;correction;uncertainty},
doi={10.1109/ISMCR51255.2020.9263722},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9636814,
author={Bujanca, Mihai and Shi, Xuesong and Spear, Matthew and Zhao, Pengpeng and Lennox, Barry and Luján, Mikel},
booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Robust SLAM Systems: Are We There Yet?},
year={2021},
volume={},
number={},
pages={5320-5327},
abstract={Progress in the last decade has brought about significant improvements in the accuracy and speed of SLAM systems, broadening their mapping capabilities. Despite these advancements, long-term operation remains a major challenge, primarily due to the wide spectrum of perturbations robotic systems may encounter.Increasing the robustness of SLAM algorithms is an ongoing effort, however it usually addresses a specific perturbation. Generalisation of robustness across a large variety of challenging scenarios is not well-studied nor understood. This paper presents a systematic evaluation of the robustness of open-source state-of-the-art SLAM algorithms with respect to challenging conditions such as fast motion, non-uniform illumination, and dynamic scenes. The experiments are performed with perturbations present both independently of each other, as well as in combination in long-term deployment settings in unconstrained environments (lifelong operation).The detailed results (approx. 20,000 experiments) along with comprehensive documentation of the benchmarking tool for integrating new datasets and evaluating SLAM algorithms not studied in this work are available at https://robustslam.github.io/evaluation.},
keywords={Simultaneous localization and mapping;Systematics;Three-dimensional displays;Heuristic algorithms;Perturbation methods;Dynamics;Lighting},
doi={10.1109/IROS51168.2021.9636814},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{7313525,
author={Dominguez, Salvador and Khomutenko, Bodgan and Garcia, Gaëtan and Martinet, Philippe},
booktitle={2015 IEEE 18th International Conference on Intelligent Transportation Systems}, title={An Optimization Technique for Positioning Multiple Maps for Self-Driving Car's Autonomous Navigation},
year={2015},
volume={},
number={},
pages={2694-2699},
abstract={Self-driving car's navigation requires a very precise localization covering wide areas and long distances. Moreover, they have to do it at faster speeds than conventional mobile robots. This paper reports on an efficient technique to optimize the position of a sequence of maps along a journey. We take advantage of the short-term precision and reduced space on disk of the localization using 2D occupancy grid maps, from now on called sub-maps, as well as, the long-term global consistency of a Kalman filter that fuses odometry and GPS measurements. In our approach, horizontal planar LiDARs and odometry measurements are used to perform 2D-SLAM generating the sub-maps, and the EKF to generate the trajectory followed by the car in global coordinates. During the trip, after finishing each sub-map, a relaxation process is applied to a set of the last sub-maps to position them globally using both, global and map's local path. The importance of this method lies on its performance, expending low computing resources, so it can work in real time on a computer with conventional characteristics and on its robustness which makes it suitable for being used on a self-driving car as it doesn't depend excessively on the availability of GPS signal or the eventual appearance of moving objects around the car. Extensive testing has been performed in the suburbs and in the down-town of Nantes (France) covering a distance of 25 kilometers with different traffic conditions obtaining satisfactory results for autonomous driving.},
keywords={Laser radar;Global Positioning System;Splines (mathematics);Force;Trajectory;Simultaneous localization and mapping;Buildings},
doi={10.1109/ITSC.2015.433},
ISSN={2153-0017},
month={Sep.},}
@INPROCEEDINGS{7989749,
author={Fourie, Dehann and Claassens, Samuel and Pillai, Sudeep and Mata, Roxana and Leonard, John},
booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)}, title={SLAMinDB: Centralized graph databases for mobile robotics},
year={2017},
volume={},
number={},
pages={6331-6337},
abstract={Robotic systems typically require memory recall mechanisms for a variety of tasks including localization, mapping, planning, visualization etc. We argue for a novel memory recall framework that enables more complex inference schemas by separating the computation from its associated data. In this work we propose a shared, centralized data persistence layer that maintains an ensemble of online, situationally-aware robot states. This is realized through a queryable graph-database with an accompanying key-value store for larger data. In turn, this approach is scalable and enables a multitude of capabilities such as experience-based learning and long-term autonomy. Using multi-modal simultaneous localization and mapping and a few example use-cases, we demonstrate the versatility and extensible nature that centralized persistence and SLAMinDB can provide. In order to support the notion of life-long autonomy, we envision robots to be endowed with such a persistence model, enabling them to revisit previous experiences and improve upon their existing task-specific capabilities.},
keywords={Simultaneous localization and mapping;Computer architecture;Relational databases;Navigation},
doi={10.1109/ICRA.2017.7989749},
ISSN={},
month={May},}
@INPROCEEDINGS{8870347,
author={Banerjee, Nandan and Lisin, Dimitri and Briggs, Jimmy and Llofriu, Martin and Munich, Mario E.},
booktitle={2019 European Conference on Mobile Robots (ECMR)}, title={Lifelong Mapping using Adaptive Local Maps},
year={2019},
volume={},
number={},
pages={1-8},
abstract={Occupancy mapping enables a mobile robot to make intelligent planning decisions to accomplish its tasks. Adaptive local maps is an algorithm which represents the occupancy information as a set of overlapping local maps anchored to poses in the robot's trajectory. At any time, a global occupancy map can be rendered from the local maps to be used for path planning. The advantage of this approach is that the occupancy information stays consistent despite the changes in the pose estimates resulting from loop closures and localization updates. The disadvantage, however, is that the number of local maps grows over time. For long robot runs, or for multiple runs in the same space, this growth will result in redundant occupancy information, which will in turn increase the time it takes to render the global map, as well as the memory footprint of the system. In this paper, we propose a novel approach for the maintenance of an adaptive local maps system, which intelligently prunes redundant local maps, ensuring the robustness and stability required for lifelong mapping.},
keywords={Simultaneous localization and mapping;Uncertainty;Mobile robots;Trajectory},
doi={10.1109/ECMR.2019.8870347},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8461042,
author={Siva, Sriram and Zhang, Hao},
booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, title={Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition},
year={2018},
volume={},
number={},
pages={5175-5181},
abstract={Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first large-scale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.},
keywords={Feature extraction;Sensor phenomena and characterization;Simultaneous localization and mapping;Optimization},
doi={10.1109/ICRA.2018.8461042},
ISSN={2577-087X},
month={May},}
@INPROCEEDINGS{7759609,
author={Bürki, Mathias and Gilitschenski, Igor and Stumm, Elena and Siegwart, Roland and Nieto, Juan},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Appearance-based landmark selection for efficient long-term visual localization},
year={2016},
volume={},
number={},
pages={4137-4143},
abstract={In this paper, we present an online landmark selection method for distributed long-term visual localization systems in bandwidth-constrained environments. Sharing a common map for online localization provides a fleet of autonomous vehicles with the possibility to maintain and access a consistent map source, and therefore reduce redundancy while increasing efficiency. However, connectivity over a mobile network imposes strict bandwidth constraints and thus the need to minimize the amount of exchanged data. The wide range of varying appearance conditions encountered during long-term visual localization offers the potential to reduce data usage by extracting only those visual cues which are relevant at the given time. Motivated by this, we propose an unsupervised method of adaptively selecting landmarks according to how likely these landmarks are to be observable under the prevailing appearance condition. The ranking function this selection is based upon exploits landmark co-observability statistics collected in past traversals through the mapped area. Evaluation is performed over different outdoor environments, large time-scales and varying appearance conditions, including the extreme transition from day-time to night-time, demonstrating that with our appearance-dependent selection method, we can significantly reduce the amount of landmarks used for localization while maintaining or even improving the localization performance.},
keywords={Vehicles;Visualization;Servers;Bandwidth;Robots;Redundancy;Mobile computing},
doi={10.1109/IROS.2016.7759609},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{9257564,
author={Oh, Jung H. and Lee, Heung-Jae},
booktitle={2019 3rd European Conference on Electrical Engineering and Computer Science (EECS)}, title={Global Alignment of Deep Features for Robot Localization in Changing Environment},
year={2019},
volume={},
number={},
pages={72-75},
abstract={Localization is an elemental requirement for autonomous navigation, simultaneous localization and mapping for mobile robots. As robots can perform long-term and large-scale tasks, finding locations in changing environment is a crucial problem. To resolve the problem, we present a robust localization system under severe appearance changes. The system consists of two stages. First, a robust feature extraction method using a deep convolutional auto-encoder is proposed. Then, global alignment of extracted feature sequences is proposed to find the actual robot's locations. Since the proposed method not only uses the condition-robust features but also considers the actual trajectory of the robot by aligning features sequences, it can show accurate localization performances in changing environments. Experiments were conducted to prove the effective of the proposed method, and the results showed that our method outperformed than existing methods.},
keywords={Robots;Feature extraction;Visualization;Image coding;Simultaneous localization and mapping;Trajectory;Image reconstruction;robotics;localization;sequence alignment;place recognition;deep learning;auto-encoder},
doi={10.1109/EECS49779.2019.00026},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5174794,
author={Hochdorfer, Siegfried and Schlegel, Christian},
booktitle={2009 International Conference on Advanced Robotics}, title={Towards a robust visual SLAM approach: Addressing the challenge of life-long operation},
year={2009},
volume={},
number={},
pages={1-6},
abstract={Localization and mapping are fundamental problems in service robotics. Knowledge about the own pose and representations of the environment are needed for a series of high level applications. Service robots should be designed for life-long and robust operation in dynamic environments. The contribution of this paper is twofold. First, an approach to address the ever growing number of landmarks in life-long operation is presented. Typically, SLAM approaches just accumulate features over time and do not discard them anymore. Therefore, the required resources in terms of memory and processing power are growing over time. In our approach, the absolute number of landmarks can be restricted by an upper bound since we introduce a method to specifically select and replace landmarks once the upper bound has been reached. The second contribution is related to improving the robustness of the landmark assignment problem in case of image based features as needed with natural landmarks. The approach has been successfully evaluated in a real world experiment on a Pioneer-3DX platform within a complex unmodified indoor environment.},
keywords={Robustness;Simultaneous localization and mapping;Upper bound;Service robots;Computer science;Application software;Indoor environments;Collaboration;Euclidean distance},
doi={},
url={https://ieeexplore.ieee.org/document/5174794},
ISSN={},
month={June},}
@ARTICLE{8963763,
author={Clement, Lee and Gridseth, Mona and Tomasi, Justin and Kelly, Jonathan},
journal={IEEE Robotics and Automation Letters}, title={Learning Matchable Image Transformations for Long-Term Metric Visual Localization},
year={2020},
volume={5},
number={2},
pages={1492-1499},
abstract={Long-term metric self-localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems due to appearance changes caused by lighting, weather, or seasonal variations. While experience-based mapping has proven to be an effective technique for bridging the `appearance gap,' the number of experiences required for reliable metric localization over days or months can be very large, and methods for reducing the necessary number of experiences are needed for this approach to scale. Taking inspiration from color constancy theory, we learn a nonlinear RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature matches for images captured under different lighting and weather conditions, and use it as a pre-processing step in a conventional single-experience localization pipeline to improve its robustness to appearance change. We train this mapping by approximating the target non-differentiable localization pipeline with a deep neural network, and find that incorporating a learned low-dimensional context feature can further improve cross-appearance feature matching. Using synthetic and real-world datasets, we demonstrate substantial improvements in localization performance across day-night cycles, enabling continuous metric localization over a 30-hour period using a single mapping experience, and allowing experience-based localization to scale to long deployments with dramatically reduced data requirements.},
keywords={Pipelines;Feature extraction;Visualization;Training;Measurement;Lighting;Robustness;Deep learning in robotics and automation;visual learning;visual-based navigation;localization},
doi={10.1109/LRA.2020.2967659},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{9340939,
author={Qin, Tong and Chen, Tongqing and Chen, Yilun and Su, Qing},
booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot},
year={2020},
volume={},
number={},
pages={5939-5945},
abstract={Autonomous valet parking is a specific application for autonomous vehicles. In this task, vehicles need to navigate in narrow, crowded and GPS-denied parking lots. Accurate localization ability is of great importance. Traditional visual-based methods suffer from tracking lost due to texture-less regions, repeated structures, and appearance changes. In this paper, we exploit robust semantic features to build the map and localize vehicles in parking lots. Semantic features contain guide signs, parking lines, speed bumps, etc, which typically appear in parking lots. Compared with traditional features, these semantic features are long-term stable and robust to the perspective and illumination change. We adopt four surround-view cameras to increase the perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system generates a global visual semantic map. This map is further used to localize vehicles at the centimeter level. We analyze the accuracy and recall of our system and compare it against other methods in real experiments. Furthermore, we demonstrate the practicability of the proposed system by the autonomous parking application.},
keywords={Location awareness;Visualization;Navigation;Semantics;Wheels;Cameras;Autonomous vehicles},
doi={10.1109/IROS45743.2020.9340939},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{7140088,
author={Arroyo, Roberto and Alcantarilla, Pablo F. and Bergasa, Luis M. and Romera, Eduardo},
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, title={Towards life-long visual localization using an efficient matching of binary sequences from images},
year={2015},
volume={},
number={},
pages={6328-6335},
abstract={Life-long visual localization is one of the most challenging topics in robotics over the last few years. The difficulty of this task is in the strong appearance changes that a place suffers due to dynamic elements, illumination, weather or seasons. In this paper, we propose a novel method (ABLE-M) to cope with the main problems of carrying out a robust visual topological localization along time. The novelty of our approach resides in the description of sequences of monocular images as binary codes, which are extracted from a global LDB descriptor and efficiently matched using FLANN for fast nearest neighbor search. Besides, an illumination invariant technique is applied. The usage of the proposed binary description and matching method provides a reduction of memory and computational costs, which is necessary for long-term performance. Our proposal is evaluated in different life-long navigation scenarios, where ABLE-M outperforms some of the main state-of-the-art algorithms, such as WI-SURF, BRIEF-Gist, FAB-MAP or SeqSLAM. Tests are presented for four public datasets where a same route is traversed at different times of day or night, along the months or across all four seasons.},
keywords={Visualization;Lighting;Robustness;Proposals;Binary codes;Cameras;Computational efficiency},
doi={10.1109/ICRA.2015.7140088},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7989538,
author={McCormac, John and Handa, Ankur and Davison, Andrew and Leutenegger, Stefan},
booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)}, title={SemanticFusion: Dense 3D semantic mapping with convolutional neural networks},
year={2017},
volume={},
number={},
pages={4628-4635},
abstract={Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need to extend beyond geometry and appearance - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state-of-the-art dense Simultaneous Localization and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondences between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of ≈25Hz.},
keywords={Semantics;Simultaneous localization and mapping;Three-dimensional displays;Geometry;Two dimensional displays;Labeling;Cameras},
doi={10.1109/ICRA.2017.7989538},
ISSN={},
month={May},}
@ARTICLE{8633942,
author={Kim, Giseop and Park, Byungjae and Kim, Ayoung},
journal={IEEE Robotics and Automation Letters}, title={1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using Scan Context Image},
year={2019},
volume={4},
number={2},
pages={1948-1955},
abstract={In this letter, we present a long-term localization method that effectively exploits the structural information of an environment via an image format. The proposed method presents a robust year-round localization performance even when learned in just a single day. The proposed localizer learns a point cloud descriptor, named Scan Context Image (SCI), and performs robot localization on a grid map by formulating the place recognition problem as place classification using a convolutional neural network. Our method is faster than existing methods proposed for place recognition because it avoids a pairwise comparison between a query and scans in a database. In addition, we provide thorough validations using publicly available long-term datasets, the NCLT dataset and the Oxford RobotCar dataset, and show that the Scan Context Image (SCI) localization attains consistent performance over a year and outperforms existing methods.},
keywords={Three-dimensional displays;Training;Laser radar;Entropy;Databases;Robot localization;Localization;range sensing;SLAM},
doi={10.1109/LRA.2019.2897340},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{9190292,
author={Stoven-Dubois, Alexis and Dziri, Aziz and Leroy, Bertrand and Chapuis, Roland},
booktitle={2020 IEEE 23rd International Conference on Information Fusion (FUSION)}, title={Graph Optimization Methods for Large-Scale Crowdsourced Mapping},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Automotive players have recently shown an increasing interest in high-precision mapping, with the aim of enhancing vehicles safety and autonomy. Nevertheless, the acquisition, processing, and updates of accurate maps remains an economic challenge. Collaborative mapping through vehicles crowdsourcing represents a promising solution to tackle this problem. However, the potential scalability and accuracy provided by such an approach have yet to be studied and assessed. In this paper, we study the use of graph optimization in the scope of collaborative mapping. We build a map of geo-localized landmarks by crowdsourcing observations from multiple vehicles, and applying several successive map updates. We present different strategies to adapt graph optimization to the crowdsourced approach, and compare their performances in terms of map quality and scalability on simulation data. We show the critical requirement, in a long-term context, to ensure consistency of the map updates, and we propose a scalable solution which is able to build an accurate map of geolocalized landmarks.},
keywords={Simultaneous localization and mapping;Optimization;Geology;Trajectory;Estimation;Scalability;Crowdsourced Mapping;Collaborative Mapping;Graph Optimization;High-Precision Mapping;Geolocal-ization},
doi={10.23919/FUSION45008.2020.9190292},
ISSN={},
month={July},}
@INPROCEEDINGS{8843095,
author={Pitschl, Meredith L. and Pryor, Mitchell W.},
booktitle={2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)}, title={Obstacle Persistent Adaptive Map Maintenance for Autonomous Mobile Robots using Spatio-temporal Reasoning},
year={2019},
volume={},
number={},
pages={1023-1028},
abstract={Mobile robotic systems operate in increasingly realistic scenarios even as users have increased expectations for the duration of autonomous tasks. Mobile robots face unique challenges when operating in environments that change over time, where systems must maintain an accurate representation of the environment with respect to both spatial and temporal dimensions. This paper describes a spatio-temporal technique for extending the autonomy of a mobile robot in a changing environment. This new technique called Obstacle Persistent Adaptive Map Maintenance (OPAMM) uses navigation data collected during normal operations to perform periodic self-maintenance of its environment model. OPAMM implements a probabilistic feature persistence model to predict the survival state of obstacles and update the world model. Maintaining an accurate world model is necessary for extending the long-term autonomy of robots in realistic scenarios. Results show that robots using OPAMM had localizations scores higher than other methods, thus reducing long-term localization degradation.},
keywords={Conferences;Automation;Computer aided software engineering},
doi={10.1109/COASE.2019.8843095},
ISSN={2161-8089},
month={Aug},}
@INPROCEEDINGS{9468884,
author={Wang, Lisai and Chen, Weidong and Wang, Jingchuan},
booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Long-Term Localization With Time Series Map Prediction for Mobile Robots in Dynamic Environments},
year={2020},
volume={},
number={},
pages={1-7},
abstract={In many applications of mobile robot, the environment is constantly changing. How to use historical information to analysis environmental changes and generate a map corresponding with current environment is important to achieve high-precision localization. Inspired by predictive mechanism of brain, this paper presents a long-term localization approach named ArmMPU (ARMA-based Map Prediction and Update) based on time series modeling and prediction. Autoregressive moving average model (ARMA), a kind of time series modeling method, is employed for environmental map modeling and prediction, then predicted map and filtered observation are fused to fix the prediction error. The simulation and experiment results show that the proposed method improves long-term localization performance in dynamic environments.},
keywords={Location awareness;Correlation;Time series analysis;Predictive models;Brain modeling;Information filters;Real-time systems},
doi={10.1109/IROS45743.2020.9468884},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{8593562,
author={Yin, Peng and Xu, Lingyun and Liu, Zhe and Li, Lu and Salman, Hadi and He, Yuqing and Xu, Weiliang and Wang, Hesheng and Choset, Howie},
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Stabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition},
year={2018},
volume={},
number={},
pages={1162-1167},
abstract={Place recognition is one of the major challenges for the LiDAR-based effective localization and mapping task. Traditional methods are usually relying on geometry matching to achieve place recognition, where a global geometry map need to be restored. In this paper, we accomplish the place recognition task based on an end-to-end feature learning framework with the LiDAR inputs. This method consists of two core modules, a dynamic octree mapping module that generates local 2D maps with the consideration of the robot's motion; and an unsupervised place feature learning module which is an improved adversarial feature learning network with additional assistance for the long-term place recognition requirement. More specially, in place feature learning, we present an additional Generative Adversarial Network with a designed Conditional Entropy Reduction module to stabilize the feature learning process in an unsupervised manner. We evaluate the proposed method on the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental results show that the proposed method outperforms state-of-the-art in place recognition tasks under long-term applications. What's more, the feature size and inference efficiency in the proposed method are applicable in real-time performance on practical robotic platforms.},
keywords={Octrees;Laser radar;Task analysis;Decoding;Simultaneous localization and mapping;Generative adversarial networks},
doi={10.1109/IROS.2018.8593562},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{8814289,
author={Berrio, Julie Stephany and Ward, James and Worrall, Stewart and Nebot, Eduardo},
booktitle={2019 IEEE Intelligent Vehicles Symposium (IV)}, title={Identifying robust landmarks in feature-based maps},
year={2019},
volume={},
number={},
pages={1166-1172},
abstract={To operate in an urban environment, an automated vehicle must be capable of accurately estimating its position within a global map reference frame. This is necessary for optimal path planning and safe navigation. To accomplish this over an extended period of time, the global map requires long term maintenance. This includes the addition of newly observable features and the removal of transient features belonging to dynamic objects. The latter is especially important for the long-term use of the map as matching against a map with features that no longer exist can result in incorrect data associations, and consequently erroneous localisation. This paper addresses the problem of removing features from the map that correspond to objects that are no longer observable/present in the environment. This is achieved by assigning a single score which depends on the geometric distribution and characteristics when the features are re-detected (or not) on different occasions. Our approach not only eliminates ephemeral features, but can also be used as a reduction algorithm for highly dense maps. We tested our approach using half a year of weekly drives over the same 500 metre section of road in an urban environment. The results presented demonstrate the validity of the long term approach to map maintenance.},
keywords={Feature extraction;Navigation;Maintenance engineering;Vehicle dynamics;Meters;Reliability;Trajectory},
doi={10.1109/IVS.2019.8814289},
ISSN={2642-7214},
month={June},}
@ARTICLE{8950110,
author={Giubilato, Riccardo and Vayugundla, Mallikarjuna and Schuster, Martin J. and Stürzl, Wolfgang and Wedler, Armin and Triebel, Rudolph and Debei, Stefano},
journal={IEEE Robotics and Automation Letters}, title={Relocalization With Submaps: Multi-Session Mapping for Planetary Rovers Equipped With Stereo Cameras},
year={2020},
volume={5},
number={2},
pages={580-587},
abstract={To enable long term exploration of extreme environments such as planetary surfaces, heterogeneous robotic teams need the ability to localize themselves on previously built maps. While the Localization and Mapping problem for single sessions can be efficiently solved with many state of the art solutions, place recognition in natural environments still poses great challenges for the perception system of a robotic agent. In this paper we propose a relocalization pipeline which exploits both 3D and visual information from stereo cameras to detect matches across local point clouds of multiple SLAM sessions. Our solution is based on a Bag of Binary Words scheme where binarized SHOT descriptors are enriched with visual cues to recall in a fast and efficient way previously visited places. The proposed relocalization scheme is validated on challenging datasets captured using a planetary rover prototype on Mount Etna, designated as a Moon analogue environment.},
keywords={Three-dimensional displays;Visualization;Simultaneous localization and mapping;Vocabulary;Pipelines;Cameras;Localization;space robotics and automation;mapping},
doi={10.1109/LRA.2020.2964157},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{5152534,
author={Lidoris, Georgios and Rohrmuller, Florian and Wollherr, Dirk and Buss, Martin},
booktitle={2009 IEEE International Conference on Robotics and Automation}, title={The Autonomous City Explorer (ACE) project — mobile robot navigation in highly populated urban environments},
year={2009},
volume={},
number={},
pages={1416-1422},
abstract={One of the greatest challenges nowadays in robotics is the advancement of robots from industrial tools to companions and helpers of humans, operating in natural, populated environments. In this respect, the Autonomous City Explorer (ACE) project aims to combine the research fields of autonomous mobile robot navigation and human robot interaction. A robot has been created that is capable of navigating in an unknown, highly populated, urban environment, based only on information extracted through interaction with passers-by and its local perception capabilities. This paper describes the algorithms and architecture that make up the navigation subsystem of ACE. More specifically, the algorithms used for Simultaneous Localization and Mapping (SLAM), path planning in dynamic environments and behavior selection are presented, as well as the system architecture that integrates them to a complete working system. Results from an extended field experiment, where the robot navigated autonomously through the downtown city area of Munich, are analyzed and show that the robot is capable of long-term, safe navigation in real-world settings.},
keywords={Cities and towns;Mobile robots;Navigation;Robot kinematics;Service robots;Robotics and automation;Human robot interaction;Data mining;Simultaneous localization and mapping;Robot vision systems},
doi={10.1109/ROBOT.2009.5152534},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{8630513,
author={Zhang, Zhongyuan and Wang, Hesheng and Chen, Weidong},
booktitle={2018 13th World Congress on Intelligent Control and Automation (WCICA)}, title={A real-time visual-inertial mapping and localization method by fusing unstable GPS},
year={2018},
volume={},
number={},
pages={1397-1402},
abstract={This paper presents a novel method which fuse visual, IMU and GPS tightly to realize high-precision real-time localization and mapping simultaneously (SLAM). Our method is based on the bundle adjustment (BA). The confidence of the GPS signal is used to determine the window size in the local mapping thread and judge whether the keyframe is reliable. The long-term unreliable keyframe linking with large uncertainty of GPS which called GPS-restricted or GPS-denied situation will cause the drift when mapping. To eliminate the drift, in contrast to use the closed-loop detection and global optimization which will increase the computational burden extremely with the size of the map enlarged, a semi-global optimization method is proposed to relieve the burden, which make the localization estimated by this method possible to be used to navigate for unmanned vehicles. In our method, the confidence of the GPS signal is significantly important, however, the covariance supplied by the GPS receiver may not be trustworthy sometimes, which cause some unnecessary mistake when optimizing, thus a semi-supervised clustering method taking the information of GPS and IMU into account synthetically is introduced to get that confidence more robustly.},
keywords={Global Positioning System;Simultaneous localization and mapping;Cameras;Receivers;Real-time systems;Fuses},
doi={10.1109/WCICA.2018.8630513},
ISSN={},
month={July},}
@INPROCEEDINGS{8623046,
author={Hu, Xiaowei and Wang, Jingchuan and Chen, Weidong},
booktitle={2018 Chinese Automation Congress (CAC)}, title={Long-term Localization of Mobile Robots in Dynamic Changing Environments},
year={2018},
volume={},
number={},
pages={384-389},
abstract={Long-term localization in dynamic changing environments is still a challenge in robotics. Traditional localization algorithms typically assume that the environment is static. However, in many real-world applications, such as parking lots and industrial plants, there are always dynamic objects (e.g. moving people) and semi-dynamic objects (e.g. parked cars and placed goods). In this paper we address this challenge by introducing a long-term localization algorithm in the environments which combine dynamic objects and semi-dynamic objects. Localizability-based-updating particle filter (LU-P F) algorithm is proposed here. Not only we use localizability matric to build an updating mechanism, but also it is used for localization system. Besides, we propose the dynamic factor as long-memory information to serve as prior knowledge, which improves the robustness of updating process. Experiments in parking lots demonstrate that our approach has better localization results with a more accurate up-to-date map compared to other methods.},
keywords={Heuristic algorithms;Robots;Measurement by laser beam;Laboratories;Particle filters;Hidden Markov models;Mathematical model;long-term localization;map updating mechanism;localizability;dynamic factor},
doi={10.1109/CAC.2018.8623046},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9341393,
author={Zhu, Pengxiang and Ren, Wei},
booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Multi-Robot Joint Visual-Inertial Localization and 3-D Moving Object Tracking},
year={2020},
volume={},
number={},
pages={11573-11580},
abstract={In this paper, we present a novel distributed algorithm to track a moving object's state by utilizing a heterogenous mobile robot network in a three-dimensional (3-D) environment, wherein the robots' poses (positions and orientations) are unknown. Each robot is equipped with a monocular camera and an inertial measurement unit (IMU), and has the ability to communicate with its neighbors. Rather than assuming a known common global frame for all the robots (which is often the case in the literature regarding multi-robot systems), we allow each robot to perform motion estimation locally. For localization, we propose a multi-robot visual-inertial navigation systems (VINS) where one robot builds a prior map and then the map is used to bound the long-term drifts of the visual-inertial odometry (VIO) running on the other robots. Moreover, a novel distributed Kalman filter is introduced and employed to cooperatively track the six degree-of-freedom (6-DoF) motion of the object which is represented as a point cloud. Further, the object can be totally invisible to some robots during the tracking period. The proposed algorithm is extensively validated in Monte-Carlo simulations.},
keywords={Location awareness;Monte Carlo methods;Three-dimensional displays;Target tracking;Robot vision systems;Sensors;Robots},
doi={10.1109/IROS45743.2020.9341393},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{8594456,
author={Rodrigues, Rômulo T. and Aguiar, A. Pedro and Pascoal, António},
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={A B-Spline Mapping Framework for Long-Term Autonomous Operations},
year={2018},
volume={},
number={},
pages={3204-3209},
abstract={This paper presents a 2D B-spline mapping framework for representing unstructured environments in a compact manner. While occupancy-grid and landmark-based maps have been successfully employed by the robotics community in indoor scenarios, outdoor long-term autonomous operations require a more compact representation of the environment. This work tackles this problem by interpolating the data of a high frequency sensor using B-spline curves. Compared to lines and circles, splines are more powerful in the sense that they allow for the description of more complex shapes in the scene. In this work, spline curves are continuously tracked and aligned across multiple sensor readings using lightweight methods, making the proposed framework suitable for robot navigation in outdoor missions. In particular, a Simultaneous Localization and Mapping (SLAM) algorithm specifically tailored for B-spline maps is presented here. The efficacy of the proposed framework is demonstrated by Software-in-the-Loop (SiL) simulations in different scenarios.},
keywords={Splines (mathematics);Simultaneous localization and mapping;Three-dimensional displays;Robot kinematics;Two dimensional displays},
doi={10.1109/IROS.2018.8594456},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{8594310,
author={Lázaro, María T. and Capobianco, Roberto and Grisetti, Giorgio},
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Efficient Long-term Mapping in Dynamic Environments},
year={2018},
volume={},
number={},
pages={153-160},
abstract={As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.},
keywords={Simultaneous localization and mapping;Cloud computing;Three-dimensional displays;Two dimensional displays;Merging;Optimization},
doi={10.1109/IROS.2018.8594310},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{7140012,
author={Dong, Jing and Nelson, Erik and Indelman, Vadim and Michael, Nathan and Dellaert, Frank},
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, title={Distributed real-time cooperative localization and mapping using an uncertainty-aware expectation maximization approach},
year={2015},
volume={},
number={},
pages={5807-5814},
abstract={We demonstrate distributed, online, and real-time cooperative localization and mapping between multiple robots operating throughout an unknown environment using indirect measurements. We present a novel Expectation Maximization (EM) based approach to efficiently identify inlier multi-robot loop closures by incorporating robot pose uncertainty, which significantly improves the trajectory accuracy over long-term navigation. An EM and hypothesis based method is used to determine a common reference frame. We detail a 2D laser scan correspondence method to form robust correspondences between laser scans shared amongst robots. The implementation is experimentally validated using teams of aerial vehicles, and analyzed to determine its accuracy, computational efficiency, scalability to many robots, and robustness to varying environments. We demonstrate through multiple experiments that our method can efficiently build maps of large indoor and outdoor environments in a distributed, online, and real-time setting.},
keywords={Trajectory;Robot kinematics;Robot sensing systems;Lasers;Robustness;Uncertainty},
doi={10.1109/ICRA.2015.7140012},
ISSN={1050-4729},
month={May},}
@ARTICLE{8751968,
author={Fridman, Lex and Brown, Daniel E. and Glazer, Michael and Angell, William and Dodd, Spencer and Jenik, Benedikt and Terwilliger, Jack and Patsekin, Aleksandr and Kindelsberger, Julia and Ding, Li and Seaman, Sean and Mehler, Alea and Sipperley, Andrew and Pettinato, Anthony and Seppelt, Bobbie D. and Angell, Linda and Mehler, Bruce and Reimer, Bryan},
journal={IEEE Access}, title={MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving Study of Driver Behavior and Interaction With Automation},
year={2019},
volume={7},
number={},
pages={102021-102038},
abstract={Today, and possibly for a long time to come, the full driving task is too complex an activity to be fully formalized as a sensing-acting robotics system that can be explicitly solved through model-based and learning-based approaches in order to achieve full unconstrained vehicle autonomy. Localization, mapping, scene perception, vehicle control, trajectory optimization, and higher-level planning decisions associated with autonomous vehicle development remain full of open challenges. This is especially true for unconstrained, real-world operation where the margin of allowable error is extremely small and the number of edge-cases is extremely large. Until these problems are solved, human beings will remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0% to just under 100% of the driving. The governing objectives of the MIT Advanced Vehicle Technology (MIT-AVT) study are to 1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning-based internal and external perception systems; 2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology; and 3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium-term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for the analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, and CAN messages, and high-definition video streams of the driver's face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15610 days of participation, 511638 mi, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.},
keywords={Task analysis;Autonomous vehicles;Automation;Instruments;Roads;Sensors;Artificial intelligence;automation;human factors;autonomous vehicles;human-robot interaction;computer vision;machine learning;neural networks},
doi={10.1109/ACCESS.2019.2926040},
ISSN={2169-3536},
month={},}
@ARTICLE{9107480,
author={Zhang, Shengkai and Wang, Wei and Tang, Sheyang and Jin, Shi and Jiang, Tao},
journal={IEEE Transactions on Wireless Communications}, title={Robot-Assisted Backscatter Localization for IoT Applications},
year={2020},
volume={19},
number={9},
pages={5807-5818},
abstract={Recent years have witnessed the rapid proliferation of backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such backscatter tags is crucial for IoT-based smart applications. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, which is laborious for deployment. To empower universal localization service, this paper presents Rover, an indoor localization system that localizes multiple backscatter tags without any start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing measurements from backscattered WiFi signals and inertial sensors to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues including interference among multiple tags, real-time processing, as well as the data marginalization problem in dealing with degenerated motions. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords={Wireless fidelity;Backscatter;Interference;Robot sensing systems;Receivers;Antenna arrays;Backscatter;localization;inertial sensor;channel state information},
doi={10.1109/TWC.2020.2997393},
ISSN={1558-2248},
month={Sep.},}
@INPROCEEDINGS{9274964,
author={Li, ZiYuan and Yu, HuaPeng and Shen, TongSheng and Li, ZhiHui},
booktitle={2020 3rd International Conference on Unmanned Systems (ICUS)}, title={Segmented Matching Method of Multi-Geophysics Field SLAM Data Based on LSTM},
year={2020},
volume={},
number={},
pages={147-151},
abstract={At present, simultaneous localization and mapping (SLAM) has become an important method for autonomous underwater vehicles (AUVs) to realize long-term navigation. However, using only bathymetric data in unknown environment has its own disadvantages, that are low precision and large computational load. To tackle with requirements of high-precision navigation under large-scale and long-term voyage condition, a SLAM method and corresponding matching algorithm for integrating multi-geophysical field data are proposed. By dividing the feature data and location data of geophysical field obtained into various submaps and sub-segments during AUV sailing, the dominant navigation data of each segment is identified using long short-term memory network. Validity of the proposed method is done by simulation experiments. During the simulation, the loop closure detection of each submap is used, and the matching counter is set to check the correct matching rate. Finally, the matching results with single geophysics field data under the same conditions are compared with multi-geophysics field data and analyzed. The experimental results have demonstrated the feasibility and correctness of the proposed method.},
keywords={Technological innovation;Underwater vehicles;Timing;Simultaneous localization and mapping;Navigation;SLAM;LSTM;navigation;multi-geophysics field data;matching},
doi={10.1109/ICUS50048.2020.9274964},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9413161,
author={Tsamis, Georgios and Kostavelis, Ioannis and Giakoumis, Dimitrios and Tzovaras, Dimitrios},
booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, title={Towards life-long mapping of dynamic environments using temporal persistence modeling},
year={2021},
volume={},
number={},
pages={10480-10485},
abstract={The contemporary SLAM mapping systems assume a static environment and build a map that is then used for mobile robot navigation disregarding the dynamic changes in this environment. The paper at hand presents a novel solution for the problem of life-long mapping that continually updates a metric map represented as a 2D occupancy grid in large scale indoor environments with movable objects such as people, robots, objects etc. suitable for industrial applications. We formalize each cell's occupancy as a failure analysis problem and contribute temporal persistence modeling (TPM), an algorithm for probabilistic prediction of the time that a cell in an observed location is expected to be “occupied” or “empty” given sparse prior observations from a task specific mobile robot. Our work is evaluated in Gazebo simulation environment against the nominal occupancy of cells and the estimated obstacles persistence. We also show that robot navigation with life-long mapping demands less replans and leads to more efficient navigation in highly dynamic environments.},
keywords={Measurement;Simultaneous localization and mapping;Navigation;Service robots;Predictive models;Probabilistic logic;Prediction algorithms},
doi={10.1109/ICPR48806.2021.9413161},
ISSN={1051-4651},
month={Jan},}
@INPROCEEDINGS{8967994,
author={Halodová, Lucie and Dvořráková, Eliška and Majer, Filip and Vintr, Tomáš and Mozos, Oscar Martinez and Dayoub, Feras and Krajník, Tomáš},
booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Predictive and adaptive maps for long-term visual navigation in changing environments},
year={2019},
volume={},
number={},
pages={7033-7039},
abstract={In this paper, we compare different map management techniques for long-term visual navigation in changing environments. In this scenario, the navigation system needs to continuously update and refine its feature map in order to adapt to the environment appearance change. To achieve reliable long-term navigation, the map management techniques have to (i) select features useful for the current navigation task, (ii) remove features that are obsolete, (iii) and add new features from the current camera view to the map. We propose several map management strategies and evaluate their performance with regard to the robot localisation accuracy in long-term teach-and-repeat navigation. Our experiments, performed over three months, indicate that strategies which model cyclic changes of the environment appearance and predict which features are going to be visible at a particular time and location, outperform strategies which do not explicitly model the temporal evolution of the changes.},
keywords={},
doi={10.1109/IROS40897.2019.8967994},
ISSN={2153-0866},
month={Nov},}
@INPROCEEDINGS{4650701,
author={Dayoub, Feras and Duckett, Tom},
booktitle={2008 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={An adaptive appearance-based map for long-term topological localization of mobile robots},
year={2008},
volume={},
number={},
pages={3364-3369},
abstract={This work considers a mobile service robot which uses an appearance-based representation of its workplace as a map, where the current view and the map are used to estimate the current position in the environment. Due to the nature of real-world environments such as houses and offices, where the appearance keeps changing, the internal representation may become out of date after some time. To solve this problem the robot needs to be able to adapt its internal representation continually to the changes in the environment. This paper presents a method for creating an adaptive map for long-term appearance-based localization of a mobile robot using long-term and short-term memory concepts, with omni-directional vision as the external sensor.},
keywords={Feature extraction;Robots;Robot sensing systems;Noise;Approximation algorithms;Robot vision systems;Cameras},
doi={10.1109/IROS.2008.4650701},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{7784311,
author={Dubé, R. and Gawel, A. and Cadena, C. and Siegwart, R. and Freda, L. and Gianni, M.},
booktitle={2016 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)}, title={3D localization, mapping and path planning for search and rescue operations},
year={2016},
volume={},
number={},
pages={272-273},
abstract={This work presents our results on 3D robot localization, mapping and path planning for the latest joint exercise of the European project “Long-Term Human-Robot Teaming for Robots Assisted Disaster Response” (TRADR)1. The full system is operated and evaluated by firemen end-users in real-world search and rescue experiments. We demonstrate that the system is able to plan a path to a goal position desired by the fireman operator in the TRADR Operational Control Unit (OCU), using a persistent 3D map created by the robot during previous sorties.},
keywords={Three-dimensional displays;Path planning;Simultaneous localization and mapping;Navigation;Lasers},
doi={10.1109/SSRR.2016.7784311},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4209130,
author={Martinson, E. and Schultz, A.},
booktitle={Proceedings 2007 IEEE International Conference on Robotics and Automation}, title={Robotic Discovery of the Auditory Scene},
year={2007},
volume={},
number={},
pages={435-440},
abstract={In this work, we describe an autonomous mobile robotic system for finding and investigating ambient noise sources in the environment. Motivated by the large negative effect of ambient noise sources on robot audition, the long-term goal is to provide awareness of the auditory scene to a robot, so that it may more effectively act to filter out the interference or re-position itself to increase the signal-to-noise ratio. Here, we concentrate on the discovery of new sources of sound through the use of mobility and directed investigation. This is performed in a two-step process. In the first step, a mobile robot first explores the surrounding acoustical environment, creating evidence grid representations to localize the most influential sound sources in the auditory scene. Then in the second step, the robot investigates each potential sound source location in the environment so as to improve the localization result, and identify volume and directionality characteristics of the sound source. Once every source has been investigated, a noise map of the entire auditory scene is created for use by the robot in avoiding areas of loud ambient noise when performing an auditory task.},
keywords={Layout;Acoustic noise;Robotics and automation;Working environment noise;Mobile robots;Microphone arrays;Signal to noise ratio;Filters;Interference;Position measurement;Sound Source Localization;Evidence Grid;Mobile Robots;Sound Mapping},
doi={10.1109/ROBOT.2007.363825},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{8541483,
author={Zaffar, Mubariz and Ehsan, Shoaib and Stolkin, Rustam and Maier, Klaus McDonald},
booktitle={2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)}, title={Sensors, SLAM and Long-term Autonomy: A Review},
year={2018},
volume={},
number={},
pages={285-290},
abstract={Simultaneous Localization and Mapping, commonly known as SLAM, has been an active research area in the field of Robotics over the past three decades. For solving the SLAM problem, every robot is equipped with either a single sensor or a combination of similar/different sensors. This paper attempts to review, discuss, evaluate and compare these sensors. Keeping an eye on future, this paper also assesses the characteristics of these sensors against factors critical to the long-term autonomy challenge.},
keywords={Cameras;Simultaneous localization and mapping;Sensor phenomena and characterization;Laser radar;Acoustic sensors;SLAM;Long-term Autonomy;Sensors},
doi={10.1109/AHS.2018.8541483},
ISSN={2471-769X},
month={Aug},}
@INPROCEEDINGS{8833730,
author={Chen, Shilang and Wu, Junjun and Wang, Yanran and Zhou, Lin and Lu, Qinghua and Zhang, Yunzhi},
booktitle={2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM)}, title={Robust Loop-Closure Detection with a Learned Illumination Invariant Representation for Robot vSLAM},
year={2019},
volume={},
number={},
pages={342-347},
abstract={Robust loop-closure detection plays a key role for the long-term robot visual Simultaneous Localization and Mapping (SLAM) in indoor or outdoor environment, due to illumination changes can greatly affect the accuracy of online image matching, and keypoints may fail to match between images taken at the same location but different seasons. In this paper, we propose a robust loop-closure detection method for robot visual SLAM, which adopts invariant representation as image descriptors composed of learned features and adapts to changes in illumination and seasons. We evaluate our method on real datasets and demonstrate its excellent ability to handle illumination changes.},
keywords={Visualization;Simultaneous localization and mapping;Lighting;Feature extraction;Mechatronics;Image matching;Visual SLAM;Loop Closure Detection;Visual Place Recognition;Illumination Invariant Feature;Moblie Robot;Convolutional Neural Network},
doi={10.1109/ICARM.2019.8833730},
ISSN={},
month={July},}
@INPROCEEDINGS{6385677,
author={Baldwin, Ian and Newman, Paul},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={Laser-only road-vehicle localization with dual 2D push-broom LIDARS and 3D priors},
year={2012},
volume={},
number={},
pages={2490-2497},
abstract={We demonstrate the viability of using 2D LIDAR data as the sole means for accurate, robust, long-term road-vehicle localization within a prior map in a complex, dynamic real-world setting. We utilize a dual-LIDAR system - one oriented horizontally, in order to infer vehicle linear and rotational velocity, and one declined to capture a dense view of the surrounds - that allows us to estimate both velocity and position within a prior map. We show how probabilistically modelling the noisy local velocity estimates from the horizontal laser feed, fusing these estimates with data from the declined LIDAR to form a dense 3D swathe and matching this swathe statistically within a map will allow for robust, long-term position estimation. We accommodate estimation errors induced by passing vehicles, pedestrians, ground-strike etc., by learning a positional-dependent sensor model - that is, a sensor-model that varies spatially - and show that learning such a model for LIDAR data allows us to deal gracefully with the complexities of realworld data. We validate the concept over more than 9 kilometres of driven distance in and around the town of Woodstock, Oxfordshire.},
keywords={Vehicles;Laser radar;Robustness;Probabilistic logic;Noise measurement;Equations;Measurement by laser beam},
doi={10.1109/IROS.2012.6385677},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{7271751,
author={Stuntz, Andrew and Liebel, David and Smith, Ryan N.},
booktitle={OCEANS 2015 - Genova}, title={Enabling persistent autonomy for underwater gliders through terrain based navigation},
year={2015},
volume={},
number={},
pages={1-10},
abstract={To effectively examine ocean processes we must often sample over the duration of long (weeks to months) oscillation patterns. Such sampling requires persistent autonomous underwater vehicles, that have a similarly long deployment duration. Actively actuated (propeller-driven) underwater vehicles have proven effective in multiple sampling scenarios, however they have limited deployment endurance. The emergence of less actuated vehicles, i.e., underwater gliders, has enabled greater energy savings and thus increased endurance. Due to reduced actuation, these vehicles are more susceptible to external forces, e.g., ocean currents, causing them to have poor navigational and localization accuracy underwater. This is exacerbated in coastal regions, where current velocities are the same order of magnitude as the vehicle velocity. In this paper, we examine a method of reducing navigation and localization error, not only for navigation, but more so for more accurately reconstructing the path that the glider traversed to contextualize the gathered data, with respect to the science question at hand. We present a set of algorithms for offline processing that accurately localizes the traversed path of an underwater glider over long-term, ocean deployments. The proposed method utilizes terrain-based navigation with only depth, altimeter and compass data compared to local bathymetry maps to provide accurate reconstructions of traversed paths in the ocean.},
keywords={Navigation;Vehicles;Accuracy;Sea measurements;Sea surface;Trajectory},
doi={10.1109/OCEANS-Genova.2015.7271751},
ISSN={},
month={May},}
@INPROCEEDINGS{7759339,
author={Spangenberg, Robert and Goehring, Daniel and Rojas, Raúl},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Pole-based localization for autonomous vehicles in urban scenarios},
year={2016},
volume={},
number={},
pages={2161-2166},
abstract={Localization is a key capability for autonomous vehicles especially in urban scenarios. We propose the use of pole-like landmarks as primary features in these environments, as they are distinct, long-term stable and can be detected reliably with a stereo camera system. Furthermore, the resulting map representation is memory efficient, allowing for easy storage and on-line updates. The localization is performed in real-time by a stereo camera system as a main sensor, using vehicle odometry and an off-the-shelf GPS as secondary information sources. Localization is performed by a particle filter approach, coupled with an Kalman filter for robustness and sensor fusion. This leads to a lateral accuracy below 20 cm in various urban test areas. The system has been included in our autonomous test vehicle and successfully demonstrated the full loop from mapping to autonomous driving.},
keywords={Vehicles;Cameras;Global Positioning System;Atmospheric measurements;Particle measurements;Kalman filters;Robustness},
doi={10.1109/IROS.2016.7759339},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{7404433,
author={Bichucher, Vittorio and Walls, Jeffrey M. and Ozog, Paul and Skinner, Katherine A. and Eustice, Ryan M.},
booktitle={OCEANS 2015 - MTS/IEEE Washington}, title={Bathymetric factor graph SLAM with sparse point cloud alignment},
year={2015},
volume={},
number={},
pages={1-7},
abstract={This paper reports on a factor graph simultaneous localization and mapping framework for autonomous underwater vehicle localization based on terrain-aided navigation. The method requires no prior bathymetric map and only assumes that the autonomous underwater vehicle has the ability to sparsely sense the local water column depth, such as with a bottom-looking Doppler velocity log. Since dead-reckoned navigation is accurate in short time windows, the vehicle accumulates several water column depth point clouds- or submaps-during the course of its survey. We propose an xy-alignment procedure between these submaps in order to enforce consistent bathymetric structure over time, and therefore attempt to bound long-term navigation drift. We evaluate the submap alignment method in simulation and present performance results from multiple autonomous underwater vehicle field trials.},
keywords={Simultaneous localization and mapping;Three-dimensional displays;Vehicles;Trajectory;Smoothing methods;Global Positioning System},
doi={10.23919/OCEANS.2015.7404433},
ISSN={},
month={Oct},}
@ARTICLE{8964341,
author={Nie, Fuyu and Zhang, Weimin and Yao, Zhuo and Shi, Yongliang and Li, Fangxing and Huang, Qiang},
journal={IEEE Access}, title={LCPF: A Particle Filter Lidar SLAM System With Loop Detection and Correction},
year={2020},
volume={8},
number={},
pages={20401-20412},
abstract={A globally consistent map is the basis of indoor robot localization and navigation. However, map built by Rao-Blackwellized Particle Filter (RBPF) doesn’t have high global consistency which is not suitable for long-term application in large scene. To address the problem, we present an improved RBPF Lidar SLAM system with loop detection and correction named LCPF. The efficiency and accuracy of loop detection depend on the segmentation of submaps. Instead of dividing the submap at fixed number of laser scan like existing method, Dynamic Submap Segmentation is proposed in LCPF. This segmentation algorithm decreases the error inside the submap by splitting the submap where there is high scan match error and later rectifies the error by an improved pose graph optimization between submaps. In order to segment the submap at appropriate point, when to create a new submap is determined by both the accumulation of scan match error and the particle distribution. Furthermore, LCPF uses branch and bound algorithm as basic detector for loop detection and multiple criteria to judge the reliability of a loop. In the criteria, a novel parameter called usable ratio was proposed to measure the useful information that a laser scan containing. Finally, comparisons to existing 2D-Lidar mapping algorithm are performed with a series of open dataset simulations and real robot experiments to demonstrate the effectiveness of LCPF.},
keywords={Simultaneous localization and mapping;Lasers;Laser radar;Particle filters;Heuristic algorithms;Optimization;Simultaneous localization and mapping;mobile robots;indoor navigation;particle filter;loop detection;dynamic submap segementation},
doi={10.1109/ACCESS.2020.2968353},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8594481,
author={Furuta, Yuki and Okada, Kei and Kakiuchi, Yohei and Inaba, Masayuki},
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={An Everyday Robotic System that Maintains Local Rules Using Semantic Map Based on Long-Term Episodic Memory},
year={2018},
volume={},
number={},
pages={1-7},
abstract={To enable robots to work on real home environments, they have to not only consider common knowledge in the global society, but also be aware of existing rules there. Since such “local rules” are not describable beforehand, robot agents must acquire them through their lives after deployment. To achieve this, we developed a framework that a) lets robots record long-term episodic memories in their deployed environments, b) autonomously builds probabilistic object localization map as structurization of logged data and c) make adapted task plans based on the map. We equipped our framework on PR2 and Fetch robots operating and recording episodic memory for 41 days with semantic common knowledge of the environment. We also conducted demonstrations in which a PR2 robot tidied up a room, showing that the robot agent can successfully plan and execute local-rule-aware home assistive tasks by using our proposed framework.},
keywords={Task analysis;Probabilistic logic;Semantics;Planning;Robot sensing systems;Solid modeling;Service Robots;Learning and Adaptive Systems;Big Data in Robotics and Automation},
doi={10.1109/IROS.2018.8594481},
ISSN={2153-0866},
month={Oct},}
@ARTICLE{9033993,
author={Tang, Dengqing and Fang, Qiang and Shen, Lincheng and Hu, Tianjiang},
journal={IEEE/ASME Transactions on Mechatronics}, title={Onboard Detection-Tracking-Localization},
year={2020},
volume={25},
number={3},
pages={1555-1565},
abstract={This article investigates long-term positioning of moving objects by monocular vision of a miniature fixed-wing unmanned aerial vehicle. It is challenging to perform a real-time onboard vision processing task, due to the strict payload capacity and power budget limitations of microflying vehicles. We propose a parallel onboard architecture that explicitly decouples the long-term positioning task into iteratively operated detection, tracking, and localization. The proposed approach is eventually called onboard detection-tracking-localization, namely oDTL. The detector automatically extracts and identifies the object from image frames captured at in-flight durations. A learning-based network is constructed to improve detection accuracy and robustness against ever-changing outdoor illumination conditions and flying viewpoints. The tracker follows the object within specified region-of-interest from frame to frame with lower computing consumption. To further reduce target-losing rate, a concept of blind zone is proposed and applied, and its boundaries in sequential images are also theoretically inferred. The position estimator maps the flying vehicle pose, the image coordinates, and calibration specifications into real-world positions of the moving target. An extended Kalman filter is developed for rough position estimation, and a smooth module is introduced for the refinement of the position. Three offline comparative experiments and three online experiments have been conducted respectively to testify the real-time capability of our approach. The collected experimental results also demonstrate the feasible accuracy and robustness of the overall solution within the specified flying onboard scenarios.},
keywords={Robustness;Real-time systems;Visualization;Lighting;Cameras;Three-dimensional displays;IEEE transactions;Detection;localization;miniature fixed-wing unmanned aerial vehicle (UAV);monocular;onboard vision;parallel architecture;positioning;tracking},
doi={10.1109/TMECH.2020.2976794},
ISSN={1941-014X},
month={June},}
@INPROCEEDINGS{8917529,
author={Hu, Jiaxin and Yang, Ming and Xu, Hanqing and He, Yuesheng and Wang, Chunxiang},
booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)}, title={Mapping and Localization using Semantic Road Marking with Centimeter-level Accuracy in Indoor Parking Lots},
year={2019},
volume={},
number={},
pages={4068-4073},
abstract={Accurate localization is one of the fundamental tasks of vehicles visual navigation in parking lots. In this paper, we propose a practical and novel solution, which exploits road marking semantic segmentation to attack the problem of long-term and high-precision visual localization. Based on the semantic data association derived from road markings segmentation, point cloud fusion and loop detection strategies are designed to improve the performance of semantic map building. Applying the generated map, we present a point cloud registration algorithm combining semantic and geometric inference to improve the localization precision. Experiments on real-world indoor parking lots prove that the semantic map created by the proposed method reveals more accurate and consistent performance. Moreover, localization error is no more than 10cm, while running in real-time performance.},
keywords={Semantics;Three-dimensional displays;Roads;Image segmentation;Visualization;Cameras;Iterative closest point algorithm},
doi={10.1109/ITSC.2019.8917529},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9303291,
author={Yin, Huan and Wang, Yue and Tang, Li and Xiong, Rong},
booktitle={2020 IEEE International Conference on Real-time Computing and Robotics (RCAR)}, title={Radar-on-Lidar: metric radar localization on prior lidar maps},
year={2020},
volume={},
number={},
pages={1-7},
abstract={Radar and lidar, provided by two different range sensors, each has pros and cons of various perception tasks on mobile robots or autonomous driving. In this paper, a Monte Carlo system is used to localize the robot with a rotating radar sensor on 2D lidar maps. We first train a conditional generative adversarial network to transfer raw radar data to lidar data, and achieve reliable radar points from generator. Then an efficient radar odometry is included in the Monte Carlo system. Combining the initial guess from odometry, a measurement model is proposed to match the radar data and prior lidar maps for final 2D positioning. We demonstrate the effectiveness of the proposed localization framework on the public multisession dataset. The experimental results show that our system can achieve high accuracy for long-term localization in outdoor scenes.},
keywords={Laser radar;Radar;Sensors;Radar imaging;Robots;Three-dimensional displays;Two dimensional displays},
doi={10.1109/RCAR49640.2020.9303291},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6630886,
author={Delmerico, Jeffrey A. and Baran, David and David, Philip and Ryde, Julian and Corso, Jason J.},
booktitle={2013 IEEE International Conference on Robotics and Automation}, title={Ascending stairway modeling from dense depth imagery for traversability analysis},
year={2013},
volume={},
number={},
pages={2283-2290},
abstract={Localization and modeling of stairways by mobile robots can enable multi-floor exploration for those platforms capable of stair traversal. Existing approaches focus on either stairway detection or traversal, but do not address these problems in the context of path planning for the autonomous exploration of multi-floor buildings. We propose a system for detecting and modeling ascending stairways while performing simultaneous localization and mapping, such that the traversability of each stairway can be assessed by estimating its physical properties. The long-term objective of our approach is to enable exploration of multiple floors of a building by allowing stairways to be considered during path planning as traversable portals to new frontiers. We design a generative model of a stairway as a single object. We localize these models with respect to the map, and estimate the dimensions of the stairway as a whole, as well as its steps. With these estimates, a robot can determine if the stairway is traversable based on its climbing capabilities. Our system consists of two parts: a computationally efficient detector that leverages geometric cues from dense depth imagery to detect sets of ascending stairs, and a stairway modeler that uses multiple detections to infer the location and parameters of a stairway that is discovered during exploration. We demonstrate the performance of this system when deployed on several mobile platforms using a Microsoft Kinect sensor.},
keywords={Robot sensing systems;Computational modeling;Legged locomotion;Cameras;Navigation;Green products},
doi={10.1109/ICRA.2013.6630886},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{4209698,
author={Filliat, David},
booktitle={Proceedings 2007 IEEE International Conference on Robotics and Automation}, title={A visual bag of words method for interactive qualitative localization and mapping},
year={2007},
volume={},
number={},
pages={3921-3926},
abstract={Localization for low cost humanoid or animal-like personal robots has to rely on cheap sensors and has to be robust to user manipulations of the robot. We present a visual localization and map-learning system that relies on vision only and that is able to incrementally learn to recognize the different rooms of an apartment from any robot position. This system is inspired by visual categorization algorithms called bag of words methods that we modified to make fully incremental and to allow a user-interactive training. Our system is able to reliably recognize the room in which the robot is after a short training time and is stable for long term use. Empirical validation on a real robot and on an image database acquired in real environments are presented.},
keywords={Robot sensing systems;Humanoid robots;Robot vision systems;Simultaneous localization and mapping;Robotics and automation;Human robot interaction;Robustness;Shape control;Costs;Image databases},
doi={10.1109/ROBOT.2007.364080},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{6849767,
author={Pérez, Javier and Caballero, Fernando and Merino, Luis},
booktitle={2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)}, title={Integration of Monte Carlo Localization and place recognition for reliable long-term robot localization},
year={2014},
volume={},
number={},
pages={85-91},
abstract={This paper proposes extending Monte Carlo Localization methods with visual information in order to build a long term robot localization system. This system is aimed to work in crowded and non-planar scenarios, where 2D laser rangefinders may not always be enough to match the robot position with the map. Thus, visual place recognition will be used in order to obtain robot position clues that can be used to detect when the robot is lost and also to reset its positions to the right one. The paper presents experimental results based on datasets gathered with a real robot in challenging scenarios.},
keywords={Robot sensing systems;Semiconductor lasers;Robot kinematics;Trajectory;Navigation},
doi={10.1109/ICARSC.2014.6849767},
ISSN={},
month={May},}
@INPROCEEDINGS{9341003,
author={Pauls, Jan-Hendrik and Petek, Kürsat and Poggenhans, Fabian and Stiller, Christoph},
booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform},
year={2020},
volume={},
number={},
pages={4595-4601},
abstract={Easy, yet robust long-term localization is still an open topic in research. Existing approaches require either dense maps, expensive sensors, specialized map features or proprietary detectors.We propose using semantic segmentation on a monocular camera to localize directly in a HD map as used for automated driving. This combines lightweight, yet powerful HD maps with the simplicity of monocular vision and the flexibility of neural networks.The major challenges arising from this combination are data association and robustness against misdetections. Association is solved efficiently by applying distance transform on binary per-class images. This provides not only a fast lookup table for a smooth gradient as needed for pose-graph optimization, but also dynamic association by default.A sliding-window pose graph optimization combines single image detections with vehicle odometry, smoothing results and helping overcome even misclassifications in consecutive frames.Evaluation against a highly accurate 6D visual localization shows that our approach can achieve accuracy levels as required for automated driving, being one of the most lightweight and flexible methods to do so.},
keywords={Location awareness;Semantics;Neural networks;Transforms;Cameras;Vehicle dynamics;Optimization},
doi={10.1109/IROS45743.2020.9341003},
ISSN={2153-0866},
month={Oct},}
@ARTICLE{8421015,
author={Bescos, Berta and Fácil, José M. and Civera, Javier and Neira, José},
journal={IEEE Robotics and Automation Letters}, title={DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes},
year={2018},
volume={3},
number={4},
pages={4076-4083},
abstract={The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this letter we present DynaSLAM, a visual SLAM system that, building on ORB-SLAM2, adds the capabilities of dynamic object detection and background inpainting. DynaSLAM is robust in dynamic scenarios for monocular, stereo, and RGB-D configurations. We are capable of detecting the moving objects either by multiview geometry, deep learning, or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo, and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynaSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments.},
keywords={Vehicle dynamics;Simultaneous localization and mapping;Heuristic algorithms;Image segmentation;Cameras;Visualization;Geometry;SLAM;visual-based navigation;localization},
doi={10.1109/LRA.2018.2860039},
ISSN={2377-3766},
month={Oct},}
@INPROCEEDINGS{4919356,
author={Siyao Fu and Guosheng Yang},
booktitle={2009 International Conference on Networking, Sensing and Control}, title={Uncalibrated monocular based simultaneous localization and mapping for indoor autonomous mobile robot navigation},
year={2009},
volume={},
number={},
pages={663-668},
abstract={This paper describes a an SLAM algorithm for the navigation for an indoor autonomous mobile robot. The main emphasis of this paper is on the ability of line extraction. A recognition method based on straight line extraction is proposed for extracting the key features on the office ceiling, in an effort to estimate the pose of mobile robot. Random Sample Consensus (RANSAC) paradigm is used to group the line segments. During the navigation, onboard odometry is used at the beginning stage to estimate the information of environment for visual reckoning, while lamps on the ceiling act as beacons for positioning to eliminate accumulation of errors after a long-term run. The data captured from infrared sensors is used for constructing a map. The proposed method scales well with respect to the size of the input image and the number and size of the shapes within the data. Moreover the algorithm is conceptually simple and easy to implement. Simulation and experimental results show that good recognition and localization can be achieved using the proposed method, allowing for the interested region correspondence matching and mapping between images from different sensors or the same sensor indifferent time phrase.},
keywords={Simultaneous localization and mapping;Mobile robots;Navigation;Data mining;Image sensors;Feature extraction;Lamps;Infrared sensors;Shape;Image recognition},
doi={10.1109/ICNSC.2009.4919356},
ISSN={},
month={March},}
@INPROCEEDINGS{7139966,
author={Mohan, Mahesh and Gálvez-López, Dorian and Monteleoni, Claire and Sibley, Gabe},
booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)}, title={Environment selection and hierarchical place recognition},
year={2015},
volume={},
number={},
pages={5487-5494},
abstract={As robots continue to create long-term maps, the amount of information that they need to handle increases over time. In terms of place recognition, this implies that the number of images being considered may increase until exceeding the computational resources of the robot. In this paper we consider a scenario where, given multiple independent large maps, possibly from different cities or locations, a robot must effectively and in real time decide whether it can localize itself in one of those known maps. Since the number of images to be handled by such a system is likely to be extremely large, we find that it is beneficial to decompose the set of images into independent groups or environments. This raises a new question: Given a query image, how do we select the best environment? This paper proposes a similarity criterion that can be used to solve this problem. It is based on the observation that, if each environment is described in terms of its co-occurrent features, similarity between environments can be established by comparing their co-occurrence matrices. We show that this leads to a novel place recognition algorithm that divides the collection of images into environments and arranges them in a hierarchy of inverted indices. By selecting first the relevant environment for the operating robot, we can reduce the number of images to perform the actual loop detection, reducing the execution time while preserving the accuracy. The practicality of this approach is shown through experimental results on several large datasets covering a combined distance of more than 750Km.},
keywords={Indexes;Vocabulary;Kernel;Accuracy;Visualization;Robots},
doi={10.1109/ICRA.2015.7139966},
ISSN={1050-4729},
month={May},}
@ARTICLE{9361194,
author={Tschopp, Florian and von Einem, Cornelius and Cramariuc, Andrei and Hug, David and Palmer, Andrew William and Siegwart, Roland and Chli, Margarita and Nieto, Juan},
journal={IEEE Robotics and Automation Letters}, title={Hough$^2$Map – Iterative Event-Based Hough Transform for High-Speed Railway Mapping},
year={2021},
volume={6},
number={2},
pages={2745-2752},
abstract={To cope with the growing demand for transportation on the railway system, accurate, robust, and high-frequency positioning is required to enable a safe and efficient utilization of the existing railway infrastructure. As a basis for a localization system we propose a complete on-board mapping pipeline able to map robust meaningful landmarks, such as poles from power lines, in the vicinity of the vehicle. Such poles are good candidates for reliable and long term landmarks even through difficult weather conditions or seasonal changes. To address the challenges of motion blur and illumination changes in railway scenarios we employ a Dynamic Vision Sensor, a novel event-based camera. Using a sideways oriented on-board camera, poles appear as vertical lines. To map such lines in a real-time event stream, we introduce Hough2Map, a novel consecutive iterative event-based Hough transform framework capable of detecting, tracking, and triangulating close-by structures. We demonstrate the mapping reliability and accuracy of Hough2Map on real-world data in typical usage scenarios and evaluate using surveyed infrastructure ground truth maps. Hough2Map achieves a detection reliability of up to $92\,\%$ and a mapping root mean square error accuracy of 1.1518 m.11The code is available at https://github.com/ethz-asl/Hough2Map.},
keywords={Real-time systems;Rail transportation;Cameras;Voltage control;Tracking;Location awareness;Reliability;Computer vision for transportation;object detection;segmentation and categorization;mapping},
doi={10.1109/LRA.2021.3061404},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{6631320,
author={Sünderhauf, Niko and Protzel, Peter},
booktitle={2013 IEEE International Conference on Robotics and Automation}, title={Switchable constraints vs. max-mixture models vs. RRR - A comparison of three approaches to robust pose graph SLAM},
year={2013},
volume={},
number={},
pages={5198-5203},
abstract={SLAM algorithms that can infer a correct map despite the presence of outliers have recently attracted increasing attention. In the context of SLAM, outlier constraints are typically caused by a failed place recognition due to perceptional aliasing. If not handled correctly, they can have catastrophic effects on the inferred map. Since robust robotic mapping and SLAM are among the key requirements for autonomous long-term operation, inference methods that can cope with such data association failures are a hot topic in current research. Our paper compares three very recently published approaches to robust pose graph SLAM, namely switchable constraints, max-mixture models and the RRR algorithm. All three methods were developed as extensions to existing factor graph-based SLAM back-ends and aim at improving the overall system's robustness to false positive loop closure constraints. Due to the novelty of the three proposed algorithms, no direct comparison has been conducted so far.},
keywords={Switches;Simultaneous localization and mapping;Trajectory;Robustness;Optimization;Measurement;Cities and towns},
doi={10.1109/ICRA.2013.6631320},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{7324181,
author={Naseer, Tayyab and Suger, Benjamin and Ruhnke, Michael and Burgard, Wolfram},
booktitle={2015 European Conference on Mobile Robots (ECMR)}, title={Vision-based Markov localization across large perceptual changes},
year={2015},
volume={},
number={},
pages={1-6},
abstract={Recently, there has been significant progress towards lifelong, autonomous operation of mobile robots, especially in the field of localization and mapping. One important challenge in this context is visual localization under substantial perceptual changes, for example, coming from different seasons. In this paper, we present an approach to localize a mobile robot with a low frequency camera with respect to an image sequence, recorded previously within a different season. Our approach uses a discrete Bayes filter and a sensor model based on whole image descriptors. Thereby it exploits sequential information to model the dynamics of the system. Since we compute a probability distribution over the whole state space, our approach can handle more complex trajectories that may include same season loop-closures as well as fragmented sub-sequences. Throughout an extensive experimental evaluation on challenging datasets, we demonstrate that our approach outperforms state-of-the-art techniques.},
keywords={Databases;Robot sensing systems;Context;Lead;Cameras;Matched filters},
doi={10.1109/ECMR.2015.7324181},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7040480,
author={Song, Zhuoyuan and Mohseni, Kamran},
booktitle={53rd IEEE Conference on Decision and Control}, title={Towards background flow based AUV localization},
year={2014},
volume={},
number={},
pages={6945-6950},
abstract={Underwater localization faces many constrains and long-term persistent global localization for autonomous underwater vehicles (AUVs) is very difficult. In this paper, we propose a novel AUV localization method taking advantage of the recent progress in ocean general circulation models (OGCMs). During navigation, the AUV performs intermittent local background flow velocity measurements or estimates using on-board sensors. A series of preloaded flow velocity forecast maps generated by OGCMs are referred by a particle filter in updating particle weights based on resemblance between forecasts and local estimation. A rigorous derivation of the problem in probability theory is presented to reveal the recursive structure of the target distribution function. Simulations in a simple double-gyre velocity field exhibit satisfactory converging localization error. Further simulations in a flow field with local flow fluctuations that are not resolved by OGCMs show similar convergent localization error with a slower converging rate. As a first step towards a new set of underwater localization methods, this work presents promising results and reveals the possibility of realizing converging global underwater localization through partial utilization of the background flow information that is easily accessible.},
keywords={Oceans;Vehicles;Velocity measurement;Vectors;Navigation;Robots;Sensors},
doi={10.1109/CDC.2014.7040480},
ISSN={0191-2216},
month={Dec},}
@INPROCEEDINGS{5339626,
author={Hochdorfer, Siegfried and Lutz, Matthias and Schlegel, Christian},
booktitle={2009 IEEE International Conference on Technologies for Practical Robot Applications}, title={Lifelong localization of a mobile service-robot in everyday indoor environments using omnidirectional vision},
year={2009},
volume={},
number={},
pages={161-166},
abstract={SLAM (Simultaneous Localization and Mapping) mechanisms are a key component towards advanced service robotics applications. Currently, a major hurdle on the way to lifelong localization is the handling of the ever growing amount of landmarks over time. Therefore, the required resources in terms of memory and processing power are also growing over time.},
keywords={Indoor environments;Simultaneous localization and mapping;Upper bound;Clustering algorithms;Uncertainty;Robot vision systems;Robot localization;Observability;Global Positioning System;Mobile computing},
doi={10.1109/TEPRA.2009.5339626},
ISSN={2325-0534},
month={Nov},}
@INPROCEEDINGS{7487237,
author={Rosen, David M. and Mason, Julian and Leonard, John J.},
booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)}, title={Towards lifelong feature-based mapping in semi-static environments},
year={2016},
volume={},
number={},
pages={1063-1070},
abstract={The feature-based graphical approach to robotic mapping provides a representationally rich and computationally efficient framework for an autonomous agent to learn a model of its environment. However, this formulation does not naturally support long-term autonomy because it lacks a notion of environmental change; in reality, “everything changes and nothing stands still, ” and any mapping and localization system that aims to support truly persistent autonomy must be similarly adaptive. To that end, in this paper we propose a novel feature-based model of environmental evolution over time. Our approach is based upon the development of an expressive probabilistic generative feature persistence model that describes the survival of abstract semi-static environmental features over time. We show that this model admits a recursive Bayesian estimator, the persistence filter, that provides an exact online method for computing, at each moment in time, an explicit Bayesian belief over the persistence of each feature in the environment. By incorporating this feature persistence estimation into current state-of-the-art graphical mapping techniques, we obtain a flexible, computationally efficient, and information-theoretically rigorous framework for lifelong environmental modeling in an ever-changing world.},
keywords={Feature extraction;Detectors;Computational modeling;Adaptation models;Simultaneous localization and mapping;Bayes methods},
doi={10.1109/ICRA.2016.7487237},
ISSN={},
month={May},}
@INPROCEEDINGS{972991,
author={Gross, H.-M. and Boehme, H.-J. and Wilhelm, T.},
booktitle={2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)}, title={Contribution to vision-based localization, tracking and navigation methods for an interactive mobile service-robot},
year={2001},
volume={2},
number={},
pages={672-677 vol.2},
abstract={Presents vision-based robot navigation and user localization techniques of our long-term research project PERSES (personal service system), which aims to develop an interactive mobile shopping assistant that allows a continuous and intuitively understandable interaction with customers in a home store. Against this background, the paper describes a number of new or improved approaches, addressing challenges arising from the characteristics of the operation area, and from the need to continuously interact with users in a complex environment. With our approaches to vision-based or visually-controlled map building, self-localization and navigation as well as user localization and tracking, we want to make a contribution to the real-world suitability of interactive mobile service-robots in non-trivial application areas and demanding human-robot interaction scenarios.},
keywords={Navigation;Robot vision systems;Infrared sensors;Mobile robots;Color;Robot sensing systems;Cameras;Sonar equipment;Sensor systems;Service robots},
doi={10.1109/ICSMC.2001.972991},
ISSN={1062-922X},
month={Oct},}
@ARTICLE{9642033,
author={Kilic, Cagri and Martinez R., Bernardo and Tatsch, Christopher A. and Beard, Jared and Strader, Jared and Das, Shounak and Ross, Derek and Gu, Yu and Pereira, Guilherme A. S. and Gross, Jason N.},
journal={IEEE Aerospace and Electronic Systems Magazine}, title={NASA Space Robotics Challenge 2 Qualification Round: An Approach to Autonomous Lunar Rover Operations},
year={2021},
volume={36},
number={12},
pages={24-41},
abstract={Plans for establishing a long-term human presence on the Moon will require substantial increases in robot autonomy and multirobot coordination to support establishing a lunar outpost. To achieve these objectives, algorithm design choices for the software developments need to be tested and validated for expected scenarios such as autonomous in situ resource utilization, localization in challenging environments, and multirobot coordination. However, real-world experiments are extremely challenging and limited for extraterrestrial environment. Also, realistic simulation demonstrations in these environments are still rare and demanded for initial algorithm testing capabilities. To help some of these needs, the NASA Centennial Challenges program established the Space Robotics Challenge Phase 2 (SRC2), which consist of virtual robotic systems in a realistic lunar simulation environment, where a group of mobile robots were tasked with reporting volatile locations within a global map, excavating and transporting these resources, and detecting and localizing a target of interest. The main goal of this article is to share our team's experiences on the design tradeoffs to perform autonomous robotic operations in a virtual lunar environment and to share strategies to complete the mission requirements posed by NASA SRC2 competition during the qualification round. Of the 114 teams that registered for participation in the NASA SRC2, team Mountaineers finished as one of only six teams to receive the top qualification round prize.},
keywords={NASA;Space vehicles;Robot kinematics;Moon;Software algorithms;Multi-robot systems;Resource management},
doi={10.1109/MAES.2021.3115897},
ISSN={1557-959X},
month={Dec},}
@INPROCEEDINGS{744447,
author={Feder, H.J.S. and Leonard, J.J. and Smith, C.M.},
booktitle={Proceedings of the 1998 Workshop on Autonomous Underwater Vehicles (Cat. No.98CH36290)}, title={Incorporating environmental measurements in navigation},
year={1998},
volume={},
number={},
pages={115-122},
abstract={Extended missions in unknown regions present a significant navigational challenge for autonomous underwater vehicles (AUV). This paper investigates the long-term performance of a concurrent mapping and localization (CML) algorithm for the scenario of an AUV making observations of point features in the environment with a forward look sonar. Simulation results demonstrate that position estimates with long-term bounded errors of a few meters can be achieved under realistic assumptions about the vehicle, its sensors, and the environment. Potential failure modes of the algorithm, such as divergence and map slip, are discussed. CML technology can provide a significant improvement in the navigational capabilities of AUVs and can enable new missions in unmapped regions without reliance on acoustic beacons or surfacing for GPS resets.},
keywords={Sonar navigation;Remotely operated vehicles;Underwater acoustics;Performance analysis;Stochastic processes;Sea measurements;Jacobian matrices;Oceans;Automotive engineering;Marine technology},
doi={10.1109/AUV.1998.744447},
ISSN={},
month={Aug},}
@INPROCEEDINGS{1307213,
author={Sabe, K. and Fukuchi, M. and Gutmann, J.-S. and Ohashi, T. and Kawamoto, K. and Yoshigahara, T.},
booktitle={IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004}, title={Obstacle avoidance and path planning for humanoid robots using stereo vision},
year={2004},
volume={1},
number={},
pages={592-597 Vol.1},
abstract={This work presents methods for path planning and obstacle avoidance for the humanoid robot QRIO, allowing the robot to autonomously walk around in a home environment. For an autonomous robot, obstacle detection and localization as well as representing them in a map are crucial tasks for the success of the robot. Our approach is based on plane extraction from data captured by a stereo-vision system that has been developed specifically for QRIO. We briefly overview the general software architecture composed of perception, short and long term memory, behavior control, and motion control, and emphasize on our methods for obstacle detection by plane extraction, occupancy grid mapping, and path planning. Experimental results complete the description of our system.},
keywords={Path planning;Humanoid robots;Stereo vision;Service robots;Legged locomotion;Robot vision systems;Motion control;Data mining;Mobile robots;Robot sensing systems},
doi={10.1109/ROBOT.2004.1307213},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{9651458,
author={Tsintotas, Konstantinos A. and Bampis, Loukas and An, Shan and Fragulis, George F. and Mouroutsos, Spyridon G. and Gasteratos, Antonios},
booktitle={2021 IEEE International Conference on Imaging Systems and Techniques (IST)}, title={Sequence-based mapping for probabilistic visual loop-closure detection},
year={2021},
volume={},
number={},
pages={1-6},
abstract={During simultaneous localization and mapping, the robot should build a map of its surroundings and simultaneously estimate its pose in the generated map. However, a fundamental task is to detect loops, i.e., previously visited areas, allowing consistent map generation. Moreover, within long-term mapping, every autonomous system needs to address its scalability in terms of storage requirements and database search. In this paper, we present a low-complexity sequence-based visual loop-closure detection pipeline. Our system dynamically segments the traversed route through a feature matching technique in order to define sub-maps. In addition, visual words are generated incrementally for the corresponding sub-maps representation. Comparisons among these sequences-of-images are performed thanks to probabilistic scores originated from a voting scheme. When a candidate sub-map is indicated, global descriptors are utilized for image-to-image pairing. Our evaluation took place on several publicly-available datasets exhibiting the system&#x2019;s low complexity and high recall compared to other state-of-the-art approaches.},
keywords={Visualization;Vocabulary;Simultaneous localization and mapping;Databases;Pipelines;Probabilistic logic;Feature extraction},
doi={10.1109/IST50367.2021.9651458},
ISSN={1558-2809},
month={Aug},}
@INPROCEEDINGS{6094414,
author={Kretzschmar, Henrik and Stachniss, Cyrill and Grisetti, Giorgio},
booktitle={2011 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={Efficient information-theoretic graph pruning for graph-based SLAM with laser range finders},
year={2011},
volume={},
number={},
pages={865-871},
abstract={In graph-based SLAM, the pose graph encodes the poses of the robot during data acquisition as well as spatial constraints between them. The size of the pose graph has a substantial influence on the runtime and the memory requirements of a SLAM system, which hinders long-term mapping. In this paper, we address the problem of efficient information-theoretic compression of pose graphs. Our approach estimates the expected information gain of laser measurements with respect to the resulting occupancy grid map. It allows for restricting the size of the pose graph depending on the information that the robot acquires about the environment or based on a given memory limit, which results in an any-space SLAM system. When discarding laser scans, our approach marginalizes out the corresponding pose nodes from the graph. To avoid a densely connected pose graph, which would result from exact marginalization, we propose an approximation to marginalization that is based on local Chow-Liu trees and maintains a sparse graph. Real world experiments suggest that our approach effectively reduces the growth of the pose graph while minimizing the loss of information in the resulting grid map.},
keywords={Measurement by laser beam;Simultaneous localization and mapping;Approximation methods;Lasers;Mutual information;Laser beams},
doi={10.1109/IROS.2011.6094414},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{8461228,
author={Sun, Li and Yan, Zhi and Mellado, Sergi Molina and Hanheide, Marc and Duckett, Tom},
booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)}, title={3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data},
year={2018},
volume={},
number={},
pages={5942-5948},
abstract={This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.},
keywords={Trajectory;Cameras;Robot kinematics;Robot vision systems;Two dimensional displays;Mobile robots},
doi={10.1109/ICRA.2018.8461228},
ISSN={2577-087X},
month={May},}
@INPROCEEDINGS{7759303,
author={Paton, Michael and MacTavish, Kirk and Warren, Michael and Barfoot, Timothy D.},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Bridging the appearance gap: Multi-experience localization for long-term visual teach and repeat},
year={2016},
volume={},
number={},
pages={1918-1925},
abstract={Vision-based, route-following algorithms enable autonomous robots to repeat manually taught paths over long distances using inexpensive vision sensors. However, these methods struggle with long-term, outdoor operation due to the challenges of environmental appearance change caused by lighting, weather, and seasons. While techniques exist to address appearance change by using multiple experiences over different environmental conditions, they either provide topological-only localization, require several manually taught experiences in different conditions, or require extensive offline mapping to produce metric localization. For real-world use, we would like to localize metrically to a single manually taught route and gather additional visual experiences during autonomous operations. Accordingly, we propose a novel multi-experience localization (MEL) algorithm developed specifically for route-following applications; it provides continuous, six-degree-of-freedom (6DoF) localization with relative uncertainty to a privileged (manually taught) path using several experiences simultaneously. We validate our algorithm through two experiments: i) an offline performance analysis on a 9km subset of a challenging 27km route-traversal dataset and ii) an online field trial where we demonstrate autonomy on a small 250m loop over the course of a sunny day. Both exhibit significant appearance change due to lighting variation. Through these experiments we show that safe localization can be achieved by bridging the appearance gap.},
keywords={Measurement;Visualization;Lighting;Uncertainty;Sensors;Robot kinematics},
doi={10.1109/IROS.2016.7759303},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{5509579,
author={Ikeda, Kouichirou and Tanaka, Kanji},
booktitle={2010 IEEE International Conference on Robotics and Automation}, title={Visual robot localization using compact binary landmarks},
year={2010},
volume={},
number={},
pages={4397-4403},
abstract={This paper is concerned with the problem of mobile robot localization using a novel compact representation of visual landmarks. With recent progress in lifelong map-learning as well as in information sharing networks, compact representation of a large-size landmark database has become crucial. In this paper, we propose a compact binary code (e.g. 32bit code) landmark representation by employing the semantic hashing technique from web-scale image retrieval. We show how well such a binary representation achieves compactness of a landmark database while maintaining efficiency of the localization system. In our contribution, we investigate the cost-performance, the semantic gap, the saliency evaluation using the presented techniques as well as challenge to further reduce the resources (#bits) per landmark. Experiments using a high-speed car-like robot show promising results.},
keywords={Robot localization;Visual databases;Binary codes;Mobile robots;Image databases;Vocabulary;Costs;Image retrieval;Information retrieval;Graphical models},
doi={10.1109/ROBOT.2010.5509579},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{976237,
author={Austin, D. and Fletcher, L. and Zelinsky, A.},
booktitle={Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)}, title={Mobile robotics in the long term-exploring the fourth dimension},
year={2001},
volume={2},
number={},
pages={613-618 vol.2},
abstract={Explores the issues involved in deployment of mobile robots in real-world situations and presents solutions and approaches under development at the Australian National University. For deployment of mobile robots outside of the laboratory, long-term operation is required. Hence, we have developed an automatic recharging system. In addition, a Web-based teleoperation system is used to provide missions to test the long-term reliability of the robot. The final aspect of real-world operation that is explored here is operations in dynamic environments. To date, researchers have assumed static environments for mapping and localisation. We propose methods to avoid this restriction.},
keywords={Mobile robots;Robotics and automation;Robot sensing systems;Laboratories;Batteries;Motion detection;Hardware;System testing;Path planning;Object detection},
doi={10.1109/IROS.2001.976237},
ISSN={},
month={Oct},}
@ARTICLE{7878680,
author={Krajník, Tomáš and Fentanes, Jaime P. and Santos, João M. and Duckett, Tom},
journal={IEEE Transactions on Robotics}, title={FreMEn: Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments},
year={2017},
volume={33},
number={4},
pages={964-977},
abstract={We present a new approach to long-term mobile robot mapping in dynamic indoor environments. Unlike traditional world models that are tailored to represent static scenes, our approach explicitly models environmental dynamics. We assume that some of the hidden processes that influence the dynamic environment states are periodic and model the uncertainty of the estimated state variables by their frequency spectra. The spectral model can represent arbitrary timescales of environment dynamics with low memory requirements. Transformation of the spectral model to the time domain allows for the prediction of the future environment states, which improves the robot's long-term performance in changing environments. Experiments performed over time periods of months to years demonstrate that the approach can efficiently represent large numbers of observations and reliably predict future environment states. The experiments indicate that the model's predictive capabilities improve mobile robot localization and navigation in changing environments.},
keywords={Hidden Markov models;Mobile robots;Uncertainty;Robustness;Harmonic analysis;Navigation;Localization;long-term autonomy;mapping},
doi={10.1109/TRO.2017.2665664},
ISSN={1941-0468},
month={Aug},}
@INPROCEEDINGS{9561701,
author={Thomas, Hugues and Agro, Ben and Gridseth, Mona and Zhang, Jian and Barfoot, Timothy D.},
booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, title={Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation},
year={2021},
volume={},
number={},
pages={14047-14053},
abstract={We present a self-supervised learning approach for the semantic segmentation of lidar frames. Our method is used to train a deep point cloud segmentation architecture without any human annotation. The annotation process is automated with the combination of simultaneous localization and mapping (SLAM) and ray-tracing algorithms. By performing multiple navigation sessions in the same environment, we are able to identify permanent structures, such as walls, and disentangle short-term and long-term movable objects, such as people and tables, respectively. New sessions can then be performed using a network trained to predict these semantic labels. We demonstrate the ability of our approach to improve itself over time, from one session to the next. With semantically filtered point clouds, our robot can navigate through more complex scenarios, which, when added to the training pool, help to improve our network predictions. We provide insights into our network predictions and show that our approach can also improve the performances of common localization techniques.},
keywords={Training;Laser radar;Simultaneous localization and mapping;Annotations;Semantics;Ray tracing;Prediction algorithms},
doi={10.1109/ICRA48506.2021.9561701},
ISSN={2577-087X},
month={May},}
@ARTICLE{8767935,
author={Quan, Meixiang and Piao, Songhao and Tan, Minglang and Huang, Shi-Sheng},
journal={IEEE Access}, title={Tightly-Coupled Monocular Visual-Odometric SLAM Using Wheels and a MEMS Gyroscope},
year={2019},
volume={7},
number={},
pages={97374-97389},
abstract={In this paper, we present a novel tightly coupled probabilistic monocular visual-odometric simultaneous localization and mapping (VOSLAM) algorithm using wheels and a MEMS gyroscope, which can provide accurate, robust, and long-term localization for ground robots. First, we present a novel odometer preintegration theory on manifold; it integrates the wheel encoder measurements and gyroscope measurements to a relative motion constraint that is independent of the linearization point and carefully addresses the uncertainty propagation and gyroscope bias correction. Based on the preintegrated odometer measurement model, we also introduce the odometer error term and tightly integrate it into the visual optimization framework. Then, in order to bootstrap the VOSLAM system, we propose a simple map initialization method. Finally, we present a complete localization mechanism to maximally exploit both sensing cues, which provides different strategies for motion tracking when: 1) both measurements are available; 2) visual measurements are not available; and 3) wheel encoders experience slippage, thereby ensuring the accurate and robust motion tracking. The proposed algorithm is evaluated by performing extensive experiments, and the experimental results demonstrate the superiority of the proposed system.},
keywords={Wheels;Visualization;Simultaneous localization and mapping;Motion measurement;Optimization;Motion estimation;sensor fusion;simultaneous localization and mapping},
doi={10.1109/ACCESS.2019.2930201},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{1511734,
author={Leblond, I. and Legris, M. and Solaiman, B.},
booktitle={Europe Oceans 2005}, title={Use of classification and segmentation of sidescan sonar images for long term registration},
year={2005},
volume={1},
number={},
pages={322-327 Vol. 1},
abstract={This article handles the possibility of using classification and segmentation of sidescan sonar images for long term registration. In our case, long term registration means to find the displacement between two images which can have been mapped with many weeks or many months between them. The aim of this study is to help AUVs (autonomous underwater vehicles) to navigate, in particular to correct the drift of navigation sensors. This type of positioning raises two sorts of problems, which come from image properties: spatial variability and temporal variability. The first one is caused above all by the sonar geometry and appears for example like a modification of the shape or the position of the shadow according to the point of view. This effect can also be seen in textures, for example on megaripples of sand, which can be more or less visible depending on the point of view of the sonar. The second one is more the consequence of the seafloor physics: between two images, mapped at different times, some elements may have changed. An obvious example is the presence of evanescent "objects" like fishes but this variability can also be seen on sediments, which borders can move due to local bottom dynamics. With the aim to solve these problems and to provide reliable landmarks for matching, we decided to classify and segment the images. The data have first been corrected from TVG (time varying gain) effects and despeckelised in order to process images which are more representative of the seafloor. The basis idea is to use a supervised method of classification. To do that, we consider some parameters which are coming from a decomposition by Gabor filters, in order to segment with linear discriminant analysis and use of the nearest neighbour method. Registration needs accurately localised landmarks: so, this operation is split in several stages, refining step by step the classification, in order to obtain a map which describes the seafloor with the most possible detailed frontiers. Then, we present the obtained results considering five texture classes: rocks, megaripples, sand, mud and shadow. These several areas and their frontiers are the basis landmarks to match the images. However, before using the segmentation, we must check its reliability. So, it appears that the frontiers, though they are realistic, are not accurate enough to make a registration precise to few pixels, especially in rock areas. Similarly, according to the orientation of the ripples, they may be seen as ripples or sand. These observations are due to the directivity of the sonar, which caused these effects on the segmentation. To do registration, we must take into account these problems. So, the results of the segmentation will be used only for a coarse registration, in order to find quickly the area of interest but also to assess the reliability of our registration (matching on ripples areas is a priori less reliable than on rocks areas). The results of registration are shown, proving the good adequacy between reference image and test image. Others methods, more quantitative, will be able to be tested, to refine the results.},
keywords={Image segmentation;Sonar navigation;Sea floor;Testing;Underwater vehicles;Geometry;Shape;Physics;Marine animals;Sediments},
doi={10.1109/OCEANSE.2005.1511734},
ISSN={},
month={June},}
@INPROCEEDINGS{884968,
author={Gross, H.-M. and Boehme, H.-J.},
booktitle={Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0}, title={PERSES-a vision-based interactive mobile shopping assistant},
year={2000},
volume={1},
number={},
pages={80-85 vol.1},
abstract={The paper describes the general idea, the application scenario, and selected methodological approaches of our long term research project PERSES (PERsonal SErvice System). The aim of the project consists of the development of an interactive mobile shopping assistant that allows a continuous and intuitively understandable interaction with a customer in a home improvement store. Typical tasks we have to tackle are to detect and contact potential users in the operation area, to guide them to desired areas or articles within the store or to follow them as a mobile information kiosk while continuously observing their behavior. Due to the specificity of the interaction-oriented scenario and the characteristics of the operation area, we have focused on vision based methods for both human-robot interaction and robot navigation. Besides some methodological approaches, we present preliminary results of experiments achieved with our mobile robot PERSES in the store with an emphasis on vision based methods for user localization, map building and self-localization.},
keywords={Navigation;Robot vision systems;Mobile robots;Robustness;Robot kinematics;Human robot interaction;Adaptation model;Context modeling;Programmable control;Adaptive control},
doi={10.1109/ICSMC.2000.884968},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{7831835,
author={Zhang, Yu and Chao, Ainong and Zhao, Boxin and Liu, Huawei and Zhao, Xiaolin},
booktitle={2016 IEEE International Conference on Information and Automation (ICIA)}, title={Migratory birds-inspired navigation system for unmanned aerial vehicles},
year={2016},
volume={},
number={},
pages={276-281},
abstract={Migration birds are able to navigate themselves during a long-distance journey without getting lost. They actually achieve just what is being sought for in the field of Unmanned Aerial Vehicles (UAVs): long-term autonomous navigation. This paper proposes an approach that combines the migration birds' sense principles with Micro-Electro-Mechanical System (MEMS) sensors to estimate UAVs position within GPS-denied environments. Camera, orientation and web-based maps (such as Google/Baidu Maps) are chosen to simulate the birds' localization cues: vision, earth magnetic field and mental maps. The visual odometry, Particle Filter theories are used in the proposed approach to integrate multiple sensor measurements. Real flying experiments are conducted both in indoor and outdoor environments. The results validate that the proposed migration-inspired visual odometry system can estimate the UAV localization effectively.},
keywords={Cameras;Visualization;Unmanned aerial vehicles;Birds;Sensors;Navigation;Optical imaging;Migration birds;Unmanned Aerial Vehicles;Navigation},
doi={10.1109/ICInfA.2016.7831835},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9615417,
author={Ostroumov, Ivan and Kuzmenko, Nataliia},
booktitle={2021 IEEE 6th International Conference on Actual Problems of Unmanned Aerial Vehicles Development (APUAVD)}, title={Vehicle Navigation by Visual Navigational Aids for Automatic Lunar Mission},
year={2021},
volume={},
number={},
pages={71-75},
abstract={Nowadays the question of Moon exploration is one of the key priorities. Many Lunar robotics missions are planned in near future by different space agencies around the world. Moon has considered to be the best place for a research station with long-term human presence for finding answers on fundamental questions about the universe. Automatic navigation of starship during a landing phase on Lunar surface is already solved with a help of inertial reference system aided visual algorithms. However, questions of automatic navigation of moving and flying vehicles on the Lunar surface are still open. Inertial navigation is limited by time, self-localization and mapping algorithms require multiple unique features of relief to guarantee required accuracy for successful automatic mission complication. In the current study, we propose the deployment of a network of visual navigational aids on the Lunar surface to support ground automatic missions. A weak atmosphere of the Moon makes effective visual beacons navigation system for long areas. A network of navigational aids includes primary and secondary ground stations which are blinking synchronously. Synchronization is supported by radio waves from the primary ground station. We consider the nature of crater relief to increase operational area of the system. The Time Difference of Arrival method is used to detect vehicle position by blinking network of visual navigational aids. In the numerical application, we consider different scenarios of network configuration to support automatic vehicle navigation inside of Tycho crater. Also, deployment of visual navigational aids network will increase the number of optical features which improve performance of already used positioning methods.},
keywords={Visualization;Time difference of arrival;Surface waves;Moon;Radio navigation;Unmanned aerial vehicles;Synchronization;visual navigational aids;landing and ground vehicles;automatic mission;Lunar mission;Time Difference of Arrival},
doi={10.1109/APUAVD53804.2021.9615417},
ISSN={},
month={Oct},}
@ARTICLE{9457132,
author={Zhang, Kaining and Jiang, Xingyu and Ma, Jiayi},
journal={IEEE Transactions on Intelligent Transportation Systems}, title={Appearance-Based Loop Closure Detection via Locality-Driven Accurate Motion Field Learning},
year={2022},
volume={23},
number={3},
pages={2350-2365},
abstract={Loop closure detection (LCD) is of significant importance in simultaneous localization and mapping. It represents the robot’s ability to recognize whether the current surrounding corresponds to a previously observed one. In this paper, we conduct this task in a two-step strategy: candidate frame selection and loop closure verification. The first step aims to search semantically similar images for the query one using features obtained by Key.Net with HardNet. Instead of adopting the traditional Bag-of-Words strategy, we utilize the aggregated selective match kernel to calculate the similarity between images. Subsequently, based on the potential property of motion field in the LCD scene, we propose a novel feature matching method, i.e., exploiting the smoothness prior and learning the motion field for an image pair in a reproducing kernel Hilbert space (RKHS), to implement loop closure verification. Concretely, we formulate the learning problem into a Bayesian framework with latent variables indicating the true/false correspondences and a mixture model accounting for the distribution of data. Furthermore, we propose a locality-driven mechanism to enhance the local relevance of motion vectors and term the algorithm as locality-driven accurate motion field learning (LAL). To satisfy the requirement of efficiency in the LCD task, we use a sparse approximation and search a suboptimal solution for the motion field in the RKHS, termed as LAL*. Extensive experiments are conducted on public datasets for feature matching and LCD tasks. The quantitative results demonstrate the effectiveness of our method over the current state-of-the-art, meanwhile showing its potential for long-term visual localization. The codes of LAL and LAL* are publicly available at https://github.com/KN-Zhang/LAL.},
keywords={Feature extraction;Liquid crystal displays;Visualization;Task analysis;Robots;Simultaneous localization and mapping;Kernel;SLAM;loop closure detection;place recognition;feature matching;autonomous vehicle},
doi={10.1109/TITS.2021.3086822},
ISSN={1558-0016},
month={March},}
@INPROCEEDINGS{8916895,
author={Luthardt, Stefan and Ziegler, Christoph and Willert, Volker and Adamy, Jürgen},
booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)}, title={How to Match Tracks of Visual Features for Automotive Long-Term SLAM},
year={2019},
volume={},
number={},
pages={934-941},
abstract={Accurate localization is a vital prerequisite for future assistance or autonomous driving functions in intelligent vehicles. To achieve the required localization accuracy and availability, long-term visual SLAM algorithms like LLama-SLAM are a promising option. In such algorithms visual feature tracks, i. e. landmark observations over several consecutive image frames, have to be matched to feature tracks recorded days, weeks or months earlier. This leads to a more challenging matching problem than in short-term visual localization and known descriptor matching methods cannot be applied directly. In this paper, we devise several approaches to compare and match feature tracks and evaluate their performance on a long-term data set. With the proposed descriptor combination and masking ("CoMa") method the best track matching performance is achieved with minor computational cost. This method creates a single combined descriptor for each feature track and furthermore increases the robustness by capturing the appearance variations of this track in a descriptor mask.},
keywords={Visualization;Feature extraction;Optimization;Simultaneous localization and mapping;Cameras;Robustness},
doi={10.1109/ITSC.2019.8916895},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6181757,
author={Herath, Damith C. and Chapman, Tawna and Tomkins, Alethea and Elliott, Larissa and David, Michelle and Cooper, Amy and Burnham, Denis and Kodagoda, Sarath},
booktitle={2011 IEEE International Conference on Robotics and Biomimetics}, title={A study on wearable robotics — Comfort is in the context},
year={2011},
volume={},
number={},
pages={2969-2974},
abstract={WITU (Wearable Indoor Tracking Unit) is a wearable robotic device that aids indoor navigation by building maps and localizing the user within them. Applications of such a device include search and rescue, travel aid in large and complex buildings, museum guides among others where external localization information such as from a GPS is not available. However, WITU relies on human intelligence both to maintain long term consistency of its location estimates and to efficiently manage its limited memory and processing capacity. This alludes to a symbiotic relationship between the user and the device and here we look at this symbiotic relationship from an end user perspective. Thus, in order to have a successful interaction, we argue that the user needs to feel comfortable wearing the device while carrying out the intended tasks. We hypothesize that this perceived comfort is dependent on the context in which the device is used. We test our hypothesis on three different scenarios; search and rescue worker, dementia patient in a long care facility and a person at a party which acts as the baseline. Results indicate an important consequence for the development of such wearable robotic systems.},
keywords={Dementia;Humans;Context;Robots;Audio recording;Loading;Educational institutions},
doi={10.1109/ROBIO.2011.6181757},
ISSN={},
month={Dec},}
@INPROCEEDINGS{1515050,
author={Mayol, W.W. and Tordoff, B.J. and de Campos, T.E. and Davison, A.J. and Murray, D.W.},
booktitle={2003 IEE Eurowearable}, title={Active vision for wearables},
year={2003},
volume={},
number={},
pages={99-104},
abstract={In this paper we report on our ongoing research on wearable active vision, where we have iteratively prototyped a wearable visual robot - a body mounted robot for which the main sensor is a camera. Two main areas have been studied: robot design and visual algorithms. In the design stage, we have analysed sensor placement through the computation of the field of view and body motion using a 3D model of the human form. A design methodology for the robot morphology was developed with the help of an optimisation algorithm based on the Pareto front. The wearability of the device has progressed over several iterations as have the sensor and control architectures. In terms of visual algorithms, we have studied methods of visual tracking fused with inertial sensors, real-time template tracking, human head pose recovery and more recently real-time simultaneous ego-localisation and autonomous 3D map building. Our main long-term application areas are enhanced remote collaboration and autonomous wearable assistants that use vision.},
keywords={},
doi={10.1049/ic:20030154},
ISSN={0537-9989},
month={Sep.},}
@INPROCEEDINGS{1308080,
author={Williams, S. and Mahon, I.},
booktitle={IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004}, title={Simultaneous localisation and mapping on the Great Barrier Reef},
year={2004},
volume={2},
number={},
pages={1771-1776 Vol.2},
abstract={This paper presents results of the application of the simultaneous localisation and mapping algorithm to data collected by an unmanned underwater vehicle operating on the Great Barrier Reef in Australia. By fusing information from the vehicle's on-board sonar and vision systems, it is possible to use the highly textured reef to provide estimates of the vehicle motion as well as to generate models of the gross structure of the underlying reefs. Terrain-aided navigation promises to revolutionise the ability of marine systems to track underwater bodies in many applications. This work represents a crucial step in the development of underwater technologies capable of long-term, reliable deployment. Results of the application of this technique to the tracking of the vehicle position are shown.},
keywords={Simultaneous localization and mapping;Underwater vehicles;Monitoring;Australia;Underwater tracking;Sonar navigation;Remotely operated vehicles;Robots;Underwater technology;Aerospace engineering},
doi={10.1109/ROBOT.2004.1308080},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{6907255,
author={Morris, Timothy and Dayoub, Feras and Corke, Peter and Wyeth, Gordon and Upcroft, Ben},
booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, title={Multiple map hypotheses for planning and navigating in non-stationary environments},
year={2014},
volume={},
number={},
pages={2765-2770},
abstract={This paper presents a method to enable a mobile robot working in non-stationary environments to plan its path and localize within multiple map hypotheses simultaneously. The maps are generated using a long-term and short-term memory mechanism that ensures only persistent configurations in the environment are selected to create the maps. In order to evaluate the proposed method, experimentation is conducted in an office environment. Compared to navigation systems that use only one map, our system produces superior path planning and navigation in a non-stationary environment where paths can be blocked periodically, a common scenario which poses significant challenges for typical planners.},
keywords={Navigation;Three-dimensional displays;Planning;Robot sensing systems;Switches;Current measurement},
doi={10.1109/ICRA.2014.6907255},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{9739362,
author={Guo, Ruibin and Liu, Xinghua},
booktitle={2021 IEEE International Conference on Robotics and Biomimetics (ROBIO)}, title={Ground Enhanced RGB-D SLAM for Dynamic Environments},
year={2021},
volume={},
number={},
pages={1171-1177},
abstract={Robust pose estimation and map reconstruction are the basic requirements of the robotics autonomous. In this paper, a static ground feature enhanced SLAM system is proposed for dynamic environments with RGB-D sensors. Compared with the typical point-based SLAM, our designed system extra introduce the ground and other plane constraints to solve the dynamic SLAM. In the front-end, the ground as a special plane feature is detected and tracked, which can provide realiable constraint for the pose estimation in dynamic environments. In the back-end, a point-ground based factor graph is constructed and optimized for more accurate map. Moreover, plane structure is exploited to repair the keyframe dynamic regions, new synthesized keyframes are used to reconstruct the static map for long-term applications. Real world dataset tests demonstrate the effectiveness of our proposed system.},
keywords={Simultaneous localization and mapping;Conferences;Biomimetics;Pose estimation;Pipelines;Object detection;Maintenance engineering},
doi={10.1109/ROBIO54168.2021.9739362},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9196906,
author={Gao, Peng and Zhang, Hao},
booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, title={Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships},
year={2020},
volume={},
number={},
pages={1070-1076},
abstract={Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.},
keywords={Visualization;Simultaneous localization and mapping;Robustness;Strain;Image recognition;Tensile stress},
doi={10.1109/ICRA40945.2020.9196906},
ISSN={2577-087X},
month={May},}
@INPROCEEDINGS{4811835,
author={Gross, H.-M. and Boehme, H.-J. and Schroeter, C. and Mueller, S. and Koenig, A. and Martin, Ch. and Merten, M. and Bley, A.},
booktitle={2008 IEEE International Conference on Systems, Man and Cybernetics}, title={ShopBot: Progress in developing an interactive mobile shopping assistant for everyday use},
year={2008},
volume={},
number={},
pages={3471-3478},
abstract={The paper describes progress achieved in our long-term research project ShopBot, which aims at the development of an intelligent and interactive mobile shopping assistant for everyday use in shopping centers or home improvement stores. It is focusing on recent progress concerning two important methodological aspects: (i) the on-line building of maps of the operation area by means of advanced Rao-Blackwellized SLAM approaches using both sonar-based gridmaps as well as vision-based graph maps as representations, and (ii) a probabilistic approach to multi-modal user detection and tracking during the guidance tour. Experimental results of both the map building characteristics and the person tracking behavior achieved in an ordinary home improvement store demonstrate the reliability of both approaches. Moreover, we present first very encouraging results of long-term field trials which have been executed with three robotic shopping assistants in another home improvement store in Bavaria since March 2008. In this field test, the robots could demonstrate their suitability for this challenging real-world application, as well as the necessary user acceptance.},
keywords={Robot sensing systems;Sensor phenomena and characterization;Mobile robots;Simultaneous localization and mapping;Sonar detection;Cognitive robotics;Intelligent robots;Testing;Robot vision systems;Navigation},
doi={10.1109/ICSMC.2008.4811835},
ISSN={1062-922X},
month={Oct},}
@ARTICLE{6153423,
author={Fraundorfer, Friedrich and Scaramuzza, Davide},
journal={IEEE Robotics Automation Magazine}, title={Visual Odometry : Part II: Matching, Robustness, Optimization, and Applications},
year={2012},
volume={19},
number={2},
pages={78-90},
abstract={Part II of the tutorial has summarized the remaining building blocks of the VO pipeline: specifically, how to detect and match salient and repeatable features across frames and robust estimation in the presence of outliers and bundle adjustment. In addition, error propagation, applications, and links to publicly available code are included. VO is a well understood and established part of robotics. VO has reached a maturity that has allowed us to successfully use it for certain classes of applications: space, ground, aerial, and underwater. In the presence of loop closures, VO can be used as a building block for a complete SLAM algorithm to reduce motion drift. Challenges that still remain are to develop and demonstrate large-scale and long-term implementations, such as driving autonomous cars for hundreds of miles. Such systems have recently been demonstrated using Lidar and Radar sensors [86]. However, for VO to be used in such systems, technical issues regarding robustness and, especially, long-term stability have to be resolved. Eventually, VO has the potential to replace Lidar-based systems for egomotion estimation, which are currently leading the state of the art in accuracy, robustness, and reliability. VO offers a cheaper and mechanically easier-to-manufacture solution for egomotion estimation, while, additionally, being fully passive. Furthermore, the ongoing miniaturization of digital cameras offers the possibility to develop smaller and smaller robotic systems capable of ego-motion estimation.},
keywords={Tutorials;Robust control;Optimization;VIsualization;Odemtry;Feature extraction;Cameras;Computer vision;Estimation},
doi={10.1109/MRA.2012.2182810},
ISSN={1558-223X},
month={June},}
@INPROCEEDINGS{4562088,
author={Andreopoulos, Alexander and Tsotsos, John K.},
booktitle={2008 Canadian Conference on Computer and Robot Vision}, title={Active Vision for Door Localization and Door Opening using Playbot: A Computer Controlled Wheelchair for People with Mobility Impairments},
year={2008},
volume={},
number={},
pages={3-10},
abstract={Playbot is a long-term, large-scale research project, whose goal is to provide a vision-based computer controlled wheelchair that enables children and adults with mobility impairments to become more independent. Within this context, we show how Playbot can actively search an indoor environment to localize a door, approach the door, use a mounted robotic arm to open the door, and go through the door, using exclusively vision-based sensors and without using a map of the environment. We demonstrate the effectiveness of active vision for localizing objects that are too large to fall within a single camerapsilas field of view and show that well-calibrated vision-based sensors are sufficient to safely pass through a door frame that is narrow enough to tolerate a wheelchair localization error of at most a few centimetres. We provide experimental results demonstrating near perfect performance in an indoor environment.},
keywords={Computer vision;Wheelchairs;Robot sensing systems;Mobile robots;Robot vision systems;Visual system;Manipulators;Robot kinematics;Force sensors;Indoor environments;active vision;visually guided robotics;object localization;assistive technology},
doi={10.1109/CRV.2008.23},
ISSN={},
month={May},}
@INPROCEEDINGS{1390045,
author={Linaker, F. and Ishikawa, M.},
booktitle={2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)}, title={Rotation invariant features from omnidirectional camera images using a polar higher-order local autocorrelation feature extractor},
year={2004},
volume={4},
number={},
pages={4026-4031 vol.4},
abstract={Proposed in this paper is a component for extracting low-dimensional rotation invariant feature vectors directly from omnidirectional camera images. The component is based on higher-order local autocorrelation (HLAC) functions, but with a modification that makes the extraction result in rotation invariant representations. As the component provides a static mapping to feature vectors, it requires no setup or learning phase and is well-suited for lifelong learning scenarios where input distributions can be nonstationary. Experiments with an actual robot system are presented and results show that the extracted feature vectors manage to capture structures in the environment. When used as the perceptual component of a sequential Monte Carlo localizer, the location of the robot can be tracked without access to long-range distance sensors. Important limitations and suitable uses for the extracted representations are also discussed.},
keywords={Cameras;Autocorrelation;Feature extraction;Robot vision systems;Data mining;Robot sensing systems;Monte Carlo methods;Mobile robots;Image storage;Continuing professional development},
doi={10.1109/IROS.2004.1390045},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7367836,
author={Ravari, Alireza Norouzzadeh and Taghirad, Hamid D.},
booktitle={2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM)}, title={Loop closure detection by compressed sensing for exploration of mobile robots in outdoor environments},
year={2015},
volume={},
number={},
pages={511-516},
abstract={In the problem of simultaneously localization and mapping (SLAM) for a mobile robot, it is required to detect previously visited locations so the estimation error shall be reduced. Sensor observations are compared by a similarity metric to detect loops. In long term navigation or exploration, the number of observations increases and so the complexity of the loop closure detection. Several techniques are proposed in order to reduce the complexity of loop closure detection. Few algorithms have considered the loop closure detection from a subset of sensor observations. In this paper, the compressed sensing approach is exploited to detect loops from few sensor measurements. In the basic compressed sensing it is assumed that a signal has a sparse representation is a basis which means that only a few elements of the signal are non-zero. Based on the compressed sensing approach a sparse signal can be recovered from few linear noisy projections by l1 minimization. The difference matrix which is widely used for loop detection has a sparse structure, where similar observations are shown by zero distance and different locations are indicated by ones. Based on the multiple measurement vector technique which is an extension of the basic compressed sensing, the loop closure detection is performed by comparison of few sensor observations. The applicability of the proposed algorithm is investigated in some outdoor environments through some publicly available data sets. It has been shown by some experiments that the proposed method can detect loops effectively.},
keywords={Robot sensing systems;Complexity theory;Sparse matrices;Compressed sensing;Cameras;Information theory;Feature extraction},
doi={10.1109/ICRoM.2015.7367836},
ISSN={},
month={Oct},}
@ARTICLE{6917066,
author={Prasanna, Prateek and Dana, Kristin J. and Gucunski, Nenad and Basily, Basily B. and La, Hung M. and Lim, Ronny Salim and Parvardeh, Hooman},
journal={IEEE Transactions on Automation Science and Engineering}, title={Automated Crack Detection on Concrete Bridges},
year={2016},
volume={13},
number={2},
pages={591-599},
abstract={Detection of cracks on bridge decks is a vital task for maintaining the structural health and reliability of concrete bridges. Robotic imaging can be used to obtain bridge surface image sets for automated on-site analysis. We present a novel automated crack detection algorithm, the STRUM (spatially tuned robust multifeature) classifier, and demonstrate results on real bridge data using a state-of-the-art robotic bridge scanning system. By using machine learning classification, we eliminate the need for manually tuning threshold parameters. The algorithm uses robust curve fitting to spatially localize potential crack regions even in the presence of noise. Multiple visual features that are spatially tuned to these regions are computed. Feature computation includes examining the scale-space of the local feature in order to represent the information and the unknown salient scale of the crack. The classification results are obtained with real bridge data from hundreds of crack regions over two bridges. This comprehensive analysis shows a peak STRUM classifier performance of 95% compared with 69% accuracy from a more typical image-based approach. In order to create a composite global view of a large bridge span, an image sequence from the robot is aligned computationally to create a continuous mosaic. A crack density map for the bridge mosaic provides a computational description as well as a global view of the spatial patterns of bridge deck cracking. The bridges surveyed for data collection and testing include Long-Term Bridge Performance program's (LTBP) pilot project bridges at Haymarket, VA, USA, and Sacramento, CA, USA.},
keywords={Bridges;Robots;Robustness;Laplace equations;Visualization;Concrete;Image segmentation;Adaboost;bridge deck inspection;bridge maintenance;computer vision;concrete;crack detection;crack pattern recognition;homography;image mosaic;image stitching;Laplacian pyramid;machine learning;random forest;robotic imaging;robotic inspection;Seekur robot;structural health monitoring;structure from motion;STRUM classifier;support vector machine},
doi={10.1109/TASE.2014.2354314},
ISSN={1558-3783},
month={April},}
@INPROCEEDINGS{6696478,
author={Carlevaris-Bianco, Nicholas and Eustice, Ryan M.},
booktitle={2013 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={Long-term simultaneous localization and mapping with generic linear constraint node removal},
year={2013},
volume={},
number={},
pages={1034-1041},
abstract={This paper reports on the use of generic linear constraint (GLC) node removal as a method to control the computational complexity of long-term simultaneous localization and mapping. We experimentally demonstrate that GLC provides a principled and flexible tool enabling a wide variety of complexity management schemes. Specifically, we consider two main classes: batch multi-session node removal, in which nodes are removed in a batch operation between mapping sessions, and online node removal, in which nodes are removed as the robot operates. Results are shown for 34.9 h of real-world indoor-outdoor data covering 147.4 km collected over 27 mapping sessions spanning a period of 15 months.},
keywords={Simultaneous localization and mapping;Approximation methods;Computational complexity;Markov processes},
doi={10.1109/IROS.2013.6696478},
ISSN={2153-0866},
month={Nov},}
@INPROCEEDINGS{7795672,
author={Arroyo, Roberto and Alcantarilla, Pablo F. and Bergasa, Luis M. and Romera, Eduardo},
booktitle={2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)}, title={OpenABLE: An open-source toolbox for application in life-long visual localization of autonomous vehicles},
year={2016},
volume={},
number={},
pages={965-970},
abstract={Visual information is a valuable asset in any perception scheme designed for an intelligent transportation system. In this regard, the camera-based recognition of locations provides a higher situational awareness of the environment, which is very useful for varied localization solutions typically needed in long-term autonomous navigation, such as loop closure detection and visual odometry or SLAM correction. In this paper we present OpenABLE, an open-source toolbox contributed to the community with the aim of helping researchers in the application of these kinds of life-long localization algorithms. The implementation follows the philosophy of the topological place recognition method named ABLE, including several new features and improvements. These functionalities allow to match locations using different global image description methods and several configuration options, which enable the users to control varied parameters in order to improve the performance of place recognition depending on their specific problem requisites. The applicability of our toolbox in visual localization purposes for intelligent vehicles is validated in the presented results, jointly with comparisons to the main state-of-the-art methods.},
keywords={Visualization;Open source software;Autonomous vehicles;Context;Cameras;Navigation},
doi={10.1109/ITSC.2016.7795672},
ISSN={2153-0017},
month={Nov},}
@INPROCEEDINGS{9636320,
author={Peltomäki, Jukka and Alijani, Farid and Puura, Jussi and Huttunen, Heikki and Rahtu, Esa and Kämäräinen, Joni-Kristian},
booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Evaluation of Long-term LiDAR Place Recognition},
year={2021},
volume={},
number={},
pages={4487-4492},
abstract={We compare a state-of-the-art deep image retrieval and a deep place recognition method for place recognition using LiDAR data. Place recognition aims to detect previously visited locations and thus provides an important tool for navigation, mapping, and localisation. Experimental comparisons are conducted using challenging outdoor and indoor datasets, Oxford Radar RobotCar and COLD, in the "long-term" setting where the test conditions differ substantially from the training and gallery data. Based on our results the image retrieval methods using LiDAR depth images can achieve accurate localization (the single best match recall 80%) within 5.00 m in urban outdoors. In office indoors the comparable accuracy is 50 cm but is more sensitive to changes in the environment.},
keywords={Training;Meters;Location awareness;Laser radar;Image recognition;Image retrieval;Radar imaging},
doi={10.1109/IROS51168.2021.9636320},
ISSN={2153-0866},
month={Sep.},}
@ARTICLE{9059034,
author={Jiao, Yanmei and Wang, Yue and Ding, Xiaqing and Fu, Bo and Huang, Shoudong and Xiong, Rong},
journal={IEEE Transactions on Industrial Electronics}, title={2-Entity Random Sample Consensus for Robust Visual Localization: Framework, Methods, and Verifications},
year={2021},
volume={68},
number={5},
pages={4519-4528},
abstract={Robust and efficient visual localization is essential for numerous robotic applications. However, it remains a challenging problem especially when significant environmental or perspective changes are present, as there are high percentage of outliers, i.e., incorrect feature matches between the query image and the map. In this article, we propose a novel 2-entity random sample consensus (RANSAC) framework using three-dimensional-two-dimensional point and line feature matches for visual localization with the aid of inertial measurements and derive minimal closed-form solutions using only 1 point 1 line or 2 point matches for both monocular and multi-camera system. The proposed 2-entity RANSAC can achieve higher robustness against outliers as multiple types of features are utilized and the number of matches needed to compute a pose is reduced. Furthermore, we propose a learning-based sampling strategy selection mechanism and a feature scoring network to be adaptive to different environmental characteristics such as structured and unstructured. Finally, both simulation and real-world experiments are performed to validate the robustness and effectiveness of the proposed method in scenarios with long-term and perspective changes.},
keywords={Cameras;Visualization;Robustness;Robot vision systems;Pose estimation;Closed-form solutions;Computational modeling;Camera pose estimation;random sample consensus (RANSAC);robust localization},
doi={10.1109/TIE.2020.2984970},
ISSN={1557-9948},
month={May},}
@INPROCEEDINGS{4209836,
author={Sola, Joan and Monin, Andre and Devy, Michel},
booktitle={Proceedings 2007 IEEE International Conference on Robotics and Automation}, title={BiCamSLAM: Two times mono is more than stereo},
year={2007},
volume={},
number={},
pages={4795-4800},
abstract={This paper is an invitation to use mono-vision techniques on stereo-vision equipped robots. By using monocular algorithms on both cameras, the advantages of mono-vision (bearing-only, with infinity range but no 3D instant information) and stereo-vision (3D information only up to a limited range) naturally add up to provide interesting possibilities, that are here developed and demonstrated using an EKF-based monocular SLAM algorithm. Mainly we obtain: a) fast 3D mapping with long term, absolute angular references; b) great landmark updating flexibility; and c) the possibility of stereo rig extrinsic self-calibration, providing a much more robust and accurate sensor. Experimental results show the pertinence of the proposed ideas, which should be easily exportable (and we encourage to do so) to other, more performing, vision-based SLAM algorithms.},
keywords={MONOS devices;Simultaneous localization and mapping;Signal processing algorithms;Cameras;Robustness;Robot vision systems;Observability;Robotics and automation;Robot sensing systems;Delay estimation},
doi={10.1109/ROBOT.2007.364218},
ISSN={1050-4729},
month={April},}
@INPROCEEDINGS{100109,
author={Sarachik, K.B.},
booktitle={Proceedings, 1989 International Conference on Robotics and Automation}, title={Characterising an indoor environment with a mobile robot and uncalibrated stereo},
year={1989},
volume={},
number={},
pages={984-989 vol.2},
abstract={The author shows how it is possible for a mobile robot to exploit the visual information obtained by scanning a room to determine its size and shape, and to orient itself continually within it. The equipment used is a very simple camera setup whose detailed initial configuration is not known but can be deduced as the algorithm runs. The approach does not require any special environment, nor is it sensitive to changes in the physical aspect of the room being inspected such as moved furniture or roaming people. The long-term goal of the project is for the robot to use the information thus acquired in order to build maps of its environment, presumed to be a single floor of an office building, and to localize itself within this framework.<>},
keywords={Indoor environments;Mobile robots;Cameras;Robot vision systems;Artificial intelligence;Floors;Contracts;Strips;Shape;Robot sensing systems},
doi={10.1109/ROBOT.1989.100109},
ISSN={},
month={May},}
@INPROCEEDINGS{6907022,
author={Murphy, Liz and Sibley, Gabe},
booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, title={Incremental unsupervised topological place discovery},
year={2014},
volume={},
number={},
pages={1312-1318},
abstract={This paper describes an online place discovery and recognition engine that fuses information over time to create topologically distinct places. A key motivation is the recognition that a single image may be a poor exemplar of what constitutes a place. Images are not `places' nor are they `documents'. Instead, by treating image-sequences as a multimodal distribution over topics - and by discovering topics incrementally and online - it is possible to both reduce the memory footprint of place recognition systems, and to improve precision and recall. Distinctive key-places are represented by a cluster topics found from the covisibility graph of a relative simultaneous localization and mapping engine - key-places inherently span many images. A dynamic vocabulary of visual words and density based clustering is used to continually estimate a set of visual topics, changes in which drive the place-recognition process. The system is evaluated using an indoor robot sequence, a standard outdoor robot sequence and a long-term sequence from a static camera. Experiments demonstrate qualitatively distinct themes associated with discovered places - from common place types such as `hallway', or `desk-area', to temporal concepts such as `dusk', `dawn' or `mid-day'. Compared to traditional image-based place-recognition, this reduces the information that must be stored without reducing place-recognition performance.},
keywords={Vocabulary;Visualization;Robots;Semantics;Streaming media;Image recognition;Engines},
doi={10.1109/ICRA.2014.6907022},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{9172595,
author={Dille, Michael and Nuch, Danny and Gupta, Shiven and McCabe, Steven and Verzic, Nicholas and Fong, Terry and Wong, Uland},
booktitle={2020 IEEE Aerospace Conference}, title={PHALANX: Expendable projectile sensor networks for planetary exploration},
year={2020},
volume={},
number={},
pages={1-12},
abstract={Technologies enabling long-term, wide-ranging measurement in hard-to-reach areas are a critical need for planetary science inquiry. Phenomena of interest include flows or variations in volatiles, gas composition or concentration, particulate density, or even simply temperature. Improved measurement of these processes enables understanding of exotic geologies and distributions or correlating indicators of trapped water or biological activity. However, such data is often needed in unsafe areas such as caves, lava tubes, or steep ravines not easily reached by current spacecraft and planetary robots. To address this capability gap, we have developed miniaturized, expendable sensors which can be ballistically lobbed from a robotic rover or static lander - or even dropped during a flyover. These projectiles can perform sensing during flight and after anchoring to terrain features. By augmenting exploration systems with these sensors, we can extend situational awareness, perform long-duration monitoring, and reduce utilization of primary mobility resources, all of which are crucial in surface missions. We call the integrated payload that includes a cold gas launcher, smart projectiles, planning software, network discovery, and science sensing: PHALANX. In this paper, we introduce the mission architecture for PHALANX and describe an exploration concept that pairs projectile sensors with a rover “mothership.” Science use cases explored include reconnaissance using ballistic cameras, volatiles detection, and building timelapse maps of temperature and illumination conditions. Strategies to autonomously coordinate constellations of deployed sensors to self-discover and localize with peer ranging (i.e. a “local GPS”) are summarized, thus providing communications infrastructure beyond-line-of-sight (BLOS) of the rover. Capabilities were demonstrated through both simulation and physical testing with a terrestrial prototype. The approach to developing a terrestrial prototype is discussed, including design of the launching mechanism, projectile optimization, micro-electronics fabrication, and sensor selection. Results from early testing and characterization of commercial-off-the-shelf (COTS) components are reported. Nodes were subjected to successful burn-in tests over 48 hours at full logging duty cycle. Integrated field tests were conducted in the Roverscape, a half-acre planetary analog environment at NASA Ames, where we tested up to 10 sensor nodes simultaneously coordinating with an exploration rover. Ranging accuracy has been demonstrated to be within +/-10cm over 20m using commodity radios when compared to high-resolution laser scanner ground truthing. Evolution of the design, including progressive miniaturization of the electronics and iterated modifications of the enclosure housing for streamlining and optimized radio performance are described. Finally, lessons learned to date, gaps toward eventual flight mission implementation, and continuing future development plans are discussed.},
keywords={},
doi={10.1109/AERO47225.2020.9172595},
ISSN={1095-323X},
month={March},}
@INPROCEEDINGS{9705975,
author={Shah, Vikrant and Nir, Jagatpreet and Kaveti, Pushyami and Singh, Hanumant},
booktitle={OCEANS 2021: San Diego – Porto}, title={Performance Analysis of Feature Detectors and Descriptors in Underwater and Polar Environments},
year={2021},
volume={},
number={},
pages={1-7},
abstract={Many scientific mapping surveys that deploy robotic platforms in underwater and polar environments perform Visual Simultaneous Localization and Mapping (VSLAM), Structure for Motion (SfM), and Image Mosaicking. These techniques heavily rely on robust and reliable feature-based vision front ends. The job of a vision front end is to provide correspondence information between different camera views which is then directly fed into a bundle adjustment step. Although the atomic steps involved in constructing a vision front end are well-known, many of the popular choices of the features and their parameters do not perform reliably in a variety of visually degraded underwater and polar environments that are characterized by low texture and contrast, and by unevenly lit and low-overlap imagery. In this paper, we develop novel metrics and quantitative analysis methods which can measure the impact of image pre-processing steps such as Contrast Limited Adaptive Histogram Equalization (CLAHE) on the improvement of vision front-end outputs. Our metrics and quantitative analysis can guide the selection between different feature detectors and descriptors to develop a reliable and robust vision front end that can operate in a wider range of underwater and polar environments. We showcase how CLAHE improves the saliency of features and feature track length on a visually degraded dataset underwater, resulting in a substantial increase in correspondence information for the SfM solution. Finally, we perform an end-to-end SfM analysis that shows reduced accumulated drift over the long term and improved accuracy.},
keywords={Measurement;Visualization;Sea surface;Simultaneous localization and mapping;Statistical analysis;Detectors;Feature extraction;Robust visual perception;Underwater and Polar environments Low texture and contrast;SLAM;CLAHE},
doi={10.23919/OCEANS44145.2021.9705975},
ISSN={0197-7385},
month={Sep.},}
@INPROCEEDINGS{7759430,
author={Su, Daobilige and Nakamura, Keisuke and Nakadai, Kazuhiro and Miro, Jaime Valls},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Robust sound source mapping using three-layered selective audio rays for mobile robots},
year={2016},
volume={},
number={},
pages={2771-2777},
abstract={This paper investigates sound source mapping in a real environment using a mobile robot. Our approach is based on audio ray tracing which integrates occupancy grids and sound source localization using a laser range finder and a microphone array. Previous audio ray tracing approaches rely on all observed rays and grids. As such observation errors caused by sound reflection, sound occlusion, wall occlusion, sounds at misdetected grids, etc. can significantly degrade the ability to locate sound sources in a map. A three-layered selective audio ray tracing mechanism is proposed in this work. The first layer conducts frame-based unreliable ray rejection (sensory rejection) considering sound reflection and wall occlusion. The second layer introduces triangulation and audio tracing to detect falsely detected sound sources, rejecting audio rays associated to these misdetected sounds sources (short-term rejection). A third layer is tasked with rejecting rays using the whole history (long-term rejection) to disambiguate sound occlusion. Experimental results under various situations are presented, which proves the effectiveness of our method.},
keywords={Robot kinematics;Ray tracing;Lasers;Simultaneous localization and mapping;Two dimensional displays},
doi={10.1109/IROS.2016.7759430},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{9636640,
author={Sheng, Diwei and Chai, Yuxiang and Li, Xinru and Feng, Chen and Lin, Jianzhe and Silva, Claudio and Rizzo, John-Ross},
booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={NYU-VPR: Long-Term Visual Place Recognition Benchmark with View Direction and Data Anonymization Influences},
year={2021},
volume={},
number={},
pages={9773-9779},
abstract={Visual place recognition (VPR) is critical in not only localization and mapping for autonomous driving vehicles, but also assistive navigation for the visually impaired population. To enable a long-term VPR system on a large scale, several challenges need to be addressed. First, different applications could require different image view directions, such as front views for self-driving cars while side views for the low vision people. Second, VPR in metropolitan scenes can often cause privacy concerns due to the imaging of pedestrian and vehicle identity information, calling for the need for data anonymization before VPR queries and database construction. Both factors could lead to VPR performance variations that are not well understood yet. To study their influences, we present the NYU-VPR dataset that contains more than 200,000 images over a 2km×2km area near the New York University campus, taken within the whole year of 2016. We present benchmark results on several popular VPR algorithms showing that side views are significantly more challenging for current VPR methods while the influence of data anonymization is almost negligible, together with our hypothetical explanations and in-depth analysis.},
keywords={Location awareness;Visualization;Data privacy;Navigation;Databases;Sociology;Imaging},
doi={10.1109/IROS51168.2021.9636640},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{8683446,
author={Wang, Chengze and Yuan, Yuan and Wang, Qi},
booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, title={Learning by Inertia: Self-supervised Monocular Visual Odometry for Road Vehicles},
year={2019},
volume={},
number={},
pages={2252-2256},
abstract={In this paper, we present iDVO (inertia-embedded deep visual odometry), a self-supervised learning based monocular visual odometry (VO) for road vehicles. When modelling the geometric consistency within adjacent frames, most deep VO methods ignore the temporal continuity of the camera pose, which results in a very severe jagged fluctuation in the velocity curves. With the observation that road vehicles tend to perform smooth dynamic characteristics in most of the time, we design the inertia loss function to describe the abnormal motion variation, which assists the model to learn the consecutiveness from long-term camera ego-motion. Based on the recurrent convolutional neural network (RCNN) architecture, our method implicitly models the dynamics of road vehicles and the temporal consecutiveness by the extended Long Short-Term Memory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to handle the non-consistency in fast camera motion by blocking the boundary part and which generates more efficiency in the whole non-consistency mask. The proposed method is evaluated on the KITTI dataset, and the results demonstrate state-of-the-art performance with respect to other monocular deep VO and SLAM approaches.},
keywords={Inertia;Self-supervised Learning;Visual Odometry;RCNN},
doi={10.1109/ICASSP.2019.8683446},
ISSN={2379-190X},
month={May},}
@INPROCEEDINGS{7889591,
author={Hashemifar, Zakieh S. and Lee, Kyung Won and Napp, Nils and Dantu, Karthik},
booktitle={2017 IEEE 11th International Conference on Semantic Computing (ICSC)}, title={Consistent Cuboid Detection for Semantic Mapping},
year={2017},
volume={},
number={},
pages={526-531},
abstract={Building and storing efficient maps is an essential feature for long-term autonomy of robots. Modern sensors (such as Kinect) tend to produce a lot of data. However, long-term autonomy requires us to store this information in a succinct manner. One way to reduce dimensionality of information is to attribute semantics. Most indoor objects are cuboidal in nature. We conjecture that cuboids are a suitable semantic feature to attribute to indoor objects for efficient mapping. We adapt a cuboid fitting algorithm previously proposedfor object recognition, for indoor mapping. Our work stems from the observation that landmark detection for mappingrequires consistent detection of those landmarks. We implement several modifications to this cuboid detection algorithm that lead to consistent detection such as emptiness, orientation, surface coverage, distance from edges, and others. We incorporate these in the identification of the cuboid candidates in a scene, as well as an optimization algorithm for finding the best set of consistent cubes to cover a given scene. Our experiments show that in comparison, the set of cuboids detected by our algorithm are at least 50% more consistent based on our metrics.SLAM.},
keywords={Image segmentation;Simultaneous localization and mapping;Semantics;Object recognition;Image edge detection;Optimization;Image color analysis},
doi={10.1109/ICSC.2017.78},
ISSN={},
month={Jan},}
@INPROCEEDINGS{4543484,
author={Leung, Cindy and Shoudong Huang and Dissanayake, Gamini},
booktitle={2008 IEEE International Conference on Robotics and Automation}, title={Active SLAM in structured environments},
year={2008},
volume={},
number={},
pages={1898-1903},
abstract={This paper considers the trajectory planning problem for line-feature based SLAM in structured indoor environments. The robot poses and line features are estimated using Smooth and Mapping (SAM) which is found to provide more consistent estimates than the Extended Kalman Filter (EKF). The objective of trajectory planning is to minimise the uncertainty of the estimates and to maximise coverage. Trajectory planning is performed using Model Predictive Control (MPC) with an attractor incorporating long term goals. This planning is demonstrated both in simulation and in a real-time experiment with a Pioneer2DX robot.},
keywords={Simultaneous localization and mapping;Robots;Trajectory;Strategic planning;Large-scale systems;Predictive models;Predictive control;Indoor environments;Laser noise;Covariance matrix},
doi={10.1109/ROBOT.2008.4543484},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{9516284,
author={Gunatilake, Amal and Galea, Mitchell and Thiyagarajan, Karthick and Kodagoda, Sarath and Piyathilaka, Lasitha and Darji, Poojaben},
booktitle={2021 IEEE 16th Conference on Industrial Electronics and Applications (ICIEA)}, title={Using UHF-RFID Signals for Robot Localization Inside Pipelines},
year={2021},
volume={},
number={},
pages={1109-1114},
abstract={Underground water pipes are important to any country's infrastructure. Overtime, the metallic pipes are prone to corrosion, which can lead to water leakage and pipe bursts. In order to prolong the service life of those assets, water utilities in Australia apply protective pipe linings. Long-term monitoring and timely intervention are crucial for maintaining those lining assets. However, the water utilities do not possess the comprehensive technology to achieve it. The main reasons for lacking such technology are the unavailability of sensors and accurate robot localization technologies. Feature based localization methods such as SLAM has limited use as the application of liners alters the features and the environment. Encoder based localization is not accurate enough to observe the evolution of defects over a long period of time requiring unique defect correspondence. This motivates us to explore accurate contact-less and wireless based localization methods. We propose a cost-effective localization method using UHF-RFID signals for robot localization inside pipelines based on Gaussian process combined particle filter. Experiments carried out in field extracted pipe samples from the Sydney water pipe network show that using the RSSI and Phase data together in the measurement model with particle filter algorithm improves the localization accuracy up to 15 centimeters precision.},
keywords={Location awareness;Wireless communication;Wireless sensor networks;Simultaneous localization and mapping;Phase measurement;Service robots;Pipelines;infrastructure robotics;linings;localization;particle filter;pipes;robotics for smart cities;RFID;robotic inspections;UHF-RFID},
doi={10.1109/ICIEA51954.2021.9516284},
ISSN={2158-2297},
month={Aug},}
@INPROCEEDINGS{7354193,
author={Limosani, R. and Morales, L. Yoichi and Even, J. and Ferreri, F. and Watanabe, A. and Cavallo, F. and Dario, P. and Hagita, N.},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Long-term human affordance maps},
year={2015},
volume={},
number={},
pages={5748-5754},
abstract={This paper presents a work on mapping the use of space by humans in long periods of time. Daily geometric maps with the same coordinate frame were generated with SLAM, and in a similar manner, daily affordance density maps (places people use) were generated with the output of a human tracker running on the robot. The contribution of the paper is two-fold: an approach to detect geometric changes to cluster them in similar geometric configurations and the building of geometric and affordance composite maps on each cluster. This approach avoids the loss of long term retrieved information. Geometric similarity was computed using a normal distance approach on the maps. The analysis was performed on data collected by a mobile robot for a period of 4 months accumulating data equivalent to 70 days. Experimental results show that the system is capable of detecting geometric changes in the environment and clustering similar geometric configurations.},
keywords={Robot kinematics;Buildings;Navigation;Robot sensing systems;Layout;Geometry},
doi={10.1109/IROS.2015.7354193},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{5986295,
author={Yang, Jin-fu and Wang, Kai and Li, Ming-ai and Liu, Lu},
booktitle={2011 IEEE International Conference on Mechatronics and Automation}, title={Research on object recognition using bag of word model for mobile robot navigation},
year={2011},
volume={},
number={},
pages={1735-1740},
abstract={Robust long term positioning for autonomous mobile robots is essential for many applications. Key to a successful visual SLAM system is correctly recognizing the objects and labeling where the robot is. Local image features are popular with constructing object recognition system, which are invariant to image scaling, translation, rotation, and partially invariant to illumination changes and affine. In this paper, we proposed an object recognition method based on the bag of word model, mainly idea includes three steps as follows: firstly, a set of local image patches are sampled using a key point detector, and each patch is a descriptor based on scale invariant feature transform. Then outliers are removed by RANSAC algorithm, and the resulting distribution of descriptors is quantified by using vector quantization against a pre-specified codebook to convert it to a histogram of votes for codebook centers. Finally, a KNN algorithm is used to classify images through the resulting global descriptor vector. The experimental results show that our proposed method has a better performance against the previous methods.},
keywords={Feature extraction;Object recognition;Computational modeling;Training;Databases;Testing;Visualization;scale invariant feature transform (SIFT);bag of word (BOW);object recognition;robot navigation},
doi={10.1109/ICMA.2011.5986295},
ISSN={2152-744X},
month={Aug},}
@INPROCEEDINGS{9568826,
author={Banerjee, Nandan and Lisin, Dimitri and Albanese, Victoria and Zhu, Zhongjian and Lenser, Scott R. and Shriver, Justin and Ramaswamy, Tyagaraja and Briggs, Jimmy and Fong, Phil},
booktitle={2021 European Conference on Mobile Robots (ECMR)}, title={Preventing and Correcting Mistakes in Lifelong Mapping},
year={2021},
volume={},
number={},
pages={1-8},
abstract={A Graph SLAM system is only as good as the edges in its pose graph. Critical mistakes in the generation of these edges can instantly render a map inconsistent, misleading, and ultimately unusable. For a lifelong mapping system, where the map is updated continuously, avoiding these errors altogether is infeasible. Instead, we propose a system for detection of and recovery from severe errors in edge generation. Our system remedies both edges created by view observations and edges created by an odometry motion model. For observation edges, we pair a novel method for monitoring ambiguous views with an intelligent graph-merging algorithm capable of rejecting a relocalization in progress. For motion edges, we propose a qualitative geometric approach for detecting structural aberrations characteristic of odometry failures. We conclude with an analysis of our results based on an empirical study of thousands of robot runs.},
keywords={Location awareness;Simultaneous localization and mapping;Image edge detection;Europe;Mobile robots;Monitoring},
doi={10.1109/ECMR50962.2021.9568826},
ISSN={},
month={Aug},}
@ARTICLE{9359460,
author={Xu, Xuecheng and Yin, Huan and Chen, Zexi and Li, Yuehua and Wang, Yue and Xiong, Rong},
journal={IEEE Robotics and Automation Letters}, title={DiSCO: Differentiable Scan Context With Orientation},
year={2021},
volume={6},
number={2},
pages={2791-2798},
abstract={Global localization is essential for robot navigation, of which the first step is to retrieve a query from the map database. This problem is called place recognition. In recent years, LiDAR scan based place recognition has drawn attention as it is robust against the appearance change. In this letter, we propose a LiDAR-based place recognition method, named Differentiable Scan Context with Orientation (DiSCO), which simultaneously finds the scan at a similar place and estimates their relative orientation. The orientation can further be used as the initial value for the down-stream local optimal metric pose estimation, improving the pose estimation especially when a large orientation between the current scan and retrieved scan exists. Our key idea is to transform the feature into the frequency domain. We utilize the magnitude of the spectrum as the place descriptor, which is theoretically rotation-invariant. In addition, based on the differentiable phase correlation, we can efficiently estimate the global optimal relative orientation using the spectrum. With such structural constraints, the network can be learned in an end-to-end manner, and the backbone is fully shared by the two tasks, achieving better interpretability and lightweight. Finally, DiSCO is validated on three datasets with long-term outdoor conditions, showing better performance than the compared methods. Codes are released at https://github.com/MaverickPeter/DiSCO-pytorch.},
keywords={Three-dimensional displays;Feature extraction;Laser radar;Pose estimation;Visualization;Transforms;Location awareness;Localization;range sensing;SLAM},
doi={10.1109/LRA.2021.3060741},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{6385879,
author={Latif, Yasir and Cadena, César and Neira, José},
booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={Realizing, reversing, recovering: Incremental robust loop closing over time using the iRRR algorithm},
year={2012},
volume={},
number={},
pages={4211-4217},
abstract={The ability to reconsider information over time allows to detect failures and is crucial for long term robust autonomous robot applications. This applies to loop closure decisions in localization and mapping systems. This paper describes a method to analyze all available information up to date in order to robustly remove past incorrect loop closures from the optimization process. The main novelties of our algorithm are: 1. incrementally reconsidering loop closures and 2. handling multi-session, spatially related or unrelated experiments. We validate our proposal in real multi-session experiments showing better results than those obtained by state of the art methods.},
keywords={Robustness;Clustering algorithms;Proposals;Optimization;Robots;Trajectory;Estimation},
doi={10.1109/IROS.2012.6385879},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{7943649,
author={Papon, Jeremie and Detry, Renaud and Vieira, Peter and Brooks, Sawyer and Srinivasan, Thirupathi and Peterson, Ariel and Kulczycki, Eric},
booktitle={2017 IEEE Aerospace Conference}, title={Martian Fetch: Finding and retrieving sample-tubes on the surface of mars},
year={2017},
volume={},
number={},
pages={1-9},
abstract={Mars Sample Return (MSR) was identified by the 2011 planetary science decadal survey as a high priority long-term goal for NASA. A three-mission campaign concept is currently being investigated. The Mars 2020 rover mission is intended to core and collect samples. These samples will be sealed in tubes and left on the surface for potential return to Earth. In the current MSR campaign concept, a Sample Retrieval and Launch (SRL) mission would collect the sample tubes left by the Mars 2020 rover and load them into a Mars Ascent Vehicle (MAV) to be launched into orbit. The third mission concept involves a spacecraft capturing the samples in Martian orbit and returning them to Earth. This paper focuses on the SRL mission concept to collect the sample tubes, addressing the problem of autonomously detecting, localizing, and grasping sample tubes deposited on the Martian surface. We employ two approaches: The first one is context-based. It would use a high precision map computed from images captured during tube release, to locate the tubes without directly observing them. The second approach directly detects the sample tubes visually and estimates their 6-DoF pose onboard from dense stereo data.},
keywords={Electron tubes;Earth;Cameras;Surface treatment;Rocks;Feature extraction;Orbits},
doi={10.1109/AERO.2017.7943649},
ISSN={},
month={March},}
@ARTICLE{8316929,
author={Cao, Fengkui and Zhuang, Yan and Zhang, Hong and Wang, Wei},
journal={IEEE Sensors Journal}, title={Robust Place Recognition and Loop Closing in Laser-Based SLAM for UGVs in Urban Environments},
year={2018},
volume={18},
number={10},
pages={4242-4252},
abstract={Robust place recognition plays a key role for the long-term autonomy of unmanned ground vehicles (UGVs) working in indoor or outdoor environments. Although most of the state-of-the-art that approaches for place recognition are vision-based, visual sensors lack adaptability in environments with poor or dynamically changing illumination. In this paper, a 3-D-laser-based place recognition algorithm is proposed to accomplish loop closure detection for simultaneous localization and mapping. An image model named bearing angle (BA) is adopted to convert 3-D laser points to 2-D images, and then ORB features extracted from BA images are utilized to perform scene matching. Since the computational cost for matching a query BA image with all the BA images in a database is too high to meet the requirement of performing real-time place recognition, a visual bag of words approach is used to improve search efficiency. Furthermore, a speed normalization algorithm and a 3-D geometry-based verification algorithm are proposed to complete the proposed place recognition algorithm. Experiments were conducted on two self-developed UGV platforms to verify the performance of the proposed method.},
keywords={Three-dimensional displays;Lasers;Visualization;Feature extraction;Sensors;Robustness;Lighting;Laser scanning;place recognition;simultaneous localization and mapping (SLAM);unmanned ground vehicles (UGVs)},
doi={10.1109/JSEN.2018.2815956},
ISSN={1558-1748},
month={May},}
@ARTICLE{8333748,
author={Tang, Yazhe and Hu, Yuchao and Cui, Jinqiang and Liao, Fang and Lao, Mingjie and Lin, Feng and Teo, Rodney S. H.},
journal={IEEE Transactions on Industrial Electronics}, title={Vision-Aided Multi-UAV Autonomous Flocking in GPS-Denied Environment},
year={2019},
volume={66},
number={1},
pages={616-626},
abstract={This paper presents a sophisticated vision-aided flocking system for unmanned aerial vehicles (UAVs), which is able to operate in GPS-denied unknown environments for exploring and searching missions, and also able to adopt two types of vision sensors, day and thermal cameras, to measure relative motion between UAVs in different lighting conditions without using wireless communication. In order to realize robust vision-aided flocking, an integrated framework of tracking-learning-detection on the basis of multifeature coded correlation filter has been developed. To achieve long-term tracking, a redetector is trained online to adaptively reinitialize target for global sensing. An advanced flocking strategy is developed to address the autonomous multi-UAVs' cooperative flight. Light detection and ranging (LiDAR)-based navigation modules are developed for autonomous localization, mapping, and obstacle avoidance. Flight experiments of a team of UAVs have been conducted to verify the performance of this flocking system in a GPS-denied environment. The extensive experiments validate the robustness of the proposed vision algorithms in challenging scenarios.},
keywords={Target tracking;Sensors;Cameras;Correlation;Trajectory;Robustness;Visualization;Flocking;unmanned system;visual sensing},
doi={10.1109/TIE.2018.2824766},
ISSN={1557-9948},
month={Jan},}
@INPROCEEDINGS{8981658,
author={Patel, Naman and Khorrami, Farshad and Krishnamurthy, Prashanth and Tzes, Anthony},
booktitle={2019 19th International Conference on Advanced Robotics (ICAR)}, title={Tightly Coupled Semantic RGB-D Inertial Odometry for Accurate Long-Term Localization and Mapping},
year={2019},
volume={},
number={},
pages={523-528},
abstract={In this paper, we utilize semantically enhanced feature matching and visual inertial bundle adjustment to improve the robustness of odometry especially in feature-sparse environments. A novel semantically enhanced feature matching algorithm is developed for robust: 1) medium and long-term tracking, and 2) loop-closing. Additionally, a semantic visual inertial bundle adjustment algorithm is introduced to robustly estimate pose in presence of ambiguous correspondences or in feature sparse environment. Our tightly coupled semantic RGB-D odometry approach is demonstrated on a real world indoor dataset collected using our unmanned ground vehicle (UGV). Our approach improves traditional visual odometry relying on low-level geometric features like corners, points, and planes for localization and mapping. Additionally, prior approaches are limited due to their sensitivity to scene geometry and changes in light intensity. The semantic inertial odometry is especially important to significantly reduce drifts in longer intervals.},
keywords={},
doi={10.1109/ICAR46387.2019.8981658},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9635991,
author={Wen, Bowen and Bekris, Kostas},
booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models},
year={2021},
volume={},
number={},
pages={8067-8074},
abstract={Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method’s reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack},
keywords={Training;Solid modeling;Target tracking;Three-dimensional displays;Simultaneous localization and mapping;Motion segmentation;Video sequences},
doi={10.1109/IROS51168.2021.9635991},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{7759504,
author={Maffei, Renan and Jorge, Vitor A. M. and Rey, Vitor F. and Kolberg, Mariana and Prestes, Edson},
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, title={Long-term place recognition using multi-level words of spatial densities},
year={2016},
volume={},
number={},
pages={3269-3274},
abstract={Proper place recognition on an environment that can change over time is fundamental for long-term SLAM. In such scenarios the observations obtained in the same region can drastically differ due to changes caused by semi-static objects, such as doors, furniture, etc. In this work, we extend a strategy that represents environment regions using words, based on spatial density information extracted from laser readings. This time, in order to deal with changes in the environment, our method not only builds words representing the real observations made by the robot, but also alternative multi-level words to account for possible changes in a place's observations generated by non-static objects. Place recognition is made by searching matches of sequences of N consecutive words (both real or alternatives). Experiments performed in real and simulated scenarios are shown, and demonstrate the advantages associated to the use of multi-level words.},
keywords={Buildings;Simultaneous localization and mapping;Kernel;Trajectory;Lasers},
doi={10.1109/IROS.2016.7759504},
ISSN={2153-0866},
month={Oct},}
@INPROCEEDINGS{7866383,
author={Taisho, Tsukamoto and Kanji, Tanaka},
booktitle={2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)}, title={Mining DCNN landmarks for long-term visual SLAM},
year={2016},
volume={},
number={},
pages={570-576},
abstract={Long-term visual SLAM, in familiar, semi-dynamic, and partially changing environments is an important area of research in robotics. The main problem we faced is the question of how to describe a scene discriminatively and compactly-both of which are necessary in order to cope with changes in appearance and a large amount of visual information. In this study, we address the above issues by mining visual experience. Our strategy is to mine a library of raw visual images, termed visual experience, to find the relevant visual patterns to effectively explain the input scene. From a practical point of view, our work offers three main contributions over the previous work. First, it is the first application of discriminative visual features from deep convolutional neural networks (DCNN) to the task of visual landmark mining. Second, we show how to interpret a high-dimensional DCNN feature to a compact semantic representation of visual word. Third, we show that our approach can turn the scene description task with any feature (including the DCNN feature) into the task of mining visual experience. Experiments on a challenging cross-domain visual place recognition validate efficacy of the proposed approach.},
keywords={Visualization;Libraries;Feature extraction;Databases;Principal component analysis;Simultaneous localization and mapping},
doi={10.1109/ROBIO.2016.7866383},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6942552,
author={Kanji, Tanaka and Yuuto, Chokushi and Masatoshi, Ando},
booktitle={2014 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={Mining visual phrases for long-term visual SLAM},
year={2014},
volume={},
number={},
pages={136-142},
abstract={We propose a discriminative and compact scene descriptor for single-view place recognition that facilitates long-term visual SLAM in familiar, semi-dynamic and partially changing environments. In contrast to popular bag-of-words scene descriptors, which rely on a library of vector quantized visual features, our proposed scene descriptor is based on a library of raw image data (such as an available visual experience, images shared by other colleague robots, and publicly available image data on the web) and directly mine it to find visual phrases (VPs) that discriminatively and compactly explain an input query / database image. Our mining approach is motivated by recent success in the field of common pattern discovery-specifically mining of common visual patterns among scenes-and requires only a single library of raw images that can be acquired at different time or day. Experimental results show that even though our scene descriptor is significantly more compact than conventional descriptors it has a relatively higher recognition performance.},
keywords={Visualization;Libraries;Simultaneous localization and mapping;Computational modeling;Vectors;Visual databases},
doi={10.1109/IROS.2014.6942552},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{6094458,
author={Argiles, Alberto and Civera, Javier and Montesano, Luis},
booktitle={2011 IEEE/RSJ International Conference on Intelligent Robots and Systems}, title={Dense multi-planar scene estimation from a sparse set of images},
year={2011},
volume={},
number={},
pages={4448-4454},
abstract={Ego-motion estimation and 3D scene reconstruction from image data has been a long term aim both in the Robotics and Computer Vision communities. Nevertheless, while both visual SLAM and Structure from Motion already provide an accurate ego-motion estimation, visual scene estimation does not offer yet such a satisfactory result; being in most cases limited to a sparse set of salient points. In this paper we propose an algorithm to densify a sparse point-based reconstruction into a dense multi-plane based one, from the only input of a set of sparse images.},
keywords={Three dimensional displays;Cameras;Image reconstruction;Estimation;Feature extraction;Visualization;Silicon},
doi={10.1109/IROS.2011.6094458},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{8354218,
author={Van Opdenbosch, Dominik and Aykut, Tamay and Alt, Nicolas and Steinbach, Eckehard},
booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)}, title={Efficient Map Compression for Collaborative Visual SLAM},
year={2018},
volume={},
number={},
pages={992-1000},
abstract={Swarm robotics is receiving increasing interest, because the collaborative completion of tasks, such as the exploration of unknown environments, leads to improved performance and reduced effort. The ability to exchange map information is an essential requirement for collaborative exploration. When moving to large-scale environments, where the communication data rate between the swarm participants is typically limited, efficient compression algorithms and an approach for discarding less informative parts of the map are key for a successful long-term operation. In this paper, we present a novel compression approach for environment maps obtained from a visual SLAM system. We apply feature coding to the visual information to compress the map efficiently. We make use of a minimum spanning tree to connect all features that serve as observations of a single map point. Thereby, we can exploit inter-feature dependencies and obtain an optimal coding order. Additionally, we add a map sparsification step to keep only useful map points by solving a linear integer programming problem, which preserves the map points that exhibit both good compression properties and high observability. We evaluate the proposed method on a standard dataset and show that our approach outperforms state-of-the-art techniques.},
keywords={Visualization;Encoding;Simultaneous localization and mapping;Feature extraction;Trajectory;Optimization},
doi={10.1109/WACV.2018.00114},
ISSN={},
month={March},}
@INPROCEEDINGS{5980317,
author={Wendel, Andreas and Irschara, Arnold and Bischof, Horst},
booktitle={2011 IEEE International Conference on Robotics and Automation}, title={Natural landmark-based monocular localization for MAVs},
year={2011},
volume={},
number={},
pages={5792-5799},
abstract={Highly accurate localization of a micro aerial vehicle (MAV) with respect to a scene is important for a wide range of applications, in particular surveillance and inspection. Most existing approaches to visual localization focus on indoor environments, while such tasks require outdoor navigation. Within this work, we introduce a novel algorithm for monocular visual localization for MAVs based on the concept of virtual views in 3D space. Under the assumption that significant parts of the scene do not alter their geometry and serve as natural landmarks, the accuracy of our visual approach outperforms consumer grade GPS systems. In an experimental setup we compare our approach to a state-of-the-art visual SLAM algorithm and evaluate the performance by geometric validation from an observer's view. As our method directly allows global registration, it is neither prone to drift nor bias. This makes it well suited for long-term autonomous navigation.},
keywords={Cameras;Three dimensional displays;Visualization;Image reconstruction;Feature extraction;Geometry;Simultaneous localization and mapping},
doi={10.1109/ICRA.2011.5980317},
ISSN={1050-4729},
month={May},}