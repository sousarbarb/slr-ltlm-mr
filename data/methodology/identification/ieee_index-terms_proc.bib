@INPROCEEDINGS{ravankar-et-al:2017:8105770,
  author = {A. A. Ravankar and A. Ravankar and T. Emaru and Y. Kobayashi},
  booktitle = {2017 56th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)},
  title = {A hybrid topological mapping and navigation method for large area robot mapping},
  volume = {},
  number = {},
  pages = {1104--1107},
  doi = {10.23919/SICE.2017.8105770},
  year = {2017},
  month = {9},
  abstract = {In this paper, we present a hybrid topological mapping and navigation method for mobile robots. The proposed method combines metric and topological information to create map and generate navigation plan for the robot. As compared to traditional approaches of robot mapping, the method is lightweight and can be used for mapping and navigation in large areas which is particularly useful for service robots operating in large buildings. The method only uses local information for navigation while maintaining the global topological graph nodes. The topological nodes are used effectively for navigation and can also be used to store semantic information of the scene such as robot poses, scans and scene properties for complete long term robot autonomy. By combining the information from the two maps (topological and grid map), autonomous navigation and mapping in large areas for robots is possible.},
  issn = {},
  keywords = {Navigation;Measurement;Simultaneous localization and mapping;Mobile robots;Robot Mapping;Topological Mapping;Navigation;Graph Theory;SLAM;Mobile Robot},
}

@INPROCEEDINGS{gunatilake-et-al:2021:9516284,
  author = {A. Gunatilake and M. Galea and K. Thiyagarajan and S. Kodagoda and L. Piyathilaka and P. Darji},
  booktitle = {2021 IEEE 16th Conference on Industrial Electronics and Applications (ICIEA)},
  title = {Using UHF-RFID Signals for Robot Localization Inside Pipelines},
  volume = {},
  number = {},
  pages = {1109--1114},
  doi = {10.1109/ICIEA51954.2021.9516284},
  year = {2021},
  month = {8},
  abstract = {Underground water pipes are important to any country's infrastructure. Overtime, the metallic pipes are prone to corrosion, which can lead to water leakage and pipe bursts. In order to prolong the service life of those assets, water utilities in Australia apply protective pipe linings. Long-term monitoring and timely intervention are crucial for maintaining those lining assets. However, the water utilities do not possess the comprehensive technology to achieve it. The main reasons for lacking such technology are the unavailability of sensors and accurate robot localization technologies. Feature based localization methods such as SLAM has limited use as the application of liners alters the features and the environment. Encoder based localization is not accurate enough to observe the evolution of defects over a long period of time requiring unique defect correspondence. This motivates us to explore accurate contact-less and wireless based localization methods. We propose a cost-effective localization method using UHF-RFID signals for robot localization inside pipelines based on Gaussian process combined particle filter. Experiments carried out in field extracted pipe samples from the Sydney water pipe network show that using the RSSI and Phase data together in the measurement model with particle filter algorithm improves the localization accuracy up to 15 centimeters precision.},
  issn = {2158-2297},
  keywords = {Location awareness;Wireless communication;Wireless sensor networks;Simultaneous localization and mapping;Phase measurement;Service robots;Pipelines;infrastructure robotics;linings;localization;particle filter;pipes;robotics for smart cities;RFID;robotic inspections;UHF-RFID},
}

@INPROCEEDINGS{glover-et-al:2010:5509547,
  author = {A. J. Glover and W. P. Maddern and M. J. Milford and G. F. Wyeth},
  booktitle = {2010 IEEE International Conference on Robotics and Automation},
  title = {FAB-MAP + RatSLAM: Appearance-based SLAM for multiple times of day},
  volume = {},
  number = {},
  pages = {3507--3512},
  doi = {10.1109/ROBOT.2010.5509547},
  year = {2010},
  month = {5},
  abstract = {Appearance-based mapping and localisation is especially challenging when separate processes of mapping and localisation occur at different times of day. The problem is exacerbated in the outdoors where continuous change in sun angle can drastically affect the appearance of a scene. We confront this challenge by fusing the probabilistic local feature based data association method of FAB-MAP with the pose cell filtering and experience mapping of RatSLAM. We evaluate the effectiveness of our amalgamation of methods using five datasets captured throughout the day from a single camera driven through a network of suburban streets. We show further results when the streets are re-visited three weeks later, and draw conclusions on the value of the system for lifelong mapping.},
  issn = {1050-4729},
  keywords = {Simultaneous localization and mapping;Layout;Cameras;Lighting;Filtering;Robustness;Probability;Sun;Robotics and automation;USA Councils},
}

@INPROCEEDINGS{jaenal-et-al:2020:9341451,
  author = {A. Jaenal and D. Zuñiga-Nöel and R. Gomez-Ojeda and J. Gonzalez-Jimenez},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Improving Visual SLAM in Car-Navigated Urban Environments with Appearance Maps},
  volume = {},
  number = {},
  pages = {4679--4685},
  doi = {10.1109/IROS45743.2020.9341451},
  year = {2020},
  month = {10},
  abstract = {This paper describes a method that corrects errors of a VSLAM-estimated trajectory for cars driving in GPS-denied environments, by applying constraints from public databases of geo-tagged images (Google Street View, Mapillary, etc). The method, dubbed Appearance-based Geo-Alignment for Simultaneous Localisation and Mapping (AGA-SLAM), encodes the available image database as an appearance map, which represents the space with a compact holistic descriptor for each image plus its associated geo-tag. The VSLAM trajectory is corrected on-line by incorporating constraints from the recognized places along the trajectory into a position-based optimization framework. The paper presents a seamless formulation to combine local and absolute metric observations with associations from Visual Place Recognition. The robustness of the holistic image descriptor to changes due to weather or illumination variations ensures a long-term consistent method to improve car localization. The proposed method has been extensively evaluated on more than 70 sequences from 4 different datasets, proving out its effectiveness and endurance to appearance challenges.},
  issn = {2153-0866},
  keywords = {Measurement;Visualization;Image databases;Visual systems;Trajectory;Internet;Automobiles},
}

@INPROCEEDINGS{rotsidis-et-al:2021:00291,
  author = {A. Rotsidis and C. Lutteroth and P. Hall and C. Richardt},
  booktitle = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title = {ExMaps: Long-Term Localization in Dynamic Scenes using Exponential Decay},
  volume = {},
  number = {},
  pages = {2866--2875},
  doi = {10.1109/WACV48630.2021.00291},
  year = {2021},
  month = {1},
  abstract = {Visual camera localization using offline maps is widespread in robotics and mobile applications. Most state-of-the-art localization approaches assume static scenes, so maps are often reconstructed once and then kept constant. However, many scenes are dynamic and as changes in the scene happen, future localization attempts may struggle or fail entirely. Therefore, it is important for successful long-term localization to update and maintain maps as new observations of the scene, and changes in it, arrive. We propose a novel method for automatically discovering which points in a map remain stable over time, and which are due to transient changes. To this end, we calculate a stability store for each point based on its visibility over time, weighted by an exponential decay over time. This allows us to consider the impact of time when scoring points, and to distinguish which points are useful for long-term localization. We evaluate our method on the CMU Extended Seasons dataset (outdoors) and a new indoor dataset of a retail shop, and show the benefit of maintaining a `live map' that integrates updates over time using our exponential decay based method over a static `base map'.},
  issn = {2642-9381},
  keywords = {Location awareness;Visualization;Computer vision;Conferences;Robot vision systems;Cameras;Mobile applications},
}

@INPROCEEDINGS{schaefer-et-al:2019:8870928,
  author = {A. Schaefer and D. Büscher and J. Vertens and L. Luft and W. Burgard},
  booktitle = {2019 European Conference on Mobile Robots (ECMR)},
  title = {Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from 3-D Lidar Scans},
  volume = {},
  number = {},
  pages = {1--7},
  doi = {10.1109/ECMR.2019.8870928},
  year = {2019},
  month = {9},
  abstract = {Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation [1]. In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15 months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy.},
  issn = {},
  keywords = {Laser radar;Detectors;Reliability;Feature extraction;Urban areas;Trajectory;Roads},
}

@INPROCEEDINGS{walcott-bryant-et-al:2012:6385561,
  author = {A. Walcott-Bryant and M. Kaess and H. Johannsson and J. J. Leonard},
  booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title = {Dynamic pose graph SLAM: Long-term mapping in low dynamic environments},
  volume = {},
  number = {},
  pages = {1871--1878},
  doi = {10.1109/IROS.2012.6385561},
  year = {2012},
  month = {10},
  abstract = {Maintaining a map of an environment that changes over time is a critical challenge in the development of persistently autonomous mobile robots. Many previous approaches to mapping assume a static world. In this work we incorporate the time dimension into the mapping process to enable a robot to maintain an accurate map while operating in dynamical environments. This paper presents Dynamic Pose Graph SLAM (DPG-SLAM), an algorithm designed to enable a robot to remain localized in an environment that changes substantially over time. Using incremental smoothing and mapping (iSAM) as the underlying SLAM state estimation engine, the Dynamic Pose Graph evolves over time as the robot explores new places and revisits previously mapped areas. The approach has been implemented for planar indoor environments, using laser scan matching to derive constraints for SLAM state estimation. Laser scans for the same portion of the environment at different times are compared to perform change detection; when sufficient change has occurred in a location, the dynamic pose graph is edited to remove old poses and scans that no longer match the current state of the world. Experimental results are shown for two real-world dynamic indoor laser data sets, demonstrating the ability to maintain an up-to-date map despite long-term environmental changes.},
  issn = {2153-0866},
  keywords = {Simultaneous localization and mapping;Measurement by laser beam;Mobile robots;Heuristic algorithms;Lasers},
}

@INPROCEEDINGS{han-et-al:2021:9636871,
  author = {B. Han and Z. Xiao and S. Huang and T. Zhang},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Multi-layer VI-GNSS Global Positioning Framework with Numerical Solution aided MAP Initialization},
  volume = {},
  number = {},
  pages = {5448--5455},
  doi = {10.1109/IROS51168.2021.9636871},
  year = {2021},
  month = {9},
  abstract = {Motivated by the goal of achieving long-term drift-free camera pose estimation in complex scenarios, we propose a global positioning framework fusing visual, inertial and Global Navigation Satellite System (GNSS) measurements in multiple layers. Different from previous loosely- and tightly-coupled methods, the proposed multi-layer fusion allows us to delicately correct the drift of visual odometry and keep reliable positioning while GNSS degrades. In particular, local motion estimation is conducted in the inner-layer, solving the problem of scale drift and inaccurate bias estimation in visual odometry by fusing the velocity of GNSS, pre-integration of Inertial Measurement Unit (IMU) and camera measurement in a tightly-coupled way. The global localization is achieved in the outer-layer, where the local motion is further fused with GNSS position and course in a long-term period in a loosely-coupled way. Furthermore, a dedicated initialization method is proposed to guarantee fast and accurate estimation for all state variables and parameters. We give exhaustive tests of the proposed framework on indoor and outdoor public datasets. The mean localization error is reduced up to 63%, with a promotion of 69% in initialization accuracy compared with state-of-the-art works. We have applied the algorithm to Augmented Reality (AR) navigation, crowd sourcing high-precision map update and other large-scale applications.},
  issn = {2153-0866},
  keywords = {Location awareness;Global navigation satellite system;Visualization;Atmospheric measurements;Particle measurements;Cameras;Real-time systems},
}

@INPROCEEDINGS{liu-et-al:2021:9561126,
  author = {B. Liu and F. Tang and Y. Fu and Y. Yang and Y. Wu},
  booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {A Flexible and Efficient Loop Closure Detection Based on Motion Knowledge},
  volume = {},
  number = {},
  pages = {11241--11247},
  doi = {10.1109/ICRA48506.2021.9561126},
  year = {2021},
  month = {5},
  abstract = {Loop closure detection (LCD) is an essential module for simultaneous localization and mapping (SLAM), which can correct accumulated errors after long-term explorations. The widely used bag-of-words (BoW) model can not satisfy well the requirements of both low time consumption and high accuracy for a mobile platform. In this paper, we propose a novel LCD algorithm based on motion knowledge. We give a flexible and efficient detection strategy and also give flexible and efficient combinations of a global binary feature extracted by convolutional neural network (CNN) and a hand-crafted local binary feature. We take a continuous motion model, grid-based motion statistics (GMS) and motion states as motion knowledge. Furthermore, we fuse the proposed LCD with a visual-inertial odometry (VIO) system to correct localization errors by a pose graph optimization. Comparative experiments with state-of-the-art LCD algorithms on typical datasets have been carried out, and the results demonstrate that our proposed method achieves quite high recall rates and quite high speed at 100% precision. Moreover, experimental results from VIO further validate the effectiveness of the proposed method.},
  issn = {2577-087X},
  keywords = {Location awareness;Simultaneous localization and mapping;Automation;Fuses;Conferences;Feature extraction;Liquid crystal displays},
}

@INPROCEEDINGS{song-et-al:2019:8968017,
  author = {B. Song and W. Chen and J. Wang and H. Wang},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Long-Term Visual Inertial SLAM based on Time Series Map Prediction},
  volume = {},
  number = {},
  pages = {5364--5369},
  doi = {10.1109/IROS40897.2019.8968017},
  year = {2019},
  month = {11},
  abstract = {With the advance in the field of mobile robots, autonomous robots are required for long-term deployment in dynamic and complex environments. However, the performance of Visual Inertial SLAM systems in long-term operation is not satisfactory, and most long-term SLAM systems assumes periodic changes in the environment. This paper presents a novel solution for long-term monocular VI SLAM system in dynamic environment based on autoregression(AR) modeling and map prediction. Map points are first classified into static and semi-static map points according to a memory model. Modeling and prediction of the different states of semi-static map points are performed that are derived from time series models. The predicted map is then fused with the current map to achieve a better forecast for the next frame if the prediction is not satisfactory enough. Experiments are carried out on an embedded system. The results indicate that the map prediction is reliable and the proposed approach improves the performance of long-term localization and mapping in dynamic environments.},
  issn = {2153-0866},
  keywords = {},
}

@INPROCEEDINGS{wei-et-al:2020:9340962,
  author = {B. Wei and W. Xu and C. Luo and G. Zoppi and D. Ma and S. Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {SolarSLAM: Battery-free Loop Closure for Indoor Localisation},
  volume = {},
  number = {},
  pages = {4485--4490},
  doi = {10.1109/IROS45743.2020.9340962},
  year = {2020},
  month = {10},
  abstract = {In this paper, we propose SolarSLAM, a batteryfree loop closure method for indoor localisation. Inertial Measurement Unit (IMU) based indoor localisation method has been widely used due to its ubiquity in mobile devices, such as mobile phones, smartwatches and wearable bands. However, it suffers from the unavoidable long term drift. To mitigate the localisation error, many loop closure solutions have been proposed using sophisticated sensors, such as cameras, laser, etc. Despite achieving high-precision localisation performance, these sensors consume a huge amount of energy. Different from those solutions, the proposed SolarSLAM takes advantage of an energy harvesting solar cell as a sensor and achieves effective battery-free loop closure method. The proposed method suggests the key-point dynamic time warping for detecting loops and uses robust simultaneous localisation and mapping (SLAM) as the optimiser to remove falsely recognised loop closures. Extensive evaluations in the real environments have been conducted to demonstrate the advantageous photocurrent characteristics for indoor localisation and good localisation accuracy of the proposed method.},
  issn = {2153-0866},
  keywords = {Performance evaluation;Simultaneous localization and mapping;Photovoltaic cells;Sensor phenomena and characterization;Mobile handsets;Sensor systems;Photoconductivity;Indoor localisation;SLAM;Solar cell},
}

@ARTICLE{cadena-et-al:2016:2624754,
  author = {C. Cadena and L. Carlone and H. Carrillo and Y. Latif and D. Scaramuzza and J. Neira and I. Reid and J. J. Leonard},
  journal = {IEEE Transactions on Robotics},
  title = {Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age},
  volume = {32},
  number = {6},
  pages = {1309--1332},
  doi = {10.1109/TRO.2016.2624754},
  year = {2016},
  month = {12},
  abstract = {Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
  issn = {1941-0468},
  keywords = {Graph theory;Simultaneous location and mapping;Service robots;Robustness;Localization;Factor graphs;localization;mapping;maximum a posteriori estimation;perception;robots;sensing;simultaneous localization and mapping (SLAM)},
}

@ARTICLE{fu-et-al:2018:2810947,
  author = {C. Fu and A. Sarabakha and E. Kayacan and C. Wagner and R. John and J. M. Garibaldi},
  journal = {IEEE/ASME Transactions on Mechatronics},
  title = {Input Uncertainty Sensitivity Enhanced Nonsingleton Fuzzy Logic Controllers for Long-Term Navigation of Quadrotor UAVs},
  volume = {23},
  number = {2},
  pages = {725--734},
  doi = {10.1109/TMECH.2018.2810947},
  year = {2018},
  month = {4},
  abstract = {Input uncertainty, e.g., noise on the on-board camera and inertial measurement unit, in vision-based control of unmanned aerial vehicles (UAVs) is an inevitable problem. In order to handle input uncertainties as well as further analyze the interaction between the input and the antecedent fuzzy sets (FSs) of nonsingleton fuzzy logic controllers (NSFLCs), an input uncertainty sensitivity enhanced NSFLC has been developed in robot operating system using the C++ programming language. Based on recent advances in nonsingleton inference, the centroid of the intersection of the input and antecedent FSs (Cen-NSFLC) is utilized to calculate the firing strength of each rule instead of the maximum of the intersection used in traditional NSFLC (Tra-NSFLC). An 8-shaped trajectory, consisting of straight and curved lines, is used for the real-time validation of the proposed controllers for a trajectory following problem. An accurate monocular keyframe-based visual-inertial simultaneous localization and mapping (SLAM) approach is used to estimate the position of the quadrotor UAV in GPS-denied unknown environments. The performance of the Cen-NSFLC is compared with a conventional proportional-integral derivative (PID) controller, a singleton FLC and a Tra-NSFLC. All controllers are evaluated for different flight speeds, thus introducing different levels of uncertainty into the control problem. Visual-inertial SLAM-based real-time quadrotor UAV flight tests demonstrate that not only does the Cen-NSFLC achieve the best control performance among the four controllers, but it also shows better control performance when compared to their singleton counterparts. Considering the bias in the use of model-based controllers, e.g., PID, for the control of UAVs, this paper advocates an alternative method, namely Cen-NSFLCs, in uncertain working environments.},
  issn = {1941-014X},
  keywords = {Uncertainty;Simultaneous localization and mapping;Frequency selective surfaces;Real-time systems;IEEE transactions;Mechatronics;Fuzzy logic controller (FLC);input uncertainty sensitivity enhanced nonsingleton FLC (NSFLC);monocular visual-inertial simultaneous localization and mapping (SLAM);NSFLC;unmanned aerial vehicle (UAV)},
}

@INPROCEEDINGS{linegar-et-al:2015:7138985,
  author = {C. Linegar and W. Churchill and P. Newman},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Work smart, not hard: Recalling relevant experiences for vast-scale but time-constrained localisation},
  volume = {},
  number = {},
  pages = {90--97},
  doi = {10.1109/ICRA.2015.7138985},
  year = {2015},
  month = {5},
  abstract = {This paper is about life-long vast-scale localisation in spite of changes in weather, lighting and scene structure. Building upon our previous work in Experience-based Navigation [1], we continually grow and curate a visual map of the world that explicitly supports multiple representations of the same place. We refer to these representations as experiences, where a single experience captures the appearance of an environment under certain conditions. Pedagogically, an experience can be thought of as a visual memory. By accumulating experiences we are able to handle cyclic appearance change (diurnal lighting, seasonal changes, and extreme weather conditions) and also adapt to slow structural change. This strategy, although elegant and effective, poses a new challenge: In a region with many stored representations - which one(s) should we try to localise against given finite computational resources? By learning from our previous use of the experience-map, we can make predictions about which memories we should consider next, conditioned on how the robot is currently localised in the experience-map. During localisation, we prioritise the loading of past experiences in order to minimise the expected computation required. We do this in a probabilistic way and show that this memory policy significantly improves localisation efficiency, enabling long-term autonomy on robots with limited computational resources. We demonstrate and evaluate our system over three challenging datasets, totalling 206km of outdoor travel. We demonstrate the system in a diverse range of lighting and weather conditions, scene clutter, camera occlusions, and permanent structural change in the environment.},
  issn = {1050-4729},
  keywords = {Robots;Visualization;Cameras;Trajectory;Meteorology;Lighting;Navigation},
}

@ARTICLE{park-et-al:2019:2895262,
  author = {C. Park and S. Kim and P. Moghadam and J. Guo and S. Sridharan and C. Fookes},
  journal = {IEEE Robotics and Automation Letters},
  title = {Robust Photogeometric Localization Over Time for Map-Centric Loop Closure},
  volume = {4},
  number = {2},
  pages = {1768--1775},
  doi = {10.1109/LRA.2019.2895262},
  year = {2019},
  month = {4},
  abstract = {Map-centric Simultaneous Localization And Mapping (SLAM) is emerging as an alternative of conventional graph-based SLAM for its accuracy and efficiency in long-term mapping problems. However, in map-centric SLAM, the process of loop closure differs from that of conventional SLAM and the result of incorrect loop closure is more destructive and is not reversible. In this letter, we present a tightly coupled photogeometric metric localization for the loop closure problem in map-centric SLAM. In particular, our method combines complementary constraints from LiDAR and camera sensors, and validates loop closure candidates with sequential observations. The proposed method provides a visual evidence-based outlier rejection where failures caused by either place recognition or localization outliers can be effectively removed. We demonstrate that the proposed method is not only more accurate than the conventional global ICP methods but is also robust to incorrect initial pose guesses.},
  issn = {2377-3766},
  keywords = {Trajectory;Uncertainty;Visualization;Laser radar;Simultaneous localization and mapping;Cameras;Measurement;Loop closure;SLAM;sensor fusion;metric localization;mapping},
}

@INPROCEEDINGS{toft-et-al:2017:83,
  author = {C. Toft and C. Olsson and F. Kahl},
  booktitle = {2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
  title = {Long-Term 3D Localization and Pose from Semantic Labellings},
  volume = {},
  number = {},
  pages = {650--659},
  doi = {10.1109/ICCVW.2017.83},
  year = {2017},
  month = {10},
  abstract = {One of the major challenges in camera pose estimation and 3D localization is identifying features that are approximately invariant across seasons and in different weather and lighting conditions. In this paper, we present a method for performing accurate and robust six degrees-of-freedom camera pose estimation based only on the pixelwise semantic labelling of a single query image. Localization is performed using a sparse 3D model consisting of semantically labelled points and curves, and an error function based on how well these project onto corresponding curves in the query image is developed. The method is evaluated on the recently released Oxford Robotcar dataset, showing that by minimizing this error function, the pose can be recovered with decimeter accuracy in many cases.},
  issn = {2473-9944},
  keywords = {Three-dimensional displays;Semantics;Solid modeling;Roads;Cameras;Labeling;Meteorology},
}

@INPROCEEDINGS{wang-et-al:2019:8683446,
  author = {C. Wang and Y. Yuan and Q. Wang},
  booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title = {Learning by Inertia: Self-supervised Monocular Visual Odometry for Road Vehicles},
  volume = {},
  number = {},
  pages = {2252--2256},
  doi = {10.1109/ICASSP.2019.8683446},
  year = {2019},
  month = {5},
  abstract = {In this paper, we present iDVO (inertia-embedded deep visual odometry), a self-supervised learning based monocular visual odometry (VO) for road vehicles. When modelling the geometric consistency within adjacent frames, most deep VO methods ignore the temporal continuity of the camera pose, which results in a very severe jagged fluctuation in the velocity curves. With the observation that road vehicles tend to perform smooth dynamic characteristics in most of the time, we design the inertia loss function to describe the abnormal motion variation, which assists the model to learn the consecutiveness from long-term camera ego-motion. Based on the recurrent convolutional neural network (RCNN) architecture, our method implicitly models the dynamics of road vehicles and the temporal consecutiveness by the extended Long Short-Term Memory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to handle the non-consistency in fast camera motion by blocking the boundary part and which generates more efficiency in the whole non-consistency mask. The proposed method is evaluated on the KITTI dataset, and the results demonstrate state-of-the-art performance with respect to other monocular deep VO and SLAM approaches.},
  issn = {2379-190X},
  keywords = {Inertia;Self-supervised Learning;Visual Odometry;RCNN},
}

@INPROCEEDINGS{yao-et-al:2021:9517646,
  author = {C. Yao and H. Zhu and S. Lv and D. Zhang and Z. Jia},
  booktitle = {2021 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
  title = {Robust Method for Static 3D Point Cloud Map Building using Multi-View Images with Multi-Resolution},
  volume = {},
  number = {},
  pages = {782--787},
  doi = {10.1109/RCAR52367.2021.9517646},
  year = {2021},
  month = {7},
  abstract = {Robots can perform various missions in multiple changing environments. The dynamic objects have significant influence on the long-term autonomy and 3D map construction, because “ghost tracks” inevitably exist due to the continuous error-accumulation of the input data. So it is critical to keep only static subsets and exclude noisy obstacles to mitigate the influence on mapping and navigation. We propose a robust static map building method, which compares the discrepancies between single scan data against the noisy map. This method focuses on the advantages of most dynamic objects of different views with unique attribution and will be easily detected in these views. Accordingly, we present the novel “Multi-View and Multi-Resolution” image-based method with BEV-RV (Bird's Eye View-Range View) modules to discriminate static/dynamic point clouds. Through two stages of iteration with different image window sizes of point level, we first collect more static points of some inevitably wrong judgments and then remove such completely unreliable dynamic points at a later stage. Experimental evaluations are conducted by using the KITTI dataset as ground truth. Qualitative analysis indicates that the proposed method is robust and reliable against state-of-the-art methodsin some dynamic regions.},
  issn = {},
  keywords = {Three-dimensional displays;Navigation;Heuristic algorithms;Buildings;Object detection;Real-time systems;Windows},
}

@INPROCEEDINGS{yu-et-al:2019:8961714,
  author = {C. Yu and Z. Liu and X.-J. Liu and F. Qiao and Y. Wang and F. Xie and Q. Wei and Y. Yang},
  booktitle = {2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  title = {A DenseNet feature-based loop closure method for visual SLAM system},
  volume = {},
  number = {},
  pages = {258--265},
  doi = {10.1109/ROBIO49542.2019.8961714},
  year = {2019},
  month = {12},
  abstract = {Loop closure is a crucial part in SLAM, especially for large and long-term scenes. Utilizing off-the-shelf networks’ features in loop closure becomes a hot spot. However, what kind of network is more suitable in loop closure and how to use their features have not been well-studied. In this paper, DenseNet is introduced in this field according to its own characters. The features of DenseNet preserve both semantic information and structure details and outweigh other popular networks’ features significantly. Based on this, a DenseNet feature-based framework, named Dense-Loop, is proposed to address the loop closure problem. Weighted Vector of Locally Aggregated Descriptor (WVLAD) method is used to encode the local descriptors as the final global descriptor, which could resist geometry structure and viewpoint changes. Furthermore, 4 max-pooling by channel and locality-sensitive hashing (LSH) are adopted to accelerate the search process. Extensive experiments are conducted on public datasets and the results demonstrate Dense-Loop could achieve state-of-the-art performance.},
  issn = {},
  keywords = {Training;Visualization;Simultaneous localization and mapping;Semantics;Lighting;Resists;Feature extraction;Convolutional Neural Network;loop closure;DenseNet;SLAM},
}

@INPROCEEDINGS{yuuto-et-al:2013:6739700,
  author = {C. Yuuto and T. Kanji and A. Masatoshi},
  booktitle = {2013 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  title = {Part-based SLAM for partially changing environments},
  volume = {},
  number = {},
  pages = {1629--1634},
  doi = {10.1109/ROBIO.2013.6739700},
  year = {2013},
  month = {12},
  abstract = {We consider the task of long-term visual SLAM, i.e., simultaneous localization and mapping, in a partially changing environment (SLAM-PCE). The main problem we face is how to obtain discriminative and compact visual landmarks, which are necessary to cope with changes in appearance in an environment and with a large amount of visual information. We address this issue by proposing the use of common object patterns, which are inherent in typical environments (e.g., indoor, street, forests, suburban, etc.), as visual landmarks for a SLAM-PCE task. In our contributions, we describe our approach, “part-based SLAM”, and validate its effectiveness within a standard problem of view image retrieval. The main novelty of this approach lies in that the common landmark objects are extracted in an unsupervised manner via common pattern discovery, and can be used for compact characterization and efficient retrieval of view images. Our method is also innovative in its use of traditional bounding box-based part annotation: an image is represented in a compact form, “bag-of-bounding-boxes (BoBB)” and then, the scene matching can be solved efficiently as a low dimensional problem of matching bounding boxes. The results of challenging experiments show that it is possible to have high retrieval performance with compact image representation with only 16 words per image.},
  issn = {},
  keywords = {Visualization;Simultaneous localization and mapping;Libraries;Computational modeling;Databases;Dictionaries},
}

@INPROCEEDINGS{fourie-et-al:2017:7989749,
  author = {D. Fourie and S. Claassens and S. Pillai and R. Mata and J. Leonard},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {SLAMinDB: Centralized graph databases for mobile robotics},
  volume = {},
  number = {},
  pages = {6331--6337},
  doi = {10.1109/ICRA.2017.7989749},
  year = {2017},
  month = {5},
  abstract = {Robotic systems typically require memory recall mechanisms for a variety of tasks including localization, mapping, planning, visualization etc. We argue for a novel memory recall framework that enables more complex inference schemas by separating the computation from its associated data. In this work we propose a shared, centralized data persistence layer that maintains an ensemble of online, situationally-aware robot states. This is realized through a queryable graph-database with an accompanying key-value store for larger data. In turn, this approach is scalable and enables a multitude of capabilities such as experience-based learning and long-term autonomy. Using multi-modal simultaneous localization and mapping and a few example use-cases, we demonstrate the versatility and extensible nature that centralized persistence and SLAMinDB can provide. In order to support the notion of life-long autonomy, we envision robots to be endowed with such a persistence model, enabling them to revisit previous experiences and improve upon their existing task-specific capabilities.},
  issn = {},
  keywords = {Simultaneous localization and mapping;Computer architecture;Relational databases;Navigation},
}

@INPROCEEDINGS{kozlov-myasnikov:2021:9649028,
  author = {D. Kozlov and V. Myasnikov},
  booktitle = {2021 International Conference on Information Technology and Nanotechnology (ITNT)},
  title = {Development of an Autonomous Robotic System Using the Graph-based SPLAM Algorithm},
  volume = {},
  number = {},
  pages = {1--5},
  doi = {10.1109/ITNT52450.2021.9649028},
  year = {2021},
  month = {9},
  abstract = {For long-term planning, localization and mapping, the robot must constantly update the map by the changing environment and new areas that the robot is exploring. At the same time, this map should not take up too much of the robot’s memory, since the robot’s performance is limited due to the small size of the robot and increased performance requirements. The robot must interact with the map on time, updating its location to build a further route to explore areas that have not been visited. In addition to compiling a map, when solving the problem of exploration rooms, the following steps are also important: forming a plan for bypassing an unknown room, calculating the trajectory, resolving collisions with obstacles, and following the trajectory. In the course of this work, an autonomous robotic system was developed, the task of which is to map previously unknown premises. For this, SPLAM algorithms, algorithms for building map and working with graphs, algorithms for following a trajectory were used.},
  issn = {},
  keywords = {Measurement;Space vehicles;Memory management;Robot vision systems;Production;Real-time systems;Trajectory;SPLAM;SLAM;robot;ROS;RTABMap;Voronoi diagram;bang-bang controller;Jetson;Zed;point cloud;odometry;Dijkstra algorithm},
}

@INPROCEEDINGS{rosen-et-al:2016:7487237,
  author = {D. M. Rosen and J. Mason and J. J. Leonard},
  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Towards lifelong feature-based mapping in semi-static environments},
  volume = {},
  number = {},
  pages = {1063--1070},
  doi = {10.1109/ICRA.2016.7487237},
  year = {2016},
  month = {5},
  abstract = {The feature-based graphical approach to robotic mapping provides a representationally rich and computationally efficient framework for an autonomous agent to learn a model of its environment. However, this formulation does not naturally support long-term autonomy because it lacks a notion of environmental change; in reality, “everything changes and nothing stands still, ” and any mapping and localization system that aims to support truly persistent autonomy must be similarly adaptive. To that end, in this paper we propose a novel feature-based model of environmental evolution over time. Our approach is based upon the development of an expressive probabilistic generative feature persistence model that describes the survival of abstract semi-static environmental features over time. We show that this model admits a recursive Bayesian estimator, the persistence filter, that provides an exact online method for computing, at each moment in time, an explicit Bayesian belief over the persistence of each feature in the environment. By incorporating this feature persistence estimation into current state-of-the-art graphical mapping techniques, we obtain a flexible, computationally efficient, and information-theoretically rigorous framework for lifelong environmental modeling in an ever-changing world.},
  issn = {},
  keywords = {Feature extraction;Detectors;Computational modeling;Adaptation models;Simultaneous localization and mapping;Bayes methods},
}

@INPROCEEDINGS{pastor-moreno-et-al:2015:7152387,
  author = {D. Pastor-Moreno and H.-S. Shin and A. Waldock},
  booktitle = {2015 International Conference on Unmanned Aircraft Systems (ICUAS)},
  title = {Optical flow localisation and appearance mapping (OFLAAM) for long-term navigation},
  volume = {},
  number = {},
  pages = {980--988},
  doi = {10.1109/ICUAS.2015.7152387},
  year = {2015},
  month = {6},
  abstract = {This paper presents a novel method to use optical flow navigation for long term navigation. Unlike standard SLAM approaches for augmented reality, OFLAAM is designed for Micro Air Vehicles (MAV). It uses a optical flow camera pointing downwards, a IMU and a monocular camera pointing frontwards. That configuration avoids the computational expensive mapping and tracking of the 3D features. It only maps these features in a vocabulary list by a localization module to tackle the optical flow drift and the lose of the navigation estimation. That module, based on the well established algorithm DBoW2, will be also used to close the loop and allow long-term navigation in previously visited areas. The combination of high speed optical flow navigation with a low rate localization algorithm allows fully autonomous navigation for MAV, at the same time it reduces the overall computational load. This framework is implemented in ROS (Robot Operating System) and tested attached to a laptop. A representative scenario is used to validate and analyze the performance of the system.},
  issn = {},
  keywords = {Cameras;Optical sensors;Optical imaging;Computers;Vehicles;Adaptive optics;High-speed optical techniques},
}

@ARTICLE{tang-et-al:2020:2976794,
  author = {D. Tang and Q. Fang and L. Shen and T. Hu},
  journal = {IEEE/ASME Transactions on Mechatronics},
  title = {Onboard Detection-Tracking-Localization},
  volume = {25},
  number = {3},
  pages = {1555--1565},
  doi = {10.1109/TMECH.2020.2976794},
  year = {2020},
  month = {6},
  abstract = {This article investigates long-term positioning of moving objects by monocular vision of a miniature fixed-wing unmanned aerial vehicle. It is challenging to perform a real-time onboard vision processing task, due to the strict payload capacity and power budget limitations of microflying vehicles. We propose a parallel onboard architecture that explicitly decouples the long-term positioning task into iteratively operated detection, tracking, and localization. The proposed approach is eventually called onboard detection-tracking-localization, namely oDTL. The detector automatically extracts and identifies the object from image frames captured at in-flight durations. A learning-based network is constructed to improve detection accuracy and robustness against ever-changing outdoor illumination conditions and flying viewpoints. The tracker follows the object within specified region-of-interest from frame to frame with lower computing consumption. To further reduce target-losing rate, a concept of blind zone is proposed and applied, and its boundaries in sequential images are also theoretically inferred. The position estimator maps the flying vehicle pose, the image coordinates, and calibration specifications into real-world positions of the moving target. An extended Kalman filter is developed for rough position estimation, and a smooth module is introduced for the refinement of the position. Three offline comparative experiments and three online experiments have been conducted respectively to testify the real-time capability of our approach. The collected experimental results also demonstrate the feasible accuracy and robustness of the overall solution within the specified flying onboard scenarios.},
  issn = {1941-014X},
  keywords = {Robustness;Real-time systems;Visualization;Lighting;Cameras;Three-dimensional displays;IEEE transactions;Detection;localization;miniature fixed-wing unmanned aerial vehicle (UAV);monocular;onboard vision;parallel architecture;positioning;tracking},
}

@INPROCEEDINGS{wilbers-et-al:2019:00013,
  author = {D. Wilbers and L. Rumberg and C. Stachniss},
  booktitle = {2019 Third IEEE International Conference on Robotic Computing (IRC)},
  title = {Approximating Marginalization with Sparse Global Priors for Sliding Window SLAM-Graphs},
  volume = {},
  number = {},
  pages = {25--31},
  doi = {10.1109/IRC.2019.00013},
  year = {2019},
  month = {2},
  abstract = {Most autonomous vehicles rely on some kind of map for localization or navigation. Outdated maps however are a risk to the performance of any map-based localization system applied in autonomous vehicles. It is necessary to update the used maps to ensure stable and long-term operation. We address the problem of computing landmark updates live in the vehicle, which requires efficient use of the computational resources. In particular, we employ a graph-based sliding window approach for simultaneous localization and incremental map refinement. We propose a novel method that approximates sliding window marginalization without inducing fill-in. Our method maintains the exact same sparsity pattern as without performing marginalization, but simultaneously improves the landmark estimates. The main novelty of this work is the derivation of sparse global priors that approximate dense marginalization. In comparison to state-of-the-art work, our approach utilizes global instead of local linearization points, but still minimizes linearization errors. We first approximate marginalization via Kullback-Leibler divergence and then recalculate the mean to compensate linearization errors. We evaluate our approach on simulated and real data from a prototype vehicle and compare our approach to state-of-the-art sliding window marginalization.},
  issn = {},
  keywords = {Microsoft Windows;Optimization;Autonomous vehicles;Jacobian matrices;Robots;Navigation;Trajectory;SLAM;Sensor Fusion;Incremental Mapping;Localization;Automated Driving},
}

@INPROCEEDINGS{einhorn-gross:2013:6698849,
  author = {E. Einhorn and H.-M. Gross},
  booktitle = {2013 European Conference on Mobile Robots},
  title = {Generic 2D/3D SLAM with NDT maps for lifelong application},
  volume = {},
  number = {},
  pages = {240--247},
  doi = {10.1109/ECMR.2013.6698849},
  year = {2013},
  month = {9},
  abstract = {In this paper, we present a new, generic approach for Simultaneous Localization and Mapping (SLAM). First of all, we propose an abstraction of the underlying sensor data using Normal Distribution Transform (NDT) maps that are suitable for making our approach independent from the used sensor and the dimension of the generated maps. We present some modifications for the original NDT mapping to handle free-space measurements explicitly and to enable its usage in dynamic environments with moving obstacles and persons. In the second part of this paper we describe our graph-based SLAM approach that is designed for lifelong usage. Therefore, the memory and computational complexity is limited by pruning the pose graph in an appropriate way.},
  issn = {},
  keywords = {Simultaneous localization and mapping;Gaussian distribution;Three-dimensional displays;Computer architecture;Microprocessors},
}

@INPROCEEDINGS{stenborg-et-al:2018:8463150,
  author = {E. Stenborg and C. Toft and L. Hammarstrand},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Long-Term Visual Localization Using Semantically Segmented Images},
  volume = {},
  number = {},
  pages = {6484--6490},
  doi = {10.1109/ICRA.2018.8463150},
  year = {2018},
  month = {5},
  abstract = {Robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles. In this paper, we exploit recent advances in semantic segmentation of images, i.e., where each pixel is assigned a label related to the type of object it represents, to attack the problem of long-term visual localization. We show that semantically labeled 3D point maps of the environment, together with semantically segmented images, can be efficiently used for vehicle localization without the need for detailed feature descriptors (SIFT, SURF, etc.), Thus, instead of depending on hand-crafted feature descriptors, we rely on the training of an image segmenter. The resulting map takes up much less storage space compared to a traditional descriptor based map. A particle filter based semantic localization solution is compared to one based on SIFT-features, and even with large seasonal variations over the year we perform on par with the larger and more descriptive SIFT-features, and are able to localize with an error below 1 m most of the time.},
  issn = {2577-087X},
  keywords = {Semantics;Cameras;Roads;Image segmentation;Visualization;Robustness;Feature extraction},
}

@INPROCEEDINGS{stenborg-et-al:2020:00104,
  author = {E. Stenborg and T. Sattler and L. Hammarstrand},
  booktitle = {2020 International Conference on 3D Vision (3DV)},
  title = {Using Image Sequences for Long-Term Visual Localization},
  volume = {},
  number = {},
  pages = {938--948},
  doi = {10.1109/3DV50981.2020.00104},
  year = {2020},
  month = {11},
  abstract = {Estimating the pose of a camera in a known scene, i.e., visual localization, is a core task for applications such as self-driving cars. In many scenarios, image sequences are available and existing work on combining single-image localization with odometry offers to unlock their potential for improving localization performance. Still, the largest part of the literature focuses on single-image localization and ignores the availability of sequence data. The goal of this paper is to demonstrate the potential of image sequences in challenging scenarios, e.g., under day-night or seasonal changes. Combining ideas from the literature, we describe a sequence-based localization pipeline that combines odometry with both a coarse and a fine localization module. Experiments on long-term localization datasets show that combining single-image global localization against a prebuilt map with a visual odometry/SLAM pipeline improves performance to a level where the extended CMU Seasons dataset can be considered solved. We show that SIFT features can perform on par with modern state-of-the-art features in our framework, despite being much weaker and a magnitude faster to compute. Our code is publicly available at github.com/rulllars.},
  issn = {2475-7888},
  keywords = {Location awareness;Visualization;Three-dimensional displays;Cameras;Trajectory;Simultaneous localization and mapping;Pose estimation},
}

@ARTICLE{cao-et-al:2021:2962416,
  author = {F. Cao and F. Yan and S. Wang and Y. Zhuang and W. Wang},
  journal = {IEEE Transactions on Industrial Electronics},
  title = {Season-Invariant and Viewpoint-Tolerant LiDAR Place Recognition in GPS-Denied Environments},
  volume = {68},
  number = {1},
  pages = {563--574},
  doi = {10.1109/TIE.2019.2962416},
  year = {2021},
  month = {1},
  abstract = {Place recognition remains a challenging problem under various perceptual conditions, e.g., all weather, times of day, seasons, and viewpoint shifts. Different from most of the existing place recognition methods using pure vision, this article studies light detection and ranging (LiDAR) based approaches. Point clouds have some benefits for place recognition since they do not suffer from illumination changes. On the other hand, they are dramatically affected by structural changes from different viewpoints or across seasons. In this article, a novel LiDAR-based place recognition system is proposed to achieve long-term robust localization, even under severe seasonal changes and viewpoint shifts. To improve the efficiency, a compact cylindrical image model is designed to convert three-dimensional point clouds to two-dimensional images representing the prominent geometric relationships of scenes. The contexts (buildings, trees, road structures, etc.) of scenes are utilized for efficient place recognition. A sequence-based temporal consistency check is also introduced for postverification. Extensive real experiments on three datasets (Oxford RobotCar [1], NCLT [2], and DUT-AS) show that the proposed system outperforms both state-of-the-art visual and LiDAR-based methods, verifying its robust performance in challenging scenarios.},
  issn = {1557-9948},
  keywords = {Three-dimensional displays;Laser radar;Visualization;Buildings;Lighting;Deep learning;Two dimensional displays;Across season;light detection and ranging (LiDAR) sensors;long-term localization;mobile robots;place recognition},
}

@ARTICLE{cao-et-al:2018:2815956,
  author = {F. Cao and Y. Zhuang and H. Zhang and W. Wang},
  journal = {IEEE Sensors Journal},
  title = {Robust Place Recognition and Loop Closing in Laser-Based SLAM for UGVs in Urban Environments},
  volume = {18},
  number = {10},
  pages = {4242--4252},
  doi = {10.1109/JSEN.2018.2815956},
  year = {2018},
  month = {5},
  abstract = {Robust place recognition plays a key role for the long-term autonomy of unmanned ground vehicles (UGVs) working in indoor or outdoor environments. Although most of the state-of-the-art that approaches for place recognition are vision-based, visual sensors lack adaptability in environments with poor or dynamically changing illumination. In this paper, a 3-D-laser-based place recognition algorithm is proposed to accomplish loop closure detection for simultaneous localization and mapping. An image model named bearing angle (BA) is adopted to convert 3-D laser points to 2-D images, and then ORB features extracted from BA images are utilized to perform scene matching. Since the computational cost for matching a query BA image with all the BA images in a database is too high to meet the requirement of performing real-time place recognition, a visual bag of words approach is used to improve search efficiency. Furthermore, a speed normalization algorithm and a 3-D geometry-based verification algorithm are proposed to complete the proposed place recognition algorithm. Experiments were conducted on two self-developed UGV platforms to verify the performance of the proposed method.},
  issn = {1558-1748},
  keywords = {Three-dimensional displays;Lasers;Visualization;Feature extraction;Sensors;Robustness;Lighting;Laser scanning;place recognition;simultaneous localization and mapping (SLAM);unmanned ground vehicles (UGVs)},
}

@INPROCEEDINGS{dayoub-duckett:2008:4650701,
  author = {F. Dayoub and T. Duckett},
  booktitle = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title = {An adaptive appearance-based map for long-term topological localization of mobile robots},
  volume = {},
  number = {},
  pages = {3364--3369},
  doi = {10.1109/IROS.2008.4650701},
  year = {2008},
  month = {9},
  abstract = {This work considers a mobile service robot which uses an appearance-based representation of its workplace as a map, where the current view and the map are used to estimate the current position in the environment. Due to the nature of real-world environments such as houses and offices, where the appearance keeps changing, the internal representation may become out of date after some time. To solve this problem the robot needs to be able to adapt its internal representation continually to the changes in the environment. This paper presents a method for creating an adaptive map for long-term appearance-based localization of a mobile robot using long-term and short-term memory concepts, with omni-directional vision as the external sensor.},
  issn = {2153-0866},
  keywords = {Feature extraction;Robots;Robot sensing systems;Noise;Approximation algorithms;Robot vision systems;Cameras},
}

@ARTICLE{han-et-al:2018:2856274,
  author = {F. Han and S. E. Beleidy and H. Wang and C. Ye and H. Zhang},
  journal = {IEEE Robotics and Automation Letters},
  title = {Learning of Holism-Landmark Graph Embedding for Place Recognition in Long-Term Autonomy},
  volume = {3},
  number = {4},
  pages = {3669--3676},
  doi = {10.1109/LRA.2018.2856274},
  year = {2018},
  month = {10},
  abstract = {Place recognition plays an important role to perform loop closure detection of large-scale, long-term simultaneous localization and mapping in loopy environments. The long-term place recognition problem is challenging because the environment appearance exhibits significant long-term variations across various times of the day, months, and seasons. In this letter, we introduce a novel place representation approach that simultaneously integrates semantic landmarks and holistic information to achieve place recognition in long-term autonomy. First, a graph is constructed for each place. The graph nodes encode all landmarks and the holistic image of the place scene recorded in different scenarios. The edges connecting the nodes indicate that these nodes represent the same landmark or place, even though places and landmarks encoded by the nodes may exhibit different appearances in the long-term periods. Then, a graph embedding is learned to preserve the locality in the feature descriptor space, i.e., finding a projection such that the same landmark and place have the identical representation in the new projected descriptor space, no matter in what scenarios they are recorded. We formulate the embedding learning as an optimization problem and implement a new solver that provides a theoretical convergence guarantee. Extensive evaluations are conducted using large-scale benchmark datasets of place recognition in long-term autonomy, which has shown our approach's promising performance.},
  issn = {2377-3766},
  keywords = {Semantics;Simultaneous localization and mapping;Robustness;Image edge detection;Optimization;Convergence;Visual learning;recognition;SLAM;localization},
}

@ARTICLE{han-et-al:2017:2662061,
  author = {F. Han and X. Yang and Y. Deng and M. Rentschler and D. Yang and H. Zhang},
  journal = {IEEE Robotics and Automation Letters},
  title = {SRAL: Shared Representative Appearance Learning for Long-Term Visual Place Recognition},
  volume = {2},
  number = {2},
  pages = {1172--1179},
  doi = {10.1109/LRA.2017.2662061},
  year = {2017},
  month = {4},
  abstract = {Place recognition, or loop closure detection, is an essential component to address the problem of visual simultaneous localization and mapping (SLAM). Long-term navigation of robots in outdoor environments introduces new challenges to enable life-long SLAM, including the strong appearance change resulting from vegetation, weather, and illumination variations across various times of the day, different days, months, or even seasons. In this paper, we propose a new shared representative appearance learning (SRAL) approach to address long-term visual place recognition. Different from previous methods using a single feature modality or a concatenation of multiple features, our SRAL method autonomously learns representative features that are shared in all scene scenarios, and then fuses the features together to represent the long-term appearance of environments observed by a robot during life-long navigation. By formulating SRAL as a regularized optimization problem, we use structured sparsity-inducing norms to model interrelationships of feature modalities. In addition, an optimization algorithm is developed to efficiently solve the formulated optimization problem, which holds a theoretical convergence guarantee. Extensive empirical study was performed to evaluate the SRAL method using large-scale benchmark datasets, including St Lucia, CMU-VL, and Nordland datasets. Experimental results have shown that our SRAL method obtains superior performance for life-long place recognition using individual images, outperforms previous single image-based methods, and is capable of estimating the importance of feature modalities.},
  issn = {2377-3766},
  keywords = {Visualization;Image recognition;Feature extraction;Simultaneous localization and mapping;Optimization;Navigation;Loop closure detection;long-term place recognition;simultaneous localization and mapping (SLAM);visual learning},
}

@INPROCEEDINGS{ott-et-al:2020:00029,
  author = {F. Ott and T. Feigl and C. Löffler and C. Mutschler},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title = {ViPR: Visual-Odometry-aided Pose Regression for 6DoF Camera Localization},
  volume = {},
  number = {},
  pages = {187--198},
  doi = {10.1109/CVPRW50498.2020.00029},
  year = {2020},
  month = {6},
  abstract = {Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with optical flow (OF), research that combines absolute pose regression with OF is limited.We propose ViPR, a novel modular architecture for longterm 6DoF VO that leverages temporal information and synergies between absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) by combining both through recurrent layers. Experiments on known datasets and on our own Industry dataset show that our modular design outperforms state ofthe art in long-term navigation tasks.},
  issn = {2160-7516},
  keywords = {Cameras;Task analysis;Pose estimation;Feature extraction;Navigation;Optical imaging;Sensors},
}

@INPROCEEDINGS{pomerleau-et-al:2014:6907397,
  author = {F. Pomerleau and P. Krüsi and F. Colas and P. Furgale and R. Siegwart},
  booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Long-term 3D map maintenance in dynamic environments},
  volume = {},
  number = {},
  pages = {3712--3719},
  doi = {10.1109/ICRA.2014.6907397},
  year = {2014},
  month = {5},
  abstract = {New applications of mobile robotics in dynamic urban areas require more than the single-session geometric maps that have dominated simultaneous localization and mapping (SLAM) research to date; maps must be updated as the environment changes and include a semantic layer (such as road network information) to aid motion planning in dynamic environments. We present an algorithm for long-term localization and mapping in real time using a three-dimensional (3D) laser scanner. The system infers the static or dynamic state of each 3D point in the environment based on repeated observations. The velocity of each dynamic point is estimated without requiring object models or explicit clustering of the points. At any time, the system is able to produce a most-likely representation of underlying static scene geometry. By storing the time history of velocities, we can infer the dominant motion patterns within the map. The result is an online mapping and localization system specifically designed to enable long-term autonomy within highly dynamic environments. We validate the approach using data collected around the campus of ETH Zurich over seven months and several kilometers of navigation. To the best of our knowledge, this is the first work to unify long-term map update with tracking of dynamic objects.},
  issn = {1050-4729},
  keywords = {Three-dimensional displays;Dynamics;Simultaneous localization and mapping;Heuristic algorithms;Laser modes;Long-term mapping;dynamic obstacles;ICP;kd-tree;registration;scan matching;robot;SLAM},
}

@ARTICLE{xue-et-al:2022:3014100,
  author = {F. Xue and X. Wang and J. Wang and H. Zha},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title = {Deep Visual Odometry With Adaptive Memory},
  volume = {44},
  number = {2},
  pages = {940--954},
  doi = {10.1109/TPAMI.2020.3014100},
  year = {2022},
  month = {2},
  abstract = {We propose a novel deep visual odometry (VO) method that considers global information by selecting memory and refining poses. Existing learning-based methods take the VO task as a pure tracking problem via recovering camera poses from image snippets, leading to severe error accumulation. Global information is crucial for alleviating accumulated errors. However, it is challenging to effectively preserve such information for end-to-end systems. To deal with this challenge, we design an adaptive memory module, which progressively and adaptively saves the information from local to global in a neural analogue of memory, enabling our system to process long-term dependency. Benefiting from global information in the memory, previous results are further refined by an additional refining module. With the guidance of previous outputs, we adopt a spatial-temporal attention to select features for each view based on the co-visibility in feature domain. Specifically, our architecture consisting of Tracking, Remembering and Refining modules works beyond tracking. Experiments on the KITTI and TUM-RGBD datasets demonstrate that our approach outperforms state-of-the-art methods by large margins and produces competitive results against classic approaches in regular scenes. Moreover, our model achieves outstanding performance in challenging scenarios such as texture-less regions and abrupt motions, where classic algorithms tend to fail.},
  issn = {1939-3539},
  keywords = {Cameras;Task analysis;Tracking;Simultaneous localization and mapping;Pose estimation;History;Visual odometry;recurrent neural networks;memory;attention},
}

@ARTICLE{kim-et-al:2019:2897340,
  author = {G. Kim and B. Park and A. Kim},
  journal = {IEEE Robotics and Automation Letters},
  title = {1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using Scan Context Image},
  volume = {4},
  number = {2},
  pages = {1948--1955},
  doi = {10.1109/LRA.2019.2897340},
  year = {2019},
  month = {4},
  abstract = {In this letter, we present a long-term localization method that effectively exploits the structural information of an environment via an image format. The proposed method presents a robust year-round localization performance even when learned in just a single day. The proposed localizer learns a point cloud descriptor, named Scan Context Image (SCI), and performs robot localization on a grid map by formulating the place recognition problem as place classification using a convolutional neural network. Our method is faster than existing methods proposed for place recognition because it avoids a pairwise comparison between a query and scans in a database. In addition, we provide thorough validations using publicly available long-term datasets, the NCLT dataset and the Oxford RobotCar dataset, and show that the Scan Context Image (SCI) localization attains consistent performance over a year and outperforms existing methods.},
  issn = {2377-3766},
  keywords = {Three-dimensional displays;Training;Laser radar;Entropy;Databases;Robot localization;Localization;range sensing;SLAM},
}

@INPROCEEDINGS{kurz-et-al:2021:9636530,
  author = {G. Kurz and M. Holoch and P. Biber},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Geometry-based Graph Pruning for Lifelong SLAM},
  volume = {},
  number = {},
  pages = {3313--3320},
  doi = {10.1109/IROS51168.2021.9636530},
  year = {2021},
  month = {9},
  abstract = {Lifelong SLAM considers long-term operation of a robot where already mapped locations are revisited many times in changing environments. As a result, traditional graph-based SLAM approaches eventually become extremely slow due to the continuous growth of the graph and the loss of sparsity. Both problems can be addressed by a graph pruning algorithm. It carefully removes vertices and edges to keep the graph size reasonable while preserving the information needed to provide good SLAM results. We propose a novel method that considers geometric criteria for choosing the vertices to be pruned. It is efficient, easy to implement, and leads to a graph with evenly spread vertices that remain part of the robot trajectory. Furthermore, we present a novel approach of marginalization that is more robust to wrong loop closures than existing methods. The proposed algorithm is evaluated on two publicly available real-world long-term datasets and compared to the unpruned case as well as ground truth. We show that even on a long dataset (25h), our approach manages to keep the graph sparse and the speed high while still providing good accuracy (40 times speed up, 6cm map error compared to unpruned case).},
  issn = {2153-0866},
  keywords = {Simultaneous localization and mapping;Three-dimensional displays;Costs;Density functional theory;Trajectory;Standards;Intelligent robots},
}

@INPROCEEDINGS{péter-kiss:2020:9263722,
  author = {G. Péter and B. Kiss},
  booktitle = {2020 23rd International Symposium on Measurement and Control in Robotics (ISMCR)},
  title = {Lightweight SLAM with automatic orientation correction using 2D LiDAR scans},
  volume = {},
  number = {},
  pages = {1--6},
  doi = {10.1109/ISMCR51255.2020.9263722},
  year = {2020},
  month = {10},
  abstract = {Simultaneous localization and mapping (SLAM) is about consistent maps in the long run. Loop closing is the most popular way for ensure long-term consistency in presence of multiple measurements by the same or multiple robots. Loop closure can be executed using raw odometrical data, but a more sophisticated, yet still light-weight method is presented in this paper: a landmark descriptor-based relative displacement calculation method for diminishing unwanted orientation errors that otherwise often lead to map inconsistency. Landmark descriptors are created using light detection and ranging (LiDAR) scans and the relation is calculated using scan-matching. The novelty of this research is a method providing long-term orientation and position correction without additional overhead between landmark detections, thus enabling simple agents to do the SLAM in a cooperative way.},
  issn = {},
  keywords = {Manganese;SLAM;LiDAR;mapping;orientation;correction;uncertainty},
}

@INPROCEEDINGS{singh-et-al:2021:9564866,
  author = {G. Singh and M. Wu and S. Lam and D. V. Minh},
  booktitle = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
  title = {Hierarchical Loop Closure Detection for Long-term Visual SLAM with Semantic-Geometric Descriptors},
  volume = {},
  number = {},
  pages = {2909--2916},
  doi = {10.1109/ITSC48978.2021.9564866},
  year = {2021},
  month = {9},
  abstract = {Modern visual Simultaneous Localization and Mapping (SLAM) systems rely on loop closure detection methods for correcting drifts in maps and poses. Existing loop closure detection methods mainly employ conventional feature descriptors to create vocabulary for describing places using bag-of-words (BOW). Such methods do not perform well in long-term SLAM applications as the scene content may change over time due to the presence of dynamic objects, even though the locations are revisited with the same viewpoint. This work enhances the loop closure detection capability of long-term visual SLAM by reducing the number of false matches through the use of location semantics. We extend a semantic visual SLAM framework to build compact global semantic-geometric location descriptors and local semantic vocabulary trees, by leveraging on the already available features and semantics. The local semantic vocabulary trees support incremental vocabulary learning, which is well-suited for long-term SLAM scenarios where the scenes encountered are not known beforehand. A novel hierarchical place recognition method that leverages the global and local location semantics is proposed to enable fast and accurate loop closure detection. The proposed method outperforms recent state-of-the-art methods (i.e., FABMAP2, SeqSLAM, iBOW-LCD, and HTMap) on all datasets considered (i.e., KITTI, Synthia, and CBD), with highest loop closure detection accuracy and lowest query time.},
  issn = {},
  keywords = {Measurement;Location awareness;Vocabulary;Visualization;Simultaneous localization and mapping;Conferences;Semantics},
}

@INPROCEEDINGS{tsamis-et-al:2021:9413161,
  author = {G. Tsamis and I. Kostavelis and D. Giakoumis and D. Tzovaras},
  booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
  title = {Towards life-long mapping of dynamic environments using temporal persistence modeling},
  volume = {},
  number = {},
  pages = {10480--10485},
  doi = {10.1109/ICPR48806.2021.9413161},
  year = {2021},
  month = {1},
  abstract = {The contemporary SLAM mapping systems assume a static environment and build a map that is then used for mobile robot navigation disregarding the dynamic changes in this environment. The paper at hand presents a novel solution for the problem of life-long mapping that continually updates a metric map represented as a 2D occupancy grid in large scale indoor environments with movable objects such as people, robots, objects etc. suitable for industrial applications. We formalize each cell's occupancy as a failure analysis problem and contribute temporal persistence modeling (TPM), an algorithm for probabilistic prediction of the time that a cell in an observed location is expected to be “occupied” or “empty” given sparse prior observations from a task specific mobile robot. Our work is evaluated in Gazebo simulation environment against the nominal occupancy of cells and the estimated obstacles persistence. We also show that robot navigation with life-long mapping demands less replans and leads to more efficient navigation in highly dynamic environments.},
  issn = {1051-4651},
  keywords = {Measurement;Simultaneous localization and mapping;Navigation;Service robots;Predictive models;Probabilistic logic;Prediction algorithms},
}

@INPROCEEDINGS{badino-et-al:2011:5940504,
  author = {H. Badino and D. Huber and T. Kanade},
  booktitle = {2011 IEEE Intelligent Vehicles Symposium (IV)},
  title = {Visual topometric localization},
  volume = {},
  number = {},
  pages = {794--799},
  doi = {10.1109/IVS.2011.5940504},
  year = {2011},
  month = {6},
  abstract = {One of the fundamental requirements of an autonomous vehicle is the ability to determine its location on a map. Frequently, solutions to this localization problem rely on GPS information or use expensive three dimensional (3D) sensors. In this paper, we describe a method for long-term vehicle localization based on visual features alone. Our approach utilizes a combination of topological and metric mapping, which we call topometric localization, to encode the coarse topology of the route as well as detailed metric information required for accurate localization. A topometric map is created by driving the route once and recording a database of visual features. The vehicle then localizes by matching features to this database at runtime. Since individual feature matches are unreliable, we employ a discrete Bayes filter to estimate the most likely vehicle position using evidence from a sequence of images along the route. We illustrate the approach using an 8.8 km route through an urban and suburban environment. The method achieves an average localization error of 2.7 m over this route, with isolated worst case errors on the order of 10 m.},
  issn = {1931-0587},
  keywords = {Vehicles;Visualization;Measurement;Databases;Feature extraction;Global Positioning System;Probability density function},
}

@INPROCEEDINGS{chen-et-al:2021:9421197,
  author = {H. Chen and Z. Wang and Q. Zhu},
  booktitle = {2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)},
  title = {Map Updating Revisited for Navigation Map : A mathematical way to perform map updating for autonomous mobile robot},
  volume = {},
  number = {},
  pages = {505--508},
  doi = {10.1109/IPEC51340.2021.9421197},
  year = {2021},
  month = {4},
  abstract = {Simultaneous localization and mapping, SLAM can product a Map for autonomous robots and self-driving vehicle in navigation. In the actual environment, the scene changes frequently, which makes the old map no long reliable. Therefore, it is necessary to update such a map by an efficient and safely way. In this paper, we review the existing map updating, long-term localization methods and discuss about the challenges in this situation. We present a Map updating method in mathematical way which can update accurately. Our proposed method are tested in five indoor dataset and demonstrated feasibility.},
  issn = {},
  keywords = {Location awareness;Computers;Simultaneous localization and mapping;Navigation;Image processing;Conferences;Reliability;map;updating;visual;point cloud},
}

@ARTICLE{hu-et-al:2022:1003907,
  author = {H. Hu and H. Wang and Z. Liu and W. Chen},
  journal = {IEEE/CAA Journal of Automatica Sinica},
  title = {Domain-Invariant Similarity Activation Map Contrastive Learning for Retrieval-Based Long-Term Visual Localization},
  volume = {9},
  number = {2},
  pages = {313--328},
  doi = {10.1109/JAS.2021.1003907},
  year = {2022},
  month = {2},
  abstract = {Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.},
  issn = {2329-9274},
  keywords = {Location awareness;Visualization;Measurement;Feature extraction;Image recognition;Pipelines;Training;Deep representation learning;place recognition;visual localization},
}

@INPROCEEDINGS{feder-et-al:1998:744447,
  author = {H. J. S. Feder and J. J. Leonard and C. M. Smith},
  booktitle = {Proceedings of the 1998 Workshop on Autonomous Underwater Vehicles (Cat. No.98CH36290)},
  title = {Incorporating environmental measurements in navigation},
  volume = {},
  number = {},
  pages = {115--122},
  doi = {10.1109/AUV.1998.744447},
  year = {1998},
  month = {8},
  abstract = {Extended missions in unknown regions present a significant navigational challenge for autonomous underwater vehicles (AUV). This paper investigates the long-term performance of a concurrent mapping and localization (CML) algorithm for the scenario of an AUV making observations of point features in the environment with a forward look sonar. Simulation results demonstrate that position estimates with long-term bounded errors of a few meters can be achieved under realistic assumptions about the vehicle, its sensors, and the environment. Potential failure modes of the algorithm, such as divergence and map slip, are discussed. CML technology can provide a significant improvement in the navigational capabilities of AUVs and can enable new missions in unmapped regions without reliance on acoustic beacons or surfacing for GPS resets.},
  issn = {},
  keywords = {Sonar navigation;Remotely operated vehicles;Underwater acoustics;Performance analysis;Stochastic processes;Sea measurements;Jacobian matrices;Oceans;Automotive engineering;Marine technology},
}

@INPROCEEDINGS{thomas-et-al:2021:9561701,
  author = {H. Thomas and B. Agro and M. Gridseth and J. Zhang and T. D. Barfoot},
  booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation},
  volume = {},
  number = {},
  pages = {14047--14053},
  doi = {10.1109/ICRA48506.2021.9561701},
  year = {2021},
  month = {5},
  abstract = {We present a self-supervised learning approach for the semantic segmentation of lidar frames. Our method is used to train a deep point cloud segmentation architecture without any human annotation. The annotation process is automated with the combination of simultaneous localization and mapping (SLAM) and ray-tracing algorithms. By performing multiple navigation sessions in the same environment, we are able to identify permanent structures, such as walls, and disentangle short-term and long-term movable objects, such as people and tables, respectively. New sessions can then be performed using a network trained to predict these semantic labels. We demonstrate the ability of our approach to improve itself over time, from one session to the next. With semantically filtered point clouds, our robot can navigate through more complex scenarios, which, when added to the training pool, help to improve our network predictions. We provide insights into our network predictions and show that our approach can also improve the performances of common localization techniques.},
  issn = {2577-087X},
  keywords = {Training;Laser radar;Simultaneous localization and mapping;Annotations;Semantics;Ray tracing;Prediction algorithms},
}

@INPROCEEDINGS{yin-et-al:2020:9303291,
  author = {H. Yin and Y. Wang and L. Tang and R. Xiong},
  booktitle = {2020 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
  title = {Radar-on-Lidar: metric radar localization on prior lidar maps},
  volume = {},
  number = {},
  pages = {1--7},
  doi = {10.1109/RCAR49640.2020.9303291},
  year = {2020},
  month = {9},
  abstract = {Radar and lidar, provided by two different range sensors, each has pros and cons of various perception tasks on mobile robots or autonomous driving. In this paper, a Monte Carlo system is used to localize the robot with a rotating radar sensor on 2D lidar maps. We first train a conditional generative adversarial network to transfer raw radar data to lidar data, and achieve reliable radar points from generator. Then an efficient radar odometry is included in the Monte Carlo system. Combining the initial guess from odometry, a measurement model is proposed to match the radar data and prior lidar maps for final 2D positioning. We demonstrate the effectiveness of the proposed localization framework on the public multisession dataset. The experimental results show that our system can achieve high accuracy for long-term localization in outdoor scenes.},
  issn = {},
  keywords = {Laser radar;Radar;Sensors;Radar imaging;Robots;Three-dimensional displays;Two dimensional displays},
}

@INPROCEEDINGS{gross-boehme:2000:884968,
  author = {H.-M. Gross and H.-J. Boehme},
  booktitle = {Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0},
  title = {PERSES-a vision-based interactive mobile shopping assistant},
  volume = {1},
  number = {},
  pages = {80--85vol.1},
  doi = {10.1109/ICSMC.2000.884968},
  year = {2000},
  month = {10},
  abstract = {The paper describes the general idea, the application scenario, and selected methodological approaches of our long term research project PERSES (PERsonal SErvice System). The aim of the project consists of the development of an interactive mobile shopping assistant that allows a continuous and intuitively understandable interaction with a customer in a home improvement store. Typical tasks we have to tackle are to detect and contact potential users in the operation area, to guide them to desired areas or articles within the store or to follow them as a mobile information kiosk while continuously observing their behavior. Due to the specificity of the interaction-oriented scenario and the characteristics of the operation area, we have focused on vision based methods for both human-robot interaction and robot navigation. Besides some methodological approaches, we present preliminary results of experiments achieved with our mobile robot PERSES in the store with an emphasis on vision based methods for user localization, map building and self-localization.},
  issn = {1062-922X},
  keywords = {Navigation;Robot vision systems;Mobile robots;Robustness;Robot kinematics;Human robot interaction;Adaptation model;Context modeling;Programmable control;Adaptive control},
}

@INPROCEEDINGS{chiu-et-al:2013:6630555,
  author = {H.-P. Chiu and S. Williams and F. Dellaert and S. Samarasekera and R. Kumar},
  booktitle = {2013 IEEE International Conference on Robotics and Automation},
  title = {Robust vision-aided navigation using Sliding-Window Factor graphs},
  volume = {},
  number = {},
  pages = {46--53},
  doi = {10.1109/ICRA.2013.6630555},
  year = {2013},
  month = {5},
  abstract = {This paper proposes a navigation algorithm that provides a low-latency solution while estimating the full nonlinear navigation state. Our approach uses Sliding-Window Factor Graphs, which extend existing incremental smoothing methods to operate on the subset of measurements and states that exist inside a sliding time window. We split the estimation into a fast short-term smoother, a slower but fully global smoother, and a shared map of 3D landmarks. A novel three-stage visual feature model is presented that takes advantage of both smoothers to optimize the 3D landmark map, while minimizing the computation required for processing tracked features in the short-term smoother. This three-stage model is formulated based on the maturity of the estimation of the 3D location of the underlying landmark in the map. Long-range associations are used as global measurements from matured landmarks in the short-term smoother and loop closure constraints in the long-term smoother. Experimental results demonstrate our approach provides highly-accurate solutions on large-scale real data sets using multiple sensors in GPS-denied settings.},
  issn = {1050-4729},
  keywords = {Three-dimensional displays;Navigation;Smoothing methods;Estimation;Sensors;Solid modeling;Current measurement},
}

@INPROCEEDINGS{ostroumov-kuzmenko:2021:9615417,
  author = {I. Ostroumov and N. Kuzmenko},
  booktitle = {2021 IEEE 6th International Conference on Actual Problems of Unmanned Aerial Vehicles Development (APUAVD)},
  title = {Vehicle Navigation by Visual Navigational Aids for Automatic Lunar Mission},
  volume = {},
  number = {},
  pages = {71--75},
  doi = {10.1109/APUAVD53804.2021.9615417},
  year = {2021},
  month = {10},
  abstract = {Nowadays the question of Moon exploration is one of the key priorities. Many Lunar robotics missions are planned in near future by different space agencies around the world. Moon has considered to be the best place for a research station with long-term human presence for finding answers on fundamental questions about the universe. Automatic navigation of starship during a landing phase on Lunar surface is already solved with a help of inertial reference system aided visual algorithms. However, questions of automatic navigation of moving and flying vehicles on the Lunar surface are still open. Inertial navigation is limited by time, self-localization and mapping algorithms require multiple unique features of relief to guarantee required accuracy for successful automatic mission complication. In the current study, we propose the deployment of a network of visual navigational aids on the Lunar surface to support ground automatic missions. A weak atmosphere of the Moon makes effective visual beacons navigation system for long areas. A network of navigational aids includes primary and secondary ground stations which are blinking synchronously. Synchronization is supported by radio waves from the primary ground station. We consider the nature of crater relief to increase operational area of the system. The Time Difference of Arrival method is used to detect vehicle position by blinking network of visual navigational aids. In the numerical application, we consider different scenarios of network configuration to support automatic vehicle navigation inside of Tycho crater. Also, deployment of visual navigational aids network will increase the number of optical features which improve performance of already used positioning methods.},
  issn = {},
  keywords = {Visualization;Time difference of arrival;Surface waves;Moon;Radio navigation;Unmanned aerial vehicles;Synchronization;visual navigational aids;landing and ground vehicles;automatic mission;Lunar mission;Time Difference of Arrival},
}

@INPROCEEDINGS{biswas-veloso:2014:6907435,
  author = {J. Biswas and M. Veloso},
  booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Episodic non-Markov localization: Reasoning about short-term and long-term features},
  volume = {},
  number = {},
  pages = {3969--3974},
  doi = {10.1109/ICRA.2014.6907435},
  year = {2014},
  month = {5},
  abstract = {Markov localization and its variants are widely used for localization of mobile robots. These methods assume Markov independence of observations, implying that observations made by a robot correspond to a static map. However, in real human environments, observations include occlusions due to unmapped objects like chairs and tables, and dynamic objects like humans. We introduce an episodic non-Markov localization algorithm that maintains estimates of the belief over the trajectory of the robot while explicitly reasoning about observations and their correlations arising from unmapped static objects, moving objects, as well as objects from the static map. Observations are classified as arising from long-term features, short-term features, or dynamic features, which correspond to mapped objects, unmapped static objects, and unmapped dynamic objects respectively. By detecting time steps along the robot's trajectory where unmapped observations prior to such time steps are unrelated to those afterwards, non-Markov localization limits the history of observations and pose estimates to “episodes” over which the belief is computed. We demonstrate non-Markov localization in challenging real world indoor and outdoor environments over multiple datasets, comparing it with alternative state-of-the-art approaches, showing it to be robust as well as accurate.},
  issn = {1050-4729},
  keywords = {Markov processes;Cost function;Correlation;Maximum likelihood estimation;Robot kinematics;History},
}

@ARTICLE{cai-et-al:2021:3109413,
  author = {J. Cai and L. Luo and Q. Yu and B. Liu and S. Hu},
  journal = {IEEE Sensors Journal},
  title = {Direct RGB-D Visual Odometry Based on Hybrid Strategy},
  volume = {21},
  number = {20},
  pages = {23278--23288},
  doi = {10.1109/JSEN.2021.3109413},
  year = {2021},
  month = {10},
  abstract = {Edge-based Direct Visual Odometry (E-DVO) plays a crucial role in robot navigation across the low-texture and rich-texture scenes, however, two essential factors are overlooked in the traditional E-DVO methods. (1) Traditional E-DVO methods seriously rely on photometric or geometric cost, thereby generating the non-robust performance under the light changing or structure-less conditions; (2) EDVO methods generally suffer drift issue mainly derived from inaccurate rotation estimation for the long term visual odometry task. In this article, a novel hybrid cost function leveraging the photometric and geometric cost within a bi-direction framework is proposed to facilitate the addressed the former issue. While the latter issue is approached through hybridization of a simple yet effective switching strategy which can guarantee both robustness and accuracy by combining the global Manhattan model and direct edge alignment. We carry out various experiments on TUM RGB-D and ICL-NUIM datasets for performance evaluation. Results show that our method has the advantage of strong robustness and high accuracy compared with state-of-the-art methods, e.g., Canny-VO and ORB-SLAM2.},
  issn = {1558-1748},
  keywords = {Visual odometry;Estimation;Image edge detection;Feature extraction;Sensors;Cost function;Robustness;Visual odometry (VO);RGB-D camera;edge feature;global Manhattan model},
}

@INPROCEEDINGS{chen-et-al:2020:9128437,
  author = {J. Chen and M. Zhu and F. Tufvesson},
  booktitle = {2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring)},
  title = {SLAM using LTE Multipath Component Delays},
  volume = {},
  number = {},
  pages = {1--5},
  doi = {10.1109/VTC2020-Spring48590.2020.9128437},
  year = {2020},
  month = {5},
  abstract = {Cellular radio based localization can be an important complement or alternative to other localization technologies, as base stations continuously transmit signals of opportunity with beneficial positioning properties. In this paper, we use the long term evolution (LTE) cell-specific reference signal for this purpose. The multipath component delays are estimated by the ESPRIT algorithm, and the estimated multipath component delays of different snapshots are associated by global nearest neighbor with a Kalman filter. Rao-Blackwellized particle filter based simultaneous localization and mapping (SLAM) is then applied to estimate the position of user equipment and that of the base station and virtual transmitters. In a measurement campaign, data from one base station was logged, and the analysis based on the data shows that, at the end of the measurement, the SLAM performance is 11 meters better than that with only inertial measurement unit (IMU).},
  issn = {2577-2465},
  keywords = {Delays;Simultaneous localization and mapping;Kalman filters;Receivers;Global navigation satellite system;Long Term Evolution;Base stations;MPC delay;SLAM;positioning;particle filter;LTE;CRS},
}

@ARTICLE{coulin-et-al:2022:3136241,
  author = {J. Coulin and R. Guillemard and V. Gay-Bellile and C. Joly and A. D. L. Fortelle},
  journal = {IEEE Robotics and Automation Letters},
  title = {Tightly-Coupled Magneto-Visual-Inertial Fusion for Long Term Localization in Indoor Environment},
  volume = {7},
  number = {2},
  pages = {952--959},
  doi = {10.1109/LRA.2021.3136241},
  year = {2022},
  month = {4},
  abstract = {We propose in this letter a tightly-coupled fusion of visual, inertial and magnetic data for long-term localization in indoor environment. Unlike state-of-the-art Visual-Inertial SLAM (VISLAM) solutions that reuse visual map to prevent drift, we present in this letter an extension of the Multi-State Constraint Kalman Filter (MSCKF) that takes advantage of a magnetic map. It makes our solution more robust to variations of the environment appearance. The experimental results demonstrate that the localization accuracy of the proposed approach is almost the same over time periods longer than a year.},
  issn = {2377-3766},
  keywords = {Bibliographies;Uniform resource locators;Standards;Databases;Documentation;Codes;Patents;Localization;sensor fusion;visual-inertial SLAM;indoor magnetic field;MSCKF},
}

@INPROCEEDINGS{dong-et-al:2015:7140012,
  author = {J. Dong and E. Nelson and V. Indelman and N. Michael and F. Dellaert},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Distributed real-time cooperative localization and mapping using an uncertainty-aware expectation maximization approach},
  volume = {},
  number = {},
  pages = {5807--5814},
  doi = {10.1109/ICRA.2015.7140012},
  year = {2015},
  month = {5},
  abstract = {We demonstrate distributed, online, and real-time cooperative localization and mapping between multiple robots operating throughout an unknown environment using indirect measurements. We present a novel Expectation Maximization (EM) based approach to efficiently identify inlier multi-robot loop closures by incorporating robot pose uncertainty, which significantly improves the trajectory accuracy over long-term navigation. An EM and hypothesis based method is used to determine a common reference frame. We detail a 2D laser scan correspondence method to form robust correspondences between laser scans shared amongst robots. The implementation is experimentally validated using teams of aerial vehicles, and analyzed to determine its accuracy, computational efficiency, scalability to many robots, and robustness to varying environments. We demonstrate through multiple experiments that our method can efficiently build maps of large indoor and outdoor environments in a distributed, online, and real-time setting.},
  issn = {1050-4729},
  keywords = {Trajectory;Robot kinematics;Robot sensing systems;Lasers;Robustness;Uncertainty},
}

@INPROCEEDINGS{leonard-durrant-whyte:1991:174711,
  author = {J. J. Leonard and H. F. Durrant-Whyte},
  booktitle = {Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91},
  title = {Simultaneous map building and localization for an autonomous mobile robot},
  volume = {},
  number = {},
  pages = {1442--1447vol.3},
  doi = {10.1109/IROS.1991.174711},
  year = {1991},
  month = {11},
  abstract = {Discusses a significant open problem in mobile robotics: simultaneous map building and localization, which the authors define as long-term globally referenced position estimation without a priori information. This problem is difficult because of the following paradox: to move precisely, a mobile robot must have an accurate environment map; however, to build an accurate map, the mobile robot's sensing locations must be known precisely. In this way, simultaneous map building and localization can be seen to present a question of 'which came first, the chicken or the egg?' (The map or the motion?) When using ultrasonic sensing, to overcome this issue the authors equip the vehicle with multiple servo-mounted sonar sensors, to provide a means in which a subset of environment features can be precisely learned from the robot's initial location and subsequently tracked to provide precise positioning.<>},
  issn = {},
  keywords = {Mobile robots;Vehicles;Sonar navigation;Robot sensing systems;Stochastic resonance;Sensor phenomena and characterization;Target tracking;Testing;National electric code;Humans},
}

@INPROCEEDINGS{jang-kim:2019:8867531,
  author = {J. Jang and J. Kim},
  booktitle = {OCEANS 2019 - Marseille},
  title = {Weighted Grid Partitioning for Panel-Based Bathymetric SLAM},
  volume = {},
  number = {},
  pages = {1--6},
  doi = {10.1109/OCEANSE.2019.8867531},
  year = {2019},
  month = {6},
  abstract = {Bathymetric navigation enables the long-term operation of autonomous underwater vehicles by reducing navigation drift errors with no need for GPS position fixes. In the case that a bathymetric map is not available, the simultaneous localization and mapping (SLAM) algorithm is required, but this increases computational complexity and memory requirement. Panel-based bathymetric SLAM could considerably reduce the computational burden. However, it may suffers from incorrect update when the vehicle does not belong to the updated panel. This study proposes a new update method, called weighted grid partitioning, which considers the probability distribution of a vehicle's location, and is more effective in terms of the map accuracy, computational burden, and memory usage compared to standard update methods. The feasibility of the proposed algorithm is verified through simulations.},
  issn = {},
  keywords = {Simultaneous localization and mapping;Navigation;Probability distribution;Signal processing algorithms;Measurement uncertainty;Uncertainty;Predictive models},
}

@ARTICLE{liu-meng:2020:3014648,
  author = {J. Liu and Z. Meng},
  journal = {IEEE Robotics and Automation Letters},
  title = {Visual SLAM With Drift-Free Rotation Estimation in Manhattan World},
  volume = {5},
  number = {4},
  pages = {6512--6519},
  doi = {10.1109/LRA.2020.3014648},
  year = {2020},
  month = {10},
  abstract = {This letter presents an efficient and accurate simultaneous localization and mapping (SLAM) system in man-made environments. The Manhattan world assumption is imposed, with which the global orientation is obtained. The drift-free rotational motion estimation is derived from the structural regularities using line features. In particular, a two-stage vanishing points (VPs) estimation method is developed, which consists of a short-term tracking module to track the clustered line features and a long-term searching module to generate abundant sets of VPs candidates and retrieve the optimal one. A least square problem is constructed and solved to provide refined VPs with the clusters of structural line features every frame. We make full use of the absolute orientation estimation to benefit the whole SLAM process. In particular, we utilize the absolute orientation estimation to increase the localization accuracy in the front end, and formulate a linear batch camera pose refinement problem with the known rotations to improve the real time performance in the back end. Experiments on both synthesized and real-world scenes reveal results with high-precision in the real time camera pose estimation process and high-speed in pose graph optimization process compared with the existing state-of-the-art methods.},
  issn = {2377-3766},
  keywords = {Cameras;Estimation;Simultaneous localization and mapping;Optimization;Feature extraction;Three-dimensional displays;Tracking;SLAM;localization;visual-based navigation},
}

@ARTICLE{aitken-et-al:2021:3115981,
  author = {J. M. Aitken and M. H. Evans and R. Worley and S. Edwards and R. Zhang and T. Dodd and L. Mihaylova and S. R. Anderson},
  journal = {IEEE Access},
  title = {Simultaneous Localization and Mapping for Inspection Robots in Water and Sewer Pipe Networks: A Review},
  volume = {9},
  number = {},
  pages = {140173--140198},
  doi = {10.1109/ACCESS.2021.3115981},
  year = {2021},
  month = {},
  abstract = {At the present time, water and sewer pipe networks are predominantly inspected manually. In the near future, smart cities will perform intelligent autonomous monitoring of buried pipe networks, using teams of small robots. These robots, equipped with all necessary computational facilities and sensors (optical, acoustic, inertial, thermal, pressure and others) will be able to inspect pipes whilst navigating, self-localising and communicating information about the pipe condition and faults such as leaks or blockages to human operators for monitoring and decision support. The predominantly manual inspection of pipe networks will be replaced with teams of autonomous inspection robots that can operate for long periods of time over a large spatial scale. Reliable autonomous navigation and reporting of faults at this scale requires effective localization and mapping, which is the estimation of the robot’s position and its surrounding environment. This survey presents an overview of state-of-the-art works on robot simultaneous localization and mapping (SLAM) with a focus on water and sewer pipe networks. It considers various aspects of the SLAM problem in pipes, from the motivation, to the water industry requirements, modern SLAM methods, map-types and sensors suited to pipes. Future challenges such as robustness for long term robot operation in pipes are discussed, including how making use of prior knowledge, e.g. geographic information systems (GIS) can be used to build map estimates, and improve multi-robot SLAM in the pipe environment.},
  issn = {2169-3536},
  keywords = {Simultaneous localization and mapping;Robots;Sensors;Inspection;Service robots;Water resources;Location awareness;Water;sewer;network;pipe networks;robots;SLAM;data fusion;Bayesian estimation;visual odometry;laser and lidar scanning},
}

@INPROCEEDINGS{mccormac-et-al:2017:7989538,
  author = {J. McCormac and A. Handa and A. Davison and S. Leutenegger},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {SemanticFusion: Dense 3D semantic mapping with convolutional neural networks},
  volume = {},
  number = {},
  pages = {4628--4635},
  doi = {10.1109/ICRA.2017.7989538},
  year = {2017},
  month = {5},
  abstract = {Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need to extend beyond geometry and appearance - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state-of-the-art dense Simultaneous Localization and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondences between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of ≈25Hz.},
  issn = {},
  keywords = {Semantics;Simultaneous localization and mapping;Three-dimensional displays;Geometry;Two dimensional displays;Labeling;Cameras},
}

@INPROCEEDINGS{peltomäki-et-al:2021:9636320,
  author = {J. Peltomäki and F. Alijani and J. Puura and H. Huttunen and E. Rahtu and J.-K. Kämäräinen},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Evaluation of Long-term LiDAR Place Recognition},
  volume = {},
  number = {},
  pages = {4487--4492},
  doi = {10.1109/IROS51168.2021.9636320},
  year = {2021},
  month = {9},
  abstract = {We compare a state-of-the-art deep image retrieval and a deep place recognition method for place recognition using LiDAR data. Place recognition aims to detect previously visited locations and thus provides an important tool for navigation, mapping, and localisation. Experimental comparisons are conducted using challenging outdoor and indoor datasets, Oxford Radar RobotCar and COLD, in the "long-term" setting where the test conditions differ substantially from the training and gallery data. Based on our results the image retrieval methods using LiDAR depth images can achieve accurate localization (the single best match recall 80%) within 5.00 m in urban outdoors. In office indoors the comparable accuracy is 50 cm but is more sensitive to changes in the environment.},
  issn = {2153-0866},
  keywords = {Training;Meters;Location awareness;Laser radar;Image recognition;Image retrieval;Radar imaging},
}

@INPROCEEDINGS{pérez-et-al:2014:6849767,
  author = {J. Pérez and F. Caballero and L. Merino},
  booktitle = {2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  title = {Integration of Monte Carlo Localization and place recognition for reliable long-term robot localization},
  volume = {},
  number = {},
  pages = {85--91},
  doi = {10.1109/ICARSC.2014.6849767},
  year = {2014},
  month = {5},
  abstract = {This paper proposes extending Monte Carlo Localization methods with visual information in order to build a long term robot localization system. This system is aimed to work in crowded and non-planar scenarios, where 2D laser rangefinders may not always be enough to match the robot position with the map. Thus, visual place recognition will be used in order to obtain robot position clues that can be used to detect when the robot is lost and also to reset its positions to the right one. The paper presents experimental results based on datasets gathered with a real robot in challenging scenarios.},
  issn = {},
  keywords = {Robot sensing systems;Semiconductor lasers;Robot kinematics;Trajectory;Navigation},
}

@INPROCEEDINGS{berrio-et-al:2019:8814189,
  author = {J. S. Berrio and J. Ward and S. Worrall and E. Nebot},
  booktitle = {2019 IEEE Intelligent Vehicles Symposium (IV)},
  title = {Updating the visibility of a feature-based map for long-term maintenance},
  volume = {},
  number = {},
  pages = {1173--1179},
  doi = {10.1109/IVS.2019.8814189},
  year = {2019},
  month = {6},
  abstract = {Mobile vehicles operating in urban navigation applications can achieve high integrity localisation with high accuracy by using maps of the surroundings. To accomplish this, the map should always have an accurate representation of the environment. Thus, it is necessary to detect and remove the map components that no longer exist in the current environment. This maintains the map compactness and dependability while simplifying the data association problem. This paper addresses the problem of deletion of transient map components by taking advantage of the geometric connection between the map and agent poses in order to establish and update the visibility of each feature. Once the map is created an initial visibility vector is associated with every map element and updated over time. The visibility of a map element which no longer exists is reduced and ultimately removed from the map. We demonstrate our approach in a 2D feature-based map composed of poles and corners extracted from information provided by a Iidar sensor. The experimental results show the map update using a seven-month data set collected in the University of Sydney campus.},
  issn = {2642-7214},
  keywords = {},
}

@ARTICLE{berrio-et-al:2021:3094485,
  author = {J. S. Berrio and S. Worrall and M. Shan and E. Nebot},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  title = {Long-Term Map Maintenance Pipeline for Autonomous Vehicles},
  volume = {},
  number = {},
  pages = {1--14},
  doi = {10.1109/TITS.2021.3094485},
  year = {2021},
  month = {},
  abstract = {For autonomous vehicles to operate persistently in a typical urban environment, it is essential to have high accuracy position information. This requires a mapping and localisation system that can adapt to changes over time. A localisation approach based on a single-survey map will not be suitable for long-term operation as it does not incorporate variations in the environment. In this paper, we present new algorithms to maintain a featured-based map. A map maintenance pipeline is proposed that can continuously update a map with the most relevant features taking advantage of the changes in the surroundings. Our pipeline detects and removes transient features based on their geometrical relationships with the vehicle's pose. Newly identified features became part of a new feature map and are assessed by the pipeline as candidates for the localisation map. By purging out-of-date features and adding newly detected features, we continually update the prior map to more accurately represent the most recent environment. We have validated our approach using the USyd Campus Dataset, which includes more than 18 months of data. The results presented demonstrate that our maintenance pipeline produces a resilient map which can provide sustained localisation performance over time.},
  issn = {1558-0016},
  keywords = {Feature extraction;Pipelines;Maintenance engineering;Transient analysis;Visualization;Autonomous vehicles;Task analysis;Long-term localisation;feature-based map;map update.},
}

@INPROCEEDINGS{werfel-et-al:2008:15,
  author = {J. Werfel and Y. Bar-Yam and D. Ingber},
  booktitle = {2008 Second IEEE International Conference on Self-Adaptive and Self-Organizing Systems Workshops},
  title = {Bioinspired Environmental Coordination in Spatial Computing Systems},
  volume = {},
  number = {},
  pages = {338--343},
  doi = {10.1109/SASOW.2008.15},
  year = {2008},
  month = {10},
  abstract = {Spatial computing systems are characterized by the extended physical environment in which they exist and function. Often this environment can be manipulated in various ways by the computing agents. We argue that it is important to consider the potential use of the environment for coordination and indirect communication in such systems. For inherently spatial problems, it can be more effective to store spatially relevant information in the environment rather than in the computing devices, as in the case of mobile agents or long-term physical structures. In scientific settings, considering the role of the environment can illuminate mechanisms or processes that might otherwise be overlooked; in engineering problems, it can provide simpler and more effective solutions than could be achieved by relying on the computing devices alone. We give as examples problems related to foraging, collective construction, simultaneous localization and mapping, object tracking, and behaviors of living tissues. We suggest in closing a classification scheme for capabilities of environmental elements, relevant to the design of physically embodied spatial computing systems.},
  issn = {},
  keywords = {Physics computing;USA Councils;Biology computing;Chemical sensors;Biomedical engineering;Engineering in medicine and biology;Simultaneous localization and mapping;Distributed computing;Mobile robots;Computer networks;foraging;collective construction;SLAM;object tracking;tissues;stormones},
}

@INPROCEEDINGS{devaux-et-al:2011:6088615,
  author = {J.-C. Devaux and P. Nadrag and E. Colle and P. Hoppenot},
  booktitle = {2011 15th International Conference on Advanced Robotics (ICAR)},
  title = {High level assisted control mode based on SLAM for a remotely controlled robot},
  volume = {},
  number = {},
  pages = {186--191},
  doi = {10.1109/ICAR.2011.6088615},
  year = {2011},
  month = {6},
  abstract = {One aim of ambient assistive technologies is to reduce long term hospitalization for elderly people, especially with pathologies such as Mild Cognitive Impairment (MCI). The smart environment assists these people and their families with safety and cognitive stimulation, so they stay as long as possible at home. The originality comes from using the robot in the elderly person's home. This robot is remote controlled by a distant user, a therapist or a relative, for determining alarming situations or for participating in stimulation exercises. Several modes are available for controlling the robot. This paper deals with an assisted control mode in which the remote user gives to the robot one goal and the robot reaches the goal by itself. During the robot movement, the user can dynamically change the current goal. An important hypothesis is that the robot has no a priori knowledge of its environment at the beginning. The knowledge will increase with time and the planned trajectory will be refreshed at two levels: a local one - faster but not always sufficient - and a global one - slower but which always finds a path if one exists. The idea is to work only with local information, using the robot sensors, the operator keeping the high level control. To assure that control, the remote operator uses video feedback and information from a laser range scanner.},
  issn = {},
  keywords = {Trajectory;Planning;Simultaneous localization and mapping;Navigation},
}

@INPROCEEDINGS{yang-et-al:2011:5986295,
  author = {J.-F. Yang and K. Wang and M.-A. Li and L. Liu},
  booktitle = {2011 IEEE International Conference on Mechatronics and Automation},
  title = {Research on object recognition using bag of word model for mobile robot navigation},
  volume = {},
  number = {},
  pages = {1735--1740},
  doi = {10.1109/ICMA.2011.5986295},
  year = {2011},
  month = {8},
  abstract = {Robust long term positioning for autonomous mobile robots is essential for many applications. Key to a successful visual SLAM system is correctly recognizing the objects and labeling where the robot is. Local image features are popular with constructing object recognition system, which are invariant to image scaling, translation, rotation, and partially invariant to illumination changes and affine. In this paper, we proposed an object recognition method based on the bag of word model, mainly idea includes three steps as follows: firstly, a set of local image patches are sampled using a key point detector, and each patch is a descriptor based on scale invariant feature transform. Then outliers are removed by RANSAC algorithm, and the resulting distribution of descriptors is quantified by using vector quantization against a pre-specified codebook to convert it to a histogram of votes for codebook centers. Finally, a KNN algorithm is used to classify images through the resulting global descriptor vector. The experimental results show that our proposed method has a better performance against the previous methods.},
  issn = {2152-744X},
  keywords = {Feature extraction;Object recognition;Computational modeling;Training;Databases;Testing;Visualization;scale invariant feature transform (SIFT);bag of word (BOW);object recognition;robot navigation},
}

@INPROCEEDINGS{pauls-et-al:2020:9341003,
  author = {J.-H. Pauls and K. Petek and F. Poggenhans and C. Stiller},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform},
  volume = {},
  number = {},
  pages = {4595--4601},
  doi = {10.1109/IROS45743.2020.9341003},
  year = {2020},
  month = {10},
  abstract = {Easy, yet robust long-term localization is still an open topic in research. Existing approaches require either dense maps, expensive sensors, specialized map features or proprietary detectors.We propose using semantic segmentation on a monocular camera to localize directly in a HD map as used for automated driving. This combines lightweight, yet powerful HD maps with the simplicity of monocular vision and the flexibility of neural networks.The major challenges arising from this combination are data association and robustness against misdetections. Association is solved efficiently by applying distance transform on binary per-class images. This provides not only a fast lookup table for a smooth gradient as needed for pose-graph optimization, but also dynamic association by default.A sliding-window pose graph optimization combines single image detections with vehicle odometry, smoothing results and helping overcome even misclassifications in consecutive frames.Evaluation against a highly accurate 6D visual localization shows that our approach can achieve accuracy levels as required for automated driving, being one of the most lightweight and flexible methods to do so.},
  issn = {2153-0866},
  keywords = {Location awareness;Semantics;Neural networks;Transforms;Cameras;Vehicle dynamics;Optimization},
}

@INPROCEEDINGS{tsintotas-et-al:2021:9651458,
  author = {K. A. Tsintotas and L. Bampis and S. An and G. F. Fragulis and S. G. Mouroutsos and A. Gasteratos},
  booktitle = {2021 IEEE International Conference on Imaging Systems and Techniques (IST)},
  title = {Sequence-based mapping for probabilistic visual loop-closure detection},
  volume = {},
  number = {},
  pages = {1--6},
  doi = {10.1109/IST50367.2021.9651458},
  year = {2021},
  month = {8},
  abstract = {During simultaneous localization and mapping, the robot should build a map of its surroundings and simultaneously estimate its pose in the generated map. However, a fundamental task is to detect loops, i.e., previously visited areas, allowing consistent map generation. Moreover, within long-term mapping, every autonomous system needs to address its scalability in terms of storage requirements and database search. In this paper, we present a low-complexity sequence-based visual loop-closure detection pipeline. Our system dynamically segments the traversed route through a feature matching technique in order to define sub-maps. In addition, visual words are generated incrementally for the corresponding sub-maps representation. Comparisons among these sequences-of-images are performed thanks to probabilistic scores originated from a voting scheme. When a candidate sub-map is indicated, global descriptors are utilized for image-to-image pairing. Our evaluation took place on several publicly-available datasets exhibiting the system&#x2019;s low complexity and high recall compared to other state-of-the-art approaches.},
  issn = {1558-2809},
  keywords = {Visualization;Vocabulary;Simultaneous localization and mapping;Databases;Pipelines;Probabilistic logic;Feature extraction},
}

@INPROCEEDINGS{tanaka:2018:00173,
  author = {K. Tanaka},
  booktitle = {2018 Joint 10th International Conference on Soft Computing and Intelligent Systems (SCIS) and 19th International Symposium on Advanced Intelligent Systems (ISIS)},
  title = {Simultaneous Localization and Change Detection for Long-Term Map Learning: A Scalable Scene Retrieval Approach},
  volume = {},
  number = {},
  pages = {1039--1045},
  doi = {10.1109/SCIS-ISIS.2018.00173},
  year = {2018},
  month = {12},
  abstract = {This paper addresses the problem of change detection from a novel perspective of long-term map learning. We are particularly interested in designing an approach that can scale to large maps and that can function under global uncertainty in the viewpoint (i.e., GPS-denied situations). Our approach, which utilizes a compact bag-of-words (BoW) scene model, makes several contributions to the problem: (1) Two kinds of prior information are extracted from the view sequence map and used for change detection. Further, we propose a novel type of prior, called motion prior, to predict the relative motions of stationary objects and anomaly ego-motion detection. The proposed prior is also useful for distinguishing stationary from non-stationary objects. (2) A small set of good reference images (e.g., 10) are efficiently retrieved from the view sequence map by employing the recently developed Bag-of-Local-Convolutional-Features (BoLCF) scene model. (3) Change detection is reformulated as a scene retrieval over these reference images to find changed objects using a novel spatial Bag-of-Words (SBoW) scene model. Evaluations conducted of individual techniques and also their combinations on a challenging dataset of highly dynamic scenes in the publicly available Malaga dataset verify their efficacy.},
  issn = {},
  keywords = {Task analysis;Uncertainty;Robots;Three-dimensional displays;Feature extraction;Solid modeling;Visualization;mobile robot;change detection;global localization},
}

@INPROCEEDINGS{wang-et-al:2019:8793499,
  author = {K. Wang and Y. Lin and L. Wang and L. Han and M. Hua and X. Wang and S. Lian and B. Huang},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  title = {A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation},
  volume = {},
  number = {},
  pages = {5224--5230},
  doi = {10.1109/ICRA.2019.8793499},
  year = {2019},
  month = {5},
  abstract = {This paper presents a novel framework for simultaneously implementing localization and segmentation, which are two of the most important vision-based tasks for robotics. While the goals and techniques used for them were considered to be different previously, we show that by making use of the intermediate results of the two modules, their performance can be enhanced at the same time. Our framework is able to handle both the instantaneous motion and long-term changes of instances in localization with the help of the segmentation result, which also benefits from the refined 3D pose information. We conduct experiments on various datasets, and prove that our framework works effectively on improving the precision and robustness of the two tasks and outperforms existing localization and segmentation algorithms.},
  issn = {2577-087X},
  keywords = {Image segmentation;Task analysis;Robot sensing systems;Motion segmentation;Feature extraction;Three-dimensional displays},
}

@ARTICLE{clement-et-al:2020:2967659,
  author = {L. Clement and M. Gridseth and J. Tomasi and J. Kelly},
  journal = {IEEE Robotics and Automation Letters},
  title = {Learning Matchable Image Transformations for Long-Term Metric Visual Localization},
  volume = {5},
  number = {2},
  pages = {1492--1499},
  doi = {10.1109/LRA.2020.2967659},
  year = {2020},
  month = {4},
  abstract = {Long-term metric self-localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems due to appearance changes caused by lighting, weather, or seasonal variations. While experience-based mapping has proven to be an effective technique for bridging the `appearance gap,' the number of experiences required for reliable metric localization over days or months can be very large, and methods for reducing the necessary number of experiences are needed for this approach to scale. Taking inspiration from color constancy theory, we learn a nonlinear RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature matches for images captured under different lighting and weather conditions, and use it as a pre-processing step in a conventional single-experience localization pipeline to improve its robustness to appearance change. We train this mapping by approximating the target non-differentiable localization pipeline with a deep neural network, and find that incorporating a learned low-dimensional context feature can further improve cross-appearance feature matching. Using synthetic and real-world datasets, we demonstrate substantial improvements in localization performance across day-night cycles, enabling continuous metric localization over a 30-hour period using a single mapping experience, and allowing experience-based localization to scale to long deployments with dramatically reduced data requirements.},
  issn = {2377-3766},
  keywords = {Pipelines;Feature extraction;Visualization;Training;Measurement;Lighting;Robustness;Deep learning in robotics and automation;visual learning;visual-based navigation;localization},
}

@INPROCEEDINGS{halodová-et-al:2019:8967994,
  author = {L. Halodová and E. Dvořráková and F. Majer and T. Vintr and O. M. Mozos and F. Dayoub and T. Krajník},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Predictive and adaptive maps for long-term visual navigation in changing environments},
  volume = {},
  number = {},
  pages = {7033--7039},
  doi = {10.1109/IROS40897.2019.8967994},
  year = {2019},
  month = {11},
  abstract = {In this paper, we compare different map management techniques for long-term visual navigation in changing environments. In this scenario, the navigation system needs to continuously update and refine its feature map in order to adapt to the environment appearance change. To achieve reliable long-term navigation, the map management techniques have to (i) select features useful for the current navigation task, (ii) remove features that are obsolete, (iii) and add new features from the current camera view to the map. We propose several map management strategies and evaluate their performance with regard to the robot localisation accuracy in long-term teach-and-repeat navigation. Our experiments, performed over three months, indicate that strategies which model cyclic changes of the environment appearance and predict which features are going to be visible at a particular time and location, outperform strategies which do not explicitly model the temporal evolution of the changes.},
  issn = {2153-0866},
  keywords = {},
}

@INPROCEEDINGS{pan-et-al:2021:9561547,
  author = {L. Pan and K. Ji and J. Zhao},
  booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Tightly-Coupled Multi-Sensor Fusion for Localization with LiDAR Feature Maps},
  volume = {},
  number = {},
  pages = {5215--5221},
  doi = {10.1109/ICRA48506.2021.9561547},
  year = {2021},
  month = {5},
  abstract = {Robust and accurate pose estimation in long-term localization is crucial to autonomous driving. In this paper, we dealt with absolute localization with a LiDAR feature map and multi-sensor measurements. We proposed a tightly-coupled fusion method with fixed-lag smoothing. A sliding window of recently maintained states is estimated by minimizing a joint cost function. This cost function includes residuals of global LiDAR registration and relative kinematic constraints from an IMU and wheel encoders. In addition, we enhance the robustness of our method by improving LiDAR registration. To achieve this goal, LiDAR feature maps with a hybrid of geometric and normal distribution features are constructed and exploited. The effectiveness of the proposed method is verified in several challenging test sequences over 200km. The experimental results demonstrate that the proposed method achieves accurate localization and high robustness in challenging scenarios even when the LiDAR observation is degraded.},
  issn = {2577-087X},
  keywords = {Location awareness;Laser radar;Smoothing methods;Pose estimation;Wheels;Kinematics;Gaussian distribution},
}

@INPROCEEDINGS{platinsky-et-al:2020:00081,
  author = {L. Platinsky and M. Szabados and F. Hlasek and R. Hemsley and L. D. Pero and A. Pancik and B. Baum and H. Grimmett and P. Ondruska},
  booktitle = {2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  title = {Collaborative Augmented Reality on Smartphones via Life-long City-scale Maps},
  volume = {},
  number = {},
  pages = {533--541},
  doi = {10.1109/ISMAR50242.2020.00081},
  year = {2020},
  month = {11},
  abstract = {In this paper we present the first published end-to-end production computer-vision system for powering city-scale shared augmented reality experiences on mobile devices. In doing so we propose a new formulation for an experience-based mapping framework as an effective solution to the key issues of city-scale SLAM scalability, robustness, map updates and all-time all-weather performance required by a production system. Furthermore, we propose an effective way of synchronising SLAM systems to deliver seamless real-time localisation of multiple edge devices at the same time. All this in the presence of network latency and bandwidth limitations. The resulting system is deployed and tested at scale in San Francisco where it delivers AR experiences in a mapped area of several hundred kilometers. To foster further development of this area we offer the data set to the public, constituting the largest of this kind to date.},
  issn = {1554-7868},
  keywords = {Production systems;Simultaneous localization and mapping;Scalability;Collaboration;Robustness;Augmented reality;Smart phones;Computer vision;Augmented reality;Structure from motion;Large-scale SLAM;Computing methodologies;Artificial intelligence;Computer vision;Tracking and Reconstruction;Computing methodologies-Mixed/augmented Reality},
}

@ARTICLE{sun-et-al:2018:2856268,
  author = {L. Sun and Z. Yan and A. Zaganidis and C. Zhao and T. Duckett},
  journal = {IEEE Robotics and Automation Letters},
  title = {Recurrent-OctoMap: Learning State-Based Map Refinement for Long-Term Semantic Mapping With 3-D-Lidar Data},
  volume = {3},
  number = {4},
  pages = {3749--3756},
  doi = {10.1109/LRA.2018.2856268},
  year = {2018},
  month = {10},
  abstract = {This letter presents a novel semantic mapping approach, Recurrent-OctoMap, learned from long-term three-dimensional (3-D) Lidar data. Most existing semantic mapping approaches focus on improving semantic understanding of single frames, rather than 3-D refinement of semantic maps (i.e. fusing semantic observations). The most widely used approach for the 3-D semantic map refinement is “Bayes update,” which fuses the consecutive predictive probabilities following a Markov-chain model. Instead, we propose a learning approach to fuse the semantic features, rather than simply fusing predictions from a classifier. In our approach, we represent and maintain our 3-D map as an OctoMap, and model each cell as a recurrent neural network, to obtain a Recurrent-OctoMap. In this case, the semantic mapping process can be formulated as a sequence-to-sequence encoding-decoding problem. Moreover, in order to extend the duration of observations in our Recurrent-OctoMap, we developed a robust 3-D localization and mapping system for successively mapping a dynamic environment using more than two weeks of data, and the system can be trained and deployed with arbitrary memory length. We validate our approach on the ETH long-term 3-D Lidar dataset. The experimental results show that our proposed approach outperforms the conventional “Bayes update” approach.},
  issn = {2377-3766},
  keywords = {Semantics;Three-dimensional displays;Laser radar;Two dimensional displays;Simultaneous localization and mapping;Feature extraction;Recurrent neural networks;Mapping;simultaneous localization and mapping (SLAM);deep learning in robotics and automation;object detection;segmentation and categorization},
}

@INPROCEEDINGS{sun-et-al:2018:8461228,
  author = {L. Sun and Z. Yan and S. M. Mellado and M. Hanheide and T. Duckett},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data},
  volume = {},
  number = {},
  pages = {5942--5948},
  doi = {10.1109/ICRA.2018.8461228},
  year = {2018},
  month = {5},
  abstract = {This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.},
  issn = {2577-087X},
  keywords = {Trajectory;Cameras;Robot kinematics;Robot vision systems;Two dimensional displays;Mobile robots},
}

@INPROCEEDINGS{wang-et-al:2020:9468884,
  author = {L. Wang and W. Chen and J. Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Long-Term Localization With Time Series Map Prediction for Mobile Robots in Dynamic Environments},
  volume = {},
  number = {},
  pages = {1--7},
  doi = {10.1109/IROS45743.2020.9468884},
  year = {2020},
  month = {10},
  abstract = {In many applications of mobile robot, the environment is constantly changing. How to use historical information to analysis environmental changes and generate a map corresponding with current environment is important to achieve high-precision localization. Inspired by predictive mechanism of brain, this paper presents a long-term localization approach named ArmMPU (ARMA-based Map Prediction and Update) based on time series modeling and prediction. Autoregressive moving average model (ARMA), a kind of time series modeling method, is employed for environmental map modeling and prediction, then predicted map and filtered observation are fused to fix the prediction error. The simulation and experiment results show that the proposed method improves long-term localization performance in dynamic environments.},
  issn = {2153-0866},
  keywords = {Location awareness;Correlation;Time series analysis;Predictive models;Brain modeling;Information filters;Real-time systems},
}

@INPROCEEDINGS{wu-wu:2019:8968599,
  author = {L. Wu and Y. Wu},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Deep Supervised Hashing with Similar Hierarchy for Place Recognition},
  volume = {},
  number = {},
  pages = {3781--3786},
  doi = {10.1109/IROS40897.2019.8968599},
  year = {2019},
  month = {11},
  abstract = {Place recognition as one of the most significant requirements for long-term simultaneous localization and mapping (SLAM) has been developed rapidly in recent years. Also, deep learning is proved to be more capable than traditional methods to extract features under some complex environments. However, in real-world environments, there are many challenging problems such as viewpoint changes and illumination changes. The existing deep learning-based place recognition in extracting feature phases and matching process is both time-consuming. Moreover, features extracted from convolution neural network (CNN) are floating-point type with high dimension. In this paper, we propose deep supervised hashing for place recognition, where we design a similar hierarchy loss function to learn a model. The model can distinguish the similar images more accurately which is well suitable to place recognition. Besides the model can learn high quality hash codes by maximizing the likelihood of triplet labels. Experiments on several benchmark datasets for place recognition show that our approach is robust to viewpoints, illuminations and season changes with high accuracy. Furthermore, the trained model can extract features and match in real time on CPU with less memory consumption.},
  issn = {2153-0866},
  keywords = {},
}

@INPROCEEDINGS{bujanca-et-al:2021:9636814,
  author = {M. Bujanca and X. Shi and M. Spear and P. Zhao and B. Lennox and M. Luján},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Robust SLAM Systems: Are We There Yet?},
  volume = {},
  number = {},
  pages = {5320--5327},
  doi = {10.1109/IROS51168.2021.9636814},
  year = {2021},
  month = {9},
  abstract = {Progress in the last decade has brought about significant improvements in the accuracy and speed of SLAM systems, broadening their mapping capabilities. Despite these advancements, long-term operation remains a major challenge, primarily due to the wide spectrum of perturbations robotic systems may encounter.Increasing the robustness of SLAM algorithms is an ongoing effort, however it usually addresses a specific perturbation. Generalisation of robustness across a large variety of challenging scenarios is not well-studied nor understood. This paper presents a systematic evaluation of the robustness of open-source state-of-the-art SLAM algorithms with respect to challenging conditions such as fast motion, non-uniform illumination, and dynamic scenes. The experiments are performed with perturbations present both independently of each other, as well as in combination in long-term deployment settings in unconstrained environments (lifelong operation).The detailed results (approx. 20,000 experiments) along with comprehensive documentation of the benchmarking tool for integrating new datasets and evaluating SLAM algorithms not studied in this work are available at https://robustslam.github.io/evaluation.},
  issn = {2153-0866},
  keywords = {Simultaneous localization and mapping;Systematics;Three-dimensional displays;Heuristic algorithms;Perturbation methods;Dynamics;Lighting},
}

@INPROCEEDINGS{bürki-et-al:2016:7759609,
  author = {M. Bürki and I. Gilitschenski and E. Stumm and R. Siegwart and J. Nieto},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Appearance-based landmark selection for efficient long-term visual localization},
  volume = {},
  number = {},
  pages = {4137--4143},
  doi = {10.1109/IROS.2016.7759609},
  year = {2016},
  month = {10},
  abstract = {In this paper, we present an online landmark selection method for distributed long-term visual localization systems in bandwidth-constrained environments. Sharing a common map for online localization provides a fleet of autonomous vehicles with the possibility to maintain and access a consistent map source, and therefore reduce redundancy while increasing efficiency. However, connectivity over a mobile network imposes strict bandwidth constraints and thus the need to minimize the amount of exchanged data. The wide range of varying appearance conditions encountered during long-term visual localization offers the potential to reduce data usage by extracting only those visual cues which are relevant at the given time. Motivated by this, we propose an unsupervised method of adaptively selecting landmarks according to how likely these landmarks are to be observable under the prevailing appearance condition. The ranking function this selection is based upon exploits landmark co-observability statistics collected in past traversals through the mapped area. Evaluation is performed over different outdoor environments, large time-scales and varying appearance conditions, including the extreme transition from day-time to night-time, demonstrating that with our appearance-dependent selection method, we can significantly reduce the amount of landmarks used for localization while maintaining or even improving the localization performance.},
  issn = {2153-0866},
  keywords = {Vehicles;Visualization;Servers;Bandwidth;Robots;Redundancy;Mobile computing},
}

@INPROCEEDINGS{bürki-et-al:2018:8500432,
  author = {M. Bürki and M. Dymczyk and I. Gilitschenski and C. Cadena and R. Siegwart and J. Nieto},
  booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},
  title = {Map Management for Efficient Long-Term Visual Localization in Outdoor Environments},
  volume = {},
  number = {},
  pages = {682--688},
  doi = {10.1109/IVS.2018.8500432},
  year = {2018},
  month = {6},
  abstract = {We present a complete map management process for a visual localization system designed for multi-vehicle long-term operations in resource constrained outdoor environments. Outdoor visual localization generates large amounts of data that need to be incorporated into a lifelong visual map in order to allow localization at all times and under all appearance conditions. Processing these large quantities of data is non-trivial, as it is subject to limited computational and storage capabilities both on the vehicle and on the mapping backend. We address this problem with a two-fold map update paradigm capable of, either, adding new visual cues to the map, or updating co-observation statistics. The former, in combination with offline map summarization techniques, allows enhancing the appearance coverage of the lifelong map while keeping the map size limited. On the other hand, the latter is able to significantly boost the appearance-based landmark selection for efficient online localization without incurring any additional computational or storage burden. Our evaluation in challenging outdoor conditions shows that our proposed map management process allows building and maintaining maps for precise visual localization over long time spans in a tractable and scalable fashion.},
  issn = {1931-0587},
  keywords = {Visualization;Buildings;Robot sensing systems;Autonomous automobiles;Maintenance engineering;Bandwidth;Intelligent vehicles},
}

@INPROCEEDINGS{dymczyk-et-al:2016:66,
  author = {M. Dymczyk and E. Stumm and J. Nieto and R. Siegwart and I. Gilitschenski},
  booktitle = {2016 Fourth International Conference on 3D Vision (3DV)},
  title = {Will It Last? Learning Stable Features for Long-Term Visual Localization},
  volume = {},
  number = {},
  pages = {572--581},
  doi = {10.1109/3DV.2016.66},
  year = {2016},
  month = {10},
  abstract = {An increasing number of simultaneous localization and mapping (SLAM) systems are using appearance-based localization to improve the quality of pose estimates. However, with the growing time-spans and size of the areas we want to cover, appearance-based maps are often becoming too large to handle and are consisting of features that are not always reliable for localization purposes. This paper presents a method for selecting map features that are persistent over time and thus suited for long-term localization. Our methodology relies on a CNN classifier based on image patches and depth maps for recognizing which features are suitable for life-long matchability. Thus, the classifier not only considers the appearance of a feature but also takes into account its expected lifetime. As a result, our feature selection approach produces more compact maps with a high fraction of temporally-stable features compared to the current state-of-the-art, while rejecting unstable features that typically harm localization. Our approach is validated on indoor and outdoor datasets, that span over a period of several months.},
  issn = {},
  keywords = {Feature extraction;Three-dimensional displays;Simultaneous localization and mapping;Training;Reliability;Visualization;Buildings;localization;place recognition;feature selection;SLAM;CNN;machine learning;mapping},
}

@INPROCEEDINGS{dymczyk-et-al:2016:7759673,
  author = {M. Dymczyk and T. Schneider and I. Gilitschenski and R. Siegwart and E. Stumm},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Erasing bad memories: Agent-side summarization for long-term mapping},
  volume = {},
  number = {},
  pages = {4572--4579},
  doi = {10.1109/IROS.2016.7759673},
  year = {2016},
  month = {10},
  abstract = {Precisely estimating the pose of an agent in a global reference frame is a crucial goal that unlocks a multitude of robotic applications, including autonomous navigation and collaboration. In order to achieve this, current state-of-the-art localization approaches collect data provided by one or more agents and create a single, consistent localization map, maintained over time. However, with the introduction of lengthier sorties and the growing size of the environments, data transfers between the backend server where the global map is stored and the agents are becoming prohibitively large. While some existing methods partially address this issue by building compact summary maps, the data transfer from the agents to the backend can still easily become unmanageable. In this paper, we propose a method that is designed to reduce the amount of data that needs to be transferred from the agent to the backend, functioning in large-scale, multi-session mapping scenarios. Our approach is based upon a landmark selection method that exploits information coming from multiple, possibly weak and correlated, landmark utility predictors; fused using learned feature coefficients. Such a selection yields a drastic reduction in data transfer while maintaining localization performance and the ability to efficiently summarize environments over time. We evaluate our approach on a data set that was autonomously collected in a dynamic indoor environment over a period of several months.},
  issn = {2153-0866},
  keywords = {Robots;Measurement;Bandwidth;Lighting;Data transfer;Servers;Reliability},
}

@INPROCEEDINGS{pitschl-pryor:2019:8843095,
  author = {M. L. Pitschl and M. W. Pryor},
  booktitle = {2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)},
  title = {Obstacle Persistent Adaptive Map Maintenance for Autonomous Mobile Robots using Spatio-temporal Reasoning},
  volume = {},
  number = {},
  pages = {1023--1028},
  doi = {10.1109/COASE.2019.8843095},
  year = {2019},
  month = {8},
  abstract = {Mobile robotic systems operate in increasingly realistic scenarios even as users have increased expectations for the duration of autonomous tasks. Mobile robots face unique challenges when operating in environments that change over time, where systems must maintain an accurate representation of the environment with respect to both spatial and temporal dimensions. This paper describes a spatio-temporal technique for extending the autonomy of a mobile robot in a changing environment. This new technique called Obstacle Persistent Adaptive Map Maintenance (OPAMM) uses navigation data collected during normal operations to perform periodic self-maintenance of its environment model. OPAMM implements a probabilistic feature persistence model to predict the survival state of obstacles and update the world model. Maintaining an accurate world model is necessary for extending the long-term autonomy of robots in realistic scenarios. Results show that robots using OPAMM had localizations scores higher than other methods, thus reducing long-term localization degradation.},
  issn = {2161-8089},
  keywords = {Conferences;Automation;Computer aided software engineering},
}

@ARTICLE{labbé-michaud:2013:2242375,
  author = {M. Labbé and F. Michaud},
  journal = {IEEE Transactions on Robotics},
  title = {Appearance-Based Loop Closure Detection for Online Large-Scale and Long-Term Operation},
  volume = {29},
  number = {3},
  pages = {734--745},
  doi = {10.1109/TRO.2013.2242375},
  year = {2013},
  month = {6},
  abstract = {In appearance-based localization and mapping, loop-closure detection is the process used to determinate if the current observation comes from a previously visited location or a new one. As the size of the internal map increases, so does the time required to compare new observations with all stored locations, eventually limiting online processing. This paper presents an online loop-closure detection approach for large-scale and long-term operation. The approach is based on a memory management method, which limits the number of locations used for loop-closure detection so that the computation time remains under real-time constraints. The idea consists of keeping the most recent and frequently observed locations in a working memory (WM) that is used for loop-closure detection, and transferring the others into a long-term memory (LTM). When a match is found between the current location and one stored in WM, associated locations that are stored in LTM can be updated and remembered for additional loop-closure detections. Results demonstrate the approach's adaptability and scalability using ten standard datasets from other appearance-based loop-closure approaches, one custom dataset using real images taken over a 2-km loop of our university campus, and one custom dataset (7 h) using virtual images from the racing video game “Need for Speed: Most Wanted”.},
  issn = {1941-0468},
  keywords = {Vocabulary;Feature extraction;Robots;Memory management;Real-time systems;Bayesian methods;Visualization;Appearance-based localization and mapping;bag-of-words approach;dynamic Bayes filtering;place recognition},
}

@ARTICLE{quan-et-al:2019:2930201,
  author = {M. Quan and S. Piao and M. Tan and S.-S. Huang},
  journal = {IEEE Access},
  title = {Tightly-Coupled Monocular Visual-Odometric SLAM Using Wheels and a MEMS Gyroscope},
  volume = {7},
  number = {},
  pages = {97374--97389},
  doi = {10.1109/ACCESS.2019.2930201},
  year = {2019},
  month = {},
  abstract = {In this paper, we present a novel tightly coupled probabilistic monocular visual-odometric simultaneous localization and mapping (VOSLAM) algorithm using wheels and a MEMS gyroscope, which can provide accurate, robust, and long-term localization for ground robots. First, we present a novel odometer preintegration theory on manifold; it integrates the wheel encoder measurements and gyroscope measurements to a relative motion constraint that is independent of the linearization point and carefully addresses the uncertainty propagation and gyroscope bias correction. Based on the preintegrated odometer measurement model, we also introduce the odometer error term and tightly integrate it into the visual optimization framework. Then, in order to bootstrap the VOSLAM system, we propose a simple map initialization method. Finally, we present a complete localization mechanism to maximally exploit both sensing cues, which provides different strategies for motion tracking when: 1) both measurements are available; 2) visual measurements are not available; and 3) wheel encoders experience slippage, thereby ensuring the accurate and robust motion tracking. The proposed algorithm is evaluated by performing extensive experiments, and the experimental results demonstrate the superiority of the proposed system.},
  issn = {2169-3536},
  keywords = {Wheels;Visualization;Simultaneous localization and mapping;Motion measurement;Optimization;Motion estimation;sensor fusion;simultaneous localization and mapping},
}

@INPROCEEDINGS{stübler-et-al:2017:8126353,
  author = {M. Stübler and S. Reuter and K. Dietmayer},
  booktitle = {2017 Sensor Data Fusion: Trends, Solutions, Applications (SDF)},
  title = {A continuously learning feature-based map using a bernoulli filtering approach},
  volume = {},
  number = {},
  pages = {1--6},
  doi = {10.1109/SDF.2017.8126353},
  year = {2017},
  month = {10},
  abstract = {One of the huge challenges of map-based localization is a rapidly changing environment. The present contribution addresses this problem by first constructing a new framework for feature-based long-term mapping using a Bernoulli filter. This framework is then applied to construct a continuously learning map. It is based on Simultaneous Localization and Mapping (SLAM) to create a short-term map which provides a momentary image of the environment during one mapping run. The proposed fusion algorithm then estimates the landmark map on a long-term basis by incorporating those short-term maps. Landmarks in the long-term map that reach a negligible spatial uncertainty can then be used again as a prior for the short-term mapping process. Since a Bernoulli filter can only handle a single object, independent groups of landmarks are constructed where only those with exactly one landmark are updated. As a result, landmarks that are part of the long-term map are quite distinct. By incorporating additional but probably outdated a priori information, the proposed method is able to restrict the inevitable error propagation of SLAM algorithms. The long-term mapping process is further distributable to several agents: every agent simultaneously localizes itself while it generates a new snapshot that is fused into the long-term map afterwards. An evaluation using real-world data completes this contribution.},
  issn = {},
  keywords = {Simultaneous localization and mapping;Markov processes;Time measurement;Mathematical model;Atmospheric measurements;Particle measurements},
}

@INPROCEEDINGS{lázaro-et-al:2018:8594310,
  author = {M. T. Lázaro and R. Capobianco and G. Grisetti},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Efficient Long-term Mapping in Dynamic Environments},
  volume = {},
  number = {},
  pages = {153--160},
  doi = {10.1109/IROS.2018.8594310},
  year = {2018},
  month = {10},
  abstract = {As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.},
  issn = {2153-0866},
  keywords = {Simultaneous localization and mapping;Cloud computing;Three-dimensional displays;Two dimensional displays;Merging;Optimization},
}

@INPROCEEDINGS{zaffar-et-al:2018:8541483,
  author = {M. Zaffar and S. Ehsan and R. Stolkin and K. M. Maier},
  booktitle = {2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)},
  title = {Sensors, SLAM and Long-term Autonomy: A Review},
  volume = {},
  number = {},
  pages = {285--290},
  doi = {10.1109/AHS.2018.8541483},
  year = {2018},
  month = {8},
  abstract = {Simultaneous Localization and Mapping, commonly known as SLAM, has been an active research area in the field of Robotics over the past three decades. For solving the SLAM problem, every robot is equipped with either a single sensor or a combination of similar/different sensors. This paper attempts to review, discuss, evaluate and compare these sensors. Keeping an eye on future, this paper also assesses the characteristics of these sensors against factors critical to the long-term autonomy challenge.},
  issn = {2471-769X},
  keywords = {Cameras;Simultaneous localization and mapping;Sensor phenomena and characterization;Laser radar;Acoustic sensors;SLAM;Long-term Autonomy;Sensors},
}

@INPROCEEDINGS{zhang-et-al:2019:8814347,
  author = {M. Zhang and Y. Chen and M. Li},
  booktitle = {2019 American Control Conference (ACC)},
  title = {SDF-Loc: Signed Distance Field based 2D Relocalization and Map Update in Dynamic Environments},
  volume = {},
  number = {},
  pages = {1997--2004},
  doi = {10.23919/ACC.2019.8814347},
  year = {2019},
  month = {7},
  abstract = {To empower an autonomous robot to perform long-term navigation in a given area, a concurrent localization and map update algorithm is required. In this paper, we tackle this problem by providing both theoretical analysis and algorithm design for robotic systems equipped with 2D laser range finders. The first key contribution of this paper is that we propose a hybrid signed distance field (SDF) framework for laser based localization. The proposed hybrid SDF integrates two methods with complementary characteristics, namely Euclidean SDF (ESDF) and Truncated SDF (TSDF). With our framework, accurate pose estimation and fast map update can be performed simultaneously. Moreover, we introduce a novel sliding window estimator which attains better accuracy by consistently utilizing sensor and map information with both scan-to-scan and scan-to-map data association. Real-world experimental results demonstrate that the proposed algorithm can be used for commercial robots in various environments with long-term usage. Experiments also show that our approach outperforms competing approaches by a wide margin.},
  issn = {2378-5861},
  keywords = {Optimization;Robot sensing systems;Lasers;Measurement by laser beam;Two dimensional displays;Pose estimation},
}

@INPROCEEDINGS{zhao-et-al:2021:9635985,
  author = {M. Zhao and X. Guo and L. Song and B. Qin and X. Shi and G. H. Lee and G. Sun},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {A General Framework for Lifelong Localization and Mapping in Changing Environment},
  volume = {},
  number = {},
  pages = {3305--3312},
  doi = {10.1109/IROS51168.2021.9635985},
  year = {2021},
  month = {9},
  abstract = {The environment of most real-world scenarios such as malls and supermarkets changes at all times. A pre-built map that does not account for these changes becomes out-of-date easily. Therefore, it is necessary to have an up-to-date model of the environment to facilitate long-term operation of a robot. To this end, this paper presents a general lifelong simultaneous localization and mapping (SLAM) framework. Our framework uses a multiple session map representation, and exploits an efficient map updating strategy that includes map building, pose graph refinement and sparsification. To mitigate the unbounded increase of memory usage, we propose a map-trimming method based on the Chow-Liu maximum-mutual-information spanning tree. The proposed SLAM framework has been comprehensively validated by over a month of robot deployment in real supermarket environment. Furthermore, we release the dataset collected from the indoor and outdoor changing environment with the hope to accelerate lifelong SLAM research in the community. Our dataset is available at https://github.com/sanduan168/lifelong-SLAM-dataset.},
  issn = {2153-0866},
  keywords = {Location awareness;Simultaneous localization and mapping;Buildings;Intelligent robots},
}

@INPROCEEDINGS{banerjee-et-al:2019:8870347,
  author = {N. Banerjee and D. Lisin and J. Briggs and M. Llofriu and M. E. Munich},
  booktitle = {2019 European Conference on Mobile Robots (ECMR)},
  title = {Lifelong Mapping using Adaptive Local Maps},
  volume = {},
  number = {},
  pages = {1--8},
  doi = {10.1109/ECMR.2019.8870347},
  year = {2019},
  month = {9},
  abstract = {Occupancy mapping enables a mobile robot to make intelligent planning decisions to accomplish its tasks. Adaptive local maps is an algorithm which represents the occupancy information as a set of overlapping local maps anchored to poses in the robot's trajectory. At any time, a global occupancy map can be rendered from the local maps to be used for path planning. The advantage of this approach is that the occupancy information stays consistent despite the changes in the pose estimates resulting from loop closures and localization updates. The disadvantage, however, is that the number of local maps grows over time. For long robot runs, or for multiple runs in the same space, this growth will result in redundant occupancy information, which will in turn increase the time it takes to render the global map, as well as the memory footprint of the system. In this paper, we propose a novel approach for the maintenance of an adaptive local maps system, which intelligently prunes redundant local maps, ensuring the robustness and stability required for lifelong mapping.},
  issn = {},
  keywords = {Simultaneous localization and mapping;Uncertainty;Mobile robots;Trajectory},
}

@INPROCEEDINGS{banerjee-et-al:2021:9568826,
  author = {N. Banerjee and D. Lisin and V. Albanese and Z. Zhu and S. R. Lenser and J. Shriver and T. Ramaswamy and J. Briggs and P. Fong},
  booktitle = {2021 European Conference on Mobile Robots (ECMR)},
  title = {Preventing and Correcting Mistakes in Lifelong Mapping},
  volume = {},
  number = {},
  pages = {1--8},
  doi = {10.1109/ECMR50962.2021.9568826},
  year = {2021},
  month = {8},
  abstract = {A Graph SLAM system is only as good as the edges in its pose graph. Critical mistakes in the generation of these edges can instantly render a map inconsistent, misleading, and ultimately unusable. For a lifelong mapping system, where the map is updated continuously, avoiding these errors altogether is infeasible. Instead, we propose a system for detection of and recovery from severe errors in edge generation. Our system remedies both edges created by view observations and edges created by an odometry motion model. For observation edges, we pair a novel method for monitoring ambiguous views with an intelligent graph-merging algorithm capable of rejecting a relocalization in progress. For motion edges, we propose a qualitative geometric approach for detecting structural aberrations characteristic of odometry failures. We conclude with an analysis of our results based on an empirical study of thousands of robot runs.},
  issn = {},
  keywords = {Location awareness;Simultaneous localization and mapping;Image edge detection;Europe;Mobile robots;Monitoring},
}

@INPROCEEDINGS{banerjee-et-al:2019:8968245,
  author = {N. Banerjee and R. C. Connolly and D. Lisin and J. Briggs and M. E. Munich},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {View management for lifelong visual maps},
  volume = {},
  number = {},
  pages = {7871--7878},
  doi = {10.1109/IROS40897.2019.8968245},
  year = {2019},
  month = {11},
  abstract = {The time complexity of making observations and loop closures in a graph-based visual SLAM system is a function of the number of views stored [1], [2]. Clever algorithms, such as approximate nearest neighbor search, can make this function sub-linear. Despite this, over time the number of views can still grow to a point at which the speed and/or accuracy of the system becomes unacceptable, especially in computation- and memory-constrained SLAM systems. However, not all views are created equal. Some views are rarely observed, because they have been created in an unusual lighting condition, or from low quality images, or in a location whose appearance has changed. These views can be removed to improve the overall performance of a SLAM system. In this paper, we propose a method for pruning views in a visual SLAM system to maintain its speed and accuracy for long term use.},
  issn = {2153-0866},
  keywords = {},
}

@ARTICLE{carlevaris-bianco-et-al:2014:2347571,
  author = {N. Carlevaris-Bianco and M. Kaess and R. M. Eustice},
  journal = {IEEE Transactions on Robotics},
  title = {Generic Node Removal for Factor-Graph SLAM},
  volume = {30},
  number = {6},
  pages = {1371--1385},
  doi = {10.1109/TRO.2014.2347571},
  year = {2014},
  month = {12},
  abstract = {This paper reports on a generic factor-based method for node removal in factor-graph simultaneous localization and mapping (SLAM), which we call generic linear constraints (GLCs). The need for a generic node removal tool is motivated by long-term SLAM applications, whereby nodes are removed in order to control the computational cost of graph optimization. GLC is able to produce a new set of linearized factors over the elimination clique that can represent either the true marginalization (i.e., dense GLC) or a sparse approximation of the true marginalization using a ChowLiu tree (i.e., sparse GLC). The proposed algorithm improves upon commonly used methods in two key ways: First, it is not limited to graphs with strictly full-state relative-pose factors and works equally well with other low-rank factors, such as those produced by monocular vision. Second, the new factors are produced in such a way that accounts for measurement correlation, which is a problem encountered in other methods that rely strictly upon pairwise measurement composition. We evaluate the proposed method over multiple real-world SLAM graphs and show that it outperforms other recently proposed methods in terms of Kullback-Leibler divergence. Additionally, we experimentally demonstrate that the proposed GLC method provides a principled and flexible tool to control the computational complexity of long-term graph SLAM, with results shown for 34.9 h of real-world indoor-outdoor data covering 147.4 km collected over 27 mapping sessions spanning a period of 15 months.},
  issn = {1941-0468},
  keywords = {Simultaneous localization and mapping;Optimization;Approximation methods;Correlation;Mobile robots;Factor-graphs;long-term autonomy;marginalization;mobile robotics;simultaneous localization and mapping (SLAM);Factor-graphs;long-term autonomy;marginalization;mobile robotics;simultaneous localization and mapping (SLAM)},
}

@INPROCEEDINGS{carlevaris-bianco-eustice:2013:6696478,
  author = {N. Carlevaris-Bianco and R. M. Eustice},
  booktitle = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title = {Long-term simultaneous localization and mapping with generic linear constraint node removal},
  volume = {},
  number = {},
  pages = {1034--1041},
  doi = {10.1109/IROS.2013.6696478},
  year = {2013},
  month = {11},
  abstract = {This paper reports on the use of generic linear constraint (GLC) node removal as a method to control the computational complexity of long-term simultaneous localization and mapping. We experimentally demonstrate that GLC provides a principled and flexible tool enabling a wide variety of complexity management schemes. Specifically, we consider two main classes: batch multi-session node removal, in which nodes are removed in a batch operation between mapping sessions, and online node removal, in which nodes are removed as the robot operates. Results are shown for 34.9 h of real-world indoor-outdoor data covering 147.4 km collected over 27 mapping sessions spanning a period of 15 months.},
  issn = {2153-0866},
  keywords = {Simultaneous localization and mapping;Approximation methods;Computational complexity;Markov processes},
}

@ARTICLE{chebrolu-et-al:2018:2849603,
  author = {N. Chebrolu and T. Läbe and C. Stachniss},
  journal = {IEEE Robotics and Automation Letters},
  title = {Robust Long-Term Registration of UAV Images of Crop Fields for Precision Agriculture},
  volume = {3},
  number = {4},
  pages = {3097--3104},
  doi = {10.1109/LRA.2018.2849603},
  year = {2018},
  month = {10},
  abstract = {Continuous crop monitoring is an important aspect of precision agriculture and requires the registration of sensor data over longer periods of time. Often, fields are monitored using cameras mounted on unmanned aerial vehicles (UAVs) but strong changes in the visual appearance of the growing crops and the field itself poses serious challenges to conventional image registration methods. In this letter, we present a method for registering images of agricultural fields taken by an UAV over the crop season and present a complete pipeline for computing temporally aligned three-dimensional (3-D) point clouds of the field. Our approach exploits the inherent geometry of the crop arrangement in the field, which remains mostly static over time. This allows us to register the images even in the presence of strong visual changes. To this end, we propose a scale invariant, geometric feature descriptor that encodes the local plant arrangement geometry. The experiments suggest that we are able to register images taken over the crop season, including situations where matching with an off-the-shelf visual descriptor fails. We evaluate the accuracy of our matching system with respect to manually labeled ground truth. We furthermore illustrate that the reconstructed 3-D models are qualitatively correct and the registration results allow for monitoring growth parameters at a per plant level.},
  issn = {2377-3766},
  keywords = {Agriculture;Visualization;Three-dimensional displays;Monitoring;Geometry;Robustness;Cameras;Robotics in agriculture and forestry;SLAM},
}

@INPROCEEDINGS{tran-nguyen:2021:9703965,
  author = {N. H. K. Tran and V.-H. Nguyen},
  booktitle = {2021 6th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE)},
  title = {An EKF-Based Fusion of Visual-Inertial Odometry and GPS for Global Robot Pose Estimation},
  volume = {6},
  number = {},
  pages = {1--5},
  doi = {10.1109/ICRAIE52900.2021.9703965},
  year = {2021},
  month = {12},
  abstract = {Globally accurate and drift-free pose estimation is essential for long-term navigation of autonomous robots. In this paper, we present an efficient method to fuse visual-inertial odometry and GPS measurements using the Extended Kalman Filter (EKF). The filter state is propagated by relative visual-inertial estimates and updated by absolute GPS readings to eliminate the accumulated drift on position and heading. We built the sensor hardware and implemented the algorithm on an embedded computer for real-time computation. The experimental datasets were collected in an outdoor environment with 6-degree-of-freedom ground truth. Evaluation results showed that the proposed algorithm achieved consistent and accurate pose estimation with different motion trajectories.},
  issn = {},
  keywords = {Technological innovation;Three-dimensional displays;Navigation;Pose estimation;Robot sensing systems;Real-time systems;Trajectory;multi-sensor fusion;robot localization;extended Kalman filter;visual-inertial odometry;GPS},
}

@INPROCEEDINGS{patel-et-al:2019:8981658,
  author = {N. Patel and F. Khorrami and P. Krishnamurthy and A. Tzes},
  booktitle = {2019 19th International Conference on Advanced Robotics (ICAR)},
  title = {Tightly Coupled Semantic RGB-D Inertial Odometry for Accurate Long-Term Localization and Mapping},
  volume = {},
  number = {},
  pages = {523--528},
  doi = {10.1109/ICAR46387.2019.8981658},
  year = {2019},
  month = {12},
  abstract = {In this paper, we utilize semantically enhanced feature matching and visual inertial bundle adjustment to improve the robustness of odometry especially in feature-sparse environments. A novel semantically enhanced feature matching algorithm is developed for robust: 1) medium and long-term tracking, and 2) loop-closing. Additionally, a semantic visual inertial bundle adjustment algorithm is introduced to robustly estimate pose in presence of ambiguous correspondences or in feature sparse environment. Our tightly coupled semantic RGB-D odometry approach is demonstrated on a real world indoor dataset collected using our unmanned ground vehicle (UGV). Our approach improves traditional visual odometry relying on low-level geometric features like corners, points, and planes for localization and mapping. Additionally, prior approaches are limited due to their sensitivity to scene geometry and changes in light intensity. The semantic inertial odometry is especially important to significantly reduce drifts in longer intervals.},
  issn = {},
  keywords = {},
}

@INPROCEEDINGS{piasco-et-al:2019:8794221,
  author = {N. Piasco and D. Sidibé and V. Gouet-Brunet and C. Demonceaux},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  title = {Learning Scene Geometry for Visual Localization in Challenging Conditions},
  volume = {},
  number = {},
  pages = {9094--9100},
  doi = {10.1109/ICRA.2019.8794221},
  year = {2019},
  month = {5},
  abstract = {We propose a new approach for outdoor large scale image based localization that can deal with challenging scenarios like cross-season, cross-weather, day/night and long-term localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We are able to increase recall@1 performances by 2.15% on cross-weather and long-term localization scenario and by 4.24% points on a challenging winter/summer localization sequence versus state-of-the-art methods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images.},
  issn = {2577-087X},
  keywords = {Training;Decoding;Feature extraction;Robots;Image reconstruction;Geometry;Visualization},
}

@INPROCEEDINGS{seiskari-et-al:2022:00036,
  author = {O. Seiskari and P. Rantalankila and J. Kannala and J. Ylilammi and E. Rahtu and A. Solin},
  booktitle = {2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  title = {HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry},
  volume = {},
  number = {},
  pages = {287--296},
  doi = {10.1109/WACV51458.2022.00036},
  year = {2022},
  month = {1},
  abstract = {We present HybVIO, a novel hybrid approach for combining filtering-based visual-inertial odometry (VIO) with optimization-based SLAM. The core of our method is highly robust, independent VIO with improved IMU bias modeling, outlier rejection, stationarity detection, and feature track selection, which is adjustable to run on embedded hardware. Long-term consistency is achieved with a loosely-coupled SLAM module. In academic benchmarks, our solution yields excellent performance in all categories, especially in the real-time use case, where we outperform the current state-of-the-art. We also demonstrate the feasibility of VIO for vehicular tracking on consumer-grade hardware using a custom dataset, and show good performance in comparison to current commercial VISLAM alternatives.},
  issn = {2642-9381},
  keywords = {Computer vision;Simultaneous localization and mapping;Benchmark testing;Feature extraction;Real-time systems;Hardware;3D Computer Vision Stereo Processing; Vision for Aerial/Drone/Underwater/Ground Vehicles},
}

@ARTICLE{araújo-et-al:2017:2730883,
  author = {P. Araújo and R. Miranda and D. Carmo and R. Alves and L. Oliveira},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  title = {Air-SSLAM: A Visual Stereo Indoor SLAM for Aerial Quadrotors},
  volume = {14},
  number = {9},
  pages = {1643--1647},
  doi = {10.1109/LGRS.2017.2730883},
  year = {2017},
  month = {9},
  abstract = {In this letter, we introduce a novel method for visual simultaneous localization and mapping (SLAM)-so-called Air-SSLAM-which exploits a stereo camera configuration. In contrast to monocular SLAM, scale definition and 3-D information are issues that can be more easily dealt with in stereo cameras. Air-SSLAM starts from computing keypoints and the correspondent descriptors over the pair of images, using good features-to-track and rotated-binary robust-independent elementary features, respectively. Then a map is created by matching each pair of right and left frames. The long-term map maintenance is continuously performed by analyzing the quality of each matching, as well as by inserting new keypoints into uncharted areas of the environment. Three main contributions can be highlighted in our method: (1) a novel method to match keypoints efficiently; (2) three quality indicators with the aim of speeding up the mapping process; and (3) map maintenance with uniform distribution performed by image zones. By using a drone equipped with a stereo camera, flying indoor, the translational average error with respect to a marked ground truth was computed, demonstrating promising results.},
  issn = {1558-0571},
  keywords = {Cameras;Simultaneous localization and mapping;Drones;Feature extraction;Visualization;Robot vision systems;Estimation;Drone;stereo vision;visual simultaneous localization and mapping (SLAM)},
}

@INPROCEEDINGS{egger-et-al:2018:8593854,
  author = {P. Egger and P. V. K. Borges and G. Catt and A. Pfrunder and R. Siegwart and R. Dubé},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization},
  volume = {},
  number = {},
  pages = {3430--3437},
  doi = {10.1109/IROS.2018.8593854},
  year = {2018},
  month = {10},
  abstract = {Reliable long-term localization is key for robotic systems in dynamic environments. In this paper, we propose a novel approach for long-term localization using 3D LiDARs, coined PoseMap. In essence, we extract distinctive features from range measurements and bundle these into local views along with observation poses. The sensor's trajectory is then estimated in a sliding window fashion by matching current and old features and minimizing the distances in-between. The map representation facilitates finding a suitable set of old features, by selecting the closest local map(s) for matching. Similarly to a visibility analysis, this procedure provides a suitable set of features for localization but at a fraction of the computational cost. PoseMap also allows for updates and extensions of the map at any time by replacing and adding local maps when necessary. We evaluate our approach using two platforms both equipped with a 3D LiDAR and an IMU, demonstrating localization at 8 Hz and robustness to changes in the environment such as moving vehicles and changing vegetation. PoseMap was implemented on an autonomous vehicle allowing it to drive autonomously over a period of 18 months through a mix of industrial and unstructured off-road environments, covering more than 100 kms without a single localization failure.},
  issn = {2153-0866},
  keywords = {Simultaneous localization and mapping;Three-dimensional displays;Laser radar;Optimization;Feature extraction},
}

@INPROCEEDINGS{gao-zhang:2020:9196906,
  author = {P. Gao and H. Zhang},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships},
  volume = {},
  number = {},
  pages = {1070--1076},
  doi = {10.1109/ICRA40945.2020.9196906},
  year = {2020},
  month = {5},
  abstract = {Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.},
  issn = {2577-087X},
  keywords = {Visualization;Simultaneous localization and mapping;Robustness;Strain;Image recognition;Tensile stress},
}

@INPROCEEDINGS{geneva-et-al:2019:8793836,
  author = {P. Geneva and K. Eckenhoff and G. Huang},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  title = {A Linear-Complexity EKF for Visual-Inertial Navigation with Loop Closures},
  volume = {},
  number = {},
  pages = {3535--3541},
  doi = {10.1109/ICRA.2019.8793836},
  year = {2019},
  month = {5},
  abstract = {Enabling real-time visual-inertial navigation in unknown environments while achieving bounded-error performance holds great potentials in robotic applications. To this end, in this paper, we propose a novel linear-complexity EKF for visual-inertial localization, which can efficiently utilize loop closure constraints, thus allowing for long-term persistent navigation. The key idea is to adapt the Schmidt-Kalman formulation within the multi-state constraint Kalman filter (MSCKF) framework, in which we selectively include keyframes as nuisance parameters in the state vector for loop closures but do not update their estimates and covariance in order to save computations while still tracking their cross-correlations with the current navigation states. As a result, the proposed Schmidt-MSCKF has only O(n) computational complexity while still incorporating loop closures into the system. The proposed approach is validated extensively on large-scale real-world experiments, showing significant performance improvements when compared to the standard MSCKF, while only incurring marginal computational overhead.},
  issn = {2577-087X},
  keywords = {Navigation;Microsoft Windows;Standards;Current measurement;Real-time systems;Trajectory;Three-dimensional displays},
}

@INPROCEEDINGS{neubert-et-al:2013:6698842,
  author = {P. Neubert and N. Sünderhauf and P. Protzel},
  booktitle = {2013 European Conference on Mobile Robots},
  title = {Appearance change prediction for long-term navigation across seasons},
  volume = {},
  number = {},
  pages = {198--203},
  doi = {10.1109/ECMR.2013.6698842},
  year = {2013},
  month = {9},
  abstract = {Changing environments pose a serious problem to current robotic systems aiming at long term operation. While place recognition systems perform reasonably well in static or low-dynamic environments, severe appearance changes that occur between day and night, between different seasons or different local weather conditions remain a challenge. In this paper we propose to learn to predict the changes in an environment. Our key insight is that the occurring appearance changes are in part systematic, repeatable and therefore predictable. The goal of our work is to support existing approaches to place recognition by learning how the visual appearance of an environment changes over time and by using this learned knowledge to predict its appearance under different environmental conditions. We describe the general idea of appearance change prediction (ACP) and a novel implementation based on vocabularies of superpixels (SP-ACP). Despite its simplicity, we can further show that the proposed approach can improve the performance of SeqSLAM and BRIEF-Gist for place recognition on a large-scale dataset that traverses an environment under extremely different conditions in winter and summer.},
  issn = {},
  keywords = {Vocabulary;Dictionaries;Visualization;Training;Image color analysis;Image segmentation;Meteorology},
}

@INPROCEEDINGS{newman:2005:ic:20050473,
  author = {P. Newman},
  booktitle = {2005 The IEE Forum on Autonomous Systems (Ref. No. 2005/11271)},
  title = {Automating answers to "where am i?"},
  volume = {},
  number = {},
  pages = {7pp.--},
  doi = {10.1049/ic:20050473},
  year = {2005},
  month = {11},
  abstract = {In many situations, large-scale, long term deployment of an autonomous vehicle requires an ability to navigate in arbitrary workspaces and must be able to establish "where am I what surrounds me?". This paper describe simultaneous localisation and mapping (SLAM)techniques and implementations in which an autonomous vehicle explores its workspace using onboard sensors and inextricably binds together the tasks of mapping and localisation.},
  issn = {0537-9989},
  keywords = {},
}

@ARTICLE{yin-et-al:2021:3061375,
  author = {P. Yin and L. Xu and J. Zhang and H. Choset},
  journal = {IEEE Robotics and Automation Letters},
  title = {FusionVLAD: A Multi-View Deep Fusion Networks for Viewpoint-Free 3D Place Recognition},
  volume = {6},
  number = {2},
  pages = {2304--2310},
  doi = {10.1109/LRA.2021.3061375},
  year = {2021},
  month = {4},
  abstract = {Real-time 3D place recognition is a crucial technology to recover from localization failure in applications like autonomous driving, last-mile delivery, and service robots. However, it is challenging for 3D place retrieval methods to be accurate, efficient, and robust to the variant viewpoints differences. In this letter, we propose FusionVLAD, a fusion-based network that encodes a multi-view representation of sparse 3D point clouds into viewpoint-free global descriptors. The system consists of two parallel branches: a spherical-view branch for orientation-invariant feature extraction, and the top-down view branch for translation-insensitive feature extraction. Furthermore, we design a parallel fusion module to enhance the combination of region-wise feature connection between the two branches. Experiments on two public datasets and two generated datasets show that our method outperforms state-of-the-art with robust place recognition accuracy and efficient inference time. Besides, FusionVLAD requires limited computation resources and makes it extremely suitable for low-cost robots' long-term place recognition task.},
  issn = {2377-3766},
  keywords = {Three-dimensional displays;Feature extraction;Robots;Laser radar;Encoding;Task analysis;Geometry;Recognition;SLAM;visual learning},
}

@INPROCEEDINGS{yin-et-al:2018:8593562,
  author = {P. Yin and L. Xu and Z. Liu and L. Li and H. Salman and Y. He and W. Xu and H. Wang and H. Choset},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Stabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition},
  volume = {},
  number = {},
  pages = {1162--1167},
  doi = {10.1109/IROS.2018.8593562},
  year = {2018},
  month = {10},
  abstract = {Place recognition is one of the major challenges for the LiDAR-based effective localization and mapping task. Traditional methods are usually relying on geometry matching to achieve place recognition, where a global geometry map need to be restored. In this paper, we accomplish the place recognition task based on an end-to-end feature learning framework with the LiDAR inputs. This method consists of two core modules, a dynamic octree mapping module that generates local 2D maps with the consideration of the robot's motion; and an unsupervised place feature learning module which is an improved adversarial feature learning network with additional assistance for the long-term place recognition requirement. More specially, in place feature learning, we present an additional Generative Adversarial Network with a designed Conditional Entropy Reduction module to stabilize the feature learning process in an unsupervised manner. We evaluate the proposed method on the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental results show that the proposed method outperforms state-of-the-art in place recognition tasks under long-term applications. What's more, the feature size and inference efficiency in the proposed method are applicable in real-time performance on practical robotic platforms.},
  issn = {2153-0866},
  keywords = {Octrees;Laser radar;Task analysis;Decoding;Simultaneous localization and mapping;Generative adversarial networks},
}

@INPROCEEDINGS{yin-et-al:2019:8793853,
  author = {P. Yin and R. A. Srivatsan and Y. Chen and X. Li and H. Zhang and L. Xu and L. Li and Z. Jia and J. Ji and Y. He},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  title = {MRS-VPR: a multi-resolution sampling based global visual place recognition method},
  volume = {},
  number = {},
  pages = {7137--7142},
  doi = {10.1109/ICRA.2019.8793853},
  year = {2019},
  month = {5},
  abstract = {Place recognition and loop closure detection are challenging for long-term visual navigation tasks. SeqSLAM is considered to be one of the most successful approaches to achieve long-term localization under varying environmental conditions and changing viewpoints. SeqSLAM uses a brute-force sequential matching method, which is computationally intensive. In this work, we introduce a multi-resolution sampling-based global visual place recognition method (MRS-VPR), which can significantly improve the matching efficiency and accuracy in sequential matching. The novelty of this method lies in the coarse-to-fine searching pipeline and a particle filter-based global sampling scheme, that can balance the matching efficiency and accuracy in the long-term navigation task. Moreover, our model works much better than SeqSLAM when the testing sequence is over a much smaller time scale than the reference sequence. Our experiments demonstrate that MRSVPR is efficient in locating short temporary trajectories within long-term reference ones without compromising on the accuracy compared to SeqSLAM.},
  issn = {2577-087X},
  keywords = {Testing;Feature extraction;Indexes;Task analysis;Trajectory;Visualization;Robots},
}

@INPROCEEDINGS{shen-et-al:2017:8248292,
  author = {Q. Shen and H. Sun and P. Ye},
  booktitle = {2017 4th International Conference on Systems and Informatics (ICSAI)},
  title = {Research of large-scale offline map management in visual SLAM},
  volume = {},
  number = {},
  pages = {215--219},
  doi = {10.1109/ICSAI.2017.8248292},
  year = {2017},
  month = {11},
  abstract = {This paper presents a novel method of visual simultaneous localization and mapping (SLAM), which is a method of real-time localization and mapping. It is important for a mobile robot to build a map while autonomously navigation. Due to the complexity of the robot work scene, the SLAM method proposed in this paper optimizes map management. It will cost a lot of time and space when a robot long-term works in a same large scene. Therefore, we propose a method in this paper to save a detail map as an offline map in advance. At the same time in order to facilitate the follow-up optimization, the offline map can be divided into several sub-graphs according to the similarity of the scene. Since the segmented offline map has been saved to local system, it can be loaded at any time to localization and obtain the pose of current frame.},
  issn = {},
  keywords = {Simultaneous localization and mapping;Image segmentation;Cameras;Real-time systems;Optimization;Symmetric matrices;SLAM;offline map;segment graph;normalized-cut},
}

@INPROCEEDINGS{tian-et-al:2019:8813720,
  author = {Q. Tian and Y. Gao and G. Li and J. Song},
  booktitle = {2019 5th International Conference on Control, Automation and Robotics (ICCAR)},
  title = {A Novel Global Relocalization Method Based on Hierarchical Registration of 3D Point Cloud Map for Mobile Robot},
  volume = {},
  number = {},
  pages = {68--73},
  doi = {10.1109/ICCAR.2019.8813720},
  year = {2019},
  month = {4},
  abstract = {Indoor service mobile robots need to relocate when they are kidnapped, powered off, or lost in long-term work, thus unable to perform daily tasks. Solving this problem is challenging, especially for 3D maps due to the computational complexity. In order to solve this issue, a novel relocalization algorithm based on hierarchical registration is proposed for a known 3D map in this paper. For 3D point cloud maps, the algorithm obtains multi-layer information in the vertical direction through hierarchical registration at the robot's current position. To obtain the best 3D pose for relocalization, we fuse the poses calculated by the multi-layered point cloud into one and use it as the initial pose of the iterative closest point algorithm. The hierarchical registration based algorithm solves the problem of unknown initial value for registration between two large point clouds, improves the recall rate, and ensures the accuracy of algorithm at the same time. The related relocalization experiments are carried out in the indoor environment and the results verify the effectiveness and robustness of the algorithm.},
  issn = {2251-2446},
  keywords = {visual SLAM;relocalization;mobile robot;point cloud registration},
}

@INPROCEEDINGS{dubé-et-al:2016:7784311,
  author = {R. Dubé and A. Gawel and C. Cadena and R. Siegwart and L. Freda and M. Gianni},
  booktitle = {2016 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)},
  title = {3D localization, mapping and path planning for search and rescue operations},
  volume = {},
  number = {},
  pages = {272--273},
  doi = {10.1109/SSRR.2016.7784311},
  year = {2016},
  month = {10},
  abstract = {This work presents our results on 3D robot localization, mapping and path planning for the latest joint exercise of the European project “Long-Term Human-Robot Teaming for Robots Assisted Disaster Response” (TRADR)1. The full system is operated and evaluated by firemen end-users in real-world search and rescue experiments. We demonstrate that the system is able to plan a path to a goal position desired by the fireman operator in the TRADR Operational Control Unit (OCU), using a persistent 3D map created by the robot during previous sorties.},
  issn = {},
  keywords = {Three-dimensional displays;Path planning;Simultaneous localization and mapping;Navigation;Lasers},
}

@ARTICLE{giubilato-et-al:2020:2964157,
  author = {R. Giubilato and M. Vayugundla and M. J. Schuster and W. Stürzl and A. Wedler and R. Triebel and S. Debei},
  journal = {IEEE Robotics and Automation Letters},
  title = {Relocalization With Submaps: Multi-Session Mapping for Planetary Rovers Equipped With Stereo Cameras},
  volume = {5},
  number = {2},
  pages = {580--587},
  doi = {10.1109/LRA.2020.2964157},
  year = {2020},
  month = {4},
  abstract = {To enable long term exploration of extreme environments such as planetary surfaces, heterogeneous robotic teams need the ability to localize themselves on previously built maps. While the Localization and Mapping problem for single sessions can be efficiently solved with many state of the art solutions, place recognition in natural environments still poses great challenges for the perception system of a robotic agent. In this paper we propose a relocalization pipeline which exploits both 3D and visual information from stereo cameras to detect matches across local point clouds of multiple SLAM sessions. Our solution is based on a Bag of Binary Words scheme where binarized SHOT descriptors are enriched with visual cues to recall in a fast and efficient way previously visited places. The proposed relocalization scheme is validated on challenging datasets captured using a planetary rover prototype on Mount Etna, designated as a Moon analogue environment.},
  issn = {2377-3766},
  keywords = {Three-dimensional displays;Visualization;Simultaneous localization and mapping;Vocabulary;Pipelines;Cameras;Localization;space robotics and automation;mapping},
}

@INPROCEEDINGS{limosani-et-al:2015:7354193,
  author = {R. Limosani and L. Y. Morales and J. Even and F. Ferreri and A. Watanabe and F. Cavallo and P. Dario and N. Hagita},
  booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Long-term human affordance maps},
  volume = {},
  number = {},
  pages = {5748--5754},
  doi = {10.1109/IROS.2015.7354193},
  year = {2015},
  month = {9},
  abstract = {This paper presents a work on mapping the use of space by humans in long periods of time. Daily geometric maps with the same coordinate frame were generated with SLAM, and in a similar manner, daily affordance density maps (places people use) were generated with the output of a human tracker running on the robot. The contribution of the paper is two-fold: an approach to detect geometric changes to cluster them in similar geometric configurations and the building of geometric and affordance composite maps on each cluster. This approach avoids the loss of long term retrieved information. Geometric similarity was computed using a normal distance approach on the maps. The analysis was performed on data collected by a mobile robot for a period of 4 months accumulating data equivalent to 70 days. Experimental results show that the system is capable of detecting geometric changes in the environment and clustering similar geometric configurations.},
  issn = {},
  keywords = {Robot kinematics;Buildings;Navigation;Robot sensing systems;Layout;Geometry},
}

@ARTICLE{mur-artal-et-al:2015:2463671,
  author = {R. Mur-Artal and J. M. M. Montiel and J. D. Tardós},
  journal = {IEEE Transactions on Robotics},
  title = {ORB-SLAM: A Versatile and Accurate Monocular SLAM System},
  volume = {31},
  number = {5},
  pages = {1147--1163},
  doi = {10.1109/TRO.2015.2463671},
  year = {2015},
  month = {10},
  abstract = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
  issn = {1941-0468},
  keywords = {Simultaneous localization and mapping;Cameras;Optimization;Feature extraction;Visualization;Real-time systems;Computational modeling;Lifelong mapping;localization;monocular vision;recognition;simultaneous localization and mapping (SLAM);Lifelong mapping;localization;monocular vision;recognition;simultaneous localization and mapping (SLAM)},
}

@INPROCEEDINGS{paul-newman:2011:5980404,
  author = {R. Paul and P. Newman},
  booktitle = {2011 IEEE International Conference on Robotics and Automation},
  title = {Self help: Seeking out perplexing images for ever improving navigation},
  volume = {},
  number = {},
  pages = {445--451},
  doi = {10.1109/ICRA.2011.5980404},
  year = {2011},
  month = {5},
  abstract = {This paper is a demonstration of how a robot can, through introspection and then targeted data retrieval, improve its own performance. It is a step in the direction of lifelong learning and adaptation and is motivated by the desire to build robots that have plastic competencies which are not baked in. They should react to and benefit from use. We consider a particular instantiation of this problem in the context of place recognition. Based on a topic based probabilistic model of images, we use a measure of perplexity to evaluate how well a working set of background images explain the robot's online view of the world. Offline, the robot then searches an external resource to seek out additional background images that bolster its ability to localise in its environment when used next. In this way the robot adapts and improves performance through use.},
  issn = {1050-4729},
  keywords = {Robots;Biological system modeling;Mathematical model;Visualization;Convergence;Databases;Redundancy},
}

@INPROCEEDINGS{rodrigues-et-al:2018:8594456,
  author = {R. T. Rodrigues and A. P. Aguiar and A. Pascoal},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {A B-Spline Mapping Framework for Long-Term Autonomous Operations},
  volume = {},
  number = {},
  pages = {3204--3209},
  doi = {10.1109/IROS.2018.8594456},
  year = {2018},
  month = {10},
  abstract = {This paper presents a 2D B-spline mapping framework for representing unstructured environments in a compact manner. While occupancy-grid and landmark-based maps have been successfully employed by the robotics community in indoor scenarios, outdoor long-term autonomous operations require a more compact representation of the environment. This work tackles this problem by interpolating the data of a high frequency sensor using B-spline curves. Compared to lines and circles, splines are more powerful in the sense that they allow for the description of more complex shapes in the scene. In this work, spline curves are continuously tracked and aligned across multiple sensor readings using lightweight methods, making the proposed framework suitable for robot navigation in outdoor missions. In particular, a Simultaneous Localization and Mapping (SLAM) algorithm specifically tailored for B-spline maps is presented here. The efficacy of the proposed framework is demonstrated by Software-in-the-Loop (SiL) simulations in different scenarios.},
  issn = {2153-0866},
  keywords = {Splines (mathematics);Simultaneous localization and mapping;Three-dimensional displays;Robot kinematics;Two dimensional displays},
}

@INPROCEEDINGS{anwar-et-al:2013:6512139,
  author = {S. Anwar and Q. Zhao and N. Qadeer and S. I. Khan},
  booktitle = {Proceedings of 2013 10th International Bhurban Conference on Applied Sciences Technology (IBCAST)},
  title = {A framework for RF-Visual SLAM},
  volume = {},
  number = {},
  pages = {103--108},
  doi = {10.1109/IBCAST.2013.6512139},
  year = {2013},
  month = {1},
  abstract = {Simultaneous Localization and Mapping, SLAM, is an important topic in the field of robotics and autonomous navigation. The metric SLAM suffers from sensor inaccuracies and thus cannot be used for long-term navigation. In such case, Visual SLAM or a Hybrid SLAM based on both metric and visual approach is a good alternative. In this paper, in order to speed up a Visual SLAM, we propose a novel concept of dynamic dictionary generated on the results of triangulation done on RF, radio frequency, signals from nearest cell towers of a cellular network. This dynamic dictionary efficiently manages the scalability of a Visual SLAM and make it possible to work in a large-scale environment. A framework is proposed along with triangulation data of a city and with simulations to support the concept.},
  issn = {},
  keywords = {Dictionaries;Simultaneous localization and mapping;Poles and towers;Navigation;Hybrid power systems},
}

@INPROCEEDINGS{arshad-kim:2020:24,
  author = {S. Arshad and G.-W. Kim},
  booktitle = {2020 IEEE International Conference on Big Data and Smart Computing (BigComp)},
  title = {Robustifying Visual Place Recognition with Semantic Scene Categorization},
  volume = {},
  number = {},
  pages = {467--469},
  doi = {10.1109/BigComp48618.2020.00-24},
  year = {2020},
  month = {2},
  abstract = {This research proposes an accurate loop closure detection method for the real-time robot navigation in changing light, viewpoint and weather conditions. It focuses on enhancing the accuracy performance of visual place recognition by integrating the semantics of image scenes with handcrafted features. This method reduces the computational cost of the feature matching process by segmentation of the dataset. The results presented depicts that scene recognition can improve the place recognition even in the large viewpoint, weather and light changes.},
  issn = {2375-9356},
  keywords = {Semantics;Feature extraction;Visualization;Navigation;Robots;Meteorology;Real-time systems;visual place recognition;scene recognition;semantic labelling;feature descriptors;long term autonomy;visual navigation;hierarchical structure;data segmentation},
}

@ARTICLE{williams-et-al:2012:2181772,
  author = {S. B. Williams and O. R. Pizarro and M. V. Jakuba and C. R. Johnson and N. S. Barrett and R. C. Babcock and G. A. Kendrick and P. D. Steinberg and A. J. Heyward and P. J. Doherty and I. Mahon and M. Johnson-Roberson and D. Steinberg and A. Friedman},
  journal = {IEEE Robotics Automation Magazine},
  title = {Monitoring of Benthic Reference Sites: Using an Autonomous Underwater Vehicle},
  volume = {19},
  number = {1},
  pages = {73--84},
  doi = {10.1109/MRA.2011.2181772},
  year = {2012},
  month = {3},
  abstract = {We have established an Australia-wide observation program that exhibits recent developments in autonomous underwater vehicle (AUV) systems to deliver precisely navigated time series benthic imagery at selected reference stations on Australia's continental shelf. These observations are designed to help characterize changes in benthic assemblage composition and cover derived from precisely registered maps collected at regular intervals. This information will provide researchers with the baseline ecological data necessary to make quantitative inferences about the long-term effects of climate change and human activities on the benthos. Incorporating a suite of observations that capitalize on the unique capabilities of AUVs into Australia's integrated marine observation system (IMOS) [1] is providing a critical link between oceanographic and benthic processes. IMOS is a nationally coordinated program designed to establish and maintain the research infrastructure required to support Australia's marine science research. It has, and will maintain, a strategic focus on the impact of major boundary currents on continental shelf environments, ecosystems, and biodiversity. The IMOS AUV facility observation program is designed to generate physical and biological observations of benthic variables that cannot be cost effectively obtained by other means.},
  issn = {1558-223X},
  keywords = {Ecosystems;Navigation;Australia;Simultaneous localization and mapping;Oceans;Biodiversity;Underwater vehicles;Meteorology;Human factors},
}

@INPROCEEDINGS{chen-et-al:2019:8833730,
  author = {S. Chen and J. Wu and Y. Wang and L. Zhou and Q. Lu and Y. Zhang},
  booktitle = {2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM)},
  title = {Robust Loop-Closure Detection with a Learned Illumination Invariant Representation for Robot vSLAM},
  volume = {},
  number = {},
  pages = {342--347},
  doi = {10.1109/ICARM.2019.8833730},
  year = {2019},
  month = {7},
  abstract = {Robust loop-closure detection plays a key role for the long-term robot visual Simultaneous Localization and Mapping (SLAM) in indoor or outdoor environment, due to illumination changes can greatly affect the accuracy of online image matching, and keypoints may fail to match between images taken at the same location but different seasons. In this paper, we propose a robust loop-closure detection method for robot visual SLAM, which adopts invariant representation as image descriptors composed of learned features and adapts to changes in illumination and seasons. We evaluate our method on real datasets and demonstrate its excellent ability to handle illumination changes.},
  issn = {},
  keywords = {Visualization;Simultaneous localization and mapping;Lighting;Feature extraction;Mechatronics;Image matching;Visual SLAM;Loop Closure Detection;Visual Place Recognition;Illumination Invariant Feature;Moblie Robot;Convolutional Neural Network},
}

@INPROCEEDINGS{dominguez-et-al:2015:433,
  author = {S. Dominguez and B. Khomutenko and G. Garcia and P. Martinet},
  booktitle = {2015 IEEE 18th International Conference on Intelligent Transportation Systems},
  title = {An Optimization Technique for Positioning Multiple Maps for Self-Driving Car's Autonomous Navigation},
  volume = {},
  number = {},
  pages = {2694--2699},
  doi = {10.1109/ITSC.2015.433},
  year = {2015},
  month = {9},
  abstract = {Self-driving car's navigation requires a very precise localization covering wide areas and long distances. Moreover, they have to do it at faster speeds than conventional mobile robots. This paper reports on an efficient technique to optimize the position of a sequence of maps along a journey. We take advantage of the short-term precision and reduced space on disk of the localization using 2D occupancy grid maps, from now on called sub-maps, as well as, the long-term global consistency of a Kalman filter that fuses odometry and GPS measurements. In our approach, horizontal planar LiDARs and odometry measurements are used to perform 2D-SLAM generating the sub-maps, and the EKF to generate the trajectory followed by the car in global coordinates. During the trip, after finishing each sub-map, a relaxation process is applied to a set of the last sub-maps to position them globally using both, global and map's local path. The importance of this method lies on its performance, expending low computing resources, so it can work in real time on a computer with conventional characteristics and on its robustness which makes it suitable for being used on a self-driving car as it doesn't depend excessively on the availability of GPS signal or the eventual appearance of moving objects around the car. Extensive testing has been performed in the suburbs and in the down-town of Nantes (France) covering a distance of 25 kilometers with different traffic conditions obtaining satisfactory results for autonomous driving.},
  issn = {2153-0017},
  keywords = {Laser radar;Global Positioning System;Splines (mathematics);Force;Trajectory;Simultaneous localization and mapping;Buildings},
}

@ARTICLE{heath-et-al:2016:2442619,
  author = {S. Heath and D. Ball and J. Wiles},
  journal = {IEEE Transactions on Cognitive and Developmental Systems},
  title = {Lingodroids: Cross-Situational Learning for Episodic Elements},
  volume = {8},
  number = {1},
  pages = {3--14},
  doi = {10.1109/TAMD.2015.2442619},
  year = {2016},
  month = {3},
  abstract = {For robots to effectively bootstrap the acquisition of language, they must handle referential uncertainty-the problem of deciding what meaning to ascribe to a given word. Typically when socially grounding terms for space and time, the underlying sensor or representation was specified within the grammar of a conversation, which constrained language learning to words for innate features. In this paper, we demonstrate that cross-situational learning resolves the issues of referential uncertainty for bootstrapping a language for episodic space and time; therefore removing the need to specify the underlying sensors or representations a priori. The requirements for robots to be able to link words to their designated meanings are presented and analyzed within the Lingodroids-language learning robots-framework. We present a study that compares predetermined associations given a priori against unconstrained learning using cross-situational learning. This study investigates the long-term coherence, immediate usability and learning time for each condition. Results demonstrate that for unconstrained learning, the long-term coherence is unaffected, though at the cost of increased learning time and hence decreased immediate usability.},
  issn = {2379-8939},
  keywords = {Grounding;Cognition;Uncertainty;Context;Simultaneous localization and mapping;Cross-situational learning;episodic;language learning;Lingodroids;robots;space;symbol grounding;time},
}

@INPROCEEDINGS{hilsenbeck-et-al:2012:6418934,
  author = {S. Hilsenbeck and A. Möller and R. Huitl and G. Schroth and M. Kranz and E. Steinbach},
  booktitle = {2012 International Conference on Indoor Positioning and Indoor Navigation (IPIN)},
  title = {Scale-preserving long-term visual odometry for indoor navigation},
  volume = {},
  number = {},
  pages = {1--10},
  doi = {10.1109/IPIN.2012.6418934},
  year = {2012},
  month = {11},
  abstract = {We present a visual odometry system for indoor navigation with a focus on long-term robustness and consistency. As our work is targeting mobile phones, we employ monocular SLAM to jointly estimate a local map and the device's trajectory. We specifically address the problem of estimating the scale factor of both, the map and the trajectory. State-of-the-art solutions approach this problem with an Extended Kalman Filter (EKF), which estimates the scale by fusing inertial and visual data, but strongly relies on good initialization and takes time to converge. Each visual tracking failure introduces a new arbitrary scale factor, forcing the filter to re-converge. We propose a fast and robust method for scale initialization that exploits basic geometric properties of the learned local map. Using random projections, we efficiently compute geometric properties from the feature point cloud produced by the visual SLAM system. From these properties (e.g., corridor width or height) we estimate scale changes caused by tracking failures and update the EKF accordingly. As a result, previously achieved convergence is preserved despite re-initializations of the map. To minimize the time required to continue tracking after failure, we perform recovery and re-initialization in parallel. This increases the time available for recovery and hence the likelihood for success, thus allowing almost seamless tracking. Moreover, fewer re-initializations are necessary. We evaluate our approach using extensive and diverse indoor datasets. Results demonstrate that errors and convergence times for scale estimation are considerably reduced, thus ensuring consistent and accurate scale estimation. This enables long-term odometry despite of tracking failures which are inevitable in realistic scenarios.},
  issn = {},
  keywords = {Visualization;Buildings;Cameras;Estimation;Navigation;Measurement;Robustness},
}

@INPROCEEDINGS{hochdorfer-schlegel:2009,
  author = {S. Hochdorfer and C. Schlegel},
  booktitle = {2009 International Conference on Advanced Robotics},
  title = {Towards a robust visual SLAM approach: Addressing the challenge of life-long operation},
  volume = {},
  number = {},
  pages = {1--6},
  doi = {},
  year = {2009},
  month = {6},
  abstract = {Localization and mapping are fundamental problems in service robotics. Knowledge about the own pose and representations of the environment are needed for a series of high level applications. Service robots should be designed for life-long and robust operation in dynamic environments. The contribution of this paper is twofold. First, an approach to address the ever growing number of landmarks in life-long operation is presented. Typically, SLAM approaches just accumulate features over time and do not discard them anymore. Therefore, the required resources in terms of memory and processing power are growing over time. In our approach, the absolute number of landmarks can be restricted by an upper bound since we introduce a method to specifically select and replace landmarks once the upper bound has been reached. The second contribution is related to improving the robustness of the landmark assignment problem in case of image based features as needed with natural landmarks. The approach has been successfully evaluated in a real world experiment on a Pioneer-3DX platform within a complex unmodified indoor environment.},
  issn = {},
  keywords = {Robustness;Simultaneous localization and mapping;Upper bound;Service robots;Computer science;Application software;Indoor environments;Collaboration;Euclidean distance},
  url = {https://ieeexplore.ieee.org/document/5174794},
}

@INPROCEEDINGS{hochdorfer-et-al:2009:5339626,
  author = {S. Hochdorfer and M. Lutz and C. Schlegel},
  booktitle = {2009 IEEE International Conference on Technologies for Practical Robot Applications},
  title = {Lifelong localization of a mobile service-robot in everyday indoor environments using omnidirectional vision},
  volume = {},
  number = {},
  pages = {161--166},
  doi = {10.1109/TEPRA.2009.5339626},
  year = {2009},
  month = {11},
  abstract = {SLAM (Simultaneous Localization and Mapping) mechanisms are a key component towards advanced service robotics applications. Currently, a major hurdle on the way to lifelong localization is the handling of the ever growing amount of landmarks over time. Therefore, the required resources in terms of memory and processing power are also growing over time.},
  issn = {2325-0534},
  keywords = {Indoor environments;Simultaneous localization and mapping;Upper bound;Clustering algorithms;Uncertainty;Robot vision systems;Robot localization;Observability;Global Positioning System;Mobile computing},
}

@INPROCEEDINGS{luthardt-et-al:2019:8916895,
  author = {S. Luthardt and C. Ziegler and V. Willert and J. Adamy},
  booktitle = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
  title = {How to Match Tracks of Visual Features for Automotive Long-Term SLAM},
  volume = {},
  number = {},
  pages = {934--941},
  doi = {10.1109/ITSC.2019.8916895},
  year = {2019},
  month = {10},
  abstract = {Accurate localization is a vital prerequisite for future assistance or autonomous driving functions in intelligent vehicles. To achieve the required localization accuracy and availability, long-term visual SLAM algorithms like LLama-SLAM are a promising option. In such algorithms visual feature tracks, i. e. landmark observations over several consecutive image frames, have to be matched to feature tracks recorded days, weeks or months earlier. This leads to a more challenging matching problem than in short-term visual localization and known descriptor matching methods cannot be applied directly. In this paper, we devise several approaches to compare and match feature tracks and evaluate their performance on a long-term data set. With the proposed descriptor combination and masking ("CoMa") method the best track matching performance is achieved with minor computational cost. This method creates a single combined descriptor for each feature track and furthermore increases the robustness by capturing the appearance variations of this track in a descriptor mask.},
  issn = {},
  keywords = {Visualization;Feature extraction;Optimization;Simultaneous localization and mapping;Cameras;Robustness},
}

@INPROCEEDINGS{luthardt-et-al:2018:8569323,
  author = {S. Luthardt and V. Willert and J. Adamy},
  booktitle = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
  title = {LLama-SLAM: Learning High-Quality Visual Landmarks for Long-Term Mapping and Localization},
  volume = {},
  number = {},
  pages = {2645--2652},
  doi = {10.1109/ITSC.2018.8569323},
  year = {2018},
  month = {11},
  abstract = {The precise localization of vehicles is an important requirement for autonomous driving or advanced driver assistance systems. Using common GNSS the ego position can be measured but not with the reliability and precision necessary. An alternative approach to achieve precise localization is the usage of visual landmarks observed by a camera mounted in the vehicle. However, this raises the necessity of reliable visual landmarks that are easily recognizable and persistent. We propose a novel SLAM algorithm that focuses on learning and mapping such visual long-term landmarks (LLamas). The algorithm therefore processes stereo image streams from several recording sessions in the same spatial area. The key part within LLama-SLAM is the assessment of the landmarks with quality values that are inferred as viewpoint dependent probabilities from observation statistics. By adding solely landmarks of high quality to the final LLama Map, it can be kept compact while still allowing reliable localization. Due to the long-term evaluation of the GNSS measurement during the sessions, the landmarks can be positioned precisely in a global referenced coordinate system. For a first assessment of the algorithm's capabilities, we present some experimental results from the mapping process combining three sessions recorded over two months on the same route.},
  issn = {2153-0017},
  keywords = {Cameras;Visualization;Simultaneous localization and mapping;Global navigation satellite system;Reliability;Probability;Optimization},
}

@INPROCEEDINGS{siva-zhang:2018:8461042,
  author = {S. Siva and H. Zhang},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition},
  volume = {},
  number = {},
  pages = {5175--5181},
  doi = {10.1109/ICRA.2018.8461042},
  year = {2018},
  month = {5},
  abstract = {Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first large-scale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.},
  issn = {2577-087X},
  keywords = {Feature extraction;Sensor phenomena and characterization;Simultaneous localization and mapping;Optimization},
}

@INPROCEEDINGS{zhang-et-al:2019:9013768,
  author = {S. Zhang and W. Wang and S. Tang and S. Jin and T. Jiang},
  booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
  title = {Localizing Backscatters by a Single Robot with Zero Start-Up Cost},
  volume = {},
  number = {},
  pages = {1--6},
  doi = {10.1109/GLOBECOM38437.2019.9013768},
  year = {2019},
  month = {12},
  abstract = {Recent years have witnessed the rapid proliferation of low- power backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such low-power backscatter tags is crucial for IoT-based smart services. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, increasing the deployment cost. To empower universal localization service, this paper presents Rover, an indoor localization system that simultaneously localizes multiple backscatter tags with zero start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing WiFi-based positioning measurements with inertial measurements to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues such as the interference among multiple tags and the real- time processing for solving the SLAM problem. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
  issn = {2576-6813},
  keywords = {Backscatter;Wireless fidelity;Antenna arrays;Interference;Simultaneous localization and mapping},
}

@ARTICLE{zhang-et-al:2020:2997393,
  author = {S. Zhang and W. Wang and S. Tang and S. Jin and T. Jiang},
  journal = {IEEE Transactions on Wireless Communications},
  title = {Robot-Assisted Backscatter Localization for IoT Applications},
  volume = {19},
  number = {9},
  pages = {5807--5818},
  doi = {10.1109/TWC.2020.2997393},
  year = {2020},
  month = {9},
  abstract = {Recent years have witnessed the rapid proliferation of backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such backscatter tags is crucial for IoT-based smart applications. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, which is laborious for deployment. To empower universal localization service, this paper presents Rover, an indoor localization system that localizes multiple backscatter tags without any start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing measurements from backscattered WiFi signals and inertial sensors to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues including interference among multiple tags, real-time processing, as well as the data marginalization problem in dealing with degenerated motions. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
  issn = {1558-2248},
  keywords = {Wireless fidelity;Backscatter;Interference;Robot sensing systems;Receivers;Antenna arrays;Backscatter;localization;inertial sensor;channel state information},
}

@INPROCEEDINGS{zhu-et-al:2021:9561584,
  author = {S. Zhu and X. Zhang and S. Guo and J. Li and H. Liu},
  booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Lifelong Localization in Semi-Dynamic Environment},
  volume = {},
  number = {},
  pages = {14389--14395},
  doi = {10.1109/ICRA48506.2021.9561584},
  year = {2021},
  month = {5},
  abstract = {Mapping and localization in non-static environments are fundamental problems in robotics. Most of previous methods mainly focus on static and highly dynamic objects in the environment, which may suffer from localization failure in semi-dynamic scenarios without considering objects with lower dynamics, such as parked cars and stopped pedestrians. In this paper, we introduce semantic mapping and lifelong localization approaches to recognize semi-dynamic objects in non-static environments. We also propose a generic framework that can integrate mainstream object detection algorithms with mapping and localization algorithms. The mapping method combines an object detection algorithm and a SLAM algorithm to detect semi-dynamic objects and constructs a semantic map that only contains semi-dynamic objects in the environment. During navigation, the localization method can classify observation corresponding to static and non-static objects respectively and evaluate whether those semi-dynamic objects have moved, to reduce the weight of invalid observation and localization fluctuation. Real-world experiments show that the proposed method can improve the localization accuracy of mobile robots in non-static scenarios.},
  issn = {2577-087X},
  keywords = {Location awareness;Simultaneous localization and mapping;Fluctuations;Navigation;Heuristic algorithms;Conferences;Semantics},
}

@INPROCEEDINGS{kanji:2019:8793482,
  author = {T. Kanji},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  title = {Detection-by-Localization: Maintenance-Free Change Object Detector},
  volume = {},
  number = {},
  pages = {4348--4355},
  doi = {10.1109/ICRA.2019.8793482},
  year = {2019},
  month = {5},
  abstract = {Recent researches demonstrate that selflocalization performance is a very useful measure of likelihood-of-change (LoC) for change detection. In this paper, this “detection-by-localization” scheme is studied in a novel generalized task of object-level change detection. In our framework, a given query image is segmented into object-level subimages (termed “scene parts”), which are then converted to subimagelevel pixel-wise LoC maps via the detection-by-localization scheme. Our approach models a self-localization system as a ranking function, outputting a ranked list of reference images, without requiring relevance score. Thanks to this new setting, we can generalize our approach to a broad class of selflocalization systems. We further propose an aggregation of different self-localization results from different queries so as to achieve higher precision. Our ranking based self-localization model allows to fuse self-localization results from different modalities via an unsupervised rank fusion derived from a field of multi-modal information retrieval (MMR). Our framework does not rely on the raw-score-merging hypothesis. Challenging experiments of cross-season change detection using the publicly available North Campus Long-Term (NCLT) dataset validates the efficacy of our proposed method.},
  issn = {2577-087X},
  keywords = {Image segmentation;Task analysis;Robots;Computational modeling;Visualization;Databases;Real-time systems},
}

@INPROCEEDINGS{kanji-et-al:2014:6942552,
  author = {T. Kanji and C. Yuuto and A. Masatoshi},
  booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title = {Mining visual phrases for long-term visual SLAM},
  volume = {},
  number = {},
  pages = {136--142},
  doi = {10.1109/IROS.2014.6942552},
  year = {2014},
  month = {9},
  abstract = {We propose a discriminative and compact scene descriptor for single-view place recognition that facilitates long-term visual SLAM in familiar, semi-dynamic and partially changing environments. In contrast to popular bag-of-words scene descriptors, which rely on a library of vector quantized visual features, our proposed scene descriptor is based on a library of raw image data (such as an available visual experience, images shared by other colleague robots, and publicly available image data on the web) and directly mine it to find visual phrases (VPs) that discriminatively and compactly explain an input query / database image. Our mining approach is motivated by recent success in the field of common pattern discovery-specifically mining of common visual patterns among scenes-and requires only a single library of raw images that can be acquired at different time or day. Experimental results show that even though our scene descriptor is significantly more compact than conventional descriptors it has a relatively higher recognition performance.},
  issn = {2153-0866},
  keywords = {Visualization;Libraries;Simultaneous localization and mapping;Computational modeling;Vectors;Visual databases},
}

@ARTICLE{krajník-et-al:2017:2665664,
  author = {T. Krajník and J. P. Fentanes and J. M. Santos and T. Duckett},
  journal = {IEEE Transactions on Robotics},
  title = {FreMEn: Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments},
  volume = {33},
  number = {4},
  pages = {964--977},
  doi = {10.1109/TRO.2017.2665664},
  year = {2017},
  month = {8},
  abstract = {We present a new approach to long-term mobile robot mapping in dynamic indoor environments. Unlike traditional world models that are tailored to represent static scenes, our approach explicitly models environmental dynamics. We assume that some of the hidden processes that influence the dynamic environment states are periodic and model the uncertainty of the estimated state variables by their frequency spectra. The spectral model can represent arbitrary timescales of environment dynamics with low memory requirements. Transformation of the spectral model to the time domain allows for the prediction of the future environment states, which improves the robot's long-term performance in changing environments. Experiments performed over time periods of months to years demonstrate that the approach can efficiently represent large numbers of observations and reliably predict future environment states. The experiments indicate that the model's predictive capabilities improve mobile robot localization and navigation in changing environments.},
  issn = {1941-0468},
  keywords = {Hidden Markov models;Mobile robots;Uncertainty;Robustness;Harmonic analysis;Navigation;Localization;long-term autonomy;mapping},
}

@INPROCEEDINGS{krajník-et-al:2016:7759671,
  author = {T. Krajník and J. P. Fentanes and M. Hanheide and T. Duckett},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Persistent localization and life-long mapping in changing environments using the Frequency Map Enhancement},
  volume = {},
  number = {},
  pages = {4558--4563},
  doi = {10.1109/IROS.2016.7759671},
  year = {2016},
  month = {10},
  abstract = {We present a lifelong mapping and localisation system for long-term autonomous operation of mobile robots in changing environments. The core of the system is a spatio-temporal occupancy grid that explicitly represents the persistence and periodicity of the individual cells and can predict the probability of their occupancy in the future. During navigation, our robot builds temporally local maps and integrates then into the global spatio-temporal grid. Through re-observation of the same locations, the spatio-temporal grid learns the long-term environment dynamics and gains the ability to predict the future environment states. This predictive ability allows to generate time-specific 2d maps used by the robot's localisation and planning modules. By analysing data from a long-term deployment of the robot in a human-populated environment, we show that the proposed representation improves localisation accuracy and the efficiency of path planning. We also show how to integrate the method into the ROS navigation stack for use by other roboticists.},
  issn = {2153-0866},
  keywords = {Navigation;Two dimensional displays;Robot sensing systems;Planning;Predictive models;Robot kinematics;mobile robotics;long-term autonomy},
}

@INPROCEEDINGS{krajník-et-al:2014:6943205,
  author = {T. Krajník and J. P. Fentanes and O. M. Mozos and T. Duckett and J. Ekekrantz and M. Hanheide},
  booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title = {Long-term topological localisation for service robots in dynamic environments using spectral maps},
  volume = {},
  number = {},
  pages = {4537--4542},
  doi = {10.1109/IROS.2014.6943205},
  year = {2014},
  month = {9},
  abstract = {This paper presents a new approach for topological localisation of service robots in dynamic indoor environments. In contrast to typical localisation approaches that rely mainly on static parts of the environment, our approach makes explicit use of information about changes by learning and modelling the spatio-temporal dynamics of the environment where the robot is acting. The proposed spatio-temporal world model is able to predict environmental changes in time, allowing the robot to improve its localisation capabilities during long-term operations in populated environments. To investigate the proposed approach, we have enabled a mobile robot to autonomously patrol a populated environment over a period of one week while building the proposed model representation. We demonstrate that the experience learned during one week is applicable for topological localization even after a hiatus of three months by showing that the localization error rate is significantly lower compared to static environment representations.},
  issn = {2153-0866},
  keywords = {Mathematical model;Three-dimensional displays;Predictive models;Fourier transforms;Feature extraction;Service robots;topological localisation;mobile robotics;spatio-temporal representations},
}

@INPROCEEDINGS{naseer-et-al:2017:7989305,
  author = {T. Naseer and G. L. Oliveira and T. Brox and W. Burgard},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Semantics-aware visual localization under challenging perceptual conditions},
  volume = {},
  number = {},
  pages = {2614--2620},
  doi = {10.1109/ICRA.2017.7989305},
  year = {2017},
  month = {5},
  abstract = {Visual place recognition under difficult perceptual conditions remains a challenging problem due to changing weather conditions, illumination and seasons. Long-term visual navigation approaches for robot localization should be robust to these dynamics of the environment. Existing methods typically leverage feature descriptions of whole images or image regions from Deep Convolutional Neural Networks. Some approaches also exploit sequential information to alleviate the problem of spatially inconsistent and non-perfect image matches. In this paper, we propose a novel approach for learning a discriminative holistic image representation which exploits the image content to create a dense and salient scene description. These salient descriptions are learnt over a variety of datasets under large perceptual changes. Such an approach enables us to precisely segment the regions of an image which are geometrically stable over large time lags. We combine features from these salient regions and an off-the-shelf holistic representation to form a more robust scene descriptor. We also introduce a semantically labeled dataset which captures extreme perceptual and structural scene dynamics over the course of 3 years. We evaluated our approach with extensive experiments on data collected over several kilometers in Freiburg and show that our learnt image representation outperforms off-the-shelf features from the deep networks and hand-crafted features.},
  issn = {},
  keywords = {Robustness;Image segmentation;Visualization;Training;Feature extraction;Semantics;Computer architecture},
}

@INPROCEEDINGS{yoshiki-et-al:2018:8569294,
  author = {T. Yoshiki and T. Kanji and Y. Naiming},
  booktitle = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
  title = {Scalable Change Detection from 3D Point Cloud Maps: Invariant Map Coordinate for Joint Viewpoint-Change Localization},
  volume = {},
  number = {},
  pages = {1115--1121},
  doi = {10.1109/ITSC.2018.8569294},
  year = {2018},
  month = {11},
  abstract = {This study addresses the problem of visual change detection using a 3D point cloud (PC) map acquired by a car-like robot. With recent advances in long-term autonomous navigation, change detection under global viewpoint uncertainty has become a topic of considerable interest. In our study, we extend the traditional two-level pipeline of change detection: (1) scene registration and (2) scene comparison, to enable scalable and efficient change detection. In the traditional pipeline, the registration stage is required to align a given scene pair (i.e., query and reference PC maps) that are taken at different times into the same coordinate system, before comparing the two PCs. However, the registration stage is a time-consuming step, which makes it harder to realize a scalable change detection. Our key concept is to transform every query or reference PC beforehand into an invariant coordinate system, which should be predefined and invariant to environment changes (e.g., dynamic objects, clutters, the mapper vehicle's trajectories), so as to enable a direct comparison of spatial layout between the two different maps. The proposed framework employs an efficient bag-of-local-features (BoLF) scene model and realizes a scalable joint viewpoint-change detection. Change detection experiments using a publicly available cross-season NCLT dataset validate the efficacy of the approach.},
  issn = {2153-0017},
  keywords = {Three-dimensional displays;Task analysis;Layout;Visualization;Feature extraction;Uncertainty;Robots},
}

@INPROCEEDINGS{bichucher-et-al:2015:7404433,
  author = {V. Bichucher and J. M. Walls and P. Ozog and K. A. Skinner and R. M. Eustice},
  booktitle = {OCEANS 2015 - MTS/IEEE Washington},
  title = {Bathymetric factor graph SLAM with sparse point cloud alignment},
  volume = {},
  number = {},
  pages = {1--7},
  doi = {10.23919/OCEANS.2015.7404433},
  year = {2015},
  month = {10},
  abstract = {This paper reports on a factor graph simultaneous localization and mapping framework for autonomous underwater vehicle localization based on terrain-aided navigation. The method requires no prior bathymetric map and only assumes that the autonomous underwater vehicle has the ability to sparsely sense the local water column depth, such as with a bottom-looking Doppler velocity log. Since dead-reckoned navigation is accurate in short time windows, the vehicle accumulates several water column depth point clouds- or submaps-during the course of its survey. We propose an xy-alignment procedure between these submaps in order to enforce consistent bathymetric structure over time, and therefore attempt to bound long-term navigation drift. We evaluate the submap alignment method in simulation and present performance results from multiple autonomous underwater vehicle field trials.},
  issn = {},
  keywords = {Simultaneous localization and mapping;Three-dimensional displays;Vehicles;Trajectory;Smoothing methods;Global Positioning System},
}

@ARTICLE{ali-et-al:2021:3100882,
  author = {W. Ali and P. Liu and R. Ying and Z. Gong},
  journal = {IEEE Sensors Journal},
  title = {A Life-Long SLAM Approach Using Adaptable Local Maps Based on Rasterized LIDAR Images},
  volume = {21},
  number = {19},
  pages = {21740--21749},
  doi = {10.1109/JSEN.2021.3100882},
  year = {2021},
  month = {10},
  abstract = {Most real-time autonomous robot applications require a robot to traverse through a dynamic space for a long time. In some cases, a robot needs to work in the same environment. Such applications give rise to the problem of a life-long SLAM system. Life-long SLAM presents two main challenges i.e. the tracking should not fail in a dynamic environment and the need for a robust and efficient mapping strategy. The system should update maps with new information; while also keeping track of older observations. But, mapping for a long time can require higher computational requirements. In this paper, we propose a solution to the problem of life-long SLAM. We represent the global map as a set of rasterized images of local maps along with a map management system responsible for updating local maps and keeping track of older values. We also present an efficient approach of using the bag of visual words method for loop closure detection and relocalization. We evaluate the performance of our system on the KITTI dataset and an indoor dataset. Our loop closure system reported recall and precision of above 90 percent. The computational cost of our system is much lower as compared to state-of-the-art methods. Our method reports lower computational requirements even for long-term operation.},
  issn = {1558-1748},
  keywords = {Simultaneous localization and mapping;Three-dimensional displays;Feature extraction;Robots;Databases;Laser radar;Sensors;Laser scanning;place recognition;bag of words;rasterization;mapping;simultaneous localization;mapping},
}

@INPROCEEDINGS{ding-et-al:2019:8968550,
  author = {X. Ding and Y. Wang and L. Tang and H. Yin and R. Xiong},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Communication constrained cloud-based long-term visual localization in real time},
  volume = {},
  number = {},
  pages = {2159--2166},
  doi = {10.1109/IROS40897.2019.8968550},
  year = {2019},
  month = {11},
  abstract = {Visual localization is one of the primary capabilities for mobile robots. Long-term visual localization in real time is particularly challenging, in which the robot is required to efficiently localize itself using visual data where appearance may change significantly over time. In this paper, we propose a cloud-based visual localization system targeting at long-term localization in real time. On the robot, we employ two estimators to achieve accurate and real-time performance. One is a sliding-window based visual inertial odometry, which integrates constraints from consecutive observations and self-motion measurements, as well as the constraints induced by localization results from the cloud. This estimator builds a local visual submap as the virtual observation which is then sent to the cloud as new localization constraints. The other one is a delayed state Extended Kalman Filter to fuse the pose of the robot localized from the cloud, the local odometry and the high-frequency inertial measurements. On the cloud, we propose a longer sliding-window based localization method to aggregate the virtual observations for larger field of view, leading to more robust alignment between virtual observations and the map. Under this architecture, the robot can achieve drift-free and real-time localization using onboard resources even in a network with limited bandwidth, high latency and existence of package loss, which enables the autonomous navigation in real-world environment. We evaluate the effectiveness of our system on a dataset with challenging seasonal and illuminative variations. We further validate the robustness of the system under challenging network conditions.},
  issn = {2153-0866},
  keywords = {},
}

@INPROCEEDINGS{hu-et-al:2018:8623046,
  author = {X. Hu and J. Wang and W. Chen},
  booktitle = {2018 Chinese Automation Congress (CAC)},
  title = {Long-term Localization of Mobile Robots in Dynamic Changing Environments},
  volume = {},
  number = {},
  pages = {384--389},
  doi = {10.1109/CAC.2018.8623046},
  year = {2018},
  month = {11},
  abstract = {Long-term localization in dynamic changing environments is still a challenge in robotics. Traditional localization algorithms typically assume that the environment is static. However, in many real-world applications, such as parking lots and industrial plants, there are always dynamic objects (e.g. moving people) and semi-dynamic objects (e.g. parked cars and placed goods). In this paper we address this challenge by introducing a long-term localization algorithm in the environments which combine dynamic objects and semi-dynamic objects. Localizability-based-updating particle filter (LU-P F) algorithm is proposed here. Not only we use localizability matric to build an updating mechanism, but also it is used for localization system. Besides, we propose the dynamic factor as long-memory information to serve as prior knowledge, which improves the robustness of updating process. Experiments in parking lots demonstrate that our approach has better localization results with a more accurate up-to-date map compared to other methods.},
  issn = {},
  keywords = {Heuristic algorithms;Robots;Measurement by laser beam;Laboratories;Particle filters;Hidden Markov models;Mathematical model;long-term localization;map updating mechanism;localizability;dynamic factor},
}

@INPROCEEDINGS{bouaziz-et-al:2021:9378614,
  author = {Y. Bouaziz and E. Royer and G. Bresson and M. Dhome},
  booktitle = {2021 IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI)},
  title = {Keyframes retrieval for robust long-term visual localization in changing conditions},
  volume = {},
  number = {},
  pages = {000093--000100},
  doi = {10.1109/SAMI50585.2021.9378614},
  year = {2021},
  month = {1},
  abstract = {Appearance changes are a challenge for visual localization in outdoor environments. Revisiting familiar places but retrieving keyframes that were taken under different environmental condition can result in inaccurate localization. To overcome this difficulty, we propose a localization approach able to take advantage of a visual landmark map composed of N sequences gathered at different times and conditions. During this localization process, we exploit information collected in the beginning of the trajectory to compute a ranking function which will be used in the rest of the trajectory to retrieve from the map the keyframes that maximise the number of matched points. The retrieval depends on the geometric distance between the pose of the keyframe and the current pose of the vehicle, and the similarity of this keyframe with the current environmental condition. The results demonstrate that our approach has significantly improved localization performance in challenging conditions (snow, rain, change of season ...).},
  issn = {},
  keywords = {Location awareness;Visualization;Rain;Navigation;Snow;Probabilistic logic;Trajectory;Visual-Based Navigation;Computer Vision for Transportation;SLAM},
}

@INPROCEEDINGS{furuta-et-al:2018:8594481,
  author = {Y. Furuta and K. Okada and Y. Kakiuchi and M. Inaba},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {An Everyday Robotic System that Maintains Local Rules Using Semantic Map Based on Long-Term Episodic Memory},
  volume = {},
  number = {},
  pages = {1--7},
  doi = {10.1109/IROS.2018.8594481},
  year = {2018},
  month = {10},
  abstract = {To enable robots to work on real home environments, they have to not only consider common knowledge in the global society, but also be aware of existing rules there. Since such “local rules” are not describable beforehand, robot agents must acquire them through their lives after deployment. To achieve this, we developed a framework that a) lets robots record long-term episodic memories in their deployed environments, b) autonomously builds probabilistic object localization map as structurization of logged data and c) make adapted task plans based on the map. We equipped our framework on PR2 and Fetch robots operating and recording episodic memory for 41 days with semantic common knowledge of the environment. We also conducted demonstrations in which a PR2 robot tidied up a room, showing that the robot agent can successfully plan and execute local-rule-aware home assistive tasks by using our proposed framework.},
  issn = {2153-0866},
  keywords = {Task analysis;Probabilistic logic;Semantics;Planning;Robot sensing systems;Solid modeling;Service Robots;Learning and Adaptive Systems;Big Data in Robotics and Automation},
}

@ARTICLE{li-et-al:2020:3015456,
  author = {Y. Li and N. Brasch and Y. Wang and N. Navab and F. Tombari},
  journal = {IEEE Robotics and Automation Letters},
  title = {Structure-SLAM: Low-Drift Monocular SLAM in Indoor Environments},
  volume = {5},
  number = {4},
  pages = {6583--6590},
  doi = {10.1109/LRA.2020.3015456},
  year = {2020},
  month = {10},
  abstract = {In this letter a low-drift monocular SLAM method is proposed targeting indoor scenarios, where monocular SLAM often fails due to the lack of textured surfaces. Our approach decouples rotation and translation estimation of the tracking process to reduce the long-term drift in indoor environments. In order to take full advantage of the available geometric information in the scene, surface normals are predicted by a convolutional neural network from each input RGB image in real-time. First, a drift-free rotation is estimated based on lines and surface normals using spherical mean-shift clustering, leveraging the weak Manhattan World assumption. Then translation is computed from point and line features. Finally, the estimated poses are refined with a map-to-frame optimization strategy. The proposed method outperforms the state of the art on common SLAM benchmarks such as ICL-NUIM and TUM RGB-D.},
  issn = {2377-3766},
  keywords = {Simultaneous localization and mapping;Feature extraction;Pose estimation;Indoor environments;Cameras;Three-dimensional displays;SLAM;visual learning},
}

@ARTICLE{tang-et-al:2019:2824766,
  author = {Y. Tang and Y. Hu and J. Cui and F. Liao and M. Lao and F. Lin and R. S. H. Teo},
  journal = {IEEE Transactions on Industrial Electronics},
  title = {Vision-Aided Multi-UAV Autonomous Flocking in GPS-Denied Environment},
  volume = {66},
  number = {1},
  pages = {616--626},
  doi = {10.1109/TIE.2018.2824766},
  year = {2019},
  month = {1},
  abstract = {This paper presents a sophisticated vision-aided flocking system for unmanned aerial vehicles (UAVs), which is able to operate in GPS-denied unknown environments for exploring and searching missions, and also able to adopt two types of vision sensors, day and thermal cameras, to measure relative motion between UAVs in different lighting conditions without using wireless communication. In order to realize robust vision-aided flocking, an integrated framework of tracking-learning-detection on the basis of multifeature coded correlation filter has been developed. To achieve long-term tracking, a redetector is trained online to adaptively reinitialize target for global sensing. An advanced flocking strategy is developed to address the autonomous multi-UAVs' cooperative flight. Light detection and ranging (LiDAR)-based navigation modules are developed for autonomous localization, mapping, and obstacle avoidance. Flight experiments of a team of UAVs have been conducted to verify the performance of this flocking system in a GPS-denied environment. The extensive experiments validate the robustness of the proposed vision algorithms in challenging scenarios.},
  issn = {1557-9948},
  keywords = {Target tracking;Sensors;Cameras;Correlation;Trajectory;Robustness;Visualization;Flocking;unmanned system;visual sensing},
}

@INPROCEEDINGS{yue-et-al:2020:9197072,
  author = {Y. Yue and C. Yang and J. Zhang and M. Wen and Z. Wu and H. Zhang and D. Wang},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Day and Night Collaborative Dynamic Mapping in Unstructured Environment Based on Multimodal Sensors},
  volume = {},
  number = {},
  pages = {2981--2987},
  doi = {10.1109/ICRA40945.2020.9197072},
  year = {2020},
  month = {5},
  abstract = {Enabling long-term operation during day and night for collaborative robots requires a comprehensive understanding of the unstructured environment. Besides, in the dynamic environment, robots must be able to recognize dynamic objects and collaboratively build a global map. This paper proposes a novel approach for dynamic collaborative mapping based on multimodal environmental perception. For each mission, robots first apply heterogeneous sensor fusion model to detect humans and separate them to acquire static observations. Then, the collaborative mapping is performed to estimate the relative position between robots and local 3D maps are integrated into a globally consistent 3D map. The experiment is conducted in the day and night rainforest with moving people. The results show the accuracy, robustness, and versatility in 3D map fusion missions.},
  issn = {2577-087X},
  keywords = {Collaboration;Three-dimensional displays;Simultaneous localization and mapping;Cameras;Robot vision systems},
}

@INPROCEEDINGS{zhang-et-al:2016:7831835,
  author = {Y. Zhang and A. Chao and B. Zhao and H. Liu and X. Zhao},
  booktitle = {2016 IEEE International Conference on Information and Automation (ICIA)},
  title = {Migratory birds-inspired navigation system for unmanned aerial vehicles},
  volume = {},
  number = {},
  pages = {276--281},
  doi = {10.1109/ICInfA.2016.7831835},
  year = {2016},
  month = {8},
  abstract = {Migration birds are able to navigate themselves during a long-distance journey without getting lost. They actually achieve just what is being sought for in the field of Unmanned Aerial Vehicles (UAVs): long-term autonomous navigation. This paper proposes an approach that combines the migration birds' sense principles with Micro-Electro-Mechanical System (MEMS) sensors to estimate UAVs position within GPS-denied environments. Camera, orientation and web-based maps (such as Google/Baidu Maps) are chosen to simulate the birds' localization cues: vision, earth magnetic field and mental maps. The visual odometry, Particle Filter theories are used in the proposed approach to integrate multiple sensor measurements. Real flying experiments are conducted both in indoor and outdoor environments. The results validate that the proposed migration-inspired visual odometry system can estimate the UAV localization effectively.},
  issn = {},
  keywords = {Cameras;Visualization;Unmanned aerial vehicles;Birds;Sensors;Navigation;Optical imaging;Migration birds;Unmanned Aerial Vehicles;Navigation},
}

@INPROCEEDINGS{zhu-et-al:2019:9010305,
  author = {Y. Zhu and B. Xue and L. Zheng and H. Huang and M. Liu and R. Fan},
  booktitle = {2019 IEEE International Conference on Imaging Systems and Techniques (IST)},
  title = {Real-Time, Environmentally-Robust 3D LiDAR Localization},
  volume = {},
  number = {},
  pages = {1--6},
  doi = {10.1109/IST48021.2019.9010305},
  year = {2019},
  month = {12},
  abstract = {Localization, or position fixing, is an important problem in robotics research. In this paper, we propose a novel approach for long-term localization in a changing environment using 3D LiDAR. We first create the map of a real environment using GPS and LiDAR. Then, we divide the map into several small parts as the targets for cloud registration, which can not only improve the robustness but also reduce the registration time. We proposed a localization method called PointLocalization. PointLocalization allows us to fuse different kinds of odometers, which can optimize the accuracy and frequency of localization results. We evaluate our algorithm on an unmanned ground vehicle (UGV) using LiDAR and a wheel encoder, and obtain the localization results at more than 20 Hz after fusion. The algorithm can also localize the UGV in a 180-degree field of view (FOV). Using an outdated map captured six months ago, this algorithm shows great robustness, and the test results show that it can achieve an accuracy of 10 cm. PointLocalization has been tested for a period of more than six months in a crowded factory and has operated successfully over a distance of more than 2000 km.},
  issn = {1558-2809},
  keywords = {Laser radar;Global Positioning System;Simultaneous localization and mapping;Wheels;Cameras;Three-dimensional displays},
}

@ARTICLE{chen-et-al:2018:2859916,
  author = {Z. Chen and L. Liu and I. Sa and Z. Ge and M. Chli},
  journal = {IEEE Robotics and Automation Letters},
  title = {Learning Context Flexible Attention Model for Long-Term Visual Place Recognition},
  volume = {3},
  number = {4},
  pages = {4015--4022},
  doi = {10.1109/LRA.2018.2859916},
  year = {2018},
  month = {10},
  abstract = {Identifying regions of interest in an image has long been of great importance in a wide range of tasks, including place recognition. In this letter, we propose a novel attention mechanism with flexible context, which can be incorporated into existing feedforward network architecture to learn image representations for long-term place recognition. In particular, in order to focus on regions that contribute positively to place recognition, we introduce a multiscale context-flexible network to estimate the importance of each spatial region in the feature map. Our model is trained end-to-end for place recognition and can detect regions of interest of arbitrary shape. Extensive experiments have been conducted to verify the effectiveness of our approach and the results demonstrate that our model can achieve consistently better performance than the state of the art on standard benchmark datasets. Finally, we visualize the learned attention maps to generate insights into what attention the network has learned.},
  issn = {2377-3766},
  keywords = {Feature extraction;Visualization;Image recognition;Task analysis;Context modeling;Shape;Data mining;Localization;deep learning in robotics and automation;visual-based navigation},
}

@INPROCEEDINGS{li-et-al:2020:9274964,
  author = {Z. Li and H. Yu and T. Shen and Z. Li},
  booktitle = {2020 3rd International Conference on Unmanned Systems (ICUS)},
  title = {Segmented Matching Method of Multi-Geophysics Field SLAM Data Based on LSTM},
  volume = {},
  number = {},
  pages = {147--151},
  doi = {10.1109/ICUS50048.2020.9274964},
  year = {2020},
  month = {11},
  abstract = {At present, simultaneous localization and mapping (SLAM) has become an important method for autonomous underwater vehicles (AUVs) to realize long-term navigation. However, using only bathymetric data in unknown environment has its own disadvantages, that are low precision and large computational load. To tackle with requirements of high-precision navigation under large-scale and long-term voyage condition, a SLAM method and corresponding matching algorithm for integrating multi-geophysical field data are proposed. By dividing the feature data and location data of geophysical field obtained into various submaps and sub-segments during AUV sailing, the dominant navigation data of each segment is identified using long short-term memory network. Validity of the proposed method is done by simulation experiments. During the simulation, the loop closure detection of each submap is used, and the matching counter is set to check the correct matching rate. Finally, the matching results with single geophysics field data under the same conditions are compared with multi-geophysics field data and analyzed. The experimental results have demonstrated the feasibility and correctness of the proposed method.},
  issn = {},
  keywords = {Technological innovation;Underwater vehicles;Timing;Simultaneous localization and mapping;Navigation;SLAM;LSTM;navigation;multi-geophysics field data;matching},
}

@INPROCEEDINGS{li-et-al:2019:8662397,
  author = {Z. Li and Y. Chen and L. Gong and L. Liu and D. Sylvester and D. Blaauw and H.-S. Kim},
  booktitle = {2019 IEEE International Solid- State Circuits Conference - (ISSCC)},
  title = {An 879GOPS 243mW 80fps VGA Fully Visual CNN-SLAM Processor for Wide-Range Autonomous Exploration},
  volume = {},
  number = {},
  pages = {134--136},
  doi = {10.1109/ISSCC.2019.8662397},
  year = {2019},
  month = {2},
  abstract = {Simultaneous localization and mapping (SLAM) estimates an agent's trajectory for all six degrees of freedom (6 DoF) and constructs a 3D map of an unknown surrounding. It is a fundamental kernel that enables head-mounted augmented/virtual reality devices and autonomous navigation of micro aerial vehicles. A noticeable recent trend in visual SLAM is to apply computationand memory-intensive convolutional neural networks (CNNs) that outperform traditional hand-designed feature-based methods [1]. For each video frame, CNN-extracted features are matched with stored keypoints to estimate the agent's 6-DoF pose by solving a perspective-n-points (PnP) non-linear optimization problem (Fig. 7.3.1, left). The agent's long-term trajectory over multiple frames is refined by a bundle adjustment process (BA, Fig. 7.3.1 right), which involves a large-scale (~120 variables) non-linear optimization. Visual SLAM requires massive computation (>250GOP/s) in the CNN-based feature extraction and matching, as well as datadependent dynamic memory access and control flow with high-precision operations, creating significant low-power design challenges. Software implementations are impractical, resulting in 0.2s runtime with a ~3GHz CPU+ GPU system with >100MB memory footprint and >100W power consumption. Prior ASICs have implemented either an incomplete SLAM system [2,3] that lacks estimation of ego-motion or employed a simplified (non-CNN) feature extraction and tracking [2,4,5] that limits SLAM quality and range. A recent ASIC [5] augments visual SLAM with an off-chip high-precision inertial measurement unit (IMU), simplifying the computational complexity, but incurring additional power and cost overhead.},
  issn = {2376-8606},
  keywords = {Simultaneous localization and mapping;Engines;Three-dimensional displays;Two dimensional displays;Feature extraction;Visualization;Trajectory},
}

@INPROCEEDINGS{xin-et-al:2017:8310121,
  author = {Z. Xin and X. Cui and J. Zhang and Y. Yang and Y. Wang},
  booktitle = {2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)},
  title = {Visual place recognition with CNNs: From global to partial},
  volume = {},
  number = {},
  pages = {1--6},
  doi = {10.1109/IPTA.2017.8310121},
  year = {2017},
  month = {11},
  abstract = {Visual place recognition is one of the most challenging problems in computer vision, due to the large diversities that real-world places can represent. Recently, visual place recognition has become a key part of loop closure detection and topological localization in long-term mobile robot autonomy. In this work, we build up a novel visual place recognition pipeline composed of a first filtering stage followed by a partial reranking process. In the filtering stage, image-wise features are utilized to find a small set of potential places. Afterwards, stable region-wise landmarks are extracted for more accurate matching in the partial reranking process. All global and partial image representations are derived from pre-trained Convolutional Neural Networks (CNNs), and the landmarks are extracted by object proposal techniques. Moreover, a new similarity measurement is provided by considering both spatial and scale distribution of landmarks. Compared with current methods only considering scale distribution, the presented similarity measurement can benefit recognition precision and robustness effectively. Experiments with varied viewpoints and environmental conditions demonstrate that the proposed method achieves superior performance against state-of-the-art methods.},
  issn = {2154-512X},
  keywords = {Feature extraction;Visualization;Robustness;Proposals;Lighting;Pipelines;Convolutional neural networks;visual place recognition;localization;convolutional neural networks;long-term environment},
}

@ARTICLE{du-et-al:2022:3028218,
  author = {Z.-J. Du and S.-S. Huang and T.-J. Mu and Q. Zhao and R. R. Martin and K. Xu},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  title = {Accurate Dynamic SLAM Using CRF-Based Long-Term Consistency},
  volume = {28},
  number = {4},
  pages = {1745--1757},
  doi = {10.1109/TVCG.2020.3028218},
  year = {2022},
  month = {4},
  abstract = {Accurate camera pose estimation is essential and challenging for real world dynamic 3D reconstruction and augmented reality applications. In this article, we present a novel RGB-D SLAM approach for accurate camera pose tracking in dynamic environments. Previous methods detect dynamic components only across a short time-span of consecutive frames. Instead, we provide a more accurate dynamic 3D landmark detection method, followed by the use of long-term consistency via conditional random fields, which leverages long-term observations from multiple frames. Specifically, we first introduce an efficient initial camera pose estimation method based on distinguishing dynamic from static points using graph-cut RANSAC. These static/dynamic labels are used as priors for the unary potential in the conditional random fields, which further improves the accuracy of dynamic 3D landmark detection. Evaluation using the TUM and Bonn RGB-D dynamic datasets shows that our approach significantly outperforms state-of-the-art methods, providing much more accurate camera trajectory estimation in a variety of highly dynamic environments. We also show that dynamic 3D reconstruction can benefit from the camera poses estimated by our RGB-D SLAM approach.},
  issn = {1941-0506},
  keywords = {Cameras;Simultaneous localization and mapping;Three-dimensional displays;Visualization;Pose estimation;Dynamics;Robustness;RGB-D SLAM;dynamic SLAM;long-term consistency;conditional random fields,graph-cut RANSAC},
}

