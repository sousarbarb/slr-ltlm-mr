@inproceedings{18955253 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {SDF-Loc: signed Distance Field based 2D Relocalization and Map Update in Dynamic Environments},
journal = {2019 American Control Conference (ACC)},
author = {Mingming Zhang and Yiming Chen and Mingyang Li},
year = {2019//},
pages = {1997 - 2004},
address = {Piscataway, NJ, USA},
abstract = {To empower an autonomous robot to perform long-term navigation in a given area, a concurrent localization and map update algorithm is required. In this paper, we tackle this problem by providing both theoretical analysis and algorithm design for robotic systems equipped with 2D laser range finders. The first key contribution of this paper is that we propose a hybrid signed distance field (SDF) framework for laser based localization. The proposed hybrid SDF integrates two methods with complementary characteristics, namely Euclidean SDF (ESDF) and Truncated SDF (TSDF). With our framework, accurate pose estimation and fast map update can be performed simultaneously. Moreover, we introduce a novel sliding window estimator which attains better accuracy by consistently utilizing sensor and map information with both scan-to-scan and scan-to-map data association. Real-world experimental results demonstrate that the proposed algorithm can be used for commercial robots in various environments with long-term usage. Experiments also show that our approach outperforms competing approaches by a wide margin.},
keywords = {laser ranging;mobile robots;path planning;pose estimation;robot vision;sensor fusion;SLAM (robots);},
note = {robotic systems;2D laser range finders;hybrid signed distance field framework;laser based localization;hybrid SDF;complementary characteristics;Euclidean SDF;fast map update;sensor;map information;scan-to-map data association;long-term usage;SDF-loc;2D relocalization;dynamic environments;autonomous robot;long-term navigation;concurrent localization;theoretical analysis;scan-to-scan data association;pose estimation;signed distance field;truncated SDF;sliding window estimator;},
} 


@article{19687591 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Onboard detection-tracking-localization},
journal = {IEEE/ASME Transactions on Mechatronics},
journal = {IEEE/ASME Trans. Mechatron. (USA)},
author = {Dengqing Tang and Qiang Fang and Lincheng Shen and Tianjiang Hu},
volume = { 25},
number = { 3},
year = {2020/06/},
pages = {1555 - 65},
issn = {1083-4435},
address = {USA},
abstract = {This article investigates long-term positioning of moving objects by monocular vision of a miniature fixed-wing unmanned aerial vehicle. It is challenging to perform a real-time onboard vision processing task, due to the strict payload capacity and power budget limitations of microflying vehicles. We propose a parallel onboard architecture that explicitly decouples the long-term positioning task into iteratively operated detection, tracking, and localization. The proposed approach is eventually called onboard detection-tracking-localization, namely oDTL. The detector automatically extracts and identifies the object from image frames captured at in-flight durations. A learning-based network is constructed to improve detection accuracy and robustness against ever-changing outdoor illumination conditions and flying viewpoints. The tracker follows the object within specified region-of-interest from frame to frame with lower computing consumption. To further reduce target-losing rate, a concept of blind zone is proposed and applied, and its boundaries in sequential images are also theoretically inferred. The position estimator maps the flying vehicle pose, the image coordinates, and calibration specifications into real-world positions of the moving target. An extended Kalman filter is developed for rough position estimation, and a smooth module is introduced for the refinement of the position. Three offline comparative experiments and three online experiments have been conducted respectively to testify the real-time capability of our approach. The collected experimental results also demonstrate the feasible accuracy and robustness of the overall solution within the specified flying onboard scenarios.},
keywords = {aerospace components;aircraft control;autonomous aerial vehicles;calibration;feature extraction;image capture;image sensors;image sequences;Kalman filters;learning (artificial intelligence);mobile robots;nonlinear filters;object detection;position control;robot vision;target tracking;},
note = {onboard detection-tracking-localization;monocular vision;miniature fixed-wing unmanned aerial vehicle;vision processing task;strict payload capacity;power budget limitations;microflying vehicles;parallel onboard architecture;long-term positioning task;iteratively operated detection;image frames;robustness;outdoor illumination conditions;position estimator maps;flying vehicle;rough position estimation;learning-based network;image sequences;extended Kalman filter;region-of-interest;},
URL = {http://dx.doi.org/10.1109/TMECH.2020.2976794},
} 


@article{18658379 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Selective memory: Recalling relevant experience for long-term visual localization},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {MacTavish, K. and Paton, M. and Barfoot, T.D.},
volume = { 35},
number = { 8},
year = {2018/12/},
pages = {1265 - 92},
issn = {1556-4959},
address = {USA},
abstract = {Visual navigation is a key enabling technology for autonomous mobile vehicles. The ability to provide large-scale, long-term navigation using low-cost, low-power vision sensors is appealing for industrial applications. A crucial requirement for long-term navigation systems is the ability to localize in environments whose appearance is constantly changing over time-due to lighting, weather, seasons, and physical changes. This paper presents a multiexperience localization (MEL) system that uses a powerful map representation-storing every visual experience in layers-that does not make assumptions about underlying appearance modalities and generators. Our localization system provides real-time performance by selecting online, a subset of experiences against which to localize. We achieve this task through a novel experience-triage algorithm based on collaborative filtering, which selects experiences relevant to the <i>live view</i>, outperforming competing techniques. Based on classical memory-based recommender systems, this technique also enables landmark-level recommendations, is entirely online, and requires no training data. We demonstrate the capabilities of the MEL system in the context of long-term autonomous path following in unstructured outdoor environments with a challenging 100-day field experiment through day, night, snow, spring, and summer. We furthermore provide offline analysis comparing our system to several state-of-the-art alternatives. We show that the combination of the novel methods presented in this paper enable full use of incredibly rich multiexperience maps, opening the door to robust long-term visual localization.},
keywords = {cartography;image sensors;mobile robots;navigation;path planning;recommender systems;robot vision;},
note = {map representation;visual experience storage;experience-triage algorithm;collaborative filtering;memory-based recommender systems;landmark-level recommendations;unstructured outdoor environments;multiexperience maps;long-term autonomous path;MEL system;appearance modalities;multiexperience localization system;long-term navigation systems;low-power vision sensors;autonomous mobile vehicles;visual navigation;long-term visual localization;selective memory;},
URL = {http://dx.doi.org/10.1002/rob.21838},
} 


@article{17199501 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Survey on advances on terrain based navigation for autonomous underwater vehicles},
journal = {Ocean Engineering},
journal = {Ocean Eng. (Netherlands)},
author = {Melo, J. and Matos, A.},
volume = { 139},
year = {2017/07/15},
pages = {250 - 64},
issn = {0029-8018},
address = {Netherlands},
abstract = {The autonomy of robotic underwater vehicles is dependent on the ability to perform long-term and long-range missions without need of human intervention. While current state-of-the-art underwater navigation techniques are able to provide sufficient levels of precision in positioning, they require the use of support vessels or acoustic beacons. This can pose limitations on the size of the survey area, but also on the whole cost of the operations. Terrain Based Navigation is a sensor-based navigation technique that bounds the error growth of dead-reckoning using a map with terrain information, provided that there is enough terrain variability. An obvious advantage of Terrain Based Navigation is the fact that no external aiding signals or devices are required. Because of this unique feature, terrain navigation has the potential to dramatically improve the autonomy of Autonomous Underwater Vehicles (AUVs). This paper consists on a comprehensive survey on the recent developments for Terrain Based Navigation methods proposed for AUVs. The survey includes a brief introduction to the original Terrain Based Navigation formulations, as well as a description of the algorithms, and a list of the different implementation alternatives found in the literature. Additionally, and due to the relevance, Bathymetric SLAM techniques will also be discussed. [All rights reserved Elsevier].},
keywords = {autonomous underwater vehicles;bathymetry;marine navigation;mobile robots;path planning;sensors;SLAM (robots);},
note = {autonomous underwater vehicles;robotic underwater vehicles;long-range missions;underwater navigation techniques;terrain information;terrain variability;Terrain Based Navigation methods;original Terrain;Navigation formulations;long-term missions;positioning;sensor-based navigation;AUV;Bathymetric SLAM;},
URL = {http://dx.doi.org/10.1016/j.oceaneng.2017.04.047},
} 


@inproceedings{15278280 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Towards life-long visual localization using an efficient matching of binary sequences from images},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Arroyo, R. and Alcantarilla, P.F. and Bergasa, L.M. and Romera, E.},
year = {2015//},
pages = {6328 - 35},
address = {Piscataway, NJ, USA},
abstract = {Life-long visual localization is one of the most challenging topics in robotics over the last few years. The difficulty of this task is in the strong appearance changes that a place suffers due to dynamic elements, illumination, weather or seasons. In this paper, we propose a novel method (ABLE-M) to cope with the main problems of carrying out a robust visual topological localization along time. The novelty of our approach resides in the description of sequences of monocular images as binary codes, which are extracted from a global LDB descriptor and efficiently matched using FLANN for fast nearest neighbor search. Besides, an illumination invariant technique is applied. The usage of the proposed binary description and matching method provides a reduction of memory and computational costs, which is necessary for long-term performance. Our proposal is evaluated in different life-long navigation scenarios, where ABLE-M outperforms some of the main state-of-the-art algorithms, such as WI-SURF, BRIEF-Gist, FAB-MAP or SeqSLAM. Tests are presented for four public datasets where a same route is traversed at different times of day or night, along the months or across all four seasons.},
keywords = {binary codes;image matching;pattern classification;robot vision;},
note = {life-long visual localization;binary sequences;image matching;robotics;ABLE-M;robust visual topological localization;binary codes;FLANN;fast nearest neighbor search;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7140088},
} 


@article{16406345 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {SemanticFusion: dense 3D semantic mapping with convolutional neural networks [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {McCormac, J. and Handa, A. and Davison, A. and Leutenegger, S.},
year = {16 Sept. 2016},
pages = {7 pp. - },
address = {USA},
abstract = {Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need extend beyond geometry and appearence - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state of the art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondence between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of approximately 25Hz.},
keywords = {human-robot interaction;image fusion;image segmentation;mobile robots;neural nets;robot vision;SLAM (robots);user interfaces;video signal processing;},
note = {SemanticFusion;dense 3D semantic mapping;convolutional neural networks;visual sensing mapping;mobile robots;robot intelligence;intuitive user interaction;simultaneous localisation and mapping system;SLAM system;ElasticFusion;indoor RGB-D video;loopy scanning trajectory;CNN semantic predictions;reconstruction dataset;single frame segmentation;baseline single frame predictions;2D semantic labelling;multiple prediction fusion;NYUv2 dataset;multiple view points;},
} 


@inproceedings{14617130 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Long-term 3D map maintenance in dynamic environments},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Pomerleau, F. and Krusi, P. and Colas, F. and Furgale, P. and Siegwart, R.},
year = {2014//},
pages = {3712 - 19},
address = {Piscataway, NJ, USA},
abstract = {New applications of mobile robotics in dynamic urban areas require more than the single-session geometric maps that have dominated simultaneous localization and mapping (SLAM) research to date; maps must be updated as the environment changes and include a semantic layer (such as road network information) to aid motion planning in dynamic environments. We present an algorithm for long-term localization and mapping in real time using a three-dimensional (3D) laser scanner. The system infers the static or dynamic state of each 3D point in the environment based on repeated observations. The velocity of each dynamic point is estimated without requiring object models or explicit clustering of the points. At any time, the system is able to produce a most-likely representation of underlying static scene geometry. By storing the time history of velocities, we can infer the dominant motion patterns within the map. The result is an online mapping and localization system specifically designed to enable long-term autonomy within highly dynamic environments. We validate the approach using data collected around the campus of ETH Zurich over seven months and several kilometers of navigation. To the best of our knowledge, this is the first work to unify long-term map update with tracking of dynamic objects.},
keywords = {image representation;mobile robots;path planning;robot vision;SLAM (robots);},
note = {long-term 3D map maintenance;mobile robotics;single-session geometric maps;simultaneous localization and mapping;SLAM;semantic layer;road network information;motion planning;long-term localization and mapping;3D laser scanner;dynamic point velocity estimation;static scene geometry representation;online mapping and localization system;ETH Zurich campus;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907397},
} 


@inproceedings{21430784 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Development of an Autonomous Robotic System Using the Graph-based SPLAM Algorithm},
journal = {2021 International Conference on Information Technology and Nanotechnology (ITNT)},
author = {Kozlov, D. and Myasnikov, V.},
year = {2021//},
pages = {5 pp. - },
address = {Piscataway, NJ, USA},
abstract = {For long-term planning, localization and mapping, the robot must constantly update the map by the changing environment and new areas that the robot is exploring. At the same time, this map should not take up too much of the robot's memory, since the robot's performance is limited due to the small size of the robot and increased performance requirements. The robot must interact with the map on time, updating its location to build a further route to explore areas that have not been visited. In addition to compiling a map, when solving the problem of exploration rooms, the following steps are also important: forming a plan for bypassing an unknown room, calculating the trajectory, resolving collisions with obstacles, and following the trajectory. In the course of this work, an autonomous robotic system was developed, the task of which is to map previously unknown premises. For this, SPLAM algorithms, algorithms for building map and working with graphs, algorithms for following a trajectory were used.},
keywords = {graph theory;mobile robots;path planning;SLAM (robots);},
note = {long-term planning;localization;changing environment;increased performance requirements;exploration rooms;autonomous robotic system;SPLAM algorithms;building map;graph-based SPLAM algorithm;},
URL = {http://dx.doi.org/10.1109/ITNT52450.2021.9649028},
} 


@inproceedings{21545854 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Sequence-based mapping for probabilistic visual loop-closure detection},
journal = {2021 IEEE International Conference on Imaging Systems and Techniques (IST)},
author = {Tsintotas, K.A. and Bampis, L. and Shan An and Fragulis, G.F. and Mouroutsos, S.G. and Gasteratos, A.},
year = {2021//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {During simultaneous localization and mapping, the robot should build a map of its surroundings and simultaneously estimate its pose in the generated map. However, a fundamental task is to detect loops, i.e., previously visited areas, allowing consistent map generation. Moreover, within long-term mapping, every autonomous system needs to address its scalability in terms of storage requirements and database search. In this paper, we present a low-complexity sequence-based visual loop-closure detection pipeline. Our system dynamically segments the traversed route through a feature matching technique in order to define sub-maps. In addition, visual words are generated incrementally for the corresponding sub-maps representation. Comparisons among these sequences-of-images are performed thanks to probabilistic scores originated from a voting scheme. When a candidate sub-map is indicated, global descriptors are utilized for image-to-image pairing. Our evaluation took place on several publicly-available datasets exhibiting the system's low complexity and high recall compared to other state-of-the-art approaches.},
keywords = {feature extraction;image matching;image segmentation;mobile robots;path planning;robot vision;SLAM (robots);},
note = {sequence-based mapping;probabilistic visual loop-closure detection;generated map;visited areas;consistent map generation;long-term mapping;autonomous system;low-complexity sequence-based visual loop-closure detection pipeline;feature matching technique;visual words;sequences-of-images;probabilistic scores;candidate sub-map;image-to-image pairing;sub-map representation;voting scheme;},
URL = {http://dx.doi.org/10.1109/IST50367.2021.9651458},
} 


@inproceedings{13851181 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Switchable constraints vs. max-mixture models vs. RRR - A comparison of three approaches to robust pose graph SLAM},
journal = {2013 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Sunderhauf, N. and Protzel, P.},
year = {2013//},
pages = {5198 - 203},
address = {Piscataway, NJ, USA},
abstract = {SLAM algorithms that can infer a correct map despite the presence of outliers have recently attracted increasing attention. In the context of SLAM, outlier constraints are typically caused by a failed place recognition due to perceptional aliasing. If not handled correctly, they can have catastrophic effects on the inferred map. Since robust robotic mapping and SLAM are among the key requirements for autonomous long-term operation, inference methods that can cope with such data association failures are a hot topic in current research. Our paper compares three very recently published approaches to robust pose graph SLAM, namely switchable constraints, max-mixture models and the RRR algorithm. All three methods were developed as extensions to existing factor graph-based SLAM back-ends and aim at improving the overall system's robustness to false positive loop closure constraints. Due to the novelty of the three proposed algorithms, no direct comparison has been conducted so far.},
keywords = {graph theory;inference mechanisms;SLAM (robots);},
note = {switchable constraints;max-mixture models;RRR;realizing-reversing-recovering algorithm;robust pose graph SLAM algorithm;simultaneous localization and mapping;outlier constraints;place recognition;perceptional aliasing;robust robotic mapping;inference methods;data association failures;factor graph-based SLAM back-ends;false positive loop closure constraints;},
URL = {http://dx.doi.org/10.1109/ICRA.2013.6631320},
} 


@inproceedings{18401110 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Stabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Peng Yin and Lingyun Xu and Zhe Liu and Lu Li and Salman, H. and Yuqing He and Weiliang Xu and Hesheng Wang and Choset, H.},
year = {2018//},
pages = {1162 - 7},
address = {Piscataway, NJ, USA},
abstract = {Place recognition is one of the major challenges for the LiDAR-based effective localization and mapping task. Traditional methods are usually relying on geometry matching to achieve place recognition, where a global geometry map need to be restored. In this paper, we accomplish the place recognition task based on an end-to-end feature learning framework with the LiDAR inputs. This method consists of two core modules, a dynamic octree mapping module that generates local 2D maps with the consideration of the robot's motion; and an unsupervised place feature learning module which is an improved adversarial feature learning network with additional assistance for the long-term place recognition requirement. More specially, in place feature learning, we present an additional Generative Adversarial Network with a designed Conditional Entropy Reduction module to stabilize the feature learning process in an unsupervised manner. We evaluate the proposed method on the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental results show that the proposed method outperforms state-of-the-art in place recognition tasks under long-term applications. What's more, the feature size and inference efficiency in the proposed method are applicable in real-time performance on practical robotic platforms.},
keywords = {entropy;feature extraction;geometry;image matching;image recognition;learning (artificial intelligence);mobile robots;octrees;optical radar;robot vision;unsupervised learning;},
note = {Generative Adversarial Network;adversarial feature;place recognition;global geometry map;Conditional Entropy Reduction module;unsupervised place feature;local 2D maps;dynamic octree mapping module;core modules;LiDAR inputs;end-to-end feature;geometry matching;traditional methods;LiDAR-based place recognition;unsupervised feature learning;feature size;place recognition task;North Campus Long-Term LiDAR dataset;feature learning process;place feature learning;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8593562},
} 


@inproceedings{16677844 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Quantitative Performance Optimisation for Corner and Edge Based Robotic Vision Systems: A Monte-Carlo Simulation},
journal = {Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10073},
author = {Jingduo Tian and Thacker, N. and Stancu, A.},
volume = {pt.II},
year = {2016//},
pages = {544 - 54},
address = {Cham, Switzerland},
abstract = {Corner and edge based robotic vision systems have achieved enormous success in various applications. To quantify and thereby improve the system performance, the standard method is to conduct cross comparisons using benchmark datasets. Such datasets, however, are usually generated for validating specific vision algorithms (e.g. monocular SLAM [1] and stereo odometry [2]). In addition, they are not capable of evaluating robotic systems which require visual feedback signals for motion control (e.g. visual servoing [3]). To develop a more generalised framework to evaluate ordinary corner and edge based robotic vision systems, we propose a novel Monte-Carlo simulation which contains various real-world geometric uncertainty sources. An edge-based global localisation algorithm is evaluated and optimised using the proposed simulation via a large scale Monte-Carlo analysis. During a long-term optimisation, the system performance is improved by around 230 times, while preserving high robustness towards all the simulated uncertainty sources.},
keywords = {edge detection;Monte Carlo methods;motion control;optimisation;robot vision;},
note = {quantitative performance optimisation;corner-based robotic vision systems;edge-based robotic vision systems;Monte Carlo simulation;system performance improvement;benchmark datasets;visual feedback signals;motion control;geometric uncertainty sources;edge-based global localisation algorithm;},
URL = {http://dx.doi.org/10.1007/978-3-319-50832-0_53},
} 


@inproceedings{16503721 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Long-term place recognition using multi-level words of spatial densities},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Maffei, R. and Jorge, V.A.M. and Rey, V.F. and Kolberg, M. and Prestes, E.},
year = {2016//},
pages = {3269 - 74},
address = {Piscataway, NJ, USA},
abstract = {Proper place recognition on an environment that can change over time is fundamental for long-term SLAM. In such scenarios the observations obtained in the same region can drastically differ due to changes caused by semi-static objects, such as doors, furniture, etc. In this work, we extend a strategy that represents environment regions using words, based on spatial density information extracted from laser readings. This time, in order to deal with changes in the environment, our method not only builds words representing the real observations made by the robot, but also alternative multi-level words to account for possible changes in a place's observations generated by non-static objects. Place recognition is made by searching matches of sequences of N consecutive words (both real or alternatives). Experiments performed in real and simulated scenarios are shown, and demonstrate the advantages associated to the use of multi-level words.},
keywords = {feature extraction;image recognition;mobile robots;SLAM (robots);},
note = {place recognition;multilevel word;spatial density;simultaneous localization and mapping;SLAM;information extraction;mobile robot;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759504},
} 


@article{18567598 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Navigation for Indoor Robot: Straight Line Movement via Navigator},
journal = {Mathematical Problems in Engineering},
journal = {Math. Probl. Eng. (UK)},
author = {Chaozheng Zhu and Ming He and Pan Chen and Kang Sun and Jinglei Wang and Qian Huang},
volume = { 2018},
year = {2018//},
pages = {8419384 (10 pp.) - },
issn = {1024-123X},
address = {UK},
abstract = {Due to the need of zigzag overlay strategy, long-term linear motion is essential for sweep robot. However, the existing indoor sweep robot navigation algorithm has many problems; for instance, algorithm with high complexity demands high hardware performance and is incapable of working at night. To overcome those problems, in this paper, a new method for indoor robot Straight Line Movement via Navigator (SLMN) is proposed to ensure long linear motion of robot with an acceptable error threshold and realize multiroom navigation. Firstly, in a short time, robot runs a suitable distance when it is covered by navigator's ultrasonic sensor. We can obtain a triangle with twice the distance between navigator and robot and the distance of robot motion. The forward angle of the robot can be conveniently obtained by the trigonometric functions. Comparing the robot's current angle with expected angle, the robot could correct itself and realize the indoor linear navigation. Secondly, discovering dozens of the magnitude gaps between the distance of robot run and the distance between navigator and robot, we propose an optimized method using approximate scaling which increases efficiency by nearly 70.8%. Finally, to realize multiroom navigation, we introduce the conception of the depth-first search stack and a unique encode rule on rooms and navigators. It is proved by extensive quantitative evaluations that the proposed method realizes indoor full coverage at a lower cost than other state-of-the-art indoor vision navigation schemes, such as ORB-SLAM.},
keywords = {mobile robots;navigation;path planning;robot vision;SLAM (robots);},
note = {long linear motion;indoor robot Straight Line Movement;high complexity demands high hardware performance;existing indoor sweep robot navigation algorithm;long-term linear motion;state-of-the-art indoor vision navigation schemes;navigators;rooms;multiroom navigation;navigator;robot run;indoor linear navigation;robot motion;},
URL = {http://dx.doi.org/10.1155/2018/8419384},
} 


@inproceedings{15798733 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Underwater Robot Visual Place Recognition in the Presence of Dramatic Appearance Change},
journal = {OCEANS 2015 - MTS/IEEE Washington},
author = {Jie Li and Eustice, R.M. and Johnson-Roberson, M.},
year = {2015//},
pages = {385 - 90},
address = {Piscataway, NJ, USA},
abstract = {This paper reports on an algorithm for underwater visual place recognition in the presence of dramatic appearance change. Long-term visual place recognition is challenging underwater due to biofouling, corrosion, and other effects that lead to dramatic visual appearance change, which often causes traditional point-based feature methods to perform poorly. Building upon the authors' earlier work, this paper presents an algorithm for underwater vehicle place recognition and relocalization that enables an autonomous underwater vehicle (AUV) to relocalize itself to a previously-built simultaneous localization and mapping (SLAM) graph. High-level structural features are learned using a supervised learning framework that retains features that have a high potential to persist in the underwater environment. Combined with a particle filtering framework, these features are used to provide a probabilistic representation of localization confidence. The algorithm is evaluated on real data, from multiple years, collected by a Hovering Autonomous Underwater Vehicle (HAUV) for ship hull inspection.},
keywords = {autonomous underwater vehicles;geophysical image processing;image recognition;learning (artificial intelligence);mobile robots;oceanographic techniques;particle filtering (numerical methods);probability;SLAM (robots);},
note = {ship hull inspection;HAUV;hovering autonomous underwater vehicle;probabilistic representation;particle filtering framework;supervised learning framework;high-level structural feature;SLAM graph;simultaneous localization and mapping graph;point-based feature method;biofouling;underwater robot visual place recognition;},
} 


@inproceedings{20755586 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Real-Time Robot Localization Based on 2D Lidar Scan-to-Submap Matching},
journal = {China Satellite Navigation Conference (CSNC 2021). Proceedings. Lecture Notes in Electrical Engineering (LNEE 773)},
author = {Qipeng Li and Jianzhu Huai and Dong Chen and Yuan Zhuang},
volume = {vol.II},
year = {2021//},
pages = {414 - 23},
address = {Singapore, Singapore},
abstract = {In this paper, we propose a real-time and low-drift localization method for lidar-equipped robot in indoor environments. State-of-the-art lidar localization research mostly uses a scan-to-scan method, which produces high drifts during the localization of the robot. It is not suitable for robots to operate indoors (such as factory environment) for a long term. Besides, the mapping and localization of this method are susceptible to the dynamic objects (such as pedestrians). To solve above problems, we propose the scan-to-submap matching method for real-time localization. Currently, this method has been used for building maps, and there are few studies to use it for localization, especially for real-time localization. In our research, we build the hardware and software platform for the scan-to-submap matching method. We extensively evaluate our approach with simulations and real-world tests. Compared with the scan-to-scan method, the results demonstrate that our approach can cope with the mapping and localization problem with high localization accuracy and low drift.},
keywords = {mobile robots;optical radar;path planning;robot vision;SLAM (robots);},
note = {time robot localization;lidar scan-to-submap matching;low-drift localization method;lidar-equipped robot;indoor environments;state-of-the-art lidar localization research;scan-to-scan method;high drifts;factory environment;scan-to-submap matching method;real-time localization;high localization accuracy;low drift;},
URL = {http://dx.doi.org/10.1007/978-981-16-3142-9_39},
} 


@inproceedings{21550585 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Vehicle Navigation by Visual Navigational Aids for Automatic Lunar Mission},
journal = {2021 IEEE 6th International Conference on Actual Problems of Unmanned Aerial Vehicles Development (APUAVD)},
author = {Ostroumov, I. and Kuzmenko, N.},
year = {2021//},
pages = {71 - 5},
address = {Piscataway, NJ, USA},
abstract = {Nowadays the question of Moon exploration is one of the key priorities. Many Lunar robotics missions are planned in near future by different space agencies around the world. Moon has considered to be the best place for a research station with long-term human presence for finding answers on fundamental questions about the universe. Automatic navigation of starship during a landing phase on Lunar surface is already solved with a help of inertial reference system aided visual algorithms. However, questions of automatic navigation of moving and flying vehicles on the Lunar surface are still open. Inertial navigation is limited by time, self-localization and mapping algorithms require multiple unique features of relief to guarantee required accuracy for successful automatic mission complication. In the current study, we propose the deployment of a network of visual navigational aids on the Lunar surface to support ground automatic missions. A weak atmosphere of the Moon makes effective visual beacons navigation system for long areas. A network of navigational aids includes primary and secondary ground stations which are blinking synchronously. Synchronization is supported by radio waves from the primary ground station. We consider the nature of crater relief to increase operational area of the system. The Time Difference of Arrival method is used to detect vehicle position by blinking network of visual navigational aids. In the numerical application, we consider different scenarios of network configuration to support automatic vehicle navigation inside of Tycho crater. Also, deployment of visual navigational aids network will increase the number of optical features which improve performance of already used positioning methods.},
keywords = {aerospace robotics;inertial navigation;lunar surface;mobile robots;path planning;position control;robot vision;space vehicle navigation;synchronisation;time-of-arrival estimation;},
note = {lunar surface;inertial reference system;visual algorithms;automatic navigation;inertial navigation;ground automatic missions;Moon exploration;automatic vehicle navigation;visual navigational aids network;long-term human presence;lunar robotics missions;automatic lunar mission;visual beacons navigation system;space agencies;starship;landing phase;flying vehicles;moving vehicles;self-localization and mapping;ground stations;synchronization;radio waves;time difference of arrival;vehicle position detection;network configuration;Tycho crater;optical features;positioning methods;},
URL = {http://dx.doi.org/10.1109/APUAVD53804.2021.9615417},
} 


@article{18842574 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Learning matchable colorspace transformations for long-term metric visual localization [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Clement, L. and Gridseth, M. and Tomasi, J. and Kelly, J.},
year = {2019/04/01},
pages = {16 pp. - },
address = {USA},
abstract = {Long-term metric localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems in the presence of appearance change caused by lighting, weather or seasonal variations. While experience-based mapping has proven to be an effective technique for enabling visual localization across appearance change, the number of experiences required for reliable long-term localization can be large, and methods for reducing the necessary number of experiences are desired. Taking inspiration from physics-based models of color constancy, we propose a method for learning a nonlinear mapping from RGB to grayscale colorspaces that maximizes the number of feature matches for images captured under varying lighting and weather conditions. Our key insight is that useful image transformations can be learned by approximating conventional non-differentiable localization pipelines with a differentiable learned model that can predict a convenient measure of localization quality, such as the number of feature matches, for a given pair of images. Moreover, we find that the generality of appearance-robust RGB-to-grayscale mappings can be improved by incorporating a learned low-dimensional context feature computed for a specific image pair. Using synthetic and real-world datasets, we show that our method substantially improves feature matching across day-night cycles and presents a viable strategy for significantly improving the efficiency of experience-based visual localization.},
keywords = {feature extraction;image colour analysis;image matching;learning (artificial intelligence);mobile robots;robot vision;},
note = {experience-based visual localization;matchable colorspace transformations;long-term metric localization;autonomous mobile robots;vision-based systems;appearance change;experience-based mapping;physics-based models;nonlinear mapping;weather conditions;nondifferentiable localization pipelines;differentiable learned model;localization quality;appearance-robust RGB-to-grayscale mappings;varying lighting conditions;image transformations;image pair;long-term metric visual localization;color constancy;grayscale colorspaces;RGB colorspaces;image feature matching;learned low-dimensional context feature;},
} 


@inproceedings{18392894 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Egger, P. and Borges, P.V.K. and Catt, G. and Pfrunder, A. and Siegwart, R. and Dube, R.},
year = {2018//},
pages = {3430 - 7},
address = {Piscataway, NJ, USA},
abstract = {Reliable long-term localization is key for robotic systems in dynamic environments. In this paper, we propose a novel approach for long-term localization using 3D LiDARs, coined PoseMap. In essence, we extract distinctive features from range measurements and bundle these into local views along with observation poses. The sensor's trajectory is then estimated in a sliding window fashion by matching current and old features and minimizing the distances in-between. The map representation facilitates finding a suitable set of old features, by selecting the closest local map(s) for matching. Similarly to a visibility analysis, this procedure provides a suitable set of features for localization but at a fraction of the computational cost. PoseMap also allows for updates and extensions of the map at any time by replacing and adding local maps when necessary. We evaluate our approach using two platforms both equipped with a 3D LiDAR and an IMU, demonstrating localization at 8 Hz and robustness to changes in the environment such as moving vehicles and changing vegetation. PoseMap was implemented on an autonomous vehicle allowing it to drive autonomously over a period of 18 months through a mix of industrial and unstructured off-road environments, covering more than 100 kms without a single localization failure.},
keywords = {feature extraction;mobile robots;optical radar;path planning;},
note = {local views;sliding window fashion;matching current;old features;map representation;local maps;off-road environments;single localization failure;distinctive features;coined PoseMap;dynamic environments;robotic systems;long-term localization;multienvironment 3D LiDAR localization;frequency 8.0 Hz;time 18.0 month;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8593854},
} 


@inproceedings{21504186 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {NYU-VPR: Long-Term Visual Place Recognition Benchmark with View Direction and Data Anonymization Influences},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Diwei Sheng and Yuxiang Chai and Xinru Li and Chen Feng and Jianzhe Lin and Silva, C. and Rizzo, J.-R.},
year = {2021//},
pages = {9773 - 9},
address = {Piscataway, NJ, USA},
abstract = {Visual place recognition (VPR) is critical in not only localization and mapping for autonomous driving vehicles, but also assistive navigation for the visually impaired population. To enable a long-term VPR system on a large scale, several challenges need to be addressed. First, different applications could require different image view directions, such as front views for self-driving cars while side views for the low vision people. Second, VPR in metropolitan scenes can often cause privacy concerns due to the imaging of pedestrian and vehicle identity information, calling for the need for data anonymization before VPR queries and database construction. Both factors could lead to VPR performance variations that are not well understood yet. To study their influences, we present the NYU-VPR dataset that contains more than 200,000 images over a 2km&times;2km area near the New York University campus, taken within the whole year of 2016. We present benchmark results on several popular VPR algorithms showing that side views are significantly more challenging for current VPR methods while the influence of data anonymization is almost negligible, together with our hypothetical explanations and in-depth analysis.},
keywords = {data privacy;image recognition;road vehicles;visual databases;},
note = {image view directions;pedestrian;metropolitan scenes;low vision people;long-term VPR system;visually impaired population;assistive navigation;autonomous driving vehicles;data anonymization influences;view direction;long-term visual place recognition benchmark;NYU-VPR dataset;VPR performance variations;VPR queries;vehicle identity information;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9636640},
} 


@article{18021116 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Long-term online multi-session graph-based SPLAM with memory management},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Labbe, M. and Michaud, F.},
volume = { 42},
number = { 6},
year = {2018/08/},
pages = {1133 - 50},
issn = {0929-5593},
address = {Germany},
abstract = {For long-term simultaneous planning, localization and mapping (SPLAM), a robot should be able to continuously update its map according to the dynamic changes of the environment and the new areas explored. With limited onboard computation capabilities, a robot should also be able to limit the size of the map used for online localization and mapping. This paper addresses these challenges using a memory management mechanism, which identifies locations that should remain in a Working Memory (WM) for online processing from locations that should be transferred to a Long-Term Memory (LTM). When revisiting previously mapped areas that are in LTM, the mechanism can retrieve these locations and place them back in WM for online SPLAM. The approach is tested on a robot equipped with a short-range laser rangefinder and a RGB-D camera, patrolling autonomously 10.5 km in an indoor environment over 11 sessions while having encountered 139 people.},
keywords = {graph theory;laser ranging;mobile robots;path planning;robot vision;SLAM (robots);},
note = {dynamic changes;onboard computation capabilities;online localization;memory management mechanism;Working Memory;WM;online processing;Long-Term Memory;LTM;mapped areas;online SPLAM;multisession graph-based SPLAM;simultaneous planning localization and mapping;RGB-D camera;},
URL = {http://dx.doi.org/10.1007/s10514-017-9682-5},
} 


@article{20423689 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {2-entity random sample consensus for robust visual localization: framework, methods, and verifications},
journal = {IEEE Transactions on Industrial Electronics},
journal = {IEEE Trans. Ind. Electron. (USA)},
author = {Yanmei Jiao and Yue Wang and Xiaqing Ding and Bo Fu and Shoudong Huang and Rong Xiong},
volume = { 68},
number = { 5},
year = {2021/05/},
pages = {4519 - 28},
issn = {0278-0046},
address = {USA},
abstract = {Robust and efficient visual localization is essential for numerous robotic applications. However, it remains a challenging problem especially when significant environmental or perspective changes are present, as there are high percentage of outliers, i.e., incorrect feature matches between the query image and the map. In this article, we propose a novel 2-entity random sample consensus (RANSAC) framework using three-dimensional-two-dimensional point and line feature matches for visual localization with the aid of inertial measurements and derive minimal closed-form solutions using only 1 point 1 line or 2 point matches for both monocular and multi-camera system. The proposed 2-entity RANSAC can achieve higher robustness against outliers as multiple types of features are utilized and the number of matches needed to compute a pose is reduced. Furthermore, we propose a learning-based sampling strategy selection mechanism and a feature scoring network to be adaptive to different environmental characteristics such as structured and unstructured. Finally, both simulation and real-world experiments are performed to validate the robustness and effectiveness of the proposed method in scenarios with long-term and perspective changes.},
keywords = {cameras;computer vision;feature extraction;image matching;learning (artificial intelligence);path planning;pose estimation;robot vision;},
note = {different environmental characteristics;learning-based sampling strategy selection mechanism;higher robustness;2-entity RANSAC;multicamera system;1 point 1 line;closed-form solutions;line feature;three-dimensional-two-dimensional point;novel 2-entity random sample consensus framework;query image;incorrect feature;significant environmental perspective changes;numerous robotic applications;efficient visual localization;robust localization;robust visual localization;},
URL = {http://dx.doi.org/10.1109/TIE.2020.2984970},
} 


@inproceedings{20334114 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Radar-on-Lidar: metric radar localization on prior lidar maps},
journal = {2020 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
author = {Huan Yin and Yue Wang and Li Tang and Rong Xiong},
year = {2020//},
pages = {1 - 7},
address = {Piscataway, NJ, USA},
abstract = {Radar and lidar, provided by two different range sensors, each has pros and cons of various perception tasks on mobile robots or autonomous driving. In this paper, a Monte Carlo system is used to localize the robot with a rotating radar sensor on 2D lidar maps. We first train a conditional generative adversarial network to transfer raw radar data to lidar data, and achieve reliable radar points from generator. Then an efficient radar odometry is included in the Monte Carlo system. Combining the initial guess from odometry, a measurement model is proposed to match the radar data and prior lidar maps for final 2D positioning. We demonstrate the effectiveness of the proposed localization framework on the public multisession dataset. The experimental results show that our system can achieve high accuracy for long-term localization in outdoor scenes.},
keywords = {distance measurement;geophysical image processing;mobile robots;Monte Carlo methods;optical radar;radar detection;radar imaging;radar tracking;remote sensing by laser beam;remote sensing by radar;robot vision;SLAM (robots);},
note = {long-term localization;localization framework;efficient radar odometry;reliable radar points;raw radar data;conditional generative adversarial network;2D lidar maps;rotating radar sensor;Monte Carlo system;autonomous driving;mobile robots;perception tasks;different range sensors;prior lidar maps;metric radar localization;radar-on-Lidar;},
URL = {http://dx.doi.org/10.1109/RCAR49640.2020.9303291},
} 


@inproceedings{18398842 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Long-term Localization of Mobile Robots in Dynamic Changing Environments},
journal = {2018 Chinese Automation Congress (CAC)},
author = {Xiaowei Hu and Jingchuan Wang and Weidong Chen},
year = {2018//},
pages = {384 - 9},
address = {Piscataway, NJ, USA},
abstract = {Long-term localization in dynamic changing environments is still a challenge in robotics. Traditional localization algorithms typically assume that the environment is static. However, in many real-world applications, such as parking lots and industrial plants, there are always dynamic objects (e.g. moving people) and semi-dynamic objects (e.g. parked cars and placed goods). In this paper we address this challenge by introducing a long-term localization algorithm in the environments which combine dynamic objects and semi-dynamic objects. Localizability-based-updating particle filter (LU-P F) algorithm is proposed here. Not only we use localizability matric to build an updating mechanism, but also it is used for localization system. Besides, we propose the dynamic factor as long-memory information to serve as prior knowledge, which improves the robustness of updating process. Experiments in parking lots demonstrate that our approach has better localization results with a more accurate up-to-date map compared to other methods.},
keywords = {mobile robots;particle filtering (numerical methods);road traffic control;},
note = {long-memory information;localizability-based-updating particle filter;long-term localization algorithm;dynamic changing environments;mobile robots;parking lots;},
URL = {http://dx.doi.org/10.1109/CAC.2018.8623046},
} 


@inproceedings{11963094 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Global localization using multiple hypothesis tracking: A real-world approach},
journal = {2011 IEEE Conference on Technologies for Practical Robot Applications (TePRA)},
author = {Lutz, M. and Hochdorfer, S. and Schlegel, C.},
year = {2011//},
pages = {127 - 32},
address = {Piscataway, NJ, USA},
abstract = {Life-long and robust operation are important challenges to be solved towards everyday usability of service robots. Global localization is of particular interest for real-world applications. If a robot would not be able to relocalize itself within a known map, all positions stored by the robot (rooms, objects, etc.) would become obsolete. Although Simultaneous Localization and Mapping (SLAM) allows to initially map new and unknown environments and to keep track of environmental changes, it does not solve the global localization problem. Each time SLAM is restarted at different locations, it introduces a new map and a new frame of reference. In this paper, we propose a solution to the global localization problem which uses a SLAM generated feature map. The approach is demonstrated with an omnicam and bearing-only features. A new way to weight hypotheses and to sort out false hypotheses results in fast convergence even with arbitrary relocalization paths. The combined approach is a further step towards life-long operation of service robots and covers every part of a robot lifecycle, ranging from a setup via SLAM to efficient global localization for reuse of maps and object poses after restart.},
keywords = {mobile robots;path planning;robot vision;service robots;SLAM (robots);},
note = {multiple hypothesis tracking;service robots;simultaneous localization and mapping;global localization problem;SLAM;feature map;},
URL = {http://dx.doi.org/10.1109/TEPRA.2011.5753494},
} 


@article{20636668 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Hough2Map - Iterative Event-Based Hough Transform for High-Speed Railway Mapping},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Tschopp, F. and von Einem, C. and Cramariuc, A. and Hug, D. and Palmer, A.W. and Siegwart, R. and Chli, M. and Nieto, J.},
volume = { 6},
number = { 2},
year = {2021/04/},
pages = {2745 - 52},
issn = {2377-3766},
address = {USA},
abstract = {To cope with the growing demand for transportation on the railway system, accurate, robust, and high-frequency positioning is required to enable a safe and efficient utilization of the existing railway infrastructure. As a basis for a localization system we propose a complete on-board mapping pipeline able to map robust meaningful landmarks, such as poles from power lines, in the vicinity of the vehicle. Such poles are good candidates for reliable and long term landmarks even through difficult weather conditions or seasonal changes. To address the challenges of motion blur and illumination changes in railway scenarios we employ a Dynamic Vision Sensor, a novel event-based camera. Using a sideways oriented on-board camera, poles appear as vertical lines. To map such lines in a real-time event stream, we introduce Hough<sup><b>2</b></sup>Map, a novel consecutive iterative event-based Hough transform framework capable of detecting, tracking, and triangulating close-by structures. We demonstrate the mapping reliability and accuracy of Hough<sup><b>2</b></sup>Map on real-world data in typical usage scenarios and evaluate using surveyed infrastructure ground truth maps. Hough<sup><b>2</b></sup>Map achieves a detection reliability of up to $92\,\%$ and a mapping root mean square error accuracy of 1.1518 m.<sup>1</sup><sup>1</sup>The code is available at https://github.com/ethz-asl/Hough2Map.},
keywords = {cameras;computer vision;geographic information systems;Hough transforms;image sensors;mean square error methods;railways;},
note = {high-speed railway mapping;iterative event-based Hough transform;Hough<sup>2</sup>Map;railway scenarios;long term landmarks;reliable term landmarks;power lines;map robust meaningful landmarks;on-board mapping pipeline;localization system;railway infrastructure;safe utilization;high-frequency positioning;railway system;mapping root;surveyed infrastructure ground truth maps;mapping reliability;consecutive iterative event-based Hough;real-time event stream;on-board camera;event-based camera;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3061404},
} 


@article{21210403 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {High-definition map update framework for intelligent autonomous transfer vehicles},
journal = {Journal of Experimental &amp; Theoretical Artificial Intelligence},
journal = {J. Exp. Theor. Artif. Intell. (UK)},
author = {Tas, M.O. and Yavuz, H.S. and Yazici, A.},
volume = { 33},
number = { 5},
year = {2021//},
pages = {847 - 65},
issn = {0952-813X},
address = {UK},
abstract = {Autonomous transfer vehicles (ATVs) can be considered as one of the critical components of context-aware structured smart factories in Industry 4.0 era. Conventional mapping methods such as grid maps can provide information for navigation, but they are not enough for complex environments that require interactions. On the other hand, high-definition (HD) mapping, which is mainly used in traffic networks, includes more information about an environment to perform excellent autonomous behaviour. In order to increase the efficiency of ATVs in flexible factories, an up-to-date environmental map information is required to perform successful long-term autonomous navigation. Therefore, when there exists a change in the environment, a simultaneous update of HD-map is as important as the creation of it. In this study, we propose an HD-map update methodology for ATVs that operates in smart factories. To the best of our knowledge, HD mapping has not been applied in smart factories. The proposed method includes the object detection and localisation tool to detect objects visually and determines their positions in connection with the conventional maps of the environment. Experimental results of a simulated factory environment demonstrate that the ATV can properly update the HD-map when a predefined sign is removed from or a new sign is added to the environment.},
keywords = {factory automation;industrial robots;mobile robots;object detection;path planning;production engineering computing;robot vision;SLAM (robots);},
note = {object detection;autonomous navigation;up-to-date environmental map information;Industry 4.0;context-aware structured smart factories;intelligent autonomous transfer vehicles;high-definition map update framework;HD mapping;ATV;},
URL = {http://dx.doi.org/10.1080/0952813X.2020.1789754},
} 


@article{20585555 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Localization in Unstructured Environments: Towards Autonomous Robots in Forests with Delaunay Triangulation},
journal = {Remote Sensing},
journal = {Remote Sens. (Switzerland)},
author = {Qingqing Li and Nevalainen, P. and Queralta, J.P. and Heikkonen, J. and Westerlund, T.},
volume = { 12},
number = { 11},
year = {2020/06/},
pages = {1870 (22 pp.) - },
issn = {2072-4292},
address = {Switzerland},
abstract = {Autonomous harvesting and transportation is a long-term goal of the forest industry. One of the main challenges is the accurate localization of both vehicles and trees in a forest. Forests are unstructured environments where it is difficult to find a group of significant landmarks for current fast feature-based place recognition algorithms. This paper proposes a novel approach where local point clouds are matched to a global tree map using the Delaunay triangularization as the representation format. Instead of point cloud based matching methods, we utilize a topology-based method. First, tree trunk positions are registered at a prior run done by a forest harvester. Second, the resulting map is Delaunay triangularized. Third, a local submap of the autonomous robot is registered, triangularized and matched using triangular similarity maximization to estimate the position of the robot. We test our method on a dataset accumulated from a forestry site at Lieksa, Finland. A total length of 200m of harvester path was recorded by an industrial harvester with a 3D laser scanner and a geolocation unit fixed to the frame. Our experiments show a 12 cm s.t.d. in the location accuracy and with real-time data processing for speeds not exceeding 0.5 m/s. The accuracy and speed limit are realistic during forest operations.},
keywords = {feature extraction;forestry;image matching;mesh generation;mobile robots;optical scanners;robot vision;SLAM (robots);},
note = {point cloud;matching methods;topology-based method;tree trunk positions;forest harvester;resulting map;local submap;autonomous robot;triangular similarity maximization;harvester path;industrial harvester;forest operations;unstructured environments;towards autonomous robots;Delaunay triangulation;transportation;forest industry;vehicles;trees;significant landmarks;current fast feature-based place recognition algorithms;local point clouds;global tree map;Delaunay triangularization;wavelength 200.0 m;size 12.0 cm;velocity 0.5 m/s;},
URL = {http://dx.doi.org/10.3390/rs12111870},
} 


@article{21472174 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {High-precision and robust localization system for mobile robots in complex and large-scale indoor scenes},
journal = {International Journal of Advanced Robotic Systems},
journal = {Int. J. Adv. Robot. Syst. (USA)},
author = {Jibo Wang and Chengpeng Li and Bangyu Li and Chenglin Pang and Zheng Fang},
volume = { 18},
number = { 5},
year = {2021//},
pages = {17298814211047690 (16 pp.) - },
issn = {1729-8814},
address = {USA},
abstract = {High-precision and robust localization is the key issue for long-term and autonomous navigation of mobile robots in industrial scenes. In this article, we propose a high-precision and robust localization system based on laser and artificial landmarks. The proposed localization system is mainly composed of three modules, namely scoring mechanism-based global localization module, laser and artificial landmark-based localization module, and relocalization trigger module. Global localization module processes the global map to obtain the map pyramid, thus improve the global localization speed and accuracy when robots are powered on or kidnapped. Laser and artificial landmark-based localization module is employed to achieve robust localization in highly dynamic scenes and high-precision localization in target areas. The relocalization trigger module is used to monitor the current localization quality in real time by matching the current laser scan with the global map and feeds it back to the global localization module to improve the robustness of the system. Experimental results show that our method can achieve robust robot localization and real-time detection of the current localization quality in indoor scenes and industrial environment. In the target area, the position error is less than 0.004 m and the angle error is less than 0.01 rad.},
keywords = {automatic guided vehicles;mobile robots;navigation;position control;robust control;velocity control;},
note = {localization quality;robust localization system;mobile robots;large-scale indoor scenes;mechanism-based global localization module;artificial landmark-based localization module;relocalization trigger module;global localization speed;highly dynamic scenes;high-precision localization;autonomous navigation;map pyramid;industrial environment;target area;AGV;automated guided vehicle;},
URL = {http://dx.doi.org/10.1177/17298814211047690},
} 


@inproceedings{18112410 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Long-Term Visual Localization Using Semantically Segmented Images},
journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Stenborg, E. and Toft, C. and Hammarstrand, L.},
year = {2018//},
pages = {6484 - 90},
address = {Piscataway, NJ, USA},
abstract = {Robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles. In this paper, we exploit recent advances in semantic segmentation of images, i.e., where each pixel is assigned a label related to the type of object it represents, to attack the problem of long-term visual localization. We show that semantically labeled 3D point maps of the environment, together with semantically segmented images, can be efficiently used for vehicle localization without the need for detailed feature descriptors (SIFT, SURF, etc.), Thus, instead of depending on hand-crafted feature descriptors, we rely on the training of an image segmenter. The resulting map takes up much less storage space compared to a traditional descriptor based map. A particle filter based semantic localization solution is compared to one based on SIFT-features, and even with large seasonal variations over the year we perform on par with the larger and more descriptive SIFT-features, and are able to localize with an error below 1 m most of the time.},
keywords = {feature extraction;image segmentation;particle filtering (numerical methods);transforms;},
note = {particle filter based semantic localization solution;SIFT-features;vehicle localization;semantically labeled 3D point maps;autonomous vehicles;long-term visual navigation;robust cross-seasonal localization;semantically segmented images;long-term visual localization;image segmenter;hand-crafted feature descriptors;},
URL = {http://dx.doi.org/10.1109/ICRA.2018.8463150},
} 


@article{18787621 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Communication constrained cloud-based long-term visual localization in real time [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Xiaqing Ding and Yue Wang and Li Tang and Huan Yin and Rong Xiong},
year = {2019/03/10},
pages = {8 pp. - },
address = {USA},
abstract = {Visual localization is one of the primary capabilities for mobile robots. Long-term visual localization in real time is particularly challenging, in which the robot is required to efficiently localize itself using visual data where appearance may change significantly over time. In this paper, we propose a cloud-based visual localization system targeting at long-term localization in real time. On the robot, we employ two estimators to achieve accurate and real-time performance. One is a sliding-window based visual inertial odometry, which integrates constraints from consecutive observations and selfmotion measurements, as well as the constraints induced by localization on the cloud. This estimator builds a local visual submap as the virtual observation which is then sent to the cloud as new localization constraints. The other one is a delayed state Extended Kalman Filter to fuse the pose of the robot localized from the cloud, the local odometry and the high-frequency inertial measurements. On the cloud, we propose a longer sliding-window based localization method to aggregate the virtual observations for larger field of view, leading to more robust alignment between virtual observations and the map. Under this architecture, the robot can achieve drift-free and real-time localization using onboard resources even in a network with limited bandwidth, high latency and existence of package loss, which enables the autonomous navigation in real-world environment. We evaluate the effectiveness of our system on a dataset with challenging seasonal and illuminative variations. We further validate the robustness of the system under challenging network conditions.},
keywords = {distance measurement;Kalman filters;mobile robots;nonlinear filters;path planning;robot vision;SLAM (robots);},
note = {cloud-based long-term;mobile robots;long-term visual localization;visual data;cloud-based visual localization system;long-term localization;real-time performance;sliding-window based visual inertial odometry;local visual submap;virtual observation;localization constraints;local odometry;high-frequency inertial measurements;localization method;real-time localization;},
} 


@inproceedings{19299148 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Communication constrained cloud-based long-term visual localization in real time},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Xiaqing Ding and Yue Wang and Li Tang and Huan Yin and Rong Xiong},
year = {2019//},
pages = {2159 - 66},
address = {Piscataway, NJ, USA},
abstract = {Visual localization is one of the primary capabilities for mobile robots. Long-term visual localization in real time is particularly challenging, in which the robot is required to efficiently localize itself using visual data where appearance may change significantly over time. In this paper, we propose a cloud-based visual localization system targeting at long-term localization in real time. On the robot, we employ two estimators to achieve accurate and real-time performance. One is a sliding-window based visual inertial odometry, which integrates constraints from consecutive observations and self-motion measurements, as well as the constraints induced by localization results from the cloud. This estimator builds a local visual submap as the virtual observation which is then sent to the cloud as new localization constraints. The other one is a delayed state Extended Kalman Filter to fuse the pose of the robot localized from the cloud, the local odometry and the high-frequency inertial measurements. On the cloud, we propose a longer sliding-window based localization method to aggregate the virtual observations for larger field of view, leading to more robust alignment between virtual observations and the map. Under this architecture, the robot can achieve drift-free and real-time localization using onboard resources even in a network with limited bandwidth, high latency and existence of package loss, which enables the autonomous navigation in real-world environment. We evaluate the effectiveness of our system on a dataset with challenging seasonal and illuminative variations. We further validate the robustness of the system under challenging network conditions.},
keywords = {cloud computing;control engineering computing;data visualisation;distance measurement;image filtering;Kalman filters;mobile robots;nonlinear filters;robot vision;SLAM (robots);},
note = {data visualization;cloud-based visual localization system;long-term localization;sliding-window based visual inertial odometry;virtual observation;localization constraints;local odometry;high-frequency inertial measurements;mobile robots;long-term visual localization;extended Kalman filter;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8968550},
} 


@inproceedings{19951507 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Graph Optimization Methods for Large-Scale Crowdsourced Mapping},
journal = {2020 IEEE 23rd International Conference on Information Fusion (FUSION). Proceedings},
author = {Stoven-Dubois, A. and Dziri, A. and Leroy, B. and Chapuis, R.},
year = {2020//},
pages = {8 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Automotive players have recently shown an increasing interest in high-precision mapping, with the aim of enhancing vehicles safety and autonomy. Nevertheless, the acquisition, processing, and updates of accurate maps remains an economic challenge. Collaborative mapping through vehicles crowdsourcing represents a promising solution to tackle this problem. However, the potential scalability and accuracy provided by such an approach have yet to be studied and assessed. In this paper, we study the use of graph optimization in the scope of collaborative mapping. We build a map of geo-localized landmarks by crowdsourcing observations from multiple vehicles, and applying several successive map updates. We present different strategies to adapt graph optimization to the crowdsourced approach, and compare their performances in terms of map quality and scalability on simulation data. We show the critical requirement, in a long-term context, to ensure consistency of the map updates, and we propose a scalable solution which is able to build an accurate map of geolocalized landmarks.},
keywords = {cartography;graph theory;optimisation;outsourcing;regression analysis;road safety;road vehicles;},
note = {collaborative mapping;vehicles crowdsourcing;potential scalability;geo-localized landmarks;multiple vehicles;successive map updates;crowdsourced approach;map quality;scalable solution;accurate map;graph optimization methods;automotive players;high-precision mapping;vehicles safety;economic challenge;large-scale crowdsourced mapping;},
URL = {http://dx.doi.org/10.23919/FUSION45008.2020.9190292},
} 


@article{19487997 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {3D LiDAR-based global localization using siamese neural network},
journal = {IEEE Transactions on Intelligent Transportation Systems},
journal = {IEEE Trans. Intell. Transp. Syst. (USA)},
author = {Huan Yin and Yue Wang and Xiaqing Ding and Li Tang and Shoudong Huang and Rong Xiong},
volume = { 21},
number = { 4},
year = {2020/04/},
pages = {1380 - 92},
issn = {1524-9050},
address = {USA},
abstract = {Global localization in 3D point clouds is a challenging task for mobile vehicles in outdoor scenarios, which requires the vehicle to localize itself correctly in a given map without prior knowledge of its pose. This is a critical component of autonomous vehicles or robots on the road for handling localization failures. In this paper, based on reduced dimension scan representations learned from neural networks, a solution to global localization is proposed by achieving place recognition first and then metric pose estimation in the global prior map. Specifically, we present a semi-handcrafted feature learning method for 3D Light detection and ranging (LiDAR) point clouds using artificial statistics and siamese network, which transforms the place recognition problem into a similarity modeling problem. Additionally, the sensor data using dimension reduced representations require less storage space and make the searching easier. With the learned representations by networks and the global poses, a prior map is built and used in the localization framework. In the localization step, position only observations obtained by place recognition are used in a particle filter algorithm to achieve precise pose estimation. To demonstrate the effectiveness of our place recognition and localization approach, KITTI benchmark and our multi-session datasets are employed for comparison with other geometric-based algorithms. The results show that our system can achieve both high accuracy and efficiency for long-term autonomy.},
keywords = {feature extraction;image filtering;image representation;learning (artificial intelligence);mobile robots;neural nets;optical radar;particle filtering (numerical methods);pose estimation;robot vision;SLAM (robots);},
note = {3D point clouds;mobile vehicles;outdoor scenarios;autonomous vehicles;localization failures;reduced dimension scan representations;neural networks;global prior map;feature learning method;3D light detection;siamese network;place recognition problem;similarity modeling problem;dimension reduced representations;learned representations;global poses;localization framework;localization step;localization approach;3D LiDAR-based global localization;siamese neural network;},
URL = {http://dx.doi.org/10.1109/TITS.2019.2905046},
} 


@article{20636724 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {DiSCO: Differentiable Scan Context With Orientation},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Xuecheng Xu and Huan Yin and Zexi Chen and Yuehua Li and Yue Wang and Rong Xiong},
volume = { 6},
number = { 2},
year = {2021/04/},
pages = {2791 - 8},
issn = {2377-3766},
address = {USA},
abstract = {Global localization is essential for robot navigation, of which the first step is to retrieve a query from the map database. This problem is called place recognition. In recent years, LiDAR scan based place recognition has drawn attention as it is robust against the appearance change. In this letter, we propose a LiDAR-based place recognition method, named Differentiable Scan Context with Orientation (DiSCO), which simultaneously finds the scan at a similar place and estimates their relative orientation. The orientation can further be used as the initial value for the down-stream local optimal metric pose estimation, improving the pose estimation especially when a large orientation between the current scan and retrieved scan exists. Our key idea is to transform the feature into the frequency domain. We utilize the magnitude of the spectrum as the place descriptor, which is theoretically rotation-invariant. In addition, based on the differentiable phase correlation, we can efficiently estimate the global optimal relative orientation using the spectrum. With such structural constraints, the network can be learned in an end-to-end manner, and the backbone is fully shared by the two tasks, achieving better interpretability and lightweight. Finally, DiSCO is validated on three datasets with long-term outdoor conditions, showing better performance than the compared methods. Codes are released at https://github.com/MaverickPeter/DiSCO-pytorch.},
keywords = {feature extraction;image recognition;mobile robots;object recognition;optical radar;pose estimation;robot vision;SLAM (robots);},
note = {map database;robot navigation;global localization;global optimal relative orientation;differentiable phase correlation;place descriptor;current scan;down-stream local optimal metric pose estimation;similar place;named Differentiable Scan Context;LiDAR-based place recognition method;appearance change;LiDAR scan;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3060741},
} 


@article{21441083 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {NASA Space Robotics Challenge 2 Qualification Round: An Approach to Autonomous Lunar Rover Operations},
journal = {IEEE Aerospace and Electronic Systems Magazine},
journal = {IEEE Aerosp. Electron. Syst. Mag. (USA)},
author = {Kilic, C. and Martinez R, B., Jr. and Tatsch, C.A. and Beard, J. and Strader, J. and Das, S. and Ross, D. and Yu Gu and Pereira, G.A.S. and Gross, J.N.},
volume = { 36},
number = { 12},
year = {2021//},
pages = {24 - 41},
issn = {0885-8985},
address = {USA},
abstract = {Plans for establishing a long-term human presence on the Moon will require substantial increases in robot autonomy and multirobot coordination to support establishing a lunar outpost. To achieve these objectives, algorithm design choices for the software developments need to be tested and validated for expected scenarios such as autonomous in situ resource utilization, localization in challenging environments, and multirobot coordination. However, real-world experiments are extremely challenging and limited for extraterrestrial environment. Also, realistic simulation demonstrations in these environments are still rare and demanded for initial algorithm testing capabilities. To help some of these needs, the NASA Centennial Challenges program established the Space Robotics Challenge Phase 2 (SRC2), which consist of virtual robotic systems in a realistic lunar simulation environment, where a group of mobile robots were tasked with reporting volatile locations within a global map, excavating and transporting these resources, and detecting and localizing a target of interest. The main goal of this article is to share our team's experiences on the design tradeoffs to perform autonomous robotic operations in a virtual lunar environment and to share strategies to complete the mission requirements posed by NASA SRC2 competition during the qualification round. Of the 114 teams that registered for participation in the NASA SRC2, team Mountaineers finished as one of only six teams to receive the top qualification round prize.},
keywords = {aerospace robotics;mobile robots;Moon;multi-robot systems;planetary rovers;},
note = {NASA Centennial Challenges program;virtual robotic systems;realistic lunar simulation environment;mobile robots;autonomous robotic operations;virtual lunar environment;NASA SRC2 competition;NASA Space Robotics Challenge 2 qualification round;autonomous lunar rover operations;long-term human presence;robot autonomy;multirobot coordination;lunar outpost;software developments;resource utilization;real-world experiments;extraterrestrial environment;realistic simulation demonstrations;algorithm testing capabilities;},
URL = {http://dx.doi.org/10.1109/MAES.2021.3115897},
} 


@inproceedings{13194899 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Dynamic pose graph SLAM: Long-term mapping in low dynamic environments},
journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2012)},
author = {Walcott-Bryant, A. and Kaess, M. and Johannsson, H. and Leonard, J.J.},
year = {2012//},
pages = {1871 - 8},
address = {Piscataway, NJ, USA},
abstract = {Maintaining a map of an environment that changes over time is a critical challenge in the development of persistently autonomous mobile robots. Many previous approaches to mapping assume a static world. In this work we incorporate the time dimension into the mapping process to enable a robot to maintain an accurate map while operating in dynamical environments. This paper presents Dynamic Pose Graph SLAM (DPG-SLAM), an algorithm designed to enable a robot to remain localized in an environment that changes substantially over time. Using incremental smoothing and mapping (iSAM) as the underlying SLAM state estimation engine, the Dynamic Pose Graph evolves over time as the robot explores new places and revisits previously mapped areas. The approach has been implemented for planar indoor environments, using laser scan matching to derive constraints for SLAM state estimation. Laser scans for the same portion of the environment at different times are compared to perform change detection; when sufficient change has occurred in a location, the dynamic pose graph is edited to remove old poses and scans that no longer match the current state of the world. Experimental results are shown for two real-world dynamic indoor laser data sets, demonstrating the ability to maintain an up-to-date map despite long-term environmental changes.},
keywords = {laser beam applications;mobile robots;optical scanners;SLAM (robots);state estimation;},
note = {long-term mapping;low dynamic environments;environment map maintenance;autonomous mobile robots;dynamical environments;dynamic pose graph SLAM;DPG-SLAM;incremental smoothing and mapping;iSAM;SLAM state estimation engine;planar indoor environments;laser scan matching;change detection;real-world dynamic indoor laser data sets;},
URL = {http://dx.doi.org/10.1109/IROS.2012.6385561},
} 


@inproceedings{16709687 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Mining DCNN landmarks for long-term visual SLAM},
journal = {2016 IEEE International Conference on Robotics and Biomimetics (ROBIO). Proceedings},
author = {Taisho, T. and Kanji, T.},
year = {2016//},
pages = {570 - 6},
address = {Piscataway, NJ, USA},
abstract = {Long-term visual SLAM, in familiar, semi-dynamic, and partially changing environments is an important area of research in robotics. The main problem we faced is the question of how to describe a scene discriminatively and compactly-both of which are necessary in order to cope with changes in appearance and a large amount of visual information. In this study, we address the above issues by mining visual experience. Our strategy is to mine a library of raw visual images, termed visual experience, to find the relevant visual patterns to effectively explain the input scene. From a practical point of view, our work offers three main contributions over the previous work. First, it is the first application of discriminative visual features from deep convolutional neural networks (DCNN) to the task of visual landmark mining. Second, we show how to interpret a high-dimensional DCNN feature to a compact semantic representation of visual word. Third, we show that our approach can turn the scene description task with any feature (including the DCNN feature) into the task of mining visual experience. Experiments on a challenging cross-domain visual place recognition validate efficacy of the proposed approach.},
keywords = {data mining;feedforward neural nets;object recognition;SLAM (robots);},
note = {DCNN landmarks mining;visual SLAM;discriminative visual features;deep convolutional neural networks;visual landmark mining;visual word semantic representation;cross-domain visual place recognition;},
URL = {http://dx.doi.org/10.1109/ROBIO.2016.7866383},
} 


@inproceedings{16503734 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Curating long-term vector maps},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Nashed, S. and Biswas, J.},
year = {2016//},
pages = {4643 - 8},
address = {Piscataway, NJ, USA},
abstract = {Autonomous service mobile robots need to consistently, accurately, and robustly localize in human environments despite changes to such environments over time. Episodic non-Markov Localization addresses the challenge of localization in such changing environments by classifying observations as arising from Long-Term, Short-Term, or Dynamic Features. However, in order to do so, EnML relies on an estimate of the Long-Term Vector Map (LTVM) that does not change over time. In this paper, we introduce a recursive algorithm to build and update the LTVM over time by reasoning about visibility constraints of objects observed over multiple robot deployments. We use a signed distance function (SDF) to filter out observations of short-term and dynamic features from multiple deployments of the robot. The remaining long-term observations are used to build a vector map by robust local linear regression. The uncertainty in the resulting LTVM is computed via Monte Carlo resampling the observations arising from long-term features. By combining occupancy-grid based SDF filtering of observations with continuous space regression of the filtered observations, our proposed approach builds, updates, and amends LTVMs over time, reasoning about all observations from all robot deployments in an environment. We present experimental results demonstrating the accuracy, robustness, and compact nature of the extracted LTVMs from several long-term robot datasets.},
keywords = {mobile robots;Monte Carlo methods;multi-robot systems;regression analysis;service robots;vectors;},
note = {long-term vector maps curating;LTVM;recursive algorithm;visibility constraints;multiple robot deployments;signed distance function;SDF;short-term features;dynamic features;robust local linear regression;Monte Carlo resampling;occupancy-grid based SDF filtering;continuous space regression;autonomous service mobile robots;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759683},
} 


@inproceedings{21623298 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Performance Analysis of Feature Detectors and Descriptors in Underwater and Polar Environments},
journal = {OCEANS 2021: San Diego - Porto},
author = {Shah, V. and Nir, J. and Kaveti, P. and Singh, H.},
year = {2021//},
pages = {7 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Many scientific mapping surveys that deploy robotic platforms in underwater and polar environments perform Visual Simultaneous Localization and Mapping (VSLAM), Structure for Motion (SfM), and Image Mosaicking. These techniques heavily rely on robust and reliable feature-based vision front ends. The job of a vision front end is to provide correspondence information between different camera views which is then directly fed into a bundle adjustment step. Although the atomic steps involved in constructing a vision front end are well-known, many of the popular choices of the features and their parameters do not perform reliably in a variety of visually degraded underwater and polar environments that are characterized by low texture and contrast, and by unevenly lit and low-overlap imagery. In this paper, we develop novel metrics and quantitative analysis methods which can measure the impact of image pre-processing steps such as Contrast Limited Adaptive Histogram Equalization (CLAHE) on the improvement of vision front-end outputs. Our metrics and quantitative analysis can guide the selection between different feature detectors and descriptors to develop a reliable and robust vision front end that can operate in a wider range of underwater and polar environments. We showcase how CLAHE improves the saliency of features and feature track length on a visually degraded dataset underwater, resulting in a substantial increase in correspondence information for the SfM solution. Finally, we perform an end-to-end SfM analysis that shows reduced accumulated drift over the long term and improved accuracy.},
keywords = {cameras;computer vision;feature extraction;image enhancement;image motion analysis;image segmentation;image sensors;mobile robots;robot vision;SLAM (robots);},
note = {performance analysis;polar environments;scientific mapping surveys;deploy robotic platforms;Visual Simultaneous Localization;Image Mosaicking;robust feature-based vision front ends;reliable feature-based vision front ends;correspondence information;bundle adjustment step;atomic steps;underwater environments;low texture;low-overlap imagery;quantitative analysis methods;image pre-processing steps;Contrast Limited Adaptive Histogram Equalization;vision front-end outputs;different feature detectors;reliable vision front end;robust vision front end;feature track length;visually degraded dataset;end-to-end SfM analysis;},
URL = {http://dx.doi.org/10.23919/OCEANS44145.2021.9705975},
} 


@article{19784652 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Robot-assisted Backscatter Localization for IoT Applications [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Shengkai Zhang and Wei Wang and Sheyang Tang and Shi Jin and Tao Jiang},
year = {2020/05/21},
pages = {13 pp. - },
address = {USA},
abstract = {Recent years have witnessed the rapid proliferation of backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such backscatter tags is crucial for IoT-based smart applications. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, which is laborious for deployment. To empower universal localization service, this paper presents Rover, an indoor localization system that localizes multiple backscatter tags without any start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing measurements from backscattered WiFi signals and inertial sensors to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues including interference among multiple tags, real-time processing, as well as the data marginalization problem in dealing with degenerated motions. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords = {indoor radio;Internet of Things;mobile robots;optimisation;radiofrequency identification;sensor placement;wireless LAN;wireless sensor networks;},
note = {backscattered WiFi signals;joint optimization framework;inertial sensors;multiple backscatter tags;indoor localization system;universal localization service;known positions;map;current backscatter localization systems;IoT-based smart applications;smart homes;smart cities;long-term connectivity;ubiquitous term connectivity;backscatter technologies;rapid proliferation;IoT applications;robot-assisted backscatter localization;localization accuracies;prototype Rover;multiple tags;design addresses practical issues including interference;connected tags;size 74.6 cm;size 39.3 cm;},
} 


@inproceedings{20424730 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Pauls, J.-H. and Petek, K. and Poggenhans, F. and Stiller, C.},
year = {2020//},
pages = {4595 - 601},
address = {Piscataway, NJ, USA},
abstract = {Easy, yet robust long-term localization is still an open topic in research. Existing approaches require either dense maps, expensive sensors, specialized map features or proprietary detectors.We propose using semantic segmentation on a monocular camera to localize directly in a HD map as used for automated driving. This combines lightweight, yet powerful HD maps with the simplicity of monocular vision and the flexibility of neural networks.The major challenges arising from this combination are data association and robustness against misdetections. Association is solved efficiently by applying distance transform on binary per-class images. This provides not only a fast lookup table for a smooth gradient as needed for pose-graph optimization, but also dynamic association by default.A sliding-window pose graph optimization combines single image detections with vehicle odometry, smoothing results and helping overcome even misclassifications in consecutive frames.Evaluation against a highly accurate 6D visual localization shows that our approach can achieve accuracy levels as required for automated driving, being one of the most lightweight and flexible methods to do so.},
keywords = {cameras;computer vision;distance measurement;feature extraction;graph theory;image segmentation;image sensors;mobile robots;object detection;optimisation;pose estimation;robot vision;SLAM (robots);table lookup;},
note = {monocular localization;HD map;semantic segmentation;distance transform;long-term localization;open topic;dense maps;expensive sensors;specialized map features;proprietary detectors;monocular camera;automated driving;powerful HD maps;monocular vision;data association;binary per-class images;pose-graph optimization;dynamic association;highly accurate 6D visual localization;lightweight methods;flexible methods;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9341003},
} 


@article{19932273 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Robot-Assisted Backscatter Localization for IoT Applications},
journal = {IEEE Transactions on Wireless Communications},
journal = {IEEE Trans. Wirel. Commun. (USA)},
author = {Shengkai Zhang and Wei Wang and Sheyang Tang and Shi Jin and Tao Jiang},
volume = { 19},
number = { 9},
year = {Sept. 2020},
pages = {5807 - 18},
issn = {1536-1276},
address = {USA},
abstract = {Recent years have witnessed the rapid proliferation of backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such backscatter tags is crucial for IoT-based smart applications. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, which is laborious for deployment. To empower universal localization service, this paper presents Rover, an indoor localization system that localizes multiple backscatter tags without any start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing measurements from backscattered WiFi signals and inertial sensors to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues including interference among multiple tags, real-time processing, as well as the data marginalization problem in dealing with degenerated motions. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords = {indoor radio;Internet of Things;mobile robots;optimisation;radiofrequency identification;sensor placement;wireless LAN;wireless sensor networks;},
note = {robot-assisted backscatter localization;IoT applications;rapid proliferation;backscatter technologies;ubiquitous term connectivity;long-term connectivity;smart cities;smart homes;IoT-based smart applications;current backscatter localization systems;map;known positions;universal localization service;indoor localization system;multiple backscatter tags;inertial sensors;joint optimization framework;backscattered WiFi signals;connected tags;design addresses practical issues including interference;multiple tags;prototype Rover;localization accuracies;size 74.6 cm;size 39.3 cm;},
URL = {http://dx.doi.org/10.1109/TWC.2020.2997393},
} 


@inproceedings{20424976 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Tong Qin and Tongqing Chen and Yilun Chen and Qing Su},
year = {2020//},
pages = {5939 - 45},
address = {Piscataway, NJ, USA},
abstract = {Autonomous valet parking is a specific application for autonomous vehicles. In this task, vehicles need to navigate in narrow, crowded and GPS-denied parking lots. Accurate localization ability is of great importance. Traditional visual-based methods suffer from tracking lost due to texture-less regions, repeated structures, and appearance changes. In this paper, we exploit robust semantic features to build the map and localize vehicles in parking lots. Semantic features contain guide signs, parking lines, speed bumps, etc, which typically appear in parking lots. Compared with traditional features, these semantic features are long-term stable and robust to the perspective and illumination change. We adopt four surround-view cameras to increase the perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system generates a global visual semantic map. This map is further used to localize vehicles at the centimeter level. We analyze the accuracy and recall of our system and compare it against other methods in real experiments. Furthermore, we demonstrate the practicability of the proposed system by the autonomous parking application.},
keywords = {cameras;data visualisation;driver information systems;feature extraction;Global Positioning System;mobile robots;object detection;road vehicles;robot vision;SLAM (robots);traffic engineering computing;},
note = {AVP-SLAM;semantic visual mapping;autonomous vehicles;parking lot;autonomous valet parking;GPS-denied parking lots;accurate localization ability;traditional visual-based methods;appearance changes;robust semantic features;localize vehicles;traditional features;global visual semantic map;autonomous parking application;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9340939},
} 


@inproceedings{15667335 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Long-term human affordance maps},
journal = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Limosani, R. and Morales, L.Y. and Even, J. and Ferreri, F. and Watanabe, A. and Cavallo, F. and Dario, P. and Hagita, N.},
year = {2015//},
pages = {5748 - 54},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a work on mapping the use of space by humans in long periods of time. Daily geometric maps with the same coordinate frame were generated with SLAM, and in a similar manner, daily affordance density maps (places people use) were generated with the output of a human tracker running on the robot. The contribution of the paper is two-fold: an approach to detect geometric changes to cluster them in similar geometric configurations and the building of geometric and affordance composite maps on each cluster. This approach avoids the loss of long term retrieved information. Geometric similarity was computed using a normal distance approach on the maps. The analysis was performed on data collected by a mobile robot for a period of 4 months accumulating data equivalent to 70 days. Experimental results show that the system is capable of detecting geometric changes in the environment and clustering similar geometric configurations.},
keywords = {mobile robots;object tracking;pattern clustering;SLAM (robots);},
note = {long-term human affordance maps;space use mapping;SLAM;daily affordance density map;human tracker;geometric change detection;geometric composite map;affordance composite map;geometric similarity;normal distance approach;mobile robot;similar geometric configuration clustering;},
URL = {http://dx.doi.org/10.1109/IROS.2015.7354193},
} 


@inproceedings{13919302 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Towards Persistent Localization and Mapping with a Continuous Appearance-Based Topology},
journal = {2012 Robotics: Science and Systems},
author = {Maddern, W. and Milford, M. and Wyeth, G.},
volume = {vol.8},
year = {2013//},
pages = {302 - 9},
address = {Cambridge, MA, USA},
abstract = {Appearance-based localization can provide loop closure detection at vast scales regardless of accumulated metric error. However, the computation time and memory requirements of current appearance-based methods scale not only with the size of the environment but also with the operation time of the platform. Additionally, repeated visits to locations will develop multiple competing representations, which will reduce recall performance over time. These properties impose severe restrictions on long-term autonomy for mobile robots, as loop closure performance will inevitably degrade with increased operation time. In this paper we present a graphical extension to CAT-SLAM, a particle filter-based algorithm for appearance-based localization and mapping, to provide constant computation and memory requirements over time and minimal degradation of recall performance during repeated visits to locations. We demonstrate loop closure detection in a large urban environment with capped computation time and memory requirements and performance exceeding previous appearance-based methods by a factor of 2. We discuss the limitations of the algorithm with respect to environment size, appearance change over time and applications in topological planning and navigation for long-term robot operation.},
keywords = {mobile robots;particle filtering (numerical methods);path planning;SLAM (robots);},
note = {topological navigation;long-term robot operation;topological planning;memory requirements;appearance- based localization and mapping;particle filter-based algorithm;CAT-SLAM;loop closure performance;mobile robots;loop closure detection;continuous appearance-based topology;persistent localization and mapping;},
} 


@inproceedings{19211254 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Mapping and Localization Using Semantic Road Marking with Centimeter-level Accuracy in Indoor Parking Lots},
journal = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
author = {Jiaxin Hu and Ming Yang and Hanqing Xu and Yuesheng He and Chunxiang Wang},
year = {2019//},
pages = {4068 - 73},
address = {Piscataway, NJ, USA},
abstract = {Accurate localization is one of the fundamental tasks of vehicles visual navigation in parking lots. In this paper, we propose a practical and novel solution, which exploits road marking semantic segmentation to attack the problem of long-term and high-precision visual localization. Based on the semantic data association derived from road markings segmentation, point cloud fusion and loop detection strategies are designed to improve the performance of semantic map building. Applying the generated map, we present a point cloud registration algorithm combining semantic and geometric inference to improve the localization precision. Experiments on real-world indoor parking lots prove that the semantic map created by the proposed method reveals more accurate and consistent performance. Moreover, localization error is no more than 10cm, while running in real-time performance.},
keywords = {feature extraction;image registration;image segmentation;mobile robots;object detection;robot vision;},
note = {loop detection strategies;semantic map building;point cloud registration;semantic inference;geometric inference;localization precision;real-world indoor parking lots;localization error;semantic road marking;centimeter-level accuracy;semantic segmentation;semantic data association;road marking segmentation;point cloud fusion;vehicle visual navigation;},
URL = {http://dx.doi.org/10.1109/ITSC.2019.8917529},
} 


@inproceedings{20425215 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Multi-Robot Joint Visual-Inertial Localization and 3-D Moving Object Tracking},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Pengxiang Zhu and Wei Ren},
year = {2020//},
pages = {11573 - 80},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we present a novel distributed algorithm to track a moving object's state by utilizing a heterogenous mobile robot network in a three-dimensional (3-D) environment, wherein the robots' poses (positions and orientations) are unknown. Each robot is equipped with a monocular camera and an inertial measurement unit (IMU), and has the ability to communicate with its neighbors. Rather than assuming a known common global frame for all the robots (which is often the case in the literature regarding multi-robot systems), we allow each robot to perform motion estimation locally. For localization, we propose a multi-robot visual-inertial navigation systems (VINS) where one robot builds a prior map and then the map is used to bound the long-term drifts of the visual-inertial odometry (VIO) running on the other robots. Moreover, a novel distributed Kalman filter is introduced and employed to cooperatively track the six degree-of-freedom (6-DoF) motion of the object which is represented as a point cloud. Further, the object can be totally invisible to some robots during the tracking period. The proposed algorithm is extensively validated in Monte-Carlo simulations.},
keywords = {computational geometry;distance measurement;distributed algorithms;inertial navigation;Kalman filters;mobile robots;Monte Carlo methods;motion estimation;multi-robot systems;object tracking;path planning;pose estimation;position control;robot vision;SLAM (robots);},
note = {visual-inertial odometry;tracking period;multirobot joint visual-inertial localization;distributed algorithm;heterogenous mobile robot network;inertial measurement unit;multirobot visual-inertial navigation systems;3D moving object tracking;three-dimensional environment;monocular camera;IMU;motion estimation;multirobot VINS;distributed Kalman filter;six degree-of-freedom motion;6-DoF motion;point cloud;Monte-Carlo simulations;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9341393},
} 


@article{17099183 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {FreMEn: Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments},
journal = {IEEE Transactions on Robotics},
journal = {IEEE Trans. Robot. (USA)},
author = {Krajnik, T. and Fentanes, J.P. and Santos, J.M. and Duckett, T.},
volume = { 33},
number = { 4},
year = {2017/08/},
pages = {964 - 77},
issn = {1552-3098},
address = {USA},
abstract = {We present a new approach to long-term mobile robot mapping in dynamic indoor environments. Unlike traditional world models that are tailored to represent static scenes, our approach explicitly models environmental dynamics. We assume that some of the hidden processes that influence the dynamic environment states are periodic and model the uncertainty of the estimated state variables by their frequency spectra. The spectral model can represent arbitrary timescales of environment dynamics with low memory requirements. Transformation of the spectral model to the time domain allows for the prediction of the future environment states, which improves the robot's long-term performance in changing environments. Experiments performed over time periods of months to years demonstrate that the approach can efficiently represent large numbers of observations and reliably predict future environment states. The experiments indicate that the model's predictive capabilities improve mobile robot localization and navigation in changing environments.},
keywords = {mobile robots;state estimation;uncertain systems;},
note = {FreMEn;frequency map enhancement;long-term mobile robot autonomy;long-term mobile robot mapping;dynamic indoor environments;environmental dynamics;uncertainty;estimated state variables;frequency spectra;spectral model;low memory requirements;time domain;},
URL = {http://dx.doi.org/10.1109/TRO.2017.2665664},
} 


@inproceedings{19410544 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Real-Time, Environmentally-Robust 3D LiDAR Localization},
journal = {2019 IEEE International Conference on Imaging Systems and Techniques (IST). Proceedings},
author = {Yilong Zhu and Bohuan Xue and Linwei Zheng and Huaiyang Huang and Ming Liu and Rui Fan},
year = {2019//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Localization, or position fixing, is an important problem in robotics research. In this paper, we propose a novel approach for long-term localization in a changing environment using 3D LiDAR. We first create the map of a real environment using GPS and LiDAR. Then, we divide the map into several small parts as the targets for cloud registration, which can not only improve the robustness but also reduce the registration time. We proposed a localization method called PointLocalization. PointLocalization allows us to fuse different kinds of odometers, which can optimize the accuracy and frequency of localization results. We evaluate our algorithm on an unmanned ground vehicle (UGV) using LiDAR and a wheel encoder, and obtain the localization results at more than 20 Hz after fusion. The algorithm can also localize the UGV in a 180-degree field of view (FOV). Using an outdated map captured six months ago, this algorithm shows great robustness, and the test results show that it can achieve an accuracy of 10 cm. PointLocalization has been tested for a period of more than six months in a crowded factory and has operated successfully over a distance of more than 2000 km.},
keywords = {cloud computing;optical radar;remotely operated vehicles;},
note = {environmentally-robust 3D LiDAR localization;position fixing;long-term localization;cloud registration;registration time;localization method;PointLocalization;unmanned ground vehicle;UGV;outdated map;wheel encoder;field of view;distance 2000.0 km;frequency 20.0 Hz;},
URL = {http://dx.doi.org/10.1109/IST48021.2019.9010305},
} 


@inproceedings{20195130 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Global Alignment of Deep Features for Robot Localization in Changing Environment},
journal = {2019 3rd European Conference on Electrical Engineering and Computer Science (EECS)},
author = {Oh, J.H. and Heung-Jae Lee},
year = {2019//},
pages = {72 - 5},
address = {Piscataway, NJ, USA},
abstract = {Localization is an elemental requirement for autonomous navigation, simultaneous localization and mapping for mobile robots. As robots can perform long-term and large-scale tasks, finding locations in changing environment is a crucial problem. To resolve the problem, we present a robust localization system under severe appearance changes. The system consists of two stages. First, a robust feature extraction method using a deep convolutional auto-encoder is proposed. Then, global alignment of extracted feature sequences is proposed to find the actual robot's locations. Since the proposed method not only uses the condition-robust features but also considers the actual trajectory of the robot by aligning features sequences, it can show accurate localization performances in changing environments. Experiments were conducted to prove the effective of the proposed method, and the results showed that our method outperformed than existing methods.},
keywords = {control engineering computing;convolutional neural nets;feature extraction;image sequences;mobile robots;navigation;robot vision;SLAM (robots);},
note = {simultaneous localization and mapping;localization performances;condition-robust features;feature sequence extraction;deep convolutional auto-encoder;robust feature extraction method;robust localization system;large-scale tasks;mobile robots;autonomous navigation;robot localization;deep features;global alignment;},
URL = {http://dx.doi.org/10.1109/EECS49779.2019.00026},
} 


@article{16246630 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Training a Convolutional Neural Network for Appearance-Invariant Place Recognition [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Gomez-Ojeda, R. and Lopez-Antequera, M. and Petkov, N. and Gonzalez-Jimenez, J.},
year = {2015/05/27},
pages = {9 pp. - },
address = {USA},
abstract = {Place recognition is one of the most challenging problems in computer vision, and has become a key part in mobile robotics and autonomous driving applications for performing loop closure in visual SLAM systems. Moreover, the difficulty of recognizing a revisited location increases with appearance changes caused, for instance, by weather or illumination variations, which hinders the long-term application of such algorithms in real environments. In this paper we present a convolutional neural network (CNN), trained for the first time with the purpose of recognizing revisited locations under severe appearance changes, which maps images to a low dimensional space where Euclidean distances represent place dissimilarity. In order for the network to learn the desired invariances, we train it with triplets of images selected from datasets which present a challenging variability in visual appearance. The triplets are selected in such way that two samples are from the same location and the third one is taken from a different place. We validate our system through extensive experimentation, where we demonstrate better performance than state-of-art algorithms in a number of popular datasets.},
keywords = {computer vision;image recognition;mobile robots;neural nets;SLAM (robots);},
note = {convolutional neural network;appearance-invariant place recognition;computer vision;mobile robotics;autonomous driving applications;visual SLAM systems;CNN;place dissimilarity;Euclidean distances;},
} 


@article{19387316 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Learning matchable image transformations for long-term metric visual localization},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Clement, L. and Gridseth, M. and Tomasi, J. and Kelly, J.},
volume = { 5},
number = { 2},
year = {2020/04/},
pages = {1492 - 9},
issn = {2377-3774},
address = {USA},
abstract = {Long-term metric self-localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems due to appearance changes caused by lighting, weather, or seasonal variations. While experience-based mapping has proven to be an effective technique for bridging the `appearance gap,' the number of experiences required for reliable metric localization over days or months can be very large, and methods for reducing the necessary number of experiences are needed for this approach to scale. Taking inspiration from color constancy theory, we learn a nonlinear RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature matches for images captured under different lighting and weather conditions, and use it as a pre-processing step in a conventional single-experience localization pipeline to improve its robustness to appearance change. We train this mapping by approximating the target non-differentiable localization pipeline with a deep neural network, and find that incorporating a learned low-dimensional context feature can further improve cross-appearance feature matching. Using synthetic and real-world datasets, we demonstrate substantial improvements in localization performance across day-night cycles, enabling continuous metric localization over a 30-hour period using a single mapping experience, and allowing experience-based localization to scale to long deployments with dramatically reduced data requirements.},
keywords = {feature extraction;image colour analysis;image matching;learning (artificial intelligence);mobile robots;neural nets;object recognition;robot vision;SLAM (robots);},
note = {matchable image transformations;long-term metric visual localization;long-term metric self-localization;autonomous mobile robots;vision-based systems;appearance change;seasonal variations;experience-based mapping;appearance gap;reliable metric localization;color constancy theory;RGB-to-grayscale mapping;inlier feature;weather conditions;single-experience localization pipeline;target nondifferentiable localization pipeline;low-dimensional context feature;cross-appearance feature matching;localization performance;day-night cycles;continuous metric localization;single mapping experience;experience-based localization;dramatically reduced data requirements;},
URL = {http://dx.doi.org/10.1109/LRA.2020.2967659},
} 


@inproceedings{15686682 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Loop closure detection by compressed sensing for exploration of mobile robots in outdoor environments},
journal = {2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM). Proceedings},
author = {Ravari, A.N. and Taghirad, H.D.},
year = {2015//},
pages = {511 - 16},
address = {Piscataway, NJ, USA},
abstract = {In the problem of simultaneously localization and mapping (SLAM) for a mobile robot, it is required to detect previously visited locations so the estimation error shall be reduced. Sensor observations are compared by a similarity metric to detect loops. In long term navigation or exploration, the number of observations increases and so the complexity of the loop closure detection. Several techniques are proposed in order to reduce the complexity of loop closure detection. Few algorithms have considered the loop closure detection from a subset of sensor observations. In this paper, the compressed sensing approach is exploited to detect loops from few sensor measurements. In the basic compressed sensing it is assumed that a signal has a sparse representation is a basis which means that only a few elements of the signal are non-zero. Based on the compressed sensing approach a sparse signal can be recovered from few linear noisy projections by l<sub>1</sub> minimization. The difference matrix which is widely used for loop detection has a sparse structure, where similar observations are shown by zero distance and different locations are indicated by ones. Based on the multiple measurement vector technique which is an extension of the basic compressed sensing, the loop closure detection is performed by comparison of few sensor observations. The applicability of the proposed algorithm is investigated in some outdoor environments through some publicly available data sets. It has been shown by some experiments that the proposed method can detect loops effectively.},
keywords = {compressed sensing;matrix algebra;mobile robots;object detection;robot vision;SLAM (robots);},
note = {loop closure detection;compressed sensing;mobile robots;SLAM;simultaneously localization and mapping;sensor observation;sensor measurement;signal representation;difference matrix;l<sub>1</sub> minimization;},
URL = {http://dx.doi.org/10.1109/ICRoM.2015.7367836},
} 


@inproceedings{19321492 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Tightly Coupled Semantic RGB-D Inertial Odometry for Accurate Long-Term Localization and Mapping},
journal = {2019 19th International Conference on Advanced Robotics (ICAR). Proceedings},
author = {Patel, N. and Khorrami, F. and Krishnamurthy, P. and Tzes, A.},
year = {2019//},
pages = {523 - 8},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we utilize semantically enhanced feature matching and visual inertial bundle adjustment to improve the robustness of odometry especially in feature-sparse environments. A novel semantically enhanced feature matching algorithm is developed for robust: 1) medium and long-term tracking, and 2) loop-closing. Additionally, a semantic visual inertial bundle adjustment algorithm is introduced to robustly estimate pose in presence of ambiguous correspondences or in feature sparse environment. Our tightly coupled semantic RGB-D odometry approach is demonstrated on a real world indoor dataset collected using our unmanned ground vehicle (UGV). Our approach improves traditional visual odometry relying on low-level geometric features like corners, points, and planes for localization and mapping. Additionally, prior approaches are limited due to their sensitivity to scene geometry and changes in light intensity. The semantic inertial odometry is especially important to significantly reduce drifts in longer intervals.},
keywords = {distance measurement;feature extraction;geometry;image matching;mobile robots;pose estimation;robot vision;SLAM (robots);},
note = {feature-sparse environments;long-term tracking;semantic visual inertial bundle adjustment algorithm;feature sparse environment;tightly coupled semantic RGB-D odometry approach;traditional visual odometry;low-level geometric features;semantic inertial odometry;semantic RGB-D inertial odometry;accurate long-term localization;semantically enhanced feature matching algorithm;},
URL = {http://dx.doi.org/10.1109/ICAR46387.2019.8981658},
} 


@inproceedings{15583143 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {An Optimization Technique for Positioning Multiple Maps for Self-Driving Car's Autonomous Navigation},
journal = {2015 IEEE 18th International Conference on Intelligent Transportation Systems (ITSC). Proceedings},
author = {Dominguez, S. and Khomutenko, B. and Garcia, G. and Martinet, P.},
year = {2015//},
pages = {2694 - 9},
address = {Los Alamitos, CA, USA},
abstract = {Self-driving car's navigation requires a very precise localization covering wide areas and long distances. Moreover, they have to do it at faster speeds than conventional mobile robots. This paper reports on an efficient technique to optimize the position of a sequence of maps along a journey. We take advantage of the short-term precision and reduced space on disk of the localization using 2D occupancy grid maps, from now on called sub-maps, as well as, the long-term global consistency of a Kalman filter that fuses odometry and GPS measurements. In our approach, horizontal planar LiDARs and odometry measurements are used to perform 2D-SLAM generating the sub-maps, and the EKF to generate the trajectory followed by the car in global coordinates. During the trip, after finishing each sub-map, a relaxation process is applied to a set of the last sub-maps to position them globally using both, global and map's local path. The importance of this method lies on its performance, expending low computing resources, so it can work in real time on a computer with conventional characteristics and on its robustness which makes it suitable for being used on a self-driving car as it doesn't depend excessively on the availability of GPS signal or the eventual appearance of moving objects around the car. Extensive testing has been performed in the suburbs and in the down-town of Nantes (France) covering a distance of 25 kilometers with different traffic conditions obtaining satisfactory results for autonomous driving.},
keywords = {cartography;control engineering computing;distance measurement;Global Positioning System;Kalman filters;mobile robots;optical radar;optimisation;road vehicles;},
note = {autonomous driving;GPS signal;computing resource;relaxation process;EKF;2D-SLAM;odometry measurement;horizontal planar LiDAR;GPS measurement;Kalman filter;long-term global consistency;2D occupancy grid map;reduced space;short-term precision;mobile robot;self-driving car autonomous navigation;positioning multiple map;optimization technique;},
URL = {http://dx.doi.org/10.1109/ITSC.2015.433},
} 


@inproceedings{12510393 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Hierarchical loop detection for mobile outdoor robots},
journal = {Proceedings of the SPIE - The International Society for Optical Engineering},
journal = {Proc. SPIE - Int. Soc. Opt. Eng. (USA)},
author = {Lang, D. and Winkens, C. and Haselich, M. and Paulus, D.},
volume = { 8301},
year = {2012//},
pages = {83010P (11 pp.) - },
issn = {0277-786X},
address = {USA},
abstract = {Loop closing is a fundamental part of 3D simultaneous localization and mapping (SLAM) that can greatly enhance the quality of long-term mapping. It is essential for the creation of globally consistent maps. Conceptually, loop closing is divided into detection and optimization. Recent approaches depend on a single sensor to recognize previously visited places in the loop detection stage. In this study, we combine data of multiple sensors such as GPS, vision, and laser range data to enhance detection results in repetitively changing environments that are not sufficiently explained by a single sensor. We present a fast and robust hierarchical loop detection algorithm for outdoor robots to achieve a reliable environment representation even if one or more sensors fail.},
keywords = {mobile robots;sensors;SLAM (robots);},
note = {mobile outdoor robots;loop closing;3D simultaneous localization and mapping;SLAM;globally consistent maps;single sensor;multiple sensors;GPS;laser range data;vision;hierarchical loop detection algorithm;},
URL = {http://dx.doi.org/10.1117/12.908277},
} 


@article{18862549 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Tightly-Coupled Monocular Visual-Odometric SLAM Using Wheels and a MEMS Gyroscope},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Meixiang Quan and Songhao Piao and Minglang Tan and Shi-Sheng Huang},
volume = { 7},
year = {2019//},
pages = {97374 - 89},
issn = {2169-3536},
address = {USA},
abstract = {In this paper, we present a novel tightly coupled probabilistic monocular visual-odometric simultaneous localization and mapping (VOSLAM) algorithm using wheels and a MEMS gyroscope, which can provide accurate, robust, and long-term localization for ground robots. First, we present a novel odometer preintegration theory on manifold; it integrates the wheel encoder measurements and gyroscope measurements to a relative motion constraint that is independent of the linearization point and carefully addresses the uncertainty propagation and gyroscope bias correction. Based on the preintegrated odometer measurement model, we also introduce the odometer error term and tightly integrate it into the visual optimization framework. Then, in order to bootstrap the VOSLAM system, we propose a simple map initialization method. Finally, we present a complete localization mechanism to maximally exploit both sensing cues, which provides different strategies for motion tracking when: 1) both measurements are available; 2) visual measurements are not available; and 3) wheel encoders experience slippage, thereby ensuring the accurate and robust motion tracking. The proposed algorithm is evaluated by performing extensive experiments, and the experimental results demonstrate the superiority of the proposed system.},
keywords = {gyroscopes;image motion analysis;mobile robots;robot vision;SLAM (robots);statistical analysis;trajectory control;},
note = {motion tracking;simple map initialization method;VOSLAM system;visual optimization framework;odometer error term;preintegrated odometer measurement model;gyroscope bias correction;uncertainty propagation;linearization point;relative motion constraint;gyroscope measurements;wheel encoder measurements;odometer preintegration theory;long-term localization;visual-odometric simultaneous localization;MEMS gyroscope;monocular visual-odometric SLAM;},
URL = {http://dx.doi.org/10.1109/ACCESS.2019.2930201},
} 


@article{19483531 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Map as the hidden sensor: fast odometry-based global localization [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Cheng Peng and Weikersdorfer, D.},
year = {20 Sept. 2019},
pages = {7 pp. - },
address = {USA},
abstract = {Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in long-term, our method using only odometry and the map converges in longterm. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~ 300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.},
keywords = {distance measurement;mobile robots;path planning;robot vision;SLAM (robots);},
note = {fast odometry-based global localization;belief tensor;map-corrected odometry localization;map traversability;robotics applications;robust global localization;odometry drift;blind robots;robot state;hidden observation;},
} 


@article{18059467 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Vision-aided Multi-UAV Autonomous Flocking in GPS-denied Environment},
journal = {IEEE Transactions on Industrial Electronics},
journal = {IEEE Trans. Ind. Electron. (USA)},
author = {Yazhe Tang and Yuchao Hu and Jinqiang Cui and Fang Liao and Mingjie Lao and Feng Lin and Teo, R.S.H.},
volume = { 66},
number = { 1},
year = {2019/01/},
pages = {616 - 26},
issn = {0278-0046},
address = {USA},
abstract = {This paper presents a sophisticated vision-aided flocking system for unmanned aerial vehicles (UAVs), which is able to operate in GPS-denied unknown environments for exploring and searching missions, and also able to adopt two types of vision sensors, day and thermal cameras, to measure relative motion between UAVs in different lighting conditions without using wireless communication. In order to realize robust vision-aided flocking, an integrated framework of tracking-learning-detection on the basis of multifeature coded correlation filter has been developed. To achieve long-term tracking, a redetector is trained online to adaptively reinitialize target for global sensing. An advanced flocking strategy is developed to address the autonomous multi-UAVs' cooperative flight. Light detection and ranging (LiDAR)-based navigation modules are developed for autonomous localization, mapping, and obstacle avoidance. Flight experiments of a team of UAVs have been conducted to verify the performance of this flocking system in a GPS-denied environment. The extensive experiments validate the robustness of the proposed vision algorithms in challenging scenarios.},
keywords = {autonomous aerial vehicles;collision avoidance;Global Positioning System;image sensors;mobile robots;multi-robot systems;optical radar;robot vision;},
note = {GPS-denied environment;vision algorithms;vision-aided multiUAV autonomous flocking;unmanned aerial vehicles;UAVs;GPS-denied unknown environments;vision sensors;tracking-learning-detection;multifeature coded correlation filter;long-term tracking;advanced flocking strategy;autonomous localization;lighting conditions;autonomous multiUAVs cooperative flight;vision-aided flocking system;searching missions;thermal cameras;wireless communication;global sensing;light detection and ranging-based navigation modules;obstacle avoidance;autonomous mapping;LIDAR;},
URL = {http://dx.doi.org/10.1109/TIE.2018.2824766},
} 


@article{19925589 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Learning to calibrate: Reinforcement learning for guided calibration of visual-inertial rigs},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Nobre, F. and Heckman, C.},
volume = { 38},
number = { 12-13},
year = {2019/10/},
pages = {1388 - 402},
issn = {0278-3649},
address = {USA},
abstract = {We present a new approach to assisted intrinsic and extrinsic calibration with an observability-aware visual-inertial calibration system that guides the user through the calibration procedure by suggesting easy-to-perform motions that render the calibration parameters observable. This is done by identifying which subset of the parameter space is rendered observable with a rank-revealing decomposition of the Fisher information matrix, modeling calibration as a Markov decision process and using reinforcement learning to establish which discrete sequence of motions optimizes for the regression of the desired parameters. The goal is to address the assumption common to most calibration solutions: that sufficiently informative motions are provided by the operator. We do not make use of a process model and instead leverage an experience-based approach that is broadly applicable to any platform in the context of simultaneous localization and mapping. This is a step in the direction of long-term autonomy and ldquopower-on-and-gordquo robotic systems, making repeatable and reliable calibration accessible to the non-expert operator.},
keywords = {calibration;cameras;inertial navigation;learning (artificial intelligence);Markov processes;mobile robots;nonlinear filters;SLAM (robots);},
note = {experience-based approach;repeatable calibration;reliable calibration;guided calibration;visual-inertial rigs;assisted intrinsic calibration;extrinsic calibration;observability-aware visual-inertial calibration system;calibration procedure;render;calibration parameters;parameter space;rank-revealing decomposition;Fisher information matrix;Markov decision process;using reinforcement learning;motions optimizes;desired parameters;calibration solutions;sufficiently informative motions;process model;},
URL = {http://dx.doi.org/10.1177/0278364919844824},
} 


@article{20229951 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {ClusterMap Building and Relocalization in Urban Environments for Unmanned Vehicles},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Zhichen Pan and Haoyao Chen and Silin Li and Yunhui Liu},
volume = { 19},
number = { 19},
year = {2019/10/},
pages = {4252 (22 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {Map building and map-based relocalization techniques are important for unmanned vehicles operating in urban environments. The existing approaches require expensive high-density laser range finders and suffer from relocalization problems in long-term applications. This study proposes a novel map format called the ClusterMap, on the basis of which an approach to achieving relocalization is developed. The ClusterMap is generated by segmenting the perceived point clouds into different point clusters and filtering out clusters belonging to dynamic objects. A location descriptor associated with each cluster is designed for differentiation. The relocalization in the global map is achieved by matching cluster descriptors between local and global maps. The solution does not require high-density point clouds and high-precision segmentation algorithms. In addition, it prevents the effects of environmental changes on illumination intensity, object appearance, and observation direction. A consistent ClusterMap without any scale problem is built by utilizing a 3D visual-LIDAR simultaneous localization and mapping solution by fusing LIDAR and visual information. Experiments on the KITTI dataset and our mobile vehicle illustrates the effectiveness of the proposed approach.},
keywords = {image segmentation;laser ranging;mobile robots;optical radar;path planning;pattern clustering;remotely operated vehicles;robot vision;SLAM (robots);},
note = {ClusterMap building;urban environments;unmanned vehicles;map building;map-based relocalization techniques;high-density laser range finders;relocalization problems;long-term applications;map format;perceived point clouds;point clusters;dynamic objects;location descriptor;cluster descriptors;local maps;global maps;high-density point clouds;high-precision segmentation algorithms;consistent ClusterMap;scale problem;mapping solution;mobile vehicle;3D visual-LIDAR simultaneous localization and mapping solution;visual information;KITTI dataset;},
URL = {http://dx.doi.org/10.3390/s19194252},
} 


@article{20115767 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Persistent Stereo Visual Localization on Cross-Modal Invariant Map},
journal = {IEEE Transactions on Intelligent Transportation Systems},
journal = {IEEE Trans. Intell. Transp. Syst. (USA)},
author = {Xiaqing Ding and Yue Wang and Rong Xiong and Dongxuan Li and Li Tang and Huan Yin and Liang Zhao},
volume = { 21},
number = { 11},
year = {2020/11/},
pages = {4646 - 58},
issn = {1524-9050},
address = {USA},
abstract = {Autonomous mobile vehicles are expected to perform persistent and accurate localization with low-cost equipment. To achieve this goal, we propose a stereo camera based visual localization method using a modified laser map, which takes the advantage of both the low cost of camera, and high geometric precision of laser data to achieve long-term performance. Considering that LiDAR and camera give measurements of the same environment in different modalities, the cross-modal invariance is investigated to modify the laser map for visual localization. Specifically, a map learning algorithm is introduced to sample the robust subsets in laser maps that are useful for visual localization using multi-session visual and laser data. Further, a generative map model is derived to describe this cross-modal invariance, based on which two types of measurements are defined to model the laser map points as appropriate visual observations. Tightly coupling these measurements within the local bundle adjustment during online sliding-window based visual odometry, the vehicle can achieve robust localization even one year after the map was built. The effectiveness of the proposed method is evaluated on both the public KITTI datasets and self-collected datasets in our campus, which include seasonal, illumination and object variations. On all experimental localization sessions, our method provides satisfactory results, even when the direction is opposite to that in the mapping session, verifying the superior performance of the laser map based visual localization method.},
keywords = {cameras;distance measurement;invariance;learning (artificial intelligence);mobile robots;path planning;road traffic control;robot vision;SLAM (robots);stereo image processing;traffic engineering computing;},
note = {persistent stereo visual localization;cross-modal invariant map;autonomous mobile vehicles;low-cost equipment;stereo camera;laser data;cross-modal invariance;map learning algorithm;multisession visual;generative map model;laser map points;local bundle adjustment;online sliding-window based visual odometry;public KITTI datasets;self-collected datasets;},
URL = {http://dx.doi.org/10.1109/TITS.2019.2942760},
} 


@inproceedings{14718688 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Mining visual phrases for long-term visual SLAM},
journal = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014)},
author = {Kanji, T. and Yuuto, C. and Masatoshi, A.},
year = {2014//},
pages = {136 - 42},
address = {Piscataway, NJ, USA},
abstract = {We propose a discriminative and compact scene descriptor for single-view place recognition that facilitates long-term visual SLAM in familiar, semi-dynamic and partially changing environments. In contrast to popular bag-of-words scene descriptors, which rely on a library of vector quantized visual features, our proposed scene descriptor is based on a library of raw image data (such as an available visual experience, images shared by other colleague robots, and publicly available image data on the web) and directly mine it to find visual phrases (VPs) that discriminatively and compactly explain an input query / database image. Our mining approach is motivated by recent success in the field of common pattern discovery-specifically mining of common visual patterns among scenes-and requires only a single library of raw images that can be acquired at different time or day. Experimental results show that even though our scene descriptor is significantly more compact than conventional descriptors it has a relatively higher recognition performance.},
keywords = {data mining;feature extraction;robot vision;SLAM (robots);},
note = {visual phrase mining;long-term visual SLAM;single-view place recognition;simultaneous localization and mapping;bag-of-words scene descriptor;vector quantized visual features;common pattern discovery;recognition performance;},
URL = {http://dx.doi.org/10.1109/IROS.2014.6942552},
} 


@article{19820626 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Multi-domain airflow modeling and ventilation characterization using mobile robots, stationary sensors and machine learning},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Hernandez Bennetts, V. and Kamarudin, K. and Wiedemann, T. and Kucner, T.P. and Somisetty, S.L. and Lilienthal, A.J.},
volume = { 19},
number = { 5},
year = {2019/03/},
pages = {1119 (21 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {Ventilation systems are critically important components of many public buildings and workspaces. Proper ventilation is often crucial for preventing accidents, such as explosions in mines and avoiding health issues, for example, through long-term exposure to harmful respirable matter. Validation and maintenance of ventilation systems is thus of key interest for plant operators and authorities. However, methods for ventilation characterization, which allow us to monitor whether the ventilation system in place works as desired, hardly exist. This article addresses the critical challenge of ventilation characterization-measuring and modelling air flow at micro-scales-that is, creating a high-resolution model of wind speed and direction from airflow measurements. Models of the near-surface micro-scale flow fields are not only useful for ventilation characterization, but they also provide critical information for planning energy-efficient paths for aerial robots and many applications in mobile robot olfaction. In this article we propose a heterogeneous measurement system composed of static, continuously sampling sensing nodes, complemented by localized measurements, collected during occasional sensing missions with a mobile robot. We introduce a novel, data-driven, multi-domain airflow modelling algorithm that estimates (1) fields of posterior distributions over wind direction and speed (ldquoventilation mapsrdquo, spatial domain); (2) sets of ventilation calendars that capture the evolution of important airflow characteristics at measurement positions (temporal domain); and (3) a frequency domain analysis that can reveal periodic changes of airflow in the environment. The ventilation map and the ventilation calendars make use of an improved estimation pipeline that incorporates a wind sensor model and a transition model to better filter out sporadic, noisy airflow changes. These sudden changes may originate from turbulence or irregular activity in the surveyed environment and can, therefore, disturb modelling of the relevant airflow patterns. We tested the proposed multi-domain airflow modelling approach with simulated data and with experiments in a semi-controlled environment and present results that verify the accuracy of our approach and its sensitivity to different turbulence levels and other disturbances. Finally, we deployed the proposed system in two different real-world industrial environments (foundry halls) with different ventilation regimes for three weeks during full operation. Since airflow ground truth cannot be obtained, we present a qualitative discussion of the generated airflow models with plant operators, who concluded that the computed models accurately depicted the expected airflow patterns and are useful to understand how pollutants spread in the work environment. This analysis may then provide the basis for decisions about corrective actions to avoid long-term exposure of workers to harmful respirable matter.},
keywords = {chemioception;computerised instrumentation;flow measurement;flow sensors;learning (artificial intelligence);mobile robots;ventilation;},
note = {high-resolution model;airflow measurements;microscale flow fields;mobile robot olfaction;heterogeneous measurement system;localized measurements;ventilation map;spatial domain analysis;ventilation calendars;measurement positions;temporal domain;frequency domain analysis;wind sensor model;transition model;sporadic airflow changes;noisy airflow changes;generated airflow models;plant operators;computed models;expected airflow patterns;harmful respirable matter;multidomain airflow modeling;stationary sensors;ventilation system characterization;airflow ground truth patterns;air flow modelling;mobile robot;machine learning;},
URL = {http://dx.doi.org/10.3390/s19051119},
} 


@article{11959739 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Comparison of Two Image and Inertial Sensor Fusion Techniques for Navigation in Unmapped Environments},
journal = {IEEE Transactions on Aerospace and Electronic Systems},
journal = {IEEE Trans. Aerosp. Electron. Syst. (USA)},
author = {Taylor, C.N. and Veth, M.J. and Raquet, J.F. and Miller, M.M.},
volume = { 47},
number = { 2},
year = {2011/04/},
pages = {946 - 58},
issn = {0018-9251},
address = {USA},
abstract = {To enable navigation of miniature aerial vehicles (MAVs) with a low-quality inertial measurement unit (IMU), external sensors are typically fused with the information generated by the low-quality IMU. Most commercial systems for MAVs currently fuse GPS measurements with IMU information to navigate the MAV. However there are many scenarios in which an MAV might prove useful, but GPS is not available (e.g., indoors, urban terrain, etc.). Therefore several approaches have recently been introduced that couple information from an IMU with visual information (usually captured by an electro-optical camera). In general the methods for fusing visual information with an IMU utilizes one of two techniques: 1) applying rigid body constraints on where landmarks should appear in a set of two images (constraint-based fusion) or 2) simultaneously estimating the location of features that are observed by the camera (mapping) and the location of the camera (simultaneous localization and mapping-SLAM-based fusion). While each technique has some nuances associated with its implementation in a true MAV environment (i.e., computational requirements, real-time implementation, feature tracking, etc.), this paper focuses solely on answering the question "Which fusion technique (constraint- or SLAM-based) enables more accurate long-term MAV navigation?" To answer this question, specific implementations of a constraint- and SLAM-based fusion technique, with novel modifications for improved results on MAVs, are described. A basic simulation environment is used to perform a comparison of the constraint- and SLAM-based fusion methods. We demonstrate the superiority of SLAM-based techniques in specific MAV flight scenarios and discuss the relative weaknesses and strengths of each fusion approach.},
keywords = {aircraft;cameras;data visualisation;Global Positioning System;image fusion;mobile robots;remotely operated vehicles;SLAM (robots);},
note = {image comparison;inertial sensor fusion technique;unmapped environment;miniature aerial vehicle;inertial measurement unit;commercial system;GPS measurement;IMU information;visual information;constraint based fusion;simultaneous localization and mapping;SLAM based fusion;},
URL = {http://dx.doi.org/10.1109/TAES.2011.5751236},
} 


@article{16660801 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Exploration and mapping technique suited for visual-features based localization of MAVs},
journal = {Journal of Intelligent &amp; Robotic Systems},
journal = {J. Intell. Robot. Syst. (Germany)},
author = {Chudoba, J. and Kulich, M. and Saska, M. and Baa, T. and Preuil, L.},
volume = { 84},
number = { 1-4},
year = {2016/12/},
pages = {351 - 69},
issn = {0921-0296},
address = {Germany},
abstract = {An approach for long term localization, stabilization, and navigation of micro-aerial vehicles (MAVs) in unknown environment is presented in this paper. The proposed method relies strictly on onboard sensors of employed MAVs and does not require any external positioning system. The core of the method consists in extraction of information from pictures consequently captured using a camera carried by the particular MAV. Visual features are obtained from images of the surface under the MAV, and stored into a map that is represented by these features. The position of the MAV is then obtained through matching with previously stored features. An important part of the proposed system is a novel approach for exploration and mapping of the workspace of robots. This method enables efficient exploring of the unknown environment, while keeping the iteratively built map of features consistent. The proposed algorithm is suitable for mapping of surfaces, both outdoor and indoor, with various density of the image features. The sufficient precision and long term persistence of the method allows its utilization for stabilization of large MAV groups that work in formations with small relative distances between particular vehicles. Numerous experiments with quadrotor helicopters and various numerical simulations have been realized for verification of the entire system and its components.},
keywords = {autonomous aerial vehicles;feature extraction;microrobots;SLAM (robots);},
note = {robot workspace;image features;microaerial vehicles;MAVs;visual-features based localization;mapping technique;exploration technique;},
URL = {http://dx.doi.org/10.1007/s10846-016-0358-8},
} 


@inproceedings{18548176 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Approximating Marginalization with Sparse Global Priors for Sliding Window SLAM-Graphs},
journal = {2019 Third IEEE International Conference on Robotic Computing (IRC). Proceedings},
author = {Wilbers, D. and Rumberg, L. and Stachniss, C.},
year = {2019//},
pages = {25 - 31},
address = {Los Alamitos, CA, USA},
abstract = {Most autonomous vehicles rely on some kind of map for localization or navigation. Outdated maps however are a risk to the performance of any map-based localization system applied in autonomous vehicles. It is necessary to update the used maps to ensure stable and long-term operation. We address the problem of computing landmark updates live in the vehicle, which requires efficient use of the computational resources. In particular, we employ a graph-based sliding window approach for simultaneous localization and incremental map refinement. We propose a novel method that approximates sliding window marginalization without inducing fill-in. Our method maintains the exact same sparsity pattern as without performing marginalization, but simultaneously improves the landmark estimates. The main novelty of this work is the derivation of sparse global priors that approximate dense marginalization. In comparison to state-of-the-art work, our approach utilizes global instead of local linearization points, but still minimizes linearization errors. We first approximate marginalization via Kullback-Leibler divergence and then recalculate the mean to compensate linearization errors. We evaluate our approach on simulated and real data from a prototype vehicle and compare our approach to state-of-the-art sliding window marginalization.},
keywords = {approximation theory;error compensation;graph theory;mobile robots;object detection;robot vision;SLAM (robots);},
note = {linearization error compensation;Kullback-Leibler divergence;graph-based sliding window approach;sliding window marginalization;sliding window SLAM-graphs;linearization error minimization;landmark updates;long-term operation;map-based localization system;outdated maps;autonomous vehicles;prototype vehicle;local linearization points;approximate dense marginalization;sparse global priors;landmark estimates;exact same sparsity pattern;incremental map refinement;simultaneous localization;computational resources;},
URL = {http://dx.doi.org/10.1109/IRC.2019.00013},
} 


@inproceedings{15286217 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Work smart, not hard: recalling relevant experiences for vast-scale but time-constrained localisation},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Linegar, C. and Churchill, W. and Newman, P.},
year = {2015//},
pages = {90 - 7},
address = {Piscataway, NJ, USA},
abstract = {This paper is about life-long vast-scale localisation in spite of changes in weather, lighting and scene structure. Building upon our previous work in Experience-based Navigation [1], we continually grow and curate a visual map of the world that explicitly supports multiple representations of the same place. We refer to these representations as experiences, where a single experience captures the appearance of an environment under certain conditions. Pedagogically, an experience can be thought of as a visual memory. By accumulating experiences we are able to handle cyclic appearance change (diurnal lighting, seasonal changes, and extreme weather conditions) and also adapt to slow structural change. This strategy, although elegant and effective, poses a new challenge: In a region with many stored representations - which one(s) should we try to localise against given finite computational resources? By learning from our previous use of the experience-map, we can make predictions about which memories we should consider next, conditioned on how the robot is currently localised in the experience-map. During localisation, we prioritise the loading of past experiences in order to minimise the expected computation required. We do this in a probabilistic way and show that this memory policy significantly improves localisation efficiency, enabling long-term autonomy on robots with limited computational resources. We demonstrate and evaluate our system over three challenging datasets, totalling 206km of outdoor travel. We demonstrate the system in a diverse range of lighting and weather conditions, scene clutter, camera occlusions, and permanent structural change in the environment.},
keywords = {environmental factors;probability;robots;},
note = {time-constrained localisation;vast-scale localisation;scene structure;lighting structure;weather structure;visual map;experience-based navigation;visual memory;cyclic appearance change;diurnal lighting;seasonal changes;extreme weather conditions;experience-map;memory policy;localisation efficiency;long-term autonomy;robots;probabilistic way;computational resources;scene clutter;camera occlusions;permanent structural change;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7138985},
} 


@inproceedings{19315104 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {FLAME: Feature-Likelihood Based Mapping and Localization for Autonomous Vehicles},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Su Pang and Kent, D. and Morris, D. and Radha, H.},
year = {2019//},
pages = {5312 - 19},
address = {Piscataway, NJ, USA},
abstract = {Accurate vehicle localization is arguably the most critical and fundamental task for autonomous vehicle navigation. While dense 3D point-cloud-based maps enable precise localization, they impose significant storage and transmission burdens when used in city-scale environments. In this paper, we propose a highly compressed representation for LiDAR maps, along with an efficient and robust real-time alignment algorithm for on-vehicle LiDAR scans. The proposed mapping framework, which we refer to as Feature Likelihood Acquisition Map Emulation (FLAME), requires less than 0.1% of the storage space of the original 3D point cloud map. In essence, FLAME emulates an original map through feature likelihood functions. In particular, FLAME models planar, pole and curb features. These three feature classes are long-term stable, distinct and common among vehicular roadways. Multiclass feature points are extracted from LiDAR scans through feature detection. A new multiclass-based point-to-distribution alignment method is proposed to find the association and alignment between the multiclass feature points and the FLAME map. The experimental results show that the proposed framework can achieve the same level of accuracy (less than 10cm) as the 3D point cloud based localization.},
keywords = {feature extraction;mobile robots;object detection;optical radar;path planning;road vehicles;robot vision;},
note = {feature likelihood acquisition map emulation;3D point-cloud-based maps;feature-likelihood based mapping;3D point cloud based localization;FLAME map;multiclass-based point-to-distribution alignment method;feature detection;multiclass feature points;feature likelihood functions;storage space;on-vehicle LiDAR scans;real-time alignment algorithm;LiDAR maps;city-scale environments;transmission burdens;autonomous vehicle navigation;vehicle localization;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8968082},
} 


@inproceedings{13336904 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Minimalistic vision-based cognitive slam},
journal = {Proceedings of the 4th International Conference on Agents and Artificial Intelligence (ICAART 2012)},
author = {Saleiro, M. and Rodrigues, J.M.F. and du Buf, J.M.H.},
volume = {vol.1},
year = {2012//},
pages = {614 - 23},
address = {Setubal, Portugal},
abstract = {The interest in cognitive robotics is still increasing, a major goal being to create a system which can adapt to dynamic environments and which can learn from its own experiences. We present a new cognitive SLAM architecture, but one which is minimalistic in terms of sensors and memory. It employs only one camera with pan and tilt control and three memories, without additional sensors nor any odometry. Short-teen memory is an egocentric map which holds information at close range at the actual robot position. Long-term memory is used for mapping the environment and registration of encountered objects. Object memory holds features of learned objects which are used as navigation landmarks and task targets. Saliency maps are used to sequentially focus important areas for object and obstacle detection, but also for selecting directions of movements. Reinforcement learning is used to consolidate or enfeeble environmental information in long-term memory. The system is able to achieve complex tasks by executing sequences of visuomotor actions, decisions being taken by goal-detection and goal-completion tasks. Experimental results show that the system is capable of executing tasks like localizing specific objects while building a map, after which it manages to return to the start position even when new obstacles have appeared.},
keywords = {collision avoidance;learning (artificial intelligence);mobile robots;robot vision;SLAM (robots);},
note = {minimalistic vision based cognitive slam;cognitive robotics;dynamic environments;autonomous mobile robots;visuomotor actions;reinforcement learning;object detection;obstacle detection;saliency maps;navigation landmarks;object memory;robot position;egocentric map;short teen memory;odometry;cognitive SLAM architecture;},
} 


@inproceedings{18372886 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {An Everyday Robotic System that Maintains Local Rules Using Semantic Map Based on Long-Term Episodic Memory},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Furuta, Y. and Okada, K. and Kakiuchi, Y. and Inaba, M.},
year = {2018//},
pages = {7641 - 7},
address = {Piscataway, NJ, USA},
abstract = {To enable robots to work on real home environments, they have to not only consider common knowledge in the global society, but also be aware of existing rules there. Since such &ldquo;local rules&rdquo; are not describable beforehand, robot agents must acquire them through their lives after deployment. To achieve this, we developed a framework that a) lets robots record long-term episodic memories in their deployed environments, b) autonomously builds probabilistic object localization map as structurization of logged data and c) make adapted task plans based on the map. We equipped our framework on PR2 and Fetch robots operating and recording episodic memory for 41 days with semantic common knowledge of the environment. We also conducted demonstrations in which a PR2 robot tidied up a room, showing that the robot agent can successfully plan and execute local-rule-aware home assistive tasks by using our proposed framework.},
keywords = {mobile robots;path planning;},
note = {robot agent;local-rule-aware home assistive tasks;semantic map;long-term episodic memory;home environments;global society;probabilistic object localization map;Fetch robots;semantic common knowledge;PR2 robot;robotic system;time 41.0 d;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8594481},
} 


@article{16740705 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Episodic non-Markov localization},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Biswas, J. and Veloso, M.M.},
volume = { 87},
year = {2017/01/},
pages = {162 - 76},
issn = {0921-8890},
address = {Netherlands},
abstract = {Markov localization and its variants are widely used for mobile robot localization. These methods assume Markov independence of observations, implying that the observations can be entirely explained by a map. However, in real human environments, robots frequently make unexpected observations due to unmapped static objects like chairs and tables, and dynamic objects like humans. We therefore introduce Episodic non-Markov Localization (EnML), which reasons about the world as consisting of three classes of objects: long-term features corresponding to permanent mapped objects, short-term features corresponding to unmapped static objects, and dynamic features corresponding to unmapped moving objects. Long-term features are represented by a static map, while short-term features are detected and tracked in real-time. To reason about unexpected observations and their correlations across poses, we augment the Dynamic Bayesian Network for Markov localization to include varying edges and nodes, resulting in a novel Varying Graphical Network representation. The maximum likelihood estimate of the belief is incrementally computed by non-linear functional optimization. By detecting timesteps along the robot's trajectory where unmapped observations prior to such time steps are unrelated to those afterwards, EnML limits the history of observations and pose estimates to &ldquo;episodes&rdquo; over which the belief is computed. We demonstrate EnML using different types of sensors including laser rangefinders and depth cameras, and over multiple datasets, comparing it with alternative approaches. We further include results of a team of indoor autonomous service mobile robots traversing hundreds of kilometers using EnML. [All rights reserved Elsevier].},
keywords = {belief networks;laser ranging;Markov processes;maximum likelihood estimation;mobile robots;optimisation;sensor placement;trajectory control;},
note = {episodic nonMarkov localization;EnML;mobile robot localization;autonomous service mobile robots;depth cameras;laser rangefinders;episodes;pose estimate limiting;robot trajectory;timestep detection;nonlinear functional optimization;maximum likelihood estimation;varying graphical network representation;dynamic Bayesian network;static map;unmapped moving objects;dynamic features;short-term features;permanent mapped objects;long-term features;unmapped static objects;Markov observation independence;},
URL = {http://dx.doi.org/10.1016/j.robot.2016.09.005},
} 


@article{18623538 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Marker-Based Multi-Sensor Fusion Indoor Localization System for Micro Air Vehicles},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Boyang Xing and Quanmin Zhu and Feng Pan and Xiaoxue Feng},
volume = { 18},
number = { 6},
year = {2018/06/},
pages = {1706 (19 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {A novel multi-sensor fusion indoor localization algorithm based on ArUco marker is designed in this paper. The proposed ArUco mapping algorithm can build and correct the map of markers online with Grubbs criterion and K-mean clustering, which avoids the map distortion due to lack of correction. Based on the conception of multi-sensor information fusion, the federated Kalman filter is utilized to synthesize the multi-source information from markers, optical flow, ultrasonic and the inertial sensor, which can obtain a continuous localization result and effectively reduce the position drift due to the long-term loss of markers in pure marker localization. The proposed algorithm can be easily implemented in a hardware of one Raspberry Pi Zero and two STM32 micro controllers produced by STMicroelectronics (Geneva, Switzerland). Thus, a small-size and low-cost marker-based localization system is presented. The experimental results show that the speed estimation result of the proposed system is better than Px4flow, and it has the centimeter accuracy of mapping and positioning. The presented system not only gives satisfying localization precision, but also has the potential to expand other sensors (such as visual odometry, ultra wideband (UWB) beacon and lidar) to further improve the localization performance. The proposed system can be reliably employed in Micro Aerial Vehicle (MAV) visual localization and robotics control.},
keywords = {aerospace robotics;autonomous aerial vehicles;image fusion;image sequences;Kalman filters;microrobots;mobile robots;object tracking;pose estimation;robot vision;SLAM (robots);},
note = {STM32 microcontrollers;low-cost marker-based localization system;positioning;localization precision;localization performance;robotics control;Microair vehicles;ArUco marker;ArUco mapping algorithm;map distortion;multisensor information fusion;inertial sensor;marker localization;multisensor fusion indoor localization algorithm;Raspberry Pi Zero;STM32 micro controllers;STMicroelectronics;Grubbs criterion;K-mean clustering;federated Kalman filter;},
URL = {http://dx.doi.org/10.3390/s18061706},
} 


@article{16846617 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {1 year, 1000 km: the Oxford RobotCar dataset},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Maddern, W. and Pascoe, G. and Linegar, C. and Newman, P.},
volume = { 36},
number = { 1},
year = {2017/01/},
pages = {3 - 15},
issn = {0278-3649},
address = {UK},
abstract = {We present a challenging new dataset for autonomous driving: the Oxford RobotCar Dataset. Over the period of May 2014 to December 2015 we traversed a route through central Oxford twice a week on average using the Oxford RobotCar platform, an autonomous Nissan LEAF. This resulted in over 1000 km of recorded driving with almost 20 million images collected from 6 cameras mounted to the vehicle, along with LIDAR, GPS and INS ground truth. Data was collected in all weather conditions, including heavy rain, night, direct sunlight and snow. Road and building works over the period of a year significantly changed sections of the route from the beginning to the end of data collection. By frequently traversing the same route over the period of a year we enable research investigating long-term localization and mapping for autonomous vehicles in real-world, dynamic urban environments. The full dataset is available for download at: http://robotcar-dataset.robots.ox.ac.uk.},
keywords = {Global Positioning System;image processing;inertial navigation;mobile robots;optical radar;rain;SLAM (robots);traffic engineering computing;},
note = {Oxford RobotCar dataset;Oxford RobotCar Dataset;size = 1000.0 km;INS;autonomous driving;Nissan LEAF;LIDAR;GPS;autonomous vehicles;size 1000.0 km;},
URL = {http://dx.doi.org/10.1177/0278364916679498},
} 


@inproceedings{16503846 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Appearance-based landmark selection for efficient long-term visual localization},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Burki, M. and Gilitschenski, I. and Stumm, E. and Siegwart, R. and Nieto, J.},
year = {2016//},
pages = {4137 - 43},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we present an online landmark selection method for distributed long-term visual localization systems in bandwidth-constrained environments. Sharing a common map for online localization provides a fleet of autonomous vehicles with the possibility to maintain and access a consistent map source, and therefore reduce redundancy while increasing efficiency. However, connectivity over a mobile network imposes strict bandwidth constraints and thus the need to minimize the amount of exchanged data. The wide range of varying appearance conditions encountered during long-term visual localization offers the potential to reduce data usage by extracting only those visual cues which are relevant at the given time. Motivated by this, we propose an unsupervised method of adaptively selecting landmarks according to how likely these landmarks are to be observable under the prevailing appearance condition. The ranking function this selection is based upon exploits landmark co-observability statistics collected in past traversals through the mapped area. Evaluation is performed over different outdoor environments, large time-scales and varying appearance conditions, including the extreme transition from day-time to night-time, demonstrating that with our appearance-dependent selection method, we can significantly reduce the amount of landmarks used for localization while maintaining or even improving the localization performance.},
keywords = {minimisation;navigation;road vehicles;statistics;traffic control;},
note = {appearance-dependent selection method;landmark coobservability statistics;ranking function;adaptive landmark selection;unsupervised method;visual cue extraction;data usage reduction;exchanged data minimization;mobile network connectivity;redundancy reduction;autonomous vehicles;online localization map;bandwidth-constrained environments;distributed long-term visual localization systems;appearance-based landmark selection;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759609},
} 


@inproceedings{12317927 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Efficient information-theoretic graph pruning for graph-based SLAM with laser range finders},
journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
author = {Kretzschmar, H. and Stachniss, C. and Grisetti, G.},
year = {2011//},
pages = {865 - 71},
address = {Piscataway, NJ, USA},
abstract = {In graph-based SLAM, the pose graph encodes the poses of the robot during data acquisition as well as spatial constraints between them. The size of the pose graph has a substantial influence on the runtime and the memory requirements of a SLAM system, which hinders long-term mapping. In this paper, we address the problem of efficient information-theoretic compression of pose graphs. Our approach estimates the expected information gain of laser measurements with respect to the resulting occupancy grid map. It allows for restricting the size of the pose graph depending on the information that the robot acquires about the environment or based on a given memory limit, which results in an any-space SLAM system. When discarding laser scans, our approach marginalizes out the corresponding pose nodes from the graph. To avoid a densely connected pose graph, which would result from exact marginalization, we propose an approximation to marginalization that is based on local Chow-Liu trees and maintains a sparse graph. Real world experiments suggest that our approach effectively reduces the growth of the pose graph while minimizing the loss of information in the resulting grid map.},
keywords = {data acquisition;graph theory;laser ranging;SLAM (robots);trees (mathematics);},
note = {information-theoretic graph pruning;graph-based SLAM;laser range finders;pose graph;data acquisition;Chow-Liu trees;sparse graph;},
URL = {http://dx.doi.org/10.1109/IROS.2011.6048060},
} 


@inproceedings{12317981 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Dense multi-planar scene estimation from a sparse set of images},
journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
author = {Argiles, A. and Civera, J. and Montesano, L.},
year = {2011//},
pages = {4448 - 54},
address = {Piscataway, NJ, USA},
abstract = {Ego-motion estimation and 3D scene reconstruction from image data has been a long term aim both in the Robotics and Computer Vision communities. Nevertheless, while both visual SLAM and Structure from Motion already provide an accurate ego-motion estimation, visual scene estimation does not offer yet such a satisfactory result; being in most cases limited to a sparse set of salient points. In this paper we propose an algorithm to densify a sparse point-based reconstruction into a dense multi-plane based one, from the only input of a set of sparse images.},
keywords = {estimation theory;image reconstruction;robot vision;set theory;SLAM (robots);},
note = {dense multiplanar scene estimation;image sparse set;egomotion estimation;3D scene reconstruction;computer vision;robotic vision;SLAM;sparse point based reconstruction;},
URL = {http://dx.doi.org/10.1109/IROS.2011.6048114},
} 


@article{17718826 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Robust Place Recognition and Loop Closing in Laser-Based SLAM for UGVs in Urban Environments},
journal = {IEEE Sensors Journal},
journal = {IEEE Sens. J. (USA)},
author = {Fengkui Cao and Yan Zhuang and Hong Zhang and Wei Wang},
volume = { 18},
number = { 10},
year = {2018/05/15},
pages = {4242 - 52},
issn = {1530-437X},
address = {USA},
abstract = {Robust place recognition plays a key role for the long-term autonomy of unmanned ground vehicles (UGVs) working in indoor or outdoor environments. Although most of the state-of-the-art that approaches for place recognition are vision-based, visual sensors lack adaptability in environments with poor or dynamically changing illumination. In this paper, a 3-D-laser-based place recognition algorithm is proposed to accomplish loop closure detection for simultaneous localization and mapping. An image model named bearing angle (BA) is adopted to convert 3-D laser points to 2-D images, and then ORB features extracted from BA images are utilized to perform scene matching. Since the computational cost for matching a query BA image with all the BA images in a database is too high to meet the requirement of performing real-time place recognition, a visual bag of words approach is used to improve search efficiency. Furthermore, a speed normalization algorithm and a 3-D geometry-based verification algorithm are proposed to complete the proposed place recognition algorithm. Experiments were conducted on two self-developed UGV platforms to verify the performance of the proposed method.},
keywords = {computational geometry;feature extraction;image matching;image sensors;mobile robots;object detection;object recognition;remotely operated vehicles;robot vision;SLAM (robots);},
note = {long-term autonomy;unmanned ground vehicles;indoor environments;outdoor environments;visual sensors lack adaptability;poor changing illumination;dynamically changing illumination;place recognition algorithm;loop closure detection;image model;2-D images;real-time place recognition;robust place recognition;loop closing;urban environments;laser-based SLAM;3-D-laser-based place recognition algorithm;simultaneous localization-and-mapping;3D laser points;query BA image matching;ORB features extraction;visual bag-of-words approach;speed normalization algorithm;3D geometry-based verification algorithm;self-developed UGV platforms;},
URL = {http://dx.doi.org/10.1109/JSEN.2018.2815956},
} 


@inproceedings{16545318 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {3D localization, mapping and path planning for search and rescue operations},
journal = {2016 IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR). Proceedings},
author = {Dube, R. and Gawel, A. and Cadena, C. and Siegwart, R. and Freda, L. and Gianni, M.},
year = {2016//},
pages = {272 - 3},
address = {Piscataway, NJ, USA},
abstract = {This work presents our results on 3D robot localization, mapping and path planning for the latest joint exercise of the European project &ldquo;Long-Term Human-Robot Teaming for Robots Assisted Disaster Response&rdquo; (TRADR)<sup>1</sup>. The full system is operated and evaluated by firemen end-users in real-world search and rescue experiments. We demonstrate that the system is able to plan a path to a goal position desired by the fireman operator in the TRADR Operational Control Unit (OCU), using a persistent 3D map created by the robot during previous sorties.},
keywords = {disasters;mobile robots;path planning;service robots;},
note = {path planning;3D mapping;rescue operations;search operations;3D robot localization;long term human robot teaming for robots assisted disaster response;TRADR;fireman operator;TRADR operational control unit;OCU;autonomous mobile robots;},
URL = {http://dx.doi.org/10.1109/SSRR.2016.7784311},
} 


@article{18880425 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving Study of Driver Behavior and Interaction With Automation},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Fridman, L. and Brown, D.E. and Glazer, M. and Angell, W. and Dodd, S. and Jenik, B. and Terwilliger, J. and Patsekin, A. and Kindelsberger, J. and Li Ding and Seaman, S. and Mehler, A. and Sipperley, A. and Pettinato, A. and Seppelt, B.D. and Angell, L. and Mehler, B. and Reimer, B.},
volume = { 7},
year = {2019//},
pages = {102021 - 38},
issn = {2169-3536},
address = {USA},
abstract = {Today, and possibly for a long time to come, the full driving task is too complex an activity to be fully formalized as a sensing-acting robotics system that can be explicitly solved through model-based and learning-based approaches in order to achieve full unconstrained vehicle autonomy. Localization, mapping, scene perception, vehicle control, trajectory optimization, and higher-level planning decisions associated with autonomous vehicle development remain full of open challenges. This is especially true for unconstrained, real-world operation where the margin of allowable error is extremely small and the number of edge-cases is extremely large. Until these problems are solved, human beings will remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0% to just under 100% of the driving. The governing objectives of the MIT Advanced Vehicle Technology (MIT-AVT) study are to 1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning-based internal and external perception systems; 2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology; and 3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium-term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for the analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, and CAN messages, and high-definition video streams of the driver's face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15610 days of participation, 511638 mi, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.},
keywords = {computer vision;data analysis;driver information systems;learning (artificial intelligence);mobile robots;remotely operated vehicles;road safety;road vehicles;},
note = {MIT Advanced Vehicle Technology study;large-scale naturalistic driving study;driver behavior;driving task;sensing-acting robotics system;unconstrained vehicle autonomy;scene perception;vehicle control;higher-level planning decisions;autonomous vehicle development;real-world operation;AI system;MIT-AVT;real-world driving data collection;external perception systems;human beings interact;vehicle automation technology;integrating video data;vehicle state data;driver characteristics;mental models;automation adoption;Model X vehicles;massive-scale dataset;instrumented vehicle fleet;recorded data streams;high-definition video streams;driver cabin;select vehicles;data collection hardware;deep learning;Tesla Model S;Cadillac CT6 vehicles;Range Rover Evoque;Volvo S90 vehicles;time 15610.0 d;},
URL = {http://dx.doi.org/10.1109/ACCESS.2019.2926040},
} 


@article{10725547 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {Experimental analysis of sample-based maps for long-term SLAM},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Biber, P. and Duckett, T.},
volume = { 28},
number = { 1},
year = {2009/01/},
pages = {20 - 33},
issn = {0278-3649},
address = {USA},
abstract = {This paper presents a system for long-term SLAM (simultaneous localization and mapping) by mobile service robots and its experimental evaluation in a real dynamic environment. To deal with the stability-plasticity dilemma (the trade-off between adaptation to new patterns and preservation of old patterns), the environment is represented by multiple timescales simultaneously (five in our experiments). A sample-based representation is proposed, where older memories fade at different rates depending on the timescale and robust statistics are used to interpret the samples. The dynamics of this representation are analyzed in a five-week experiment, measuring the relative influence of short- and long-term memories over time and further demonstrating the robustness of the approach.},
keywords = {mobile robots;robot dynamics;service robots;SLAM (robots);stability;},
note = {sample-based maps;long-term SLAM;experimental analysis;mobile service robots;stability-plasticity dilemma;sample-based representation;robust statistics;},
URL = {http://dx.doi.org/10.1177/0278364908096286},
} 


@article{18501967 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Real-time dense map fusion for stereo SLAM},
journal = {Robotica},
journal = {Robotica (UK)},
author = {Pire, T. and Baravalle, R. and D'Alessandro, A. and Civera, J.},
volume = { 36},
number = { 10},
year = {2018/10/},
pages = {1510 - 26},
issn = {0263-5747},
address = {UK},
abstract = {A robot should be able to estimate an accurate and dense 3D model of its environment (a map), along with its pose relative to it, all of it in real time, in order to be able to navigate autonomously without collisions. As the robot moves from its starting position and the estimated map grows, the computational and memory footprint of a dense 3D map increases and might exceed the robot capabilities in a short time. However, a global map is still needed to maintain its consistency and plan for distant goals, possibly out of the robot field of view. In this work, we address such problem by proposing a real-time stereo mapping pipeline, feasible for standard CPUs, which is locally dense and globally sparse and accurate. Our algorithm is based on a graph relating poses and salient visual points, in order to maintain a long-term accuracy with a small cost. Within such framework, we propose an efficient dense fusion of several stereo depths in the locality of the current robot pose. We evaluate the performance and the accuracy of our algorithm in the public datasets of Tsukuba and KITTI, and demonstrate that it outperforms single-view stereo depth. We release the code as open-source, in order to facilitate the system use and comparisons.},
keywords = {image reconstruction;mobile robots;pose estimation;robot vision;SLAM (robots);stereo image processing;},
note = {environment;dense 3D model;accurate D model;stereo SLAM;time dense map fusion;single-view stereo depth;current robot;stereo depths;efficient dense fusion;graph relating poses;real-time stereo mapping pipeline;global map;short time;robot capabilities;dense 3D map increases;computational memory footprint;estimated map;starting position;a map;},
URL = {http://dx.doi.org/10.1017/S0263574718000528},
} 


@inproceedings{15589670 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Vision-based Markov localization across large perceptual changes},
journal = {2015 European Conference on Mobile Robots (ECMR). Proceedings},
author = {Naseer, T. and Suger, B. and Ruhnke, M. and Burgard, W.},
year = {2015//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Recently, there has been significant progress towards lifelong, autonomous operation of mobile robots, especially in the field of localization and mapping. One important challenge in this context is visual localization under substantial perceptual changes, for example, coming from different seasons. In this paper, we present an approach to localize a mobile robot with a low frequency camera with respect to an image sequence, recorded previously within a different season. Our approach uses a discrete Bayes filter and a sensor model based on whole image descriptors. Thereby it exploits sequential information to model the dynamics of the system. Since we compute a probability distribution over the whole state space, our approach can handle more complex trajectories that may include same season loop-closures as well as fragmented sub-sequences. Throughout an extensive experimental evaluation on challenging datasets, we demonstrate that our approach outperforms state-of-the-art techniques.},
keywords = {Bayes methods;image filtering;image sequences;Markov processes;mobile robots;robot vision;SLAM (robots);},
note = {vision-based Markov localization;mobile robot autonomous operation;substantial perceptual changes;low frequency camera;image sequence;discrete Bayes filter;sensor model;probability distribution;},
URL = {http://dx.doi.org/10.1109/ECMR.2015.7324181},
} 


@inproceedings{11689801 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {6 DoF SLAM using a ToF camera: the challenge of a continuously growing number of landmarks},
journal = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2010)},
author = {Hochdorfer, S. and Schlegel, C.},
year = {2010//},
pages = {3981 - 6},
address = {Piscataway, NJ, USA},
abstract = {Localization and mapping are fundamental problems in service robotics since representations of the environment and knowledge about the own pose significantly simplify the implementation of a series of high-level applications. ToF (time-of-flight) cameras are a relatively new kind of sensors in robotics. They enable the real-time capture of the distance and the grayscale information of a scene. Due to the increase of the image resolution of ToF cameras, now highlevel computer vision algorithms for visual feature extraction (e.g. SIFT or SURF) can be applied to the captured images. These visual features combined with the corresponding distance information give a full measurement of 3D landmarks. An obvious problem to be solved is the continuously growing number of landmarks. So far, all ever seen landmarks are just accumulated irrespective of their utility and the then required resources. Rather, one should keep only really useful landmarks, e.g. such that localization quality in the whole operational area is kept above a given threshold. In fact a lifelong running SLAM approach is dependent on means to select and discard landmarks. That is even more acute in case of feature-rich sensor data as provided with high update rates by sensors like a ToF camera. We run our SLAM approach in a real-world experiment within an indoor environment. The experiment was performed on a P3DX-platform equipped with a PMD CamCube 2.0 and a Xsens IMU.},
keywords = {cameras;feature extraction;image resolution;robot vision;service robots;SLAM (robots);},
note = {DoF SLAM;ToF camera;localization and mapping;service robotics;time-of-flight;sensors;image resolution;computer vision;feature extraction;3D landmark;},
URL = {http://dx.doi.org/10.1109/IROS.2010.5651229},
} 


@inproceedings{19891622 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {PHALANX: Expendable projectile sensor networks for planetary exploration},
journal = {2020 IEEE Aerospace Conference},
author = {Dille, M. and Nuch, D. and Gupta, S. and McCabe, S. and Verzic, N. and Fong, T. and Wong, U.},
year = {2020//},
pages = {12 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Technologies enabling long-term, wide-ranging measurement in hard-to-reach areas are a critical need for planetary science inquiry. Phenomena of interest include flows or variations in volatiles, gas composition or concentration, particulate density, or even simply temperature. Improved measurement of these processes enables understanding of exotic geologies and distributions or correlating indicators of trapped water or biological activity. However, such data is often needed in unsafe areas such as caves, lava tubes, or steep ravines not easily reached by current spacecraft and planetary robots. To address this capability gap, we have developed miniaturized, expendable sensors which can be ballistically lobbed from a robotic rover or static lander - or even dropped during a flyover. These projectiles can perform sensing during flight and after anchoring to terrain features. By augmenting exploration systems with these sensors, we can extend situational awareness, perform long-duration monitoring, and reduce utilization of primary mobility resources, all of which are crucial in surface missions. We call the integrated payload that includes a cold gas launcher, smart projectiles, planning software, network discovery, and science sensing: PHALANX. In this paper, we introduce the mission architecture for PHALANX and describe an exploration concept that pairs projectile sensors with a rover ldquomothership.rdquo Science use cases explored include reconnaissance using ballistic cameras, volatiles detection, and building timelapse maps of temperature and illumination conditions. Strategies to autonomously coordinate constellations of deployed sensors to self-discover and localize with peer ranging (i.e. a ldquolocal GPSrdquo) are summarized, thus providing communications infrastructure beyond-line-of-sight (BLOS) of the rover. Capabilities were demonstrated through both simulation and physical testing with a terrestrial prototype. The approach to developing a terrestrial prototype is discussed, including design of the launching mechanism, projectile optimization, micro-electronics fabrication, and sensor selection. Results from early testing and characterization of commercial-off-the-shelf (COTS) components are reported. Nodes were subjected to successful burn-in tests over 48 hours at full logging duty cycle. Integrated field tests were conducted in the Roverscape, a half-acre planetary analog environment at NASA Ames, where we tested up to 10 sensor nodes simultaneously coordinating with an exploration rover. Ranging accuracy has been demonstrated to be within +/-10cm over 20m using commodity radios when compared to high-resolution laser scanner ground truthing. Evolution of the design, including progressive miniaturization of the electronics and iterated modifications of the enclosure housing for streamlining and optimized radio performance are described. Finally, lessons learned to date, gaps toward eventual flight mission implementation, and continuing future development plans are discussed.},
keywords = {aerospace robotics;Global Positioning System;mobile robots;planetary rovers;space research;space vehicles;wireless sensor networks;},
note = {expendable sensors;robotic rover;static lander;commercial-off-the-shelf components;burn-in tests;integrated field tests;half-acre planetary analog environment;sensor nodes;exploration rover;ranging accuracy;streamlining radio performance;optimized radio performance;eventual flight mission implementation;future development plans;Expendable projectile sensor networks;planetary exploration;hard-to-reach areas;planetary science inquiry;particulate density;improved measurement;exotic geologies;correlating indicators;trapped water;unsafe areas;lava tubes;steep ravines;current spacecraft;planetary robots;capability gap;sensor selection;projectile optimization;terrestrial prototype;physical testing;communications infrastructure beyond-line-of-sight;peer ranging;deployed sensors;volatiles detection;ballistic cameras;rover mothership;exploration concept;mission architecture;PHALANX;science sensing;network discovery;planning software;smart projectiles;cold gas launcher;integrated payload;surface missions;primary mobility resources;long-duration monitoring;situational awareness;augmenting exploration systems;time 48.0 hour;},
URL = {http://dx.doi.org/10.1109/AERO47225.2020.9172595},
} 


@article{18185001 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Distributed and collaborative monocular simultaneous localization and mapping for multi-robot systems in large-scale environments},
journal = {International Journal of Advanced Robotic Systems},
journal = {Int. J. Adv. Robot. Syst. (UK)},
author = {Hui Zhang and Xieyuanli Chen and Huimin Lu and Junhao Xiao},
volume = { 15},
number = { 3},
year = {2018/05/},
pages = {173 - 92},
issn = {1729-8814},
address = {UK},
abstract = {In this article, we propose a distributed and collaborative monocular simultaneous localization and mapping system for the multi-robot system in large-scale environments, where monocular vision is the only exteroceptive sensor. Each robot estimates its pose and reconstructs the environment simultaneously using the same monocular simultaneous localization and mapping algorithm. Meanwhile, they share the results of their incremental maps by streaming keyframes through the robot operating system messages and the wireless network. Subsequently, each robot in the group can obtain the global map with high efficiency. To build the collaborative simultaneous localization and mapping architecture, two novel approaches are proposed. One is a robust relocalization method based on active loop closure, and the other is a vision-based multi-robot relative pose estimating and map merging method. The former is used to solve the problem of tracking failures when robots carry out long-term monocular simultaneous localization and mapping in large-scale environments, while the latter uses the appearance-based place recognition method to determine multi-robot relative poses and build the large-scale global map by merging each robot's local map. Both KITTI data set and our own data set acquired by a handheld camera are used to evaluate the proposed system. Experimental results show that the proposed distributed multi-robot collaborative monocular simultaneous localization and mapping system can be used in both indoor small-scale and outdoor large-scale environments.},
keywords = {cameras;mobile robots;multi-robot systems;pose estimation;robot vision;SLAM (robots);},
note = {local map;large-scale global map;multirobot relative poses;long-term monocular simultaneous localization;robots;map merging method;vision-based multirobot;mapping architecture;collaborative simultaneous localization;robot operating system messages;incremental maps;mapping algorithm;monocular vision;multirobot system;mapping system;collaborative monocular simultaneous localization;large-scale environments;},
URL = {http://dx.doi.org/10.1177/1729881418780178},
} 


@article{12773099 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Visual Odometry : Part II: Matching, Robustness, Optimization, and Applications},
journal = {IEEE Robotics &amp; Automation Magazine},
journal = {IEEE Robot. Autom. Mag. (USA)},
author = {Fraundorfer, F. and Scaramuzza, D.},
volume = { 19},
number = { 2},
year = {2012/06/},
pages = {78 - 90},
issn = {1070-9932},
address = {USA},
abstract = {Part II of the tutorial has summarized the remaining building blocks of the VO pipeline: specifically, how to detect and match salient and repeatable features across frames and robust estimation in the presence of outliers and bundle adjustment. In addition, error propagation, applications, and links to publicly available code are included. VO is a well understood and established part of robotics. VO has reached a maturity that has allowed us to successfully use it for certain classes of applications: space, ground, aerial, and underwater. In the presence of loop closures, VO can be used as a building block for a complete SLAM algorithm to reduce motion drift. Challenges that still remain are to develop and demonstrate large-scale and long-term implementations, such as driving autonomous cars for hundreds of miles. Such systems have recently been demonstrated using Lidar and Radar sensors [86]. However, for VO to be used in such systems, technical issues regarding robustness and, especially, long-term stability have to be resolved. Eventually, VO has the potential to replace Lidar-based systems for egomotion estimation, which are currently leading the state of the art in accuracy, robustness, and reliability. VO offers a cheaper and mechanically easier-to-manufacture solution for egomotion estimation, while, additionally, being fully passive. Furthermore, the ongoing miniaturization of digital cameras offers the possibility to develop smaller and smaller robotic systems capable of ego-motion estimation.},
keywords = {cameras;distance measurement;motion estimation;robot vision;SLAM (robots);},
note = {visual odometry;VO pipeline;robust estimation;error propagation;space application;ground application;aerial application;underwater application;loop closures;SLAM algorithm;motion drift;driving autonomous car;radar sensor;Lidar-based system;egomotion estimation;easier-to-manufacture solution;digital camera;robotic system;},
URL = {http://dx.doi.org/10.1109/MRA.2012.2182810},
} 


@inproceedings{16639425 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Migratory birds-inspired navigation system for unmanned aerial vehicles},
journal = {2016 IEEE International Conference on Information and Automation (ICIA). Proceedings},
author = {Yu Zhang and Ainong Chao and Boxin Zhao and Huawei Liu and Xiaolin Zhao},
year = {2016//},
pages = {276 - 81},
address = {Piscataway, NJ, USA},
abstract = {Migration birds are able to navigate themselves during a long-distance journey without getting lost. They actually achieve just what is being sought for in the field of Unmanned Aerial Vehicles (UAVs): long-term autonomous navigation. This paper proposes an approach that combines the migration birds' sense principles with Micro-Electro-Mechanical System (MEMS) sensors to estimate UAVs position within GPS-denied environments. Camera, orientation and web-based maps (such as Google/Baidu Maps) are chosen to simulate the birds' localization cues: vision, earth magnetic field and mental maps. The visual odometry, Particle Filter theories are used in the proposed approach to integrate multiple sensor measurements. Real flying experiments are conducted both in indoor and outdoor environments. The results validate that the proposed migration-inspired visual odometry system can estimate the UAV localization effectively.},
keywords = {aerospace navigation;autonomous aerial vehicles;cameras;distance measurement;geomagnetism;Internet;microsensors;particle filtering (numerical methods);robot vision;},
note = {long-distance journey;unmanned aerial vehicles;long-term autonomous navigation;migration bird sense principles;microelectromechanical system sensors;MEMS sensors;UAV position estimation;GPS-denied environments;camera;Web-based maps;bird localization cues;bird vision;earth magnetic field;mental maps;visual odometry;particle filter theories;multiple sensor measurements;real flying experiments;migration-inspired visual odometry system;UAV localization;},
URL = {http://dx.doi.org/10.1109/ICInfA.2016.7831835},
} 


@inproceedings{16932695 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Martian Fetch: Finding and retrieving sample-tubes on the surface of Mars},
journal = {2017 IEEE Aerospace Conference},
author = {Papon, J. and Detry, R. and Vieira, P. and Brooks, S. and Srinivasan, T. and Peterson, A. and Kulczycki, E.},
year = {2017//},
pages = {9 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Mars Sample Return (MSR) was identified by the 2011 planetary science decadal survey as a high priority long-term goal for NASA. A three-mission campaign concept is currently being investigated. The Mars 2020 rover mission is intended to core and collect samples. These samples will be sealed in tubes and left on the surface for potential return to Earth. In the current MSR campaign concept, a Sample Retrieval and Launch (SRL) mission would collect the sample tubes left by the Mars 2020 rover and load them into a Mars Ascent Vehicle (MAV) to be launched into orbit. The third mission concept involves a spacecraft capturing the samples in Martian orbit and returning them to Earth. This paper focuses on the SRL mission concept to collect the sample tubes, addressing the problem of autonomously detecting, localizing, and grasping sample tubes deposited on the Martian surface. We employ two approaches: The first one is context-based. It would use a high precision map computed from images captured during tube release, to locate the tubes without directly observing them. The second approach directly detects the sample tubes visually and estimates their 6-DoF pose onboard from dense stereo data.},
keywords = {Mars;planetary rovers;planetary surfaces;space vehicles;},
note = {Mars Sample Return;planetary science decadal survey;NASA;three-mission campaign concept;Mars 2020 rover mission;Earth;MSR campaign concept;Sample Retrieval and Launch mission;sample tubes;Mars Ascent Vehicle;spacecraft;Martian orbit;SRL mission concept;sample tubes;Martian surface;high precision map;stereo data;martian fetch;},
URL = {http://dx.doi.org/10.1109/AERO.2017.7943649},
} 


@inproceedings{16503885 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Pole-based Localization for Autonomous Vehicles in Urban Scenarios},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Spangenberg, R. and Goehring, D. and Rojas, R.},
year = {2016//},
pages = {2161 - 6},
address = {Piscataway, NJ, USA},
abstract = {Localization is a key capability for autonomous vehicles especially in urban scenarios. We propose the use of pole-like landmarks as primary features in these environments, as they are distinct, long-term stable and can be detected reliably with a stereo camera system. Furthermore, the resulting map representation is memory efficient, allowing for easy storage and on-line updates. The localization is performed in real-time by a stereo camera system as a main sensor, using vehicle odometry and an off-the-shelf GPS as secondary information sources. Localization is performed by a particle filter approach, coupled with an Kalman filter for robustness and sensor fusion. This leads to a lateral accuracy below 20 cm in various urban test areas. The system has been included in our autonomous test vehicle and successfully demonstrated the full loop from mapping to autonomous driving.},
keywords = {cameras;Global Positioning System;Kalman filters;mobile robots;particle filtering (numerical methods);road vehicles;robot vision;sensor fusion;SLAM (robots);stereo image processing;},
note = {autonomous driving;sensor fusion;Kalman filter;particle filter approach;off-the-shelf GPS;vehicle odometry;stereo camera system;map representation;pole-like landmark;urban scenario;autonomous vehicle;pole-based localization;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759339},
} 


@article{16848819 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Unsupervised Place Discovery for Visual Place Classification [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Xiaoxiao, F. and Kanji, T. and Kouya, I.},
year = {2016/12/20},
pages = {5 pp. - },
address = {USA},
abstract = {In this study, we explore the use of deep convolutional neural networks (DCNNs) in visual place classification for robotic mapping and localization. An open question is how to partition the robot's workspace into places to maximize the performance (e.g., accuracy, precision, recall) of potential DCNN classifiers. This is a chicken and egg problem: If we had a well-trained DCNN classifier, it is rather easy to partition the robot's workspace into places, but the training of a DCNN classifier requires a set of pre-defined place classes. In this study, we address this problem and present several strategies for unsupervised discovery of place classes ("time cue," "location cue," "time-appearance cue," and "location-appearance cue"). We also evaluate the efficacy of the proposed methods using the publicly available University of Michigan North Campus Long-Term (NCLT) Dataset.},
keywords = {feedforward neural nets;image classification;mobile robots;motion control;robot vision;},
note = {unsupervised place discovery;visual place classification;deep convolutional neural networks;DCNN;robotic mapping;robotic localization;predefined place classes;University of Michigan north campus long-term dataset;NCLT;},
} 


@inproceedings{15286147 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Distributed real-time cooperative localization and mapping using an uncertainty-aware expectation maximization approach},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Jing Dong and Nelson, E. and Indelman, V. and Michael, N. and Dellaert, F.},
year = {2015//},
pages = {5807 - 14},
address = {Piscataway, NJ, USA},
abstract = {We demonstrate distributed, online, and real-time cooperative localization and mapping between multiple robots operating throughout an unknown environment using indirect measurements. We present a novel Expectation Maximization (EM) based approach to efficiently identify inlier multi-robot loop closures by incorporating robot pose uncertainty, which significantly improves the trajectory accuracy over long-term navigation. An EM and hypothesis based method is used to determine a common reference frame. We detail a 2D laser scan correspondence method to form robust correspondences between laser scans shared amongst robots. The implementation is experimentally validated using teams of aerial vehicles, and analyzed to determine its accuracy, computational efficiency, scalability to many robots, and robustness to varying environments. We demonstrate through multiple experiments that our method can efficiently build maps of large indoor and outdoor environments in a distributed, online, and real-time setting.},
keywords = {distributed control;expectation-maximisation algorithm;multi-robot systems;uncertain systems;},
note = {distributed real-time cooperative localization;cooperative mapping;uncertainty-aware expectation maximization approach;EM based approach;multiple robots;indirect measurements;inlier multirobot loop closures;robot pose uncertainty;trajectory accuracy;long-term navigation;hypothesis based method;common reference frame;2D laser scan correspondence method;aerial vehicles;computational efficiency;robot scalability;indoor environments;outdoor environments;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7140012},
} 


@article{18467666 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Explorations on visual localization from active to passive},
journal = {Multimedia Tools and Applications},
journal = {Multimed. Tools Appl. (Germany)},
author = {Yongquan Yang and Yang Wu and Ning Chen},
volume = { 78},
number = { 2},
year = {2019/01/},
pages = {2269 - 309},
issn = {1380-7501},
address = {Germany},
abstract = {In this paper, we novelly consider visual localization in active and passive two ways, with simple definition that active localization assists device to estimate location of its interest while passive localization aids device to estimate its own location in environment. Expecting to indicate some insights into visual localization, we specifically performed two explorations on active localization and more importantly explored to upgrade them from active to passive localization with extra geometry information available. In order to produce unconstrained and accurate 2D location estimation of interested object, we constructed an active localization system by fusing detection, tracking and recognition. Based on recognition, we proposed a collaborative strategy making mutual enhancement between detection and tracking possible to obtain better performance on 2D location estimation. Meanwhile, to actively estimate semantic location of interested visual region, we employed latest state-of-the-art light weight CNN models specifically designed for efficiency and trained two of them with large place dataset in perspective of scene recognition. What's more, using depth information available from RGB-D camera, we improved the active system for 2D location of interested object to a passive system for relative 3D location of device to the interested object. Firstly estimated was the 3D location of the interested object in the coordinate system of device, then relative location of device to the interested object in world coordinate system was deduced with appropriate assumption. Evaluations both subjectively on a RGB-D sequence obtained in a lab environment and practically on a robotic platform in an office environment indicated that the improved system was suitable for autonomous following robot. As well, the active system for rough semantic location estimation of interested visual region was promoted to a passive system for fine location estimation of device, with available 3D map describing the visited environment. In perspective of place recognition, we first adopted one of the efficient CNN models previously trained for semantic location estimation as a base to generate CNN features for both retrieval of candidate loops in the map and geometrical consistency checking of retrieved loops, then true loops were used to deduce fine location of device itself in environment. Comparison with state-of-the-art results reflected that the promoted system was adequate for long-term robotic autonomy. Achieving favorable performances, the presented four explorations have implied adequacy for elaborating on some insights into visual localization.},
keywords = {convolutional neural nets;image colour analysis;mobile robots;object detection;object tracking;robot vision;},
note = {visual region;light weight CNN models;place recognition;rough semantic location estimation;relative location;coordinate system;passive system;active system;active localization system;passive localization aids device;active localization assists device;passive two ways;active ways;visual localization;explorations;},
URL = {http://dx.doi.org/10.1007/s11042-018-6347-0},
} 


@inproceedings{10801924 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {Towards a robust visual SLAM approach: addressing the challenge of life-long operation},
journal = {2009 14th International Conference on Advanced Robotics (ICAR 2009)},
author = {Hochdorfer, S. and Schlegel, C.},
year = {2009//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Localization and mapping are fundamental problems in service robotics. Knowledge about the own pose and representations of the environment are needed for a series of high level applications. Service robots should be designed for life-long and robust operation in dynamic environments. The contribution of this paper is twofold. First, an approach to address the ever growing number of landmarks in life-long operation is presented. Typically, SLAM approaches just accumulate features over time and do not discard them anymore. Therefore, the required resources in terms of memory and processing power are growing over time. In our approach, the absolute number of landmarks can be restricted by an upper bound since we introduce a method to specifically select and replace landmarks once the upper bound has been reached. The second contribution is related to improving the robustness of the landmark assignment problem in case of image based features as needed with natural landmarks. The approach has been successfully evaluated in a real world experiment on a Pioneer-3DX platform within a complex unmodified indoor environment.},
keywords = {mobile robots;robot vision;robust control;service robots;SLAM (robots);},
note = {robust visual SLAM approach;life-long operation;service robotics;dynamic environment;landmark assignment problem;robustness;image-based feature;Pioneer-3DX platform;complex unmodified indoor environment;},
} 


@article{14069148 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Selective Combination of Visual and Thermal Imaging for Resilient Localization in Adverse Conditions: Day and Night, Smoke and Fire},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Brunner, C. and Peynot, T. and Vidal-Calleja, T. and Underwood, J.},
volume = { 30},
number = { 4},
year = {2013/07/},
pages = {641 - 6},
issn = {1556-4959},
address = {USA},
abstract = {Long-term autonomy in robotics requires perception systems that are resilient to unusual but realistic conditions that <i>will</i> eventually occur during extended missions. For example, unmanned ground vehicles (UGVs) need to be capable of operating safely in adverse and low-visibility conditions, such as at night or in the presence of smoke. The key to a resilient UGV perception system lies in the use of multiple sensor modalities, e.g., operating at different frequencies of the electromagnetic spectrum, to compensate for the limitations of a single sensor type. In this paper, visual and infrared imaging are combined in a Visual-SLAM algorithm to achieve localization. We propose to evaluate the quality of data provided by each sensor modality prior to data combination. This evaluation is used to discard low-quality data, i.e., data most likely to induce large localization errors. In this way, perceptual failures are anticipated and mitigated. An extensive experimental evaluation is conducted on data sets collected with a UGV in a range of environments and adverse conditions, including the presence of smoke (obstructing the visual camera), fire, extreme heat (saturating the infrared camera), low-light conditions (dusk), and at night with sudden variations of artificial light. A total of 240 trajectory estimates are obtained using five different variations of data sources and data combination strategies in the localization method. In particular, the proposed approach for selective data combination is compared to methods using a single sensor type or combining both modalities without preselection. We show that the proposed framework allows for camera-based localization resilient to a large range of low-visibility conditions.},
keywords = {infrared imaging;remotely operated vehicles;robot vision;visual perception;},
note = {camera based localization;selective data combination;artificial light;low light conditions;extreme heat;sensor modality;visual-SLAM algorithm;infrared imaging;resilient UGV perception system;unmanned ground vehicles;robotics;resilient localization;thermal imaging;visual imaging;},
URL = {http://dx.doi.org/10.1002/rob.21464},
} 


@inproceedings{12375648 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Natural landmark-based monocular localization for MAVs},
journal = {2011 IEEE International Conference on Robotics and Automation (ICRA 2011)},
author = {Wendel, A. and Irschara, A. and Bischof, H.},
year = {2011//},
pages = {5792 - 9},
address = {Piscataway, NJ, USA},
abstract = {Highly accurate localization of a micro aerial vehicle (MAV) with respect to a scene is important for a wide range of applications, in particular surveillance and inspection. Most existing approaches to visual localization focus on indoor environments, while such tasks require outdoor navigation. Within this work, we introduce a novel algorithm for monocular visual localization for MAVs based on the concept of virtual views in 3D space. Under the assumption that significant parts of the scene do not alter their geometry and serve as natural landmarks, the accuracy of our visual approach outperforms consumer grade GPS systems. In an experimental setup we compare our approach to a state-of-the-art visual SLAM algorithm and evaluate the performance by geometric validation from an observer's view. As our method directly allows global registration, it is neither prone to drift nor bias. This makes it well suited for long-term autonomous navigation.},
keywords = {aircraft;geometry;image reconstruction;microrobots;navigation;robot vision;SLAM (robots);solid modelling;surveillance;},
note = {micro aerial vehicle;surveillance;inspection;outdoor navigation;3D space;visual SLAM algorithm;geometric algorithm;global registration;autonomous navigation;natural landmark based monocular visual localization;MAV;},
URL = {http://dx.doi.org/10.1109/ICRA.2011.5980317},
} 


@inproceedings{16503990 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Robust sound source mapping using three-layered selective audio rays for mobile robots},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Daobilige Su and Nakamura, K. and Nakadai, K. and Miro, J.V.},
year = {2016//},
pages = {2771 - 7},
address = {Piscataway, NJ, USA},
abstract = {This paper investigates sound source mapping in a real environment using a mobile robot. Our approach is based on audio ray tracing which integrates occupancy grids and sound source localization using a laser range finder and a microphone array. Previous audio ray tracing approaches rely on all observed rays and grids. As such observation errors caused by sound reflection, sound occlusion, wall occlusion, sounds at misdetected grids, etc. can significantly degrade the ability to locate sound sources in a map. A three-layered selective audio ray tracing mechanism is proposed in this work. The first layer conducts frame-based unreliable ray rejection (sensory rejection) considering sound reflection and wall occlusion. The second layer introduces triangulation and audio tracing to detect falsely detected sound sources, rejecting audio rays associated to these misdetected sounds sources (short-term rejection). A third layer is tasked with rejecting rays using the whole history (long-term rejection) to disambiguate sound occlusion. Experimental results under various situations are presented, which proves the effectiveness of our method.},
keywords = {acoustic signal detection;acoustic signal processing;audio signal processing;mobile robots;ray tracing;},
note = {robust sound source mapping;mobile robots;occupancy grids;sound source localization;laser range finder;microphone array;observation errors;sound reflection;sound occlusion;wall occlusion;three-layered selective audio ray tracing mechanism;frame-based unreliable ray rejection;triangulation;sound source detection;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759430},
} 


@inproceedings{15798792 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Bathymetric Factor Graph SLAM with Sparse Point Cloud Alignment},
journal = {OCEANS 2015 - MTS/IEEE Washington},
author = {Bichucher, V. and Walls, J.M. and Ozog, P. and Skinner, K.A. and Eustice, R.M.},
year = {2015//},
pages = {732 - 8},
address = {Piscataway, NJ, USA},
abstract = {This paper reports on a factor graph simultaneous localization and mapping framework for autonomous underwater vehicle localization based on terrain-aided navigation. The method requires no prior bathymetric map and only assumes that the autonomous underwater vehicle has the ability to sparsely sense the local water column depth, such as with a bottom-looking Doppler velocity log. Since dead-reckoned navigation is accurate in short time windows, the vehicle accumulates several water column depth point clouds- or submaps-during the course of its survey. We propose an xy-alignment procedure between these submaps in order to enforce consistent bathymetric structure over time, and therefore attempt to bound long-term navigation drift. We evaluate the submap alignment method in simulation and present performance results from multiple autonomous underwater vehicle field trials.},
keywords = {autonomous underwater vehicles;bathymetry;marine navigation;SLAM (robots);terrain mapping;},
note = {bathymetric factor graph SLAM;sparse point cloud alignment;simultaneous localization and mapping framework;terrain-aided navigation;autonomous underwater vehicle localization;bathymetric map;local water column depth sparse sensing;dead-reckoned navigation;short time windows;xy-alignment procedure;long-term navigation drift;consistent bathymetric structure;submap alignment method;multiple autonomous underwater vehicle field trial;},
} 


@inproceedings{12194882 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Research on object recognition using bag of word model for mobile robot navigation},
journal = {Proceedings of the 2011 IEEE International Conference on Mechatronics and Automation (ICMA 2011)},
author = {Jin-fu Yang and Kai Wang and Ming-ai Li and Lu Liu},
year = {2011//},
pages = {1735 - 40},
address = {Piscataway, NJ, USA},
abstract = {Robust long term positioning for autonomous mobile robots is essential for many applications. Key to a successful visual SLAM system is correctly recognizing the objects and labeling where the robot is. Local image features are popular with constructing object recognition system, which are invariant to image scaling, translation, rotation, and partially invariant to illumination changes and affine. In this paper, we proposed an object recognition method based on the bag of word model, mainly idea includes three steps as follows: firstly, a set of local image patches are sampled using a key point detector, and each patch is a descriptor based on scale invariant feature transform. Then outliers are removed by RANSAC algorithm, and the resulting distribution of descriptors is quantified by using vector quantization against a pre-specified codebook to convert it to a histogram of votes for codebook centers. Finally, a KNN algorithm is used to classify images through the resulting global descriptor vector. The experimental results show that our proposed method has a better performance against the previous methods.},
keywords = {image classification;image coding;mobile robots;object recognition;path planning;random processes;robot vision;SLAM (robots);vector quantisation;},
note = {object recognition;bag of word model;mobile robot navigation;long term positioning;autonomous mobile robot;visual SLAM system;local image feature;image scaling;image translation;image rotation;local image patch;key point detector;scale invariant feature transform;RANSAC algorithm;vector quantization;codebook center;KNN algorithm is;image classification;},
URL = {http://dx.doi.org/10.1109/ICMA.2011.5986295},
} 


@inproceedings{16503797 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Bridging the Appearance Gap: Multi-Experience Localization for Long-Term Visual Teach and Repeat},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Paton, M. and MacTavish, K. and Warren, M. and Barfoot, T.D.},
year = {2016//},
pages = {1918 - 25},
address = {Piscataway, NJ, USA},
abstract = {Vision-based, route-following algorithms enable autonomous robots to repeat manually taught paths over long distances using inexpensive vision sensors. However, these methods struggle with long-term, outdoor operation due to the challenges of environmental appearance change caused by lighting, weather, and seasons. While techniques exist to address appearance change by using multiple experiences over different environmental conditions, they either provide topological-only localization, require several manually taught experiences in different conditions, or require extensive offline mapping to produce metric localization. For real-world use, we would like to localize metrically to a single manually taught route and gather additional visual experiences during autonomous operations. Accordingly, we propose a novel multi-experience localization (MEL) algorithm developed specifically for route-following applications; it provides continuous, six-degree-of-freedom (6DoF) localization with relative uncertainty to a privileged (manually taught) path using several experiences simultaneously. We validate our algorithm through two experiments: i) an offline performance analysis on a 9km subset of a challenging 27km route-traversal dataset and ii) an online field trial where we demonstrate autonomy on a small 250m loop over the course of a sunny day. Both exhibit significant appearance change due to lighting variation. Through these experiments we show that safe localization can be achieved by bridging the appearance gap.},
keywords = {image sensors;mobile robots;path planning;robot vision;SLAM (robots);},
note = {environmental appearance;multiexperience localization;MEL algorithm;visual teach and repeat;VT&amp;R;vision-based algorithm;route-following algorithm;autonomous robot;vision sensor;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759303},
} 


@inproceedings{14447140 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Integration of Monte Carlo Localization and Place Recognition for Reliable Long-Term Robot Localization},
journal = {2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
author = {Perez, J. and Caballero, F. and Merino, L.},
year = {2014//},
pages = {85 - 91},
address = {Piscataway, NJ, USA},
abstract = {This paper proposes extending Monte Carlo Localization methods with visual information in order to build a long term robot localization system. This system is aimed to work in crowded and non-planar scenarios, where 2D laser rangefinders may not always be enough to match the robot position with the map. Thus, visual place recognition will be used in order to obtain robot position clues that can be used to detect when the robot is lost and also to reset its positions to the right one. The paper presents experimental results based on datasets gathered with a real robot in challenging scenarios.},
keywords = {mobile robots;Monte Carlo methods;navigation;path planning;pose estimation;robot vision;SLAM (robots);},
note = {robot position;nonplanar scenarios;crowded scenarios;long-term robot localization system;visual place recognition;Monte Carlo localization methods;},
URL = {http://dx.doi.org/10.1109/ICARSC.2014.6849767},
} 


@inproceedings{14616887 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Multiple map hypotheses for planning and navigating in non-stationary environments},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Morris, T. and Dayoub, F. and Corke, P. and Wyeth, G. and Upcroft, B.},
year = {2014//},
pages = {2765 - 70},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a method to enable a mobile robot working in non-stationary environments to plan its path and localize within multiple map hypotheses simultaneously. The maps are generated using a long-term and short-term memory mechanism that ensures only persistent configurations in the environment are selected to create the maps. In order to evaluate the proposed method, experimentation is conducted in an office environment. Compared to navigation systems that use only one map, our system produces superior path planning and navigation in a non-stationary environment where paths can be blocked periodically, a common scenario which poses significant challenges for typical planners.},
keywords = {mobile robots;path planning;},
note = {multiple map hypotheses;nonstationary environments;mobile robot working;short-term memory mechanism;long-term memory mechanism;path planning;path navigation;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907255},
} 


@article{15918125 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Automated Crack Detection on Concrete Bridges},
journal = {IEEE Transactions on Automation Science and Engineering},
journal = {IEEE Trans. Autom. Sci. Eng. (USA)},
author = {Prasanna, P. and Dana, K.J. and Gucunski, N. and Basily, B.B. and La, H.M. and Lim, R.S. and Parvardeh, H.},
volume = { 13},
number = { 2},
year = {2016/04/},
pages = {591 - 9},
issn = {1545-5955},
address = {USA},
abstract = {Detection of cracks on bridge decks is a vital task for maintaining the structural health and reliability of concrete bridges. Robotic imaging can be used to obtain bridge surface image sets for automated on-site analysis. We present a novel automated crack detection algorithm, the STRUM (spatially tuned robust multifeature) classifier, and demonstrate results on real bridge data using a state-of-the-art robotic bridge scanning system. By using machine learning classification, we eliminate the need for manually tuning threshold parameters. The algorithm uses robust curve fitting to spatially localize potential crack regions even in the presence of noise. Multiple visual features that are spatially tuned to these regions are computed. Feature computation includes examining the scale-space of the local feature in order to represent the information and the unknown salient scale of the crack. The classification results are obtained with real bridge data from hundreds of crack regions over two bridges. This comprehensive analysis shows a peak STRUM classifier performance of 95% compared with 69% accuracy from a more typical image-based approach. In order to create a composite global view of a large bridge span, an image sequence from the robot is aligned computationally to create a continuous mosaic. A crack density map for the bridge mosaic provides a computational description as well as a global view of the spatial patterns of bridge deck cracking. The bridges surveyed for data collection and testing include Long-Term Bridge Performance program's (LTBP) pilot project bridges at Haymarket, VA, USA, and Sacramento, CA, USA.},
keywords = {bridges (structures);condition monitoring;crack detection;curve fitting;image classification;image segmentation;learning (artificial intelligence);structural engineering computing;},
note = {automated crack detection;concrete bridges;structural health;reliability;Robotic imaging;bridge surface image;spatially tuned robust multifeature classifier;STRUM classifier;machine learning classification;image thresholding;bridge mosaic;long-term bridge performance program;Haymarket;USA;Sacramento;curve fitting;},
URL = {http://dx.doi.org/10.1109/TASE.2014.2354314},
} 


@article{16393824 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Deformable map matching for uncertain loop-less maps [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Kanji, T.},
year = {8 Sept. 2016},
pages = {7 pp. - },
address = {USA},
abstract = {In the classical context of robotic mapping and localization, map matching is typically defined as the task of finding a rigid transformation (i.e., 3DOF rotation/translation on the 2D moving plane) that aligns the query and reference maps built by mobile robots. This definition is valid in loop-rich trajectories that enable a mapper robot to close many loops, for which precise maps can be assumed. The same cannot be said about the newly emerging autonomous navigation and driving systems, which typically operate in loop-less trajectories that have no large loop (e.g., straight paths). In this paper, we propose a solution that overcomes this limitation by merging the two maps. Our study is motivated by the observation that even when there is no large loop in either the query or reference map, many loops can often be obtained in the merged map. We add two new aspects to map matching: (1) image retrieval with discriminative deep convolutional neural network (DCNN) features, which efficiently generates a small number of good initial alignment hypotheses; and (2) map merge, which jointly deforms the two maps to minimize differences in shape between them. To realize practical computation time, we also present a preemption scheme that avoids excessive evaluation of useless map-matching hypotheses. To verify our approach experimentally, we created a novel collection of uncertain loop-less maps by utilizing the recently published North Campus Long-Term (NCLT) dataset and its ground-truth GPS data. The results obtained using these map collections confirm that our approach improves on previous map-matching approaches.},
keywords = {convolution;image matching;image retrieval;mobile robots;neural nets;path planning;robot vision;},
note = {deformable map matching;uncertain loop-less maps;robotic mapping;robotic localization;2D moving plane;mobile robots;loop-rich trajectories;mapper robot;autonomous navigation and driving systems;loop-less trajectories;image retrieval;discriminative deep convolutional neural network features;DCNN features;map merge;shape difference minimization;uncertain loopless maps;North Campus Long-Term dataset;NCLT dataset;GPS data;map collections;},
} 


@inproceedings{10014880 ,
language = {English},
copyright = {Copyright 2008, The Institution of Engineering and Technology},
title = {Active SLAM in structured environments},
journal = {2008 IEEE International Conference on Robotics and Automation. The Half-Day Workshop on: Towards Autonomous Agriculture of Tomorrow},
author = {Leung, C. and Shoudong Huang and Dissanayake, G.},
year = {2008//},
pages = {1898 - 903},
address = {Piscataway, NJ, USA},
abstract = {This paper considers the trajectory planning problem for line-feature based SLAM in structured indoor environments. The robot poses and line features are estimated using smooth and mapping (SAM) which is found to provide more consistent estimates than the extended Kalman filter (EKF) The objective of trajectory planning is to minimise the uncertainty of the estimates and to maximise coverage. Trajectory planning is performed using model predictive control (MPC) with an attractor incorporating long term goals. This planning is demonstrated both in simulation and in a real-time experiment with a Pioneer2DX robot.},
keywords = {Kalman filters;path planning;predictive control;SLAM (robots);},
note = {active SLAM;structured indoor environments;trajectory planning problem;extended Kalman filter;model predictive control;Pioneer2DX robot;},
} 


@inproceedings{10749152 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {The autonomous city explorer (ACE) project - mobile robot navigation in highly populated urban environments},
journal = {2009 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Lidoris, G. and Rohrmuller, F. and Wollherr, D. and Buss, M.},
year = {2009//},
pages = {1416 - 22},
address = {Piscataway, NJ, USA},
abstract = {One of the greatest challenges nowadays in robotics is the advancement of robots from industrial tools to companions and helpers of humans, operating in natural, populated environments. In this respect, the Autonomous City Explorer (ACE) project aims to combine the research fields of autonomous mobile robot navigation and human robot interaction. A robot has been created that is capable of navigating in an unknown, highly populated, urban environment, based only on information extracted through interaction with passers-by and its local perception capabilities. This paper describes the algorithms and architecture that make up the navigation subsystem of ACE. More specifically, the algorithms used for Simultaneous Localization and Mapping (SLAM), path planning in dynamic environments and behavior selection are presented, as well as the system architecture that integrates them to a complete working system. Results from an extended field experiment, where the robot navigated autonomously through the downtown city area of Munich, are analyzed and show that the robot is capable of long-term, safe navigation in real-world settings.},
keywords = {human-robot interaction;mobile robots;path planning;SLAM (robots);},
note = {autonomous city explorer project;autonomous mobile robot navigation;highly populated urban environment;human robot interaction;simultaneous localization-and-mapping;SLAM;path planning;dynamic environment;},
URL = {http://dx.doi.org/10.1109/ROBOT.2009.5152534},
} 


@article{15947956 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Localizing ground penetrating RADAR: a step toward robust autonomous ground vehicle localization},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Cornick, M. and Koechling, J. and Stanley, B. and Beijia Zhang},
volume = { 33},
number = { 1},
year = {2016/01/},
pages = {82 - 102},
issn = {1556-4959},
address = {USA},
abstract = {Autonomous ground vehicles navigating on road networks require robust and accurate localization over long-term operation and in a wide range of adverse weather and environmental conditions. GPS/INS (inertial navigation system) solutions, which are insufficient alone to maintain a vehicle within a lane, can fail because of significant radio frequency noise or jamming, tall buildings, trees, and other blockage or multipath scenarios. LIDAR and camera map-based vehicle localization can fail when optical features become obscured, such as with snow or dust, or with changes to gravel or dirt road surfaces. Localizing ground penetrating radar (LGPR) is a new mode of a priori map-based vehicle localization designed to complement existing approaches with a low sensitivity to failure modes of LIDAR, camera, and GPS/INS sensors due to its low-frequency RF energy, which couples deep into the ground. Most subsurface features detected are inherently stable over time. Significant research, discussed herein, remains to prove general utility. We have developed a novel low-profile ultra-low power LGPR system and demonstrated real-time operation underneath a passenger vehicle. A correlation maximizing optimization technique was developed to allow real-time localization at 126 Hz. Here we present the detailed design and results from highway testing, which uses a simple heuristic for fusing LGPR estimates with a GPS/INS system. Cross-track localization accuracies of 4.3 cm RMS relative to a &ldquo;truth&rdquo; RTK GPS/INS unit at speeds up to 100 km/h (60 mph) are demonstrated. These results, if generalizable, introduce a widely scalable real-time localization method with cross-track accuracy as good as or better than current localization methods.},
keywords = {Global Positioning System;ground penetrating radar;inertial navigation;position control;remotely operated vehicles;road vehicles;},
note = {low-profile ultra-low power LGPR system;camera map-based vehicle localization;light detection and ranging;LIDAR;inertial navigation system;Global Positioning System;INS solution;GPS solution;road networks;autonomous ground vehicle localization;ground penetrating radar localization;frequency 126 Hz;},
URL = {http://dx.doi.org/10.1002/rob.21605},
} 


@inproceedings{15458068 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Enabling persistent autonomy for underwater gliders through terrain based navigation},
journal = {OCEANS 2015},
author = {Stuntz, A. and Liebel, D. and Smith, R.N.},
year = {2015//},
pages = {10 pp. - },
address = {Piscataway, NJ, USA},
abstract = {To effectively examine ocean processes we must often sample over the duration of long (weeks to months) oscillation patterns. Such sampling requires persistent autonomous underwater vehicles, that have a similarly long deployment duration. Actively actuated (propeller-driven) underwater vehicles have proven effective in multiple sampling scenarios, however they have limited deployment endurance. The emergence of less actuated vehicles, i.e., underwater gliders, has enabled greater energy savings and thus increased endurance. Due to reduced actuation, these vehicles are more susceptible to external forces, e.g., ocean currents, causing them to have poor navigational and localization accuracy underwater. This is exacerbated in coastal regions, where current velocities are the same order of magnitude as the vehicle velocity. In this paper, we examine a method of reducing navigation and localization error, not only for navigation, but more so for more accurately reconstructing the path that the glider traversed to contextualize the gathered data, with respect to the science question at hand. We present a set of algorithms for offline processing that accurately localizes the traversed path of an underwater glider over long-term, ocean deployments. The proposed method utilizes terrain-based navigation with only depth, altimeter and compass data compared to local bathymetry maps to provide accurate reconstructions of traversed paths in the ocean.},
keywords = {autonomous underwater vehicles;mobile robots;path planning;telerobotics;},
note = {underwater gliders autonomy;terrain-based navigation;},
URL = {http://dx.doi.org/10.1109/OCEANS-Genova.2015.7271751},
} 


@inproceedings{10646060 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {Uncalibrated monocular based simultaneous localization and mapping for indoor autonomous mobile robot navigation},
journal = {2009 International Conference on Networking, Sensing and Control},
author = {Siyao Fu and Guosheng Yang},
year = {2009//},
pages = {663 - 8},
address = {Piscataway, NJ, USA},
abstract = {This paper describes a an SLAM algorithm for the navigation for an indoor autonomous mobile robot. The main emphasis of this paper is on the ability of line extraction. A recognition method based on straight line extraction is proposed for extracting the key features on the office ceiling, in an effort to estimate the pose of mobile robot. Random sample consensus (RANSAC) paradigm is used to group the line segments. During the navigation, onboard odometry is used at the beginning stage to estimate the information of environment for visual reckoning, while lamps on the ceiling act as beacons for positioning to eliminate accumulation of errors after a long-term run. The data captured from infrared sensors is used for constructing a map. The proposed method scales well with respect to the size of the input image and the number and size of the shapes within the data. Moreover the algorithm is conceptually simple and easy to implement. Simulation and experimental results show that good recognition and localization can be achieved using the proposed method, allowing for the interested region correspondence matching and mapping between images from different sensors or the same sensor indifferent time phrase.},
keywords = {distance measurement;feature extraction;image fusion;image matching;mobile robots;navigation;pose estimation;random processes;robot vision;SLAM (robots);},
note = {simultaneous localization and mapping algorithm;indoor autonomous mobile robot navigation;recognition method;straight line extraction;key feature extraction;pose estimation;random sample consensus paradigm;onboard odometry;visual reckoning;infrared sensors;image matching;},
URL = {http://dx.doi.org/10.1109/ICNSC.2009.4919356},
} 


@inproceedings{14002387 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Long-term simultaneous localization and mapping with generic linear constraint node removal},
journal = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2013)},
author = {Carlevaris-Bianco, N. and Eustice, R.M.},
year = {2013//},
pages = {1034 - 41},
address = {Piscataway, NJ, USA},
abstract = {This paper reports on the use of generic linear constraint (GLC) node removal as a method to control the computational complexity of long-term simultaneous localization and mapping. We experimentally demonstrate that GLC provides a principled and flexible tool enabling a wide variety of complexity management schemes. Specifically, we consider two main classes: batch multi-session node removal, in which nodes are removed in a batch operation between mapping sessions, and online node removal, in which nodes are removed as the robot operates. Results are shown for 34.9 h of real-world indoor-outdoor data covering 147.4 km collected over 27 mapping sessions spanning a period of 15 months.},
keywords = {graph theory;SLAM (robots);},
note = {long-term simultaneous localization and mapping;generic linear constraint node removal;GLC node removal;complexity management scheme;batch multi-session node removal;},
URL = {http://dx.doi.org/10.1109/IROS.2013.6696478},
} 


@inproceedings{9552909 ,
language = {English},
copyright = {Copyright 2007, The Institution of Engineering and Technology},
title = {BiCamSLAM: two times mono is more than stereo},
journal = {2007 IEEE International Conference on Robotics and Automation (IEEE Cat No. 07CH37836D)},
author = {Sola, J. and Monin, A. and Devy, M.},
year = {2007//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {This paper is an invitation to use mono-vision techniques on stereo-vision equipped robots. By using monocular algorithms on both cameras, the advantages of mono-vision (bearing-only, with infinity range but no 3D instant information) and stereo-vision (3D information only up to a limited range) naturally add up to provide interesting possibilities, that are here developed and demonstrated using an EKF-based monocular SLAM algorithm. Mainly we obtain: a) fast 3D mapping with long term, absolute angular references; b) great landmark updating flexibility; and c) the possibility of stereo rig extrinsic self-calibration, providing a much more robust and accurate sensor. Experimental results show the pertinence of the proposed ideas, which should be easily exportable (and we encourage to do so) to other, more performing, vision-based SLAM algorithms.},
keywords = {Kalman filters;robot vision;SLAM (robots);stereo image processing;},
note = {BiCamSLAM;monovision technique;stereo-vision equipped robot;extended Kalman filter;3D mapping;absolute angular reference;landmark update;},
} 


@inproceedings{13195014 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Laser-only road-vehicle localization with dual 2D push-broom LIDARS and 3D priors},
journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2012)},
author = {Baldwin, I. and Newman, P.},
year = {2012//},
pages = {2490 - 7},
address = {Piscataway, NJ, USA},
abstract = {We demonstrate the viability of using 2D LIDAR data as the sole means for accurate, robust, long-term road-vehicle localization within a prior map in a complex, dynamic real-world setting. We utilize a dual-LIDAR system - one oriented horizontally, in order to infer vehicle linear and rotational velocity, and one declined to capture a dense view of the surrounds - that allows us to estimate both velocity and position within a prior map. We show how probabilistically modelling the noisy local velocity estimates from the horizontal laser feed, fusing these estimates with data from the declined LIDAR to form a dense 3D swathe and matching this swathe statistically within a map will allow for robust, long-term position estimation. We accommodate estimation errors induced by passing vehicles, pedestrians, ground-strike etc., by learning a positional-dependent sensor model - that is, a sensor-model that varies spatially - and show that learning such a model for LIDAR data allows us to deal gracefully with the complexities of realworld data. We validate the concept over more than 9 kilometres of driven distance in and around the town of Woodstock, Oxfordshire.},
keywords = {navigation;optical radar;optical sensors;probability;road vehicles;},
note = {laser-only road-vehicle localization;dual 2D push-broom LIDARS;3D priors;dynamic real-world setting;rotational velocity;vehicle linear velocity;probabilistic modelling;estimation errors;pedestrians;ground-strike;sensor-model;horizontal laser feed;long-term road-vehicle localization;},
URL = {http://dx.doi.org/10.1109/IROS.2012.6385677},
} 


@inproceedings{10571037 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {ShopBot: progress in developing an interactive mobile shopping assistant for everyday use},
journal = {2008 IEEE International Conference on Systems, Man and Cybernetics (SMC 2008)},
author = {Gross, H.-M. and Boehme, H.-J. and Schroeter, C. and Mueller, S. and Koenig, A. and Martin, C. and Merten, M. and Bley, A.},
year = {2008//},
pages = {3471 - 8},
address = {Piscataway, NJ, USA},
abstract = {The paper describes progress achieved in our long-term research project ShopBot, which aims at the development of an intelligent and interactive mobile shopping assistant for everyday use in shopping centers or home improvement stores. It is focusing on recent progress concerning two important methodological aspects: (i) the on-line building of maps of the operation area by means of advanced Rao-Blackwellized SLAM approaches using both sonar-based gridmaps as well as vision-based graph maps as representations, and (ii) a probabilistic approach to multi-modal user detection and tracking during the guidance tour. Experimental results of both the map building characteristics and the person tracking behavior achieved in an ordinary home improvement store demonstrate the reliability of both approaches. Moreover, we present first very encouraging results of long-term field trials which have been executed with three robotic shopping assistants in another home improvement store in Bavaria since March 2008. In this field test, the robots could demonstrate their suitability for this challenging real-world application, as well as the necessary user acceptance.},
keywords = {graph theory;intelligent robots;mobile robots;probability;robot vision;service robots;sonar detection;},
note = {interactive mobile shopping assistant;home improvement store;sonar-based gridmap;vision-based graph map;probabilistic approach;multimodal user detection;robotic shopping assistant;},
URL = {http://dx.doi.org/10.1109/ICSMC.2008.4811835},
} 


@inproceedings{9109904 ,
language = {English},
copyright = {Copyright 2006, The Institution of Engineering and Technology},
copyright = {CD-ROM},
title = {Active airborne localisation and exploration in unknown environments using inertial SLAM},
journal = {2006 IEEE Aerospace Conference (IEEE Cat. No. 05TH8853C)},
author = {Bryson, M. and Salah Sukkarieh},
year = {2006//},
pages = {13 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Future unmanned aerial vehicle (UAV) applications will require high-accuracy localisation in environments in which navigation infrastructure such as the Global Positioning System (GPS) and prior terrain maps may be unavailable or unreliable. In these applications, long-term operation requires the vehicle to build up a spatial map of the environment while simultaneously localising itself within the map, a task known as simultaneous localisation and mapping (SLAM). In the first part of this paper we present an architecture for performing inertial-sensor based SLAM on an aerial vehicle. We demonstrate an on-line path planning scheme that intelligently plans the vehicle's trajectory while exploring unknown terrain in order to maximise the quality of both the resulting SLAM map and localisation estimates necessary for the autonomous control of the UAV. Two important performance properties and their relationship to the dynamic motion and path planning systems on-board the UAV are analysed. Firstly we analyse information-based measures such as entropy. Secondly we perform an observability analysis of inertial SLAM by recasting the algorithms into an indirect error model form. Qualitative knowledge gained from the observability analysis is used to assist in the design of an information-based trajectory planner for the UAV. Results of the online path planning algorithm are presented using a high-fidelity 6-DoF simulation of a UAV during a simulated navigation and mapping task.},
keywords = {aircraft instrumentation;aircraft navigation;Global Positioning System;path planning;remotely operated vehicles;sensors;terrain mapping;},
note = {active airborne localisation;active airborne exploration;unmanned aerial vehicle applications;navigation infrastructure;spatial map;Simultaneous Localisation And Mapping;inertial sensor;on-line path planning scheme;vehicle trajectory;autonomous control;dynamic motion;path planning systems;entropy;observability analysis;indirect error model form;information-based trajectory planner;online path planning algorithm;high-fidelity 6-DoF simulation;mapping task;},
} 


@inproceedings{15278375 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Environment selection and hierarchical place recognition},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Mohan, M. and Galvez-Lopez, D. and Monteleoni, C. and Sibley, G.},
year = {2015//},
pages = {5487 - 94},
address = {Piscataway, NJ, USA},
abstract = {As robots continue to create long-term maps, the amount of information that they need to handle increases over time. In terms of place recognition, this implies that the number of images being considered may increase until exceeding the computational resources of the robot. In this paper we consider a scenario where, given multiple independent large maps, possibly from different cities or locations, a robot must effectively and in real time decide whether it can localize itself in one of those known maps. Since the number of images to be handled by such a system is likely to be extremely large, we find that it is beneficial to decompose the set of images into independent groups or environments. This raises a new question: Given a query image, how do we select the best environment? This paper proposes a similarity criterion that can be used to solve this problem. It is based on the observation that, if each environment is described in terms of its co-occurrent features, similarity between environments can be established by comparing their co-occurrence matrices. We show that this leads to a novel place recognition algorithm that divides the collection of images into environments and arranges them in a hierarchy of inverted indices. By selecting first the relevant environment for the operating robot, we can reduce the number of images to perform the actual loop detection, reducing the execution time while preserving the accuracy. The practicality of this approach is shown through experimental results on several large datasets covering a combined distance of more than 750Km.},
keywords = {cartography;image recognition;matrix algebra;robot vision;},
note = {environment selection;hierarchical place recognition algorithm;multiple independent large maps;image decomposition;query image;similarity criterion;co-occurrent features;co-occurrence matrices;image collection;inverted indices hierarchy;loop detection;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7139966},
} 


@article{10358724 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {Subjective local maps for hybrid metric-topological SLAM},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Blanco, J.L. and Gonzalez, J. and Fernandez-Madrigal, J.-A.},
volume = { 57},
number = { 1},
year = {2009/01/31},
pages = {64 - 74},
issn = {0921-8890},
address = {Netherlands},
abstract = {Hybrid maps where local metric submaps are kept in the nodes of a graph-based topological structure are gaining relevance as the focus of robot Simultaneous Localization and Mapping (SLAM) shifts towards spatial scalability and long-term operation. In this paper we examine the applicability of spectral graph partitioning techniques to the automatic generation of metric submaps by establishing groups in the sequence of observations gathered by the robot. One of the main aims of this work is to provide a probabilistically grounded interpretation of such a partitioning technique in the context of generating local maps. We also discuss how to apply it to different kinds of sensory data (landmarks extracted from stereo images and laser range scans) and how to consider them simultaneously. An important feature of our approach is that the partitioning takes into account the intrinsic characteristics of the sensors, such as the sensor field of view, instead of applying heuristics supplied by a human as in other works. Thus the robot builds &ldquo;subjective&rdquo; local maps whose size will be determined by the nature of the sensors. The ideas presented here are supported by experimental results from a real mobile robot as well as simulations for statistical analysis. We discuss the effects of considering different combinations of sensors in the resulting clustering of the environment.[All rights reserved Elsevier].},
keywords = {mobile robots;path planning;SLAM (robots);statistical analysis;},
note = {subjective local maps;hybrid metric-topological SLAM;local metric submaps;graph-based topological structure;robot simultaneous localization and mapping;spatial scalability;mobile robot;statistical analysis;},
URL = {http://dx.doi.org/10.1016/j.robot.2008.02.002},
} 


@inproceedings{14921027 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Towards background flow based AUV localization},
journal = {2014 IEEE 53rd Annual Conference on Decision and Control (CDC)},
author = {Zhuoyuan Song and Mohseni, K.},
year = {2014//},
pages = {6945 - 50},
address = {Piscataway, NJ, USA},
abstract = {Underwater localization faces many constrains and long-term persistent global localization for autonomous underwater vehicles (AUVs) is very difficult. In this paper, we propose a novel AUV localization method taking advantage of the recent progress in ocean general circulation models (OGCMs). During navigation, the AUV performs intermittent local background flow velocity measurements or estimates using on-board sensors. A series of preloaded flow velocity forecast maps generated by OGCMs are referred by a particle filter in updating particle weights based on resemblance between forecasts and local estimation. A rigorous derivation of the problem in probability theory is presented to reveal the recursive structure of the target distribution function. Simulations in a simple double-gyre velocity field exhibit satisfactory converging localization error. Further simulations in a flow field with local flow fluctuations that are not resolved by OGCMs show similar convergent localization error with a slower converging rate. As a first step towards a new set of underwater localization methods, this work presents promising results and reveals the possibility of realizing converging global underwater localization through partial utilization of the background flow information that is easily accessible.},
keywords = {autonomous underwater vehicles;mobile robots;particle filtering (numerical methods);telerobotics;velocity measurement;},
note = {autonomous underwater vehicles;AUV localization method;ocean general circulation models;intermittent local background flow velocity measurements;on-board sensors;preloaded flow velocity forecast maps;particle filter;local estimation;probability theory;target distribution function recursive structure;double-gyre velocity field;converging localization error;underwater localization methods;},
URL = {http://dx.doi.org/10.1109/CDC.2014.7040480},
} 


@inproceedings{14617151 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Episodic non-Markov localization: Reasoning about short-term and long-term features},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Biswas, J. and Veloso, M.},
year = {2014//},
pages = {3969 - 74},
address = {Piscataway, NJ, USA},
abstract = {Markov localization and its variants are widely used for localization of mobile robots. These methods assume Markov independence of observations, implying that observations made by a robot correspond to a static map. However, in real human environments, observations include occlusions due to unmapped objects like chairs and tables, and dynamic objects like humans. We introduce an episodic non-Markov localization algorithm that maintains estimates of the belief over the trajectory of the robot while explicitly reasoning about observations and their correlations arising from unmapped static objects, moving objects, as well as objects from the static map. Observations are classified as arising from long-term features, short-term features, or dynamic features, which correspond to mapped objects, unmapped static objects, and unmapped dynamic objects respectively. By detecting time steps along the robot's trajectory where unmapped observations prior to such time steps are unrelated to those afterwards, non-Markov localization limits the history of observations and pose estimates to &ldquo;episodes&rdquo; over which the belief is computed. We demonstrate non-Markov localization in challenging real world indoor and outdoor environments over multiple datasets, comparing it with alternative state-of-the-art approaches, showing it to be robust as well as accurate.},
keywords = {inference mechanisms;Markov processes;mobile robots;trajectory control;},
note = {episodic nonMarkov localization;short-term features;long-term features;mobile robots;Markov observation independence;static map;robot trajectory;unmapped static objects;static map;mapped objects;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907435},
} 


@article{11955798 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Indoor navigation to support the blind person using true pathway within the map},
journal = {Journal of Computer Sciences},
journal = {J. Comput. Sci. (USA)},
author = {Ali, A.M. and Nordin, M.J.},
volume = { 6},
number = { 7},
year = {2010//},
pages = {740 - 7},
issn = {1549-3636},
address = {USA},
abstract = {Problem statement: Map creation remains a very active field in the robotics and AI community, however it contains some challenge points like data association and the high degree of accuracy of localization which are seems to be difficult in some cases, more than that, most of these study focus on the robot navigation, without any consideration for the semantic of the environment, to serve human like blind persons. Approach: This study introduced a monocular SLAM method, which uses the Scale Invariant Features Transform (SIFT) representation for the scene. The scene represented as clouds of sift features within the map; this hierarchical representation of space, serving to estimate the current direction in the environment within the current session. The system exploited the tracking of the same features of successive frames to calculate scalar weights for these features, to build a map of the environment indicating the camera movement, then by comparing the camera movement of the current moving with the true pathway within the same session the system can help and advice the blind person to navigate more confidently, through auditory information for the path way in the surroundings. Extended Kalman Filter (EKF) used to estimate the camera movement within the successive frames. Results: The experimental work tested using the proposed method with a hand-held camera walking in indoor environment. The results show a good estimation on the spatial locations of the camera within few milliseconds. Tracking of the true pathway in addition to semantic environment within the session can give a good support to the blind person for navigation. Conclusion: The study presented new semantic features model helping the blind person for navigation environment using these clouds of features, for long-term appearance-based localization of a cane with web camera vision as the external sensor.},
keywords = {handicapped aids;Kalman filters;sensor fusion;SLAM (robots);},
note = {indoor navigation;blind person support;true pathway;map creation;AI community;robotic community;data association;SLAM method;scale invariant features transform;SIFT;extended Kalman filter;EKF;},
URL = {http://dx.doi.org/10.3844/jcssp.2010.740.747},
} 


@inproceedings{14616670 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Incremental unsupervised topological place discovery},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Murphy, L. and Sibley, G.},
year = {2014//},
pages = {1312 - 18},
address = {Piscataway, NJ, USA},
abstract = {This paper describes an online place discovery and recognition engine that fuses information over time to create topologically distinct places. A key motivation is the recognition that a single image may be a poor exemplar of what constitutes a place. Images are not `places' nor are they `documents'. Instead, by treating image-sequences as a multimodal distribution over topics - and by discovering topics incrementally and online - it is possible to both reduce the memory footprint of place recognition systems, and to improve precision and recall. Distinctive key-places are represented by a cluster topics found from the covisibility graph of a relative simultaneous localization and mapping engine - key-places inherently span many images. A dynamic vocabulary of visual words and density based clustering is used to continually estimate a set of visual topics, changes in which drive the place-recognition process. The system is evaluated using an indoor robot sequence, a standard outdoor robot sequence and a long-term sequence from a static camera. Experiments demonstrate qualitatively distinct themes associated with discovered places - from common place types such as `hallway', or `desk-area', to temporal concepts such as `dusk', `dawn' or `mid-day'. Compared to traditional image-based place-recognition, this reduces the information that must be stored without reducing place-recognition performance.},
keywords = {graph theory;image sensors;image sequences;object recognition;pattern clustering;robot vision;SLAM (robots);},
note = {incremental unsupervised topological place discovery;online place discovery-and-recognition engine;memory footprint reduction;place recognition systems;precision improvement;recall improvement;cluster topics;covisibility graph;relative simultaneous localization-and-mapping engine;dynamic visual words vocabulary;density based clustering;visual topics estimation;indoor robot sequence;standard outdoor robot sequence;static camera;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907022},
} 


@inproceedings{8724429 ,
language = {English},
copyright = {Copyright 2006, IEE},
title = {Automating answers to "where am i?"},
journal = {IEE Forum on: Autonomous Systems},
author = {Newman, P.},
year = {2005//},
pages = {7 pp. - },
address = {Stevenage, UK},
abstract = {In many situations, large-scale, long term deployment of an autonomous vehicle requires an ability to navigate in arbitrary workspaces and must be able to establish "where am I what surrounds me?". This paper describe simultaneous localisation and mapping (SLAM)techniques and implementations in which an autonomous vehicle explores its workspace using onboard sensors and inextricably binds together the tasks of mapping and localisation.},
keywords = {path planning;remotely operated vehicles;},
note = {autonomous vehicle;simultaneous localisation and mapping techniques;long term deployment;arbitrary workspaces;},
} 


@inproceedings{13195216 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Realizing, reversing, recovering: Incremental robust loop closing over time using the iRRR algorithm},
journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2012)},
author = {Latif, Y. and Cadena, C. and Neira, J.},
year = {2012//},
pages = {4211 - 17},
address = {Piscataway, NJ, USA},
abstract = {The ability to reconsider information over time allows to detect failures and is crucial for long term robust autonomous robot applications. This applies to loop closure decisions in localization and mapping systems. This paper describes a method to analyze all available information up to date in order to robustly remove past incorrect loop closures from the optimization process. The main novelties of our algorithm are: 1. incrementally reconsidering loop closures and 2. handling multi-session, spatially related or unrelated experiments. We validate our proposal in real multi-session experiments showing better results than those obtained by state of the art methods.},
keywords = {mobile robots;robust control;},
note = {incremental robust loop;iRRR algorithm;robust autonomous robot applications;mapping systems;localization systems;mobile robot;realizing;reversing;recovering;},
URL = {http://dx.doi.org/10.1109/IROS.2012.6385879},
} 


@inproceedings{13998964 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {A metric approach for environments mapping},
journal = {2013 International Conference on Control, Decision and Information Technologies (CoDIT)},
author = {Slimane, N. and Khireddine, M.S. and Chafaa, K.},
year = {2013//},
pages = {647 - 52},
address = {Piscataway, NJ, USA},
abstract = {One of the main issues in mobile robotics is the autonomous navigation of a mobile robot in an unknown environment. Concurrent mapping and localisation or simultaneous localisation and mapping is a stochastic map building method which permits consistent robot navigation without requiring an a priori map. The governing idea which guides autonomous robotics consists in saying that the vehicle builds its chart progressively during exploration enabling it to evolve in the long term in unknown places in advance. When the robot environment chart is not known a priori, a generation module of incremental chart must obligatorily be integrated into the navigation system. The map is built incrementally as the robot observes the environment with its on-board sensors and, at the same time, is used to localise the robot. Unfortunately, the inaccuracy of the odometric sensors does not allow a sufficiently correct positioning of the robot. In this paper, simultaneous localisation and map building is performed with a metric approach which permits both precision and robustness. The most important innovation of the approach is the way how errors in the robot localisation control are handled by map building using the landmarks localisation information. The method uses data from a laser scanner to extract distances and orientations of landmarks and combines control localisation and metric paradigm. The metric approach, based on the Kalman filter, uses a new concept to avoid the problem of the drift in odometry. The simulation section will validate the maps representation approach and presents different aspect of environments.},
keywords = {distance measurement;Kalman filters;mobile robots;navigation;object detection;position control;robot vision;SLAM (robots);stochastic processes;},
note = {metric approach;environment mapping;mobile robotics;autonomous navigation;unknown environment;concurrent mapping and localisation;simultaneous localisation and mapping;stochastic map building method;robot navigation;autonomous robotics;vehicle;exploration;robot environment chart;incremental chart;navigation system;robot on-board sensors;odometric sensors;robot positioning;robustness;robot localisation control;landmark localisation information;laser scanner data;landmark distance extraction;landmark orientation extraction;control localisation;Kalman filter;odometry drift;},
URL = {http://dx.doi.org/10.1109/CoDIT.2013.6689619},
} 


@inproceedings{11431257 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Visual robot localization using compact binary landmarks},
journal = {2010 IEEE International Conference on Robotics and Automation (ICRA 2010)},
author = {Ikeda, K. and Tanaka, K.},
year = {2010//},
pages = {4397 - 403},
address = {Piscataway, NJ, USA},
abstract = {This paper is concerned with the problem of mobile robot localization using a novel compact representation of visual landmarks. With recent progress in lifelong map-learning as well as in information sharing networks, compact representation of a large-size landmark database has become crucial. In this paper, we propose a compact binary code (e.g. 32bit code) landmark representation by employing the semantic hashing technique from web-scale image retrieval. We show how well such a binary representation achieves compactness of a landmark database while maintaining efficiency of the localization system. In our contribution, we investigate the cost-performance, the semantic gap, the saliency evaluation using the presented techniques as well as challenge to further reduce the resources (#bits) per landmark. Experiments using a high-speed car-like robot show promising results.},
keywords = {control engineering computing;image retrieval;mobile robots;path planning;robot vision;},
note = {visual robot localization;compact binary landmarks;mobile robot localization;information sharing networks;landmark database;visual landmarks;compact binary code;semantic hashing technique;web-scale image retrieval;binary representation;semantic gap;saliency evaluation;high-speed car-like robot;},
URL = {http://dx.doi.org/10.1109/ROBOT.2010.5509579},
} 


@article{12647728 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Appearance-based mapping and localization for mobile robots using a feature stability histogram},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Bacca, B. and Salvi, J. and Cufi, X.},
volume = { 59},
number = { 10},
year = {2011/10/},
pages = {840 - 57},
issn = {0921-8890},
address = {Netherlands},
abstract = {The strength of appearance-based mapping models for mobile robots lies in their ability to represent the environment through high-level image features and to provide human-readable information. However, developing a mapping and a localization method using these kinds of models is very challenging, especially if robots must deal with long-term mapping, localization, navigation, occlusions, and dynamic environments. In other words, the mobile robot has to deal with environmental appearance change, which modifies its representation of the environment. This paper proposes an indoor appearance-based mapping and a localization method for mobile robots based on the human memory model, which was used to build a Feature Stability Histogram (FSH) at each node in the robot topological map. This FSH registers local feature stability over time through a voting scheme, and the most stable features were considered for mapping, for Bayesian localization and for incrementally updating the current appearance reference view in the topological map. The experimental results are presented using an omnidirectional images dataset acquired over the long-term and considering: illumination changes (time of day, different seasons), occlusions, random removal of features, and perceptual aliasing. The results include a comparison with the approach proposed by Dayoub and Duckett (2008) [19] and the popular Bag-of-Words (Bazeille and Filliat, 2010) [35] approach. The obtained results confirm the viability of our method and indicate that it can adapt the internal map representation over time to localize the robot both globally and locally. [All rights reserved Elsevier].},
keywords = {Bayes methods;mobile robots;path planning;robot vision;SLAM (robots);},
note = {indoor appearance-based mapping model;mobile robots;feature stability histogram;high-level image features;human-readable information;localization method;long-term mapping;navigation;occlusions;dynamic environments;environmental appearance change;human memory model;robot topological map;voting scheme;Bayesian localization;omnidirectional images dataset;illumination changes;feature random removal;perceptual aliasing;bag-of-words approach;},
URL = {http://dx.doi.org/10.1016/j.robot.2011.06.008},
} 


@inproceedings{13851266 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Ascending stairway modeling from dense depth imagery for traversability analysis},
journal = {2013 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Delmerico, J.A. and Baran, D. and David, P. and Ryde, J. and Corso, J.J.},
year = {2013//},
pages = {2283 - 90},
address = {Piscataway, NJ, USA},
abstract = {Localization and modeling of stairways by mobile robots can enable multi-floor exploration for those platforms capable of stair traversal. Existing approaches focus on either stairway detection or traversal, but do not address these problems in the context of path planning for the autonomous exploration of multi-floor buildings. We propose a system for detecting and modeling ascending stairways while performing simultaneous localization and mapping, such that the traversability of each stairway can be assessed by estimating its physical properties. The long-term objective of our approach is to enable exploration of multiple floors of a building by allowing stairways to be considered during path planning as traversable portals to new frontiers. We design a generative model of a stairway as a single object. We localize these models with respect to the map, and estimate the dimensions of the stairway as a whole, as well as its steps. With these estimates, a robot can determine if the stairway is traversable based on its climbing capabilities. Our system consists of two parts: a computationally efficient detector that leverages geometric cues from dense depth imagery to detect sets of ascending stairs, and a stairway modeler that uses multiple detections to infer the location and parameters of a stairway that is discovered during exploration. We demonstrate the performance of this system when deployed on several mobile platforms using a Microsoft Kinect sensor.},
keywords = {image sensors;mobile robots;object detection;path planning;robot vision;SLAM (robots);},
note = {ascending stairway modeling;dense depth imagery;traversability analysis;stairway localization;multifloor exploration;stairway detection;stairway traversal;simultaneous localization and mapping;path planning;stairway generative model;robot climbing capability;computationally efficient detector;geometric cues;stairway modeler;Microsoft Kinect sensor;mobile platforms;},
URL = {http://dx.doi.org/10.1109/ICRA.2013.6630886},
} 


@article{14533643 ,
language = {Japanese},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Development of Localization Method Using Magnetic Sensor and LIDAR},
journal = {Transactions of the Society of Instrument and Control Engineers},
journal = {Trans. Soc. Instrum. Control Eng. (Japan)},
author = {Shinohara, M. and Rahok, S.A. and Inoue, K. and Ozaki, K.},
volume = { 49},
number = { 8},
year = {2013/08/},
pages = {795 - 801},
issn = {0453-4654},
address = {Japan},
abstract = {In mobile robot's localization, it is well known that odometry can provide a reliable accuracy in short term navigation and a very high sampling rate. However, odometry produces cumulative error because of uneven terrains or wheel slippage and this error increases proportionally with the distance traveled by the mobile robot. Therefore, it is necessary to augment odometry with other sensors to improve its accuracy. This paper proposes an estimation method of mobile robot orientation using an environmental magnetic field (magnetic field that occurs in the environment). A three-axis magnetic sensor is utilized to scan the environmental magnetic field to build a magnetic database on a grid map called "a magnetic map" with the mobile robot operated with a joystick on a desired route. The mobile robot then estimates its orientation by comparing the magnetic sensor readings with the magnetic data stored in the magnetic map. However, even if the proposed method can improve the accuracy of the odometry, positioning error still remains as a major problem in long term navigation. In this work, a localization method using Monte Carlo Localization (MCL) based on a Light Detection and Ranging (LIDAR) is utilized to fix the positioning error at the areas where landmark can be observed. The experimental results showed that the mobile robot could localize robustly in any environments with the proposed method.},
keywords = {distance measurement;magnetic sensors;mobile robots;Monte Carlo methods;optical radar;},
note = {positioning error;Monte Carlo localization;magnetic map;grid map;magnetic database;three-axis magnetic sensor;environmental magnetic field;mobile robot orientation estimation method;short term navigation;odometry;light detection and ranging;LIDAR;mobile robot localization method;},
} 


@article{11266231 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Keypoint design and evaluation for place recognition in 2D lidar maps},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Bosse, M. and Zlot, R.},
volume = { 57},
number = { 12},
year = {2009/12/31},
pages = {1211 - 24},
issn = {0921-8890},
address = {Netherlands},
abstract = {We address the place recognition problem, which we define as the problem of establishing whether an observed location has been previously seen, and if so, determining the transformation aligning the current observations to an existing map. In the contexts of robot navigation and mapping, place recognition amounts to globally localizing a robot or map segment without being given any prior estimate. An efficient method of solving this problem involves first selecting a set of keypoints in the scene which store an encoding of their local region, and then utilizing a sublinear-time search into a database of keypoints previously generated from the global map to identify places with common features. We present an algorithm to embed arbitrary keypoint descriptors in a reduced-dimension metric space, in order to frame the problem as an efficient nearest neighbor search. Given that there are a multitude of possibilities for keypoint design, we propose a general methodology for comparing keypoint location selection heuristics and descriptor models that describe the region around the keypoint. With respect to selecting keypoint locations, we introduce a metric that encodes how likely it is that the keypoint will be found in the presence of noise and occlusions during mapping passes. Metrics for keypoint descriptors are used to assess the distinguishability between the distributions of matches and non-matches and the probability the correct match will be found in an approximate k-nearest neighbors search. Verification of the test outcomes is done by comparing the various keypoint designs on a kilometers-scale place recognition problem. We apply our design evaluation methodology to three keypoint selection heuristics and six keypoint descriptor models. A full place recognition system is presented, including a series of match verification algorithms which effectively filter out false positives. Results from city-scale and long-term mapping problems illustrate our approach for both offline and online SLAM, map merging, and global localization and demonstrate that our algorithm is able to produce accurate maps over trajectories of hundreds of kilometers. [All rights reserved Elsevier].},
keywords = {optical radar;pattern recognition;SLAM (robots);},
note = {keypoint design;place recognition;2D lidar maps;robot navigation;sublinear time search;arbitrary keypoint descriptors;k-nearest neighbors search;match verification algorithms;mapping problems;SLAM;},
URL = {http://dx.doi.org/10.1016/j.robot.2009.07.009},
} 


@article{12401560 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Long-term experiments with an adaptive spherical view representation for navigation in changing environments},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Dayoub, F. and Cielniak, G. and Duckett, T.},
volume = { 59},
number = { 5},
year = {2011/05/},
pages = {285 - 95},
issn = {0921-8890},
address = {Netherlands},
abstract = {Real-world environments such as houses and offices change over time, meaning that a mobile robot's map will become out of date. In this work, we introduce a method to update the reference views in a hybrid metric-topological map so that a mobile robot can continue to localize itself in a changing environment. The updating mechanism, based on the multi-store model of human memory, incorporates a spherical metric representation of the observed visual features for each node in the map, which enables the robot to estimate its heading and navigate using multi-view geometry, as well as representing the local 3D geometry of the environment. A series of experiments demonstrate the persistence performance of the proposed system in real changing environments, including analysis of the long-term stability. [All rights reserved Elsevier].},
keywords = {mobile robots;path planning;},
note = {adaptive spherical view representation;changing environment navigation;real world environments;mobile robots;hybrid metric topological map;human memory;spherical metric representation;3D geometry;},
URL = {http://dx.doi.org/10.1016/j.robot.2011.02.013},
} 


@article{14744625 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Localization and navigation of the CoBots over long-term deployments},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Biswas, J. and Veloso, M.M.},
volume = { 32},
number = { 14},
year = {2013/12/},
pages = {1679 - 94},
issn = {0278-3649},
address = {UK},
abstract = {For the last three years, we have developed and researched multiple collaborative robots, CoBots, which have been autonomously traversing our multi-floor buildings. We pursue the goal of long-term autonomy for indoor service mobile robots as the ability for them to be deployed indefinitely while they perform tasks in an evolving environment. The CoBots include several levels of autonomy, and in this paper we focus on their localization and navigation algorithms. We present the Corrective Gradient Refinement (CGR) algorithm, which refines the proposal distribution of the particle filter used for localization with sensor observations using analytically computed state space derivatives on a vector map. We also present the Fast Sampling Plane Filtering algorithm that extracts planar regions from depth images in real time. These planar regions are then projected onto the 2D vector map of the building, and along with the laser rangefinder observations, used with CGR for localization. For navigation, we present a hierarchical planner, which computes a topological policy using a graph representation of the environment, computes motion commands based on the topological policy, and then modifies the motion commands to side-step perceived obstacles. We started logging the deployments of the CoBots one and a halfyears ago, and have since collected logs of the CoBots traversing more than 130 km over 1082 deployments and a total run time of 182 h, which we publish as a dataset consisting of more than 10 million laser scans. The logs show that although there have been continuous changes in the environment, the robots are robust to most of them, and there exist only afew locations where changes in the environment cause increased uncertainty in localization.},
keywords = {mobile robots;multi-robot systems;navigation;particle filtering (numerical methods);path planning;service robots;topology;},
note = {motion commands;graph representation;topological policy;hierarchical planner;laser rangefinder observations;2D-vector map;fast sampling plane filtering algorithm;analytically computed state space derivatives;sensor observations;particle filter;CGR algorithm;corrective gradient refinement algorithm;navigation algorithms;localization algorithms;indoor service mobile robots;multifloor buildings;multiple collaborative robots;long-term deployments;},
URL = {http://dx.doi.org/10.1177/0278364913503892},
} 


@article{11447252 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Appearance-based mapping and localisation using feature stability histograms},
journal = {Electronics Letters},
journal = {Electron. Lett. (UK)},
author = {Bacca, B. and Salvi, J. and Batlle, J. and Cufi, X.},
volume = { 46},
number = { 16},
year = {2010/08/05},
pages = {1120 - 1},
issn = {0013-5194},
address = {UK},
abstract = {Proposed is an appearance-based mapping and localisation method based on the human memory model, which is used to build a feature stability histogram (FSH) at each node in the robot topological map. FSH registers local feature stability over time through a voting scheme, and most stable features are considered for mapping and Bayesian localisation. Experimental results are presented using omnidirectional images acquired through long-term acquisition considering: illumination changes, occlusions, random removal of features and perceptual aliasing. This method is able to adapt the internal node's representation through time to achieve global and local robot localisation.},
keywords = {Bayes methods;mobile robots;path planning;robot vision;SLAM (robots);stability;},
note = {appearance based mapping;localisation;feature stability histograms;human memory model;robot topological map;voting scheme;Bayesian localisation;omnidirectional images;},
URL = {http://dx.doi.org/10.1049/el.2010.1599},
} 


@inproceedings{8412039 ,
language = {English},
copyright = {Copyright 2005, IEE},
title = {Towards exteroceptive based localisation},
journal = {2004 IEEE Conference on Robotics, Automation and Mechatronics (IEEE Cat. No.04EX913)},
author = {Spero, D.J. and Jarvis, R.A.},
volume = {vol.2},
year = {2005//},
pages = {822 - 7},
address = {Piscataway, NJ, USA},
abstract = {The intelligent application of a mobile robot, outside the experimental laboratory, requires a robust locomotive strategy that is rarely conducive to stringent kinematic modeling. Localisation methods that rely upon such modeling often fail, as model boundaries succumb to unpredictable events. This paper presents the development of a self-contained localisation system that purposely obviates the need for odometric information, and an associated kinematic model, to provide robot anonymity. Without odometry, the system is oblivious to the non-systematic vagaries of the robotic platform interacting with a natural domain. The proposed system hypothesises about the robot's absolute pose by algorithmically solving the kidnapped robot problem using exteroceptive based perception. Since no a priori information is assumed, long-term pose fixes are derived within a simultaneous localisation and mapping (SLAM) framework. Preliminary results were gathered using a skid steering mobile robot, equipped with a scanning laser rangefinder, in an outdoor environment. This novel localisation approach was found to be efficient and robust, while exhibiting the capacity for widespread applicability.},
keywords = {intelligent robots;laser ranging;mobile robots;path planning;position control;robot kinematics;},
note = {exteroceptive based localisation;intelligent mobile robot;robust locomotive strategy;self-contained localisation system;kinematic model;robot anonymity;kidnapped robot problem;exteroceptive based perception;simultaneous localisation and mapping;skid steering mobile robot;scanning laser rangefinder;outdoor environment;},
} 


@inproceedings{12095240 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Visual topometric localization},
journal = {2011 IEEE Intelligent Vehicles Symposium (IV)},
author = {Badino, H. and Huber, D. and Kanade, T.},
year = {2011//},
pages = {794 - 9},
address = {Piscataway, NJ, USA},
abstract = {One of the fundamental requirements of an autonomous vehicle is the ability to determine its location on a map. Frequently, solutions to this localization problem rely on GPS information or use expensive three dimensional (3D) sensors. In this paper, we describe a method for long-term vehicle localization based on visual features alone. Our approach utilizes a combination of topological and metric mapping, which we call topometric localization, to encode the coarse topology of the route as well as detailed metric information required for accurate localization. A topometric map is created by driving the route once and recording a database of visual features. The vehicle then localizes by matching features to this database at runtime. Since individual feature matches are unreliable, we employ a discrete Bayes filter to estimate the most likely vehicle position using evidence from a sequence of images along the route. We illustrate the approach using an 8.8 km route through an urban and suburban environment. The method achieves an average localization error of 2.7 m over this route, with isolated worst case errors on the order of 10 m.},
keywords = {automated highways;Bayes methods;feature extraction;filtering theory;geographic information systems;image matching;image motion analysis;image sequences;remotely operated vehicles;road vehicles;},
note = {visual topometric localization;autonomous vehicle;map location;localization problem;long-term vehicle localization;visual feature;topological mapping;metric mapping;route driving;feature matching;discrete Bayes filter;vehicle position estimation;image sequence;suburban environment;localization error;},
URL = {http://dx.doi.org/10.1109/IVS.2011.5940504},
} 


@inproceedings{12688545 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {A study on wearable robotics - comfort is in the context},
journal = {2011 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
author = {Herath, D.C. and Chapman, T. and Tomkins, A. and Elliott, L. and David, M. and Cooper, A. and Burnham, D. and Kodagoda, S.},
year = {2011//},
pages = {2969 - 74},
address = {Piscataway, NJ, USA},
abstract = {WITU (Wearable Indoor Tracking Unit) is a wearable robotic device that aids indoor navigation by building maps and localizing the user within them. Applications of such a device include search and rescue, travel aid in large and complex buildings, museum guides among others where external localization information such as from a GPS is not available. However, WITU relies on human intelligence both to maintain long term consistency of its location estimates and to efficiently manage its limited memory and processing capacity. This alludes to a symbiotic relationship between the user and the device and here we look at this symbiotic relationship from an end user perspective. Thus, in order to have a successful interaction, we argue that the user needs to feel comfortable wearing the device while carrying out the intended tasks. We hypothesize that this perceived comfort is dependent on the context in which the device is used. We test our hypothesis on three different scenarios; search and rescue worker, dementia patient in a long care facility and a person at a party which acts as the baseline. Results indicate an important consequence for the development of such wearable robotic systems.},
keywords = {human-robot interaction;service robots;},
note = {wearable indoor tracking unit;wearable robotic device;travel aid;complex buildings;museum guides;external localization information;GPS;search-and-rescue;symbiotic relationship;perceived comfort;},
URL = {http://dx.doi.org/10.1109/ROBIO.2011.6181757},
} 


@article{12156923 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Online and Incremental Appearance-based SLAM in Highly Dynamic Environments},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Kawewong, A. and Tongprasit, N. and Tangruamsub, S. and Hasegawa, O.},
volume = { 30},
number = { 1},
year = {2011/01/},
pages = {33 - 55},
issn = {0278-3649},
address = {USA},
abstract = {In this paper we present a novel method for online and incremental appearance-based localization and mapping in a highly dynamic environment. Using position-invariant robust features (PIRFs), the method can achieve a high rate of recall with 100% precision. It can handle both strong perceptual aliasing and dynamic changes of places efficiently. Its performance also extends beyond conventional images; it is applicable to omnidirectional images for which the major portions of scenes are similar for most places. The proposed PIRF-based Navigation method named PIRF-Nav is evaluated by testing it on two standard datasets in a similar manner as in FAB-MAP and on an additional omnidirectional image dataset that we collected. This extra dataset was collected on 2 days with different specific events, i.e. an open-campus event, to present challenges related to illumination variance and strong dynamic changes, and to test assessment of dynamic scene changes. Results show that PIRF-Nav outperforms FAB-MAP; PIRF-Nav at precision-1 yields a recall rate about twice as high (approximately 80% increase) than that of FAB-MAP. Its computation time is sufficiently short for real-time applications. The method is fully incremental, and requires no offline process for dictionary creation. Additional testing using combined datasets proves that PIRF-Nav can function over the long term and can solve the kidnapped robot problem.},
keywords = {feature extraction;mobile robots;path planning;SLAM (robots);},
note = {online appearance-based SLAM;incremental appearance-based SLAM;highly dynamic environments;position-invariant robust features;PIRF-based navigation method;FAB-MAP;omnidirectional image dataset;kidnapped robot problem;perceptual aliasing;},
URL = {http://dx.doi.org/10.1177/0278364910371855},
} 


@article{11573958 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Position-invariant robust features for long-term recognition of dynamic outdoor scenes},
journal = {IEICE Transactions on Information and Systems},
journal = {IEICE Trans. Inf. Syst. (Japan)},
author = {Kawewong, A. and Tangruamsub, S. and Hasegawa, O.},
volume = { E93-D},
number = { 9},
year = {2010//},
pages = {2587 - 601},
issn = {0916-8532},
address = {Japan},
abstract = {A novel Position-Invariant Robust Feature, designated as PIRF, is presented to address the problem of highly dynamic scene recognition. The PIRF is obtained by identifying existing local features (i.e. SIFT) that have a wide baseline visibility within a place (one place contains more than one sequential images). These wide-baseline visible features are then represented as a single PIRF, which is computed as an average of all descriptors associated with the PIRF. Particularly, PIRFs are robust against highly dynamical changes in scene: a single PIRF can be matched correctly against many features from many dynamical images. This paper also describes an approach to using these features for scene recognition. Recognition proceeds by matching an individual PIRF to a set of features from test images, with subsequent majority voting to identify a place with the highest matched PIRF. The PIRF system is trained and tested on 2000+ outdoor omnidirectional images and on COLD datasets. Despite its simplicity, PIRF offers a markedly better rate of recognition for dynamic outdoor scenes (ca. 90%) than the use of other features. Additionally, a robot navigation system based on PIRF (PIRF-Nav) can outperform other incremental topological mapping methods in terms of time (70% less) and memory. The number of PIRFs can be reduced further to reduce the time while retaining high accuracy, which makes it suitable for long-term recognition and localization.},
keywords = {image recognition;},
note = {position-invariant robust features;long-term recognition;dynamic outdoor scenes;scale invariant feature transformation;topological mapping;},
URL = {http://dx.doi.org/10.1587/transinf.E93.D.2587},
} 


@article{12422242 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Robust mobile robot localization in highly non-static environments},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Netherlands)},
author = {Jung-Suk Lee and Wan Kyun Chung},
volume = { 29},
number = { 1},
year = {2010/07/},
pages = {1 - 16},
issn = {0929-5593},
address = {Netherlands},
abstract = {In this paper, we propose a robust pose tracking method for mobile robot localization with an incomplete map in a highly non-static environment. This algorithm will work with a simple map that does not include complete in- formation about the non-static environment. With only an initial incomplete map, a mobile robot cannot estimate its pose because of the inconsistency between the real observations from the environment and the predicted observations on the incomplete map. The proposed localization algorithm uses the approach of sampling from a non-corrupted window, which allows the mobile robot to estimate its pose more robustly in a non-static environment even when subjected to severe corruption of observations. The algorithm sequence involves identifying the corruption by comparing the real observations with the corresponding predicted observations of all particles, sampling particles from a non- corrupted window that consists of multiple non-corrupted sets, and filtering sensor measurements to provide weights to particles in the corrupted sets. After localization, the estimated path may still contain some errors due to long-term corruption. These errors can be corrected using nonlinear constrained least-squares optimization. The incomplete map is then updated using both the corrected path and the stored sensor information. The performance of the proposed algorithm was verified via simulations and experiments in various highly non-static environments. Our localization algorithm can increase the success rate of tracking its pose to more than 95% compared to estimates made without its use. After that, the initial incomplete map is updated based on the localization result.},
keywords = {filtering theory;image sampling;least squares approximations;mobile robots;Monte Carlo methods;object tracking;pose estimation;robot vision;SLAM (robots);},
note = {robust mobile robot localization;nonstatic environments;robust pose tracking method;incomplete map;noncorrupted window;pose estimation;multiple noncorrupted sets;sensor measurement filtering;nonlinear constrained least-squares optimization;Monte Carlo localization;},
URL = {http://dx.doi.org/10.1007/s10514-010-9184-1},
} 


@inproceedings{10117381 ,
language = {English},
copyright = {Copyright 2008, The Institution of Engineering and Technology},
title = {Active vision for door localization and door opening using Playbot: a computer controlled wheelchair for people with mobility impairments},
journal = {Proceedings of the Fifth Canadian Conference on Computer and Robot Vision},
author = {Andreopoulos, A. and Tsotsos, J.K.},
year = {2008//},
pages = {3 - 10},
address = {Piscataway, NJ, USA},
abstract = {Playbot is a long-term, large-scale research project, whose goal is to provide a vision-based computer controlled wheelchair that enables children and adults with mobility impairments to become more independent. Within this context, we show how Playbot can actively search an indoor environment to localize a door, approach the door, use a mounted robotic arm to open the door, and go through the door, using exclusively vision-based sensors and without using a map of the environment. We demonstrate the effectiveness of active vision for localizing objects that are too large to fall within a single camera's field of view and show that well-calibrated vision-based sensors are sufficient to safely pass through a door frame that is narrow enough to tolerate a wheelchair localization error of at most a few centimetres. We provide experimental results demonstrating near perfect performance in an indoor environment.},
keywords = {control engineering computing;handicapped aids;medical robotics;mobile robots;robot vision;},
note = {active vision;door localization;door opening;Playbot;mobility impairments;vision-based computer controlled wheelchair;mounted robotic arm;vision-based sensors;wheelchair localization error;},
URL = {http://dx.doi.org/10.1109/CRV.2008.23},
} 


@inproceedings{9561798 ,
language = {English},
copyright = {Copyright 2007, The Institution of Engineering and Technology},
title = {A visual bag of words method for interactive qualitative localization and mapping},
journal = {2007 IEEE International Conference on Robotics and Automation (IEEE Cat No. 07CH37836D)},
author = {Filliat, D.},
year = {2007//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Localization for low cost humanoid or animal-like personal robots has to rely on cheap sensors and has to be robust to user manipulations of the robot. We present a visual localization and map-learning system that relies on vision only and that is able to incrementally learn to recognize the different rooms of an apartment from any robot position. This system is inspired by visual categorization algorithms called bag of words methods that we modified to make fully incremental and to allow a user-interactive training. Our system is able to reliably recognize the room in which the robot is after a short training time and is stable for long term use. Empirical validation on a real robot and on an image database acquired in real environments are presented.},
keywords = {humanoid robots;image recognition;learning (artificial intelligence);robot vision;SLAM (robots);},
note = {visual bag of words method;room recognition;user-interactive training;visual categorization;incremental learning;robot vision;map-learning system;animal-like personal robots;humanoid robots;interactive mapping;interactive qualitative localization;},
} 


@article{11159164 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Robust outdoor visual localization using a three-dimensional-edge map},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Nuske, S. and Roberts, J. and Wyeth, G.},
volume = { 26},
number = { 9},
year = {Sept. 2009},
pages = {728 - 56},
issn = {1556-4959},
address = {USA},
abstract = {Visual localization systems that are practical for autonomous vehicles in outdoor industrial applications must perform reliably in a wide range of conditions. Changing outdoor conditions cause difficulty by drastically altering the information available in the camera images. To confront the problem, we have developed a visual localization system that uses a surveyed three-dimensional (3D)-edge map of permanent structures in the environment. The map has the invariant properties necessary to achieve long-term robust operation. Previous 3D-edge map localization systems usually maintain a single pose hypothesis, making it difficult to initialize without an accurate prior pose estimate and also making them susceptible to misalignment with unmapped edges detected in the camera image. A multihypothesis particle filter is employed here to perform the initialization procedure with significant uncertainty in the vehicle's initial pose. A novel observation function for the particle filter is developed and evaluated against two existing functions. The new function is shown to further improve the abilities of the particle filter to converge given a very coarse estimate of the vehicle's initial pose. An intelligent exposure control algorithm is also developed that improves the quality of the pertinent information in the image. Results gathered over an entire sunny day and also during rainy weather illustrate that the localization system can operate in a wide range of outdoor conditions. The conclusion is that an invariant map, a robust multihypothesis localization algorithm, and an intelligent exposure control algorithm all combine to enable reliable visual localization through challenging outdoor conditions. &copy; 2009 Wiley Periodicals, Inc.},
keywords = {artificial intelligence;cameras;mobile robots;particle filtering (numerical methods);robust control;},
note = {robust outdoor visual localization;autonomous vehicles;industrial applications;camera images;robust operation;3D edge map localization systems;single pose hypothesis;multihypothesis particle filter;intelligent exposure control algorithm;},
URL = {http://dx.doi.org/10.1002/rob.20306},
} 


@inproceedings{9517353 ,
language = {English},
copyright = {Copyright 2007, The Institution of Engineering and Technology},
title = {Robotic discovery of the auditory scene},
journal = {2007 IEEE International Conference on Robotics and Automation (IEEE Cat No. 07CH37836D)},
author = {Martinson, E. and Schultz, A.},
year = {2007//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {In this work, we describe an autonomous mobile robotic system for finding and investigating ambient noise sources in the environment. Motivated by the large negative effect of ambient noise sources on robot audition, the long-term goal is to provide awareness of the auditory scene to a robot, so that it may more effectively act to filter out the interference or re-position itself to increase the signal-to-noise ratio. Here, we concentrate on the discovery of new sources of sound through the use of mobility and directed investigation. This is performed in a two-step process. In the first step, a mobile robot first explores the surrounding acoustical environment, creating evidence grid representations to localize the most influential sound sources in the auditory scene. Then in the second step, the robot investigates each potential sound source location in the environment so as to improve the localization result, and identify volume and directionality characteristics of the sound source. Once every source has been investigated, a noise map of the entire auditory scene is created for use by the robot in avoiding areas of loud ambient noise when performing an auditory task.},
keywords = {audio signal processing;hearing;mobile robots;},
note = {autonomous mobile robotic system;ambient noise sources;robot audition;signal-to-noise ratio;sound source localization;},
} 


@inproceedings{8348591 ,
language = {English},
copyright = {Copyright 2005, IEE},
title = {Rotation invariant features from omnidirectional camera images using a polar higher-order local autocorrelation feature extractor},
journal = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)},
author = {Linaker, F. and Ishikawa, M.},
volume = {vol.4},
year = {2004//},
pages = {4026 - 31},
address = {Piscataway, NJ, USA},
abstract = {Proposed in this paper is a component for extracting low-dimensional rotation invariant feature vectors directly from omnidirectional camera images. The component is based on higher-order local autocorrelation (HLAC) functions, but with a modification that makes the extraction result in rotation invariant representations. As the component provides a static mapping to feature vectors, it requires no setup or learning phase and is well-suited for lifelong learning scenarios where input distributions can be nonstationary. Experiments with an actual robot system are presented and results show that the extracted feature vectors manage to capture structures in the environment. When used as the perceptual component of a sequential Monte Carlo localizer, the location of the robot can be tracked without access to long-range distance sensors. Important limitations and suitable uses for the extracted representations are also discussed.},
keywords = {feature extraction;image representation;mobile robots;Monte Carlo methods;robot vision;},
note = {rotation invariant feature;omnidirectional camera image;autocorrelation feature extractor;higher-order local autocorrelation function;robot system;sequential Monte Carlo localizer;},
} 


@inproceedings{8313734 ,
language = {English},
copyright = {Copyright 2005, IEE},
title = {Obstacle avoidance and path planning for humanoid robots using stereo vision},
journal = {2004 IEEE International Conference on Robotics and Automation (IEEE Cat. No.04CH37508)},
author = {Sabe, K. and Fukuchi, M. and Gutmann, J.-S. and Ohashi, T. and Kawamoto, K. and Yoshigahara, T.},
volume = {Vol.1},
year = {2004//},
pages = {592 - 7},
address = {Piscataway, NJ, USA},
abstract = {This paper presents methods for path planning and obstacle avoidance for the humanoid robot QRIO, allowing the robot to autonomously walk around in a home environment. For an autonomous robot, obstacle detection and localization as well as representing them in a map are crucial tasks for the success of the robot. Our approach is based on plane extraction from data captured by a stereo-vision system that has been developed specifically for QRIO. We briefly overview the general software architecture composed of perception, short and long term memory, behavior control, and motion control, and emphasize on our methods for obstacle detection by plane extraction, occupancy grid mapping, and path planning. Experimental results complete the description of our system.},
keywords = {collision avoidance;humanoid robots;mobile robots;motion control;robot vision;},
note = {obstacle avoidance;path planning;QRIO humanoid robot;stereo vision;autonomous robot;plane extraction;behavior control;motion control;occupancy grid mapping;},
} 


@inproceedings{8065726 ,
language = {English},
copyright = {Copyright 2004, IEE},
title = {Simultaneous localisation and mapping on the Great Barrier Reef},
journal = {2004 IEEE International Conference on Robotics and Automation (IEEE Cat. No.04CH37508)},
author = {Williams, S. and Mahon, I.},
volume = {Vol.2},
year = {2004//},
pages = {1771 - 6},
address = {Piscataway, NJ, USA},
abstract = {This paper presents results of the application of the simultaneous localisation and mapping algorithm to data collected by an unmanned underwater vehicle operating on the Great Barrier Reef in Australia. By fusing information from the vehicle's on-board sonar and vision systems, it is possible to use the highly textured reef to provide estimates of the vehicle motion as well as to generate models of the gross structure of the underlying reefs. Terrain-aided navigation promises to revolutionise the ability of marine systems to track underwater bodies in many applications. This work represents a crucial step in the development of underwater technologies capable of long-term, reliable deployment. Results of the application of this technique to the tracking of the vehicle position are shown.},
keywords = {mobile robots;motion estimation;navigation;remotely operated vehicles;robot vision;sonar tracking;terrain mapping;underwater vehicles;},
note = {simultaneous localisation and mapping algorithm;unmanned underwater vehicle;onboard sonar systems;vision systems;vehicle motion estimation;terrain aided navigation;marine systems;vehicle position tracking;great barrier reef;underwater bodies;underwater technologies;reliable deployment;},
} 


@inproceedings{7169545 ,
language = {English},
copyright = {Copyright 2002, IEE},
title = {Mobile robotics in the long term-exploring the fourth dimension},
journal = {Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)},
author = {Austin, D. and Fletcher, L. and Zelinsky, A.},
volume = {vol.2},
year = {2001//},
pages = {613 - 18},
address = {Piscataway, NJ, USA},
abstract = {Explores the issues involved in deployment of mobile robots in real-world situations and presents solutions and approaches under development at the Australian National University. For deployment of mobile robots outside of the laboratory, long-term operation is required. Hence, we have developed an automatic recharging system. In addition, a Web-based teleoperation system is used to provide missions to test the long-term reliability of the robot. The final aspect of real-world operation that is explored here is operations in dynamic environments. To date, researchers have assumed static environments for mapping and localisation. We propose methods to avoid this restriction.},
keywords = {active vision;Internet;mobile robots;reliability;robot vision;telerobotics;},
note = {mobile robotics;real-world situations;Australian National University;long-term operation;automatic recharging system;Web-based teleoperation system;long-term reliability;dynamic environments;},
URL = {http://dx.doi.org/10.1109/IROS.2001.976237},
} 


@inproceedings{7832648 ,
language = {English},
copyright = {Copyright 2004, IEE},
title = {Active vision for wearables},
journal = {IEE Eurowearable '03},
author = {Mayol, W.W. and Tordoff, B.J. and de Campos, T.E. and Davison, A.J. and Murray, D.W.},
year = {2004//},
pages = {99 - 104},
address = {London, UK},
abstract = {In this paper we report on our ongoing research on wearable active vision, where we have iteratively prototyped a wearable visual robot - a body mounted robot for which the main sensor is a camera. Two main areas have been studied: robot design and visual algorithms. In the design stage, we have analysed sensor placement through the computation of the field of view and body motion using a 3D model of the human form. A design methodology for the robot morphology was developed with the help of an optimisation algorithm based on the Pareto front. The wearability of the device has progressed over several iterations as have the sensor and control architectures. In terms of visual algorithms, we have studied methods of visual tracking fused with inertial sensors, real-time template tracking, human head pose recovery and more recently real-time simultaneous ego-localisation and autonomous 3D map building. Our main long-term application areas are enhanced remote collaboration and autonomous wearable assistants that use vision.},
keywords = {active vision;mobile computing;Pareto optimisation;real-time systems;robot vision;sensor fusion;software prototyping;tracking;visual programming;wearable computers;},
note = {wearable active vision;iterative prototyping;wearable visual robot;body mounted robot;camera sensor;robot design;visual algorithms;sensor placement;field of view;body motion;3D model;robot morphology;Pareto optimisation;control architectures;visual tracking;inertial sensor fusion;real-time template tracking;human head pose recovery;real-time simultaneous ego-localisation;autonomous 3D map building;enhanced remote collaboration;autonomous wearable assistants;},
} 


@inproceedings{8634390 ,
language = {English},
copyright = {Copyright 2005, IEE},
title = {Use of classification and segmentation of sidescan sonar images for long term registration},
journal = {Oceans 2005 - Europe (IEEE Cat. No. 05EX1042)},
author = {Leblond, I. and Legris, M. and Solaiman, B.},
volume = {Vol. 1},
year = {2005//},
pages = {322 - 7},
address = {Piscataway, NJ, USA},
abstract = {This article handles the possibility of using classification and segmentation of sidescan sonar images for long term registration. In our case, long term registration means to find the displacement between two images which can have been mapped with many weeks or many months between them. The aim of this study is to help AUVs (autonomous underwater vehicles) to navigate, in particular to correct the drift of navigation sensors. This type of positioning raises two sorts of problems, which come from image properties: spatial variability and temporal variability. The first one is caused above all by the sonar geometry and appears for example like a modification of the shape or the position of the shadow according to the point of view. This effect can also be seen in textures, for example on megaripples of sand, which can be more or less visible depending on the point of view of the sonar. The second one is more the consequence of the seafloor physics: between two images, mapped at different times, some elements may have changed. An obvious example is the presence of evanescent "objects" like fishes but this variability can also be seen on sediments, which borders can move due to local bottom dynamics. With the aim to solve these problems and to provide reliable landmarks for matching, we decided to classify and segment the images. The data have first been corrected from TVG (time varying gain) effects and despeckelised in order to process images which are more representative of the seafloor. The basis idea is to use a supervised method of classification. To do that, we consider some parameters which are coming from a decomposition by Gabor filters, in order to segment with linear discriminant analysis and use of the nearest neighbour method. Registration needs accurately localised landmarks: so, this operation is split in several stages, refining step by step the classification, in order to obtain a map which describes the seafloor with the most possible detailed frontiers. Then, we present the obtained results considering five texture classes: rocks, megaripples, sand, mud and shadow. These several areas and their frontiers are the basis landmarks to match the images. However, before using the segmentation, we must check its reliability. So, it appears that the frontiers, though they are realistic, are not accurate enough to make a registration precise to few pixels, especially in rock areas. Similarly, according to the orientation of the ripples, they may be seen as ripples or sand. These observations are due to the directivity of the sonar, which caused these effects on the segmentation. To do registration, we must take into account these problems. So, the results of the segmentation will be used only for a coarse registration, in order to find quickly the area of interest but also to assess the reliability of our registration (matching on ripples areas is a priori less reliable than on rocks areas). The results of registration are shown, proving the good adequacy between reference image and test image. Others methods, more quantitative, will be able to be tested, to refine the results.},
keywords = {image classification;image registration;image segmentation;rocks;sand;sonar imaging;underwater vehicles;},
note = {sidescan sonar image classification;sidescan sonar image segmentation;sonar image registration;AUV;autonomous underwater vehicles;navigation sensor drift correction;image property;spatial variability;temporal variability;sonar geometry;sand megaripples;seafloor physics;evanescent objects;fish;sediments;seabottom dynamics;TVG effects;time varying gain effects;data despeckelised;image processing;supervised method;Gabor filters;linear discriminant analysis;nearest neighbour method;texture class;rocks;mud;shadow position;reference image;test image;},
} 


@inproceedings{7162264 ,
language = {English},
copyright = {Copyright 2002, IEE},
title = {Contribution to vision-based localization, tracking and navigation methods for an interactive mobile service-robot},
journal = {2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)},
author = {Gross, H.-M. and Boehme, H.-J. and Wilhelm, T.},
volume = {vol.2},
year = {2001//},
pages = {672 - 7},
address = {Piscataway, NJ, USA},
abstract = {Presents vision-based robot navigation and user localization techniques of our long-term research project PERSES (personal service system), which aims to develop an interactive mobile shopping assistant that allows a continuous and intuitively understandable interaction with customers in a home store. Against this background, the paper describes a number of new or improved approaches, addressing challenges arising from the characteristics of the operation area, and from the need to continuously interact with users in a complex environment. With our approaches to vision-based or visually-controlled map building, self-localization and navigation as well as user localization and tracking, we want to make a contribution to the real-world suitability of interactive mobile service-robots in non-trivial application areas and demanding human-robot interaction scenarios.},
keywords = {mobile robots;object detection;path planning;robot vision;},
note = {vision-based localization;vision-based tracking;vision-based navigation;interactive mobile service-robot;PERSES;personal service system;interactive mobile shopping assistant;home store;person detection;},
URL = {http://dx.doi.org/10.1109/ICSMC.2001.972991},
} 


@inproceedings{6182595 ,
language = {English},
copyright = {Copyright 1999, IEE},
title = {Incorporating environmental measurements in navigation},
journal = {Proceedings of the 1998 Workshop on Autonomous Underwater Vehicles (Cat. No.98CH36290)},
author = {Feder, H.J.S. and Leonard, J.J. and Smith, C.M.},
year = {1998//},
pages = {115 - 22},
address = {Piscataway, NJ, USA},
abstract = {Extended missions in unknown regions present a significant navigational challenge for autonomous underwater vehicles (AUV). This paper investigates the long-term performance of a concurrent mapping and localization (CML) algorithm for the scenario of an AUV making observations of point features in the environment with a forward look sonar. Simulation results demonstrate that position estimates with long-term bounded errors of a few meters can be achieved under realistic assumptions about the vehicle, its sensors, and the environment. Potential failure modes of the algorithm, such as divergence and map slip, are discussed. CML technology can provide a significant improvement in the navigational capabilities of AUVs and can enable new missions in unmapped regions without reliance on acoustic beacons or surfacing for GPS resets.},
keywords = {computerised navigation;mobile robots;sonar;underwater vehicles;},
note = {environmental measurements;navigation;autonomous underwater vehicles;AUV;mapping;localization;CML;forward look sonar;long-term bounded errors;failure modes;divergence;map slip;acoustic beacons;forward-looking sonar;},
URL = {http://dx.doi.org/10.1109/AUV.1998.744447},
} 


@inproceedings{6771194 ,
language = {English},
copyright = {Copyright 2000, IEE},
title = {PERSES-a vision-based interactive mobile shopping assistant},
journal = {SMC 2000 Conference Proceedings. 2000 IEEE International Conference on Systems, Man and Cybernetics. `Cybernetics Evolving to Systems, Humans, Organizations, and their Complex Interactions' (Cat. No.00CH37166)},
author = {Gross, H.-M. and Boehme, H.-J.},
volume = {vol.1},
year = {2000//},
pages = {80 - 5},
address = {Piscataway, NJ, USA},
abstract = {The paper describes the general idea, the application scenario, and selected methodological approaches of our long term research project PERSES (PERsonal SErvice System). The aim of the project consists of the development of an interactive mobile shopping assistant that allows a continuous and intuitively understandable interaction with a customer in a home improvement store. Typical tasks we have to tackle are to detect and contact potential users in the operation area, to guide them to desired areas or articles within the store or to follow them as a mobile information kiosk while continuously observing their behavior. Due to the specificity of the interaction-oriented scenario and the characteristics of the operation area, we have focused on vision based methods for both human-robot interaction and robot navigation. Besides some methodological approaches, we present preliminary results of experiments achieved with our mobile robot PERSES in the store with an emphasis on vision based methods for user localization, map building and self-localization.},
keywords = {computerised navigation;interactive systems;mobile robots;retail data processing;robot vision;user interfaces;},
note = {PERSES;vision based interactive mobile shopping assistant;application scenario;long term research project;PERsonal SErvice System;intuitively understandable interaction;home improvement store;potential users;operation area;mobile information kiosk;interaction-oriented scenario;vision based methods;human-robot interaction;robot navigation;methodological approaches;mobile robot;user localization;map building;self-localization;},
URL = {http://dx.doi.org/10.1109/ICSMC.2000.884968},
} 


@inproceedings{5680121 ,
language = {English},
copyright = {Copyright 1997, IEE},
title = {Continuous localization in changing environments},
journal = {Proceedings. 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. `Towards New Computational Principles for Robotics and Automation' (Cat. No.97TB100176)},
author = {Graves, K. and Adams, W. and Schultz, A.},
year = {1997//},
pages = {28 - 33},
address = {Los Alamitos, CA, USA},
abstract = {Continuous localization is a technique that allows a robot to maintain an accurate estimate of its location by performing regular small corrections to its odometry. Continuous localization uses an evidence grid representation, a common representation scheme that is used by other map-dependent processes, such as path planning. Although techniques exist for building evidence grid maps, most are not adaptive to changes in the environment. In this research, we extend the continuous localization technique by adding a learning component. This allows continuous localization to update the long-term map (evidence grid) with current sensor readings. Results show that the addition of the learning behavior to continuous localization allows the system to adapt to changes in its environment without a loss in its ability to remain localized. This system was tested on a Nomad 200 mobile robot.},
keywords = {adaptive control;learning (artificial intelligence);mobile robots;path planning;position measurement;},
note = {continuous localization;robot;odometry corrections;evidence grid representation;map-dependent processes;Nomad 200 mobile robot;},
URL = {http://dx.doi.org/10.1109/CIRA.1997.613834},
} 


@inproceedings{3288082 ,
language = {English},
copyright = {Copyright 1989, IEE},
title = {Unusual applications of SLAM in management decision making},
journal = {Simulation in the Factory of the Future and Simulation in Traffic Control. Proceedings of the European Simulation Multiconference},
author = {Prekel, H.L.},
year = {1988//},
pages = {105 - 10},
address = {Ghent, Belgium},
abstract = {SLAM, the Simulation Language for Alternate Modelling, is one of a growing number of popular simulation languages. The increasing power and sophistication of the microcomputer is making these languages accessible to a growing number of users, and has led to the development of affordable software packages. Most of the applications, however, are used to model physical processing type situations, such as production lines, inventory systems, vehicle scheduling, and mining operations. Two entirely different types of problems are analysed with the help of SLAM in this paper. In the first, Topchem, it is shown how SLAM can be used to compare two investment opportunities by generating distributions of possible internal rates of return for each of them, and analysing the results. The second, Manpower, shows how SLAM can be used to analyse the effects of various manpower planning policies on the long term staffing position of a company.},
keywords = {complete computer programs;decision support systems;financial data processing;investment;microcomputer applications;personnel;simulation languages;},
note = {management decision making;SLAM;Simulation Language;microcomputer;software packages;Topchem;investment opportunities;internal rates of return;Manpower;manpower planning policies;long term staffing position;},
} 


@inproceedings{4214157 ,
language = {English},
copyright = {Copyright 1992, IEE},
title = {Simultaneous map building and localization for an autonomous mobile robot},
journal = {Proceedings IROS '91. IEEE/RSJ International Workshop on Intelligent Robots and Systems '91. Intelligence for Mechanical Systems (Cat. No.91TH0375-6)},
author = {Leonard, J.J. and Durrant-Whyte, H.F.},
year = {1991//},
pages = {1442 - 7},
address = {New York, NY, USA},
abstract = {Discusses a significant open problem in mobile robotics: simultaneous map building and localization, which the authors define as long-term globally referenced position estimation without a priori information. This problem is difficult because of the following paradox: to move precisely, a mobile robot must have an accurate environment map; however, to build an accurate map, the mobile robot's sensing locations must be known precisely. In this way, simultaneous map building and localization can be seen to present a question of `which came first, the chicken or the egg?' (The map or the motion?) When using ultrasonic sensing, to overcome this issue the authors equip the vehicle with multiple servo-mounted sonar sensors, to provide a means in which a subset of environment features can be precisely learned from the robot's initial location and subsequently tracked to provide precise positioning.},
keywords = {computerised navigation;mobile robots;planning (artificial intelligence);},
note = {path planning;computerised navigation;map building;localization;autonomous mobile robot;long-term globally referenced position estimation;ultrasonic sensing;multiple servo-mounted sonar sensors;precise positioning;},
URL = {http://dx.doi.org/10.1109/IROS.1991.174711},
} 


@inproceedings{3471189 ,
language = {English},
copyright = {Copyright 1989, IEE},
title = {Characterising an indoor environment with a mobile robot and uncalibrated stereo},
journal = {Proceedings. 1989 IEEE International Conference on Robotics and Automation (Cat. No.89CH2750-8)},
author = {Sarachik, K.B.},
year = {1989//},
pages = {984 - 9},
address = {Washington, DC, USA},
abstract = {The author shows how it is possible for a mobile robot to exploit the visual information obtained by scanning a room to determine its size and shape, and to orient itself continually within it. The equipment used is a very simple camera setup whose detailed initial configuration is not known but can be deduced as the algorithm runs. The approach does not require any special environment, nor is it sensitive to changes in the physical aspect of the room being inspected such as moved furniture or roaming people. The long-term goal of the project is for the robot to use the information thus acquired in order to build maps of its environment, presumed to be a single floor of an office building, and to localize itself within this framework.},
keywords = {computer vision;computerised pattern recognition;computerised picture processing;mobile robots;},
note = {computer vision;unstructured environment;map-building;indoor environment;mobile robot;uncalibrated stereo;room;camera setup;},
URL = {http://dx.doi.org/10.1109/ROBOT.1989.100109},
} 


@inproceedings{19986830 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM},
journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Xuesong Shi and Dongjiang Li and Pengpeng Zhao and Qinbin Tian and Yuxin Tian and Qiwei Long and Chunhao Zhu and Jingwei Song and Fei Qiao and Le Song and Yangquan Guo and Zhigang Wang and Yimin Zhang and Baoxing Qin and Wei Yang and Fangshi Wang and Chan, R.H.M. and Qi She},
year = {2020//},
pages = {3139 - 45},
address = {Piscataway, NJ, USA},
abstract = {Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot's long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.},
keywords = {mobile robots;path planning;pose estimation;robot vision;service robots;SLAM (robots);},
note = {simultaneous localization and mapping;data sequences;robotic autonomy;service robots;real-world indoor scenes;OpenLORIS-Scene datasets;SLAM problems;pose estimation;},
URL = {http://dx.doi.org/10.1109/ICRA40945.2020.9196638},
} 


@inproceedings{20778617 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Long-Term Localization With Time Series Map Prediction for Mobile Robots in Dynamic Environments},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Lisai Wang and Weidong Chen and Jingchuan Wang},
year = {2020//},
pages = {7 pp. - },
address = {Piscataway, NJ, USA},
abstract = {In many applications of mobile robot, the environment is constantly changing. How to use historical information to analysis environmental changes and generate a map corresponding with current environment is important to achieve high-precision localization. Inspired by predictive mechanism of brain, this paper presents a long-term localization approach named ArmMPU (ARMA-based Map Prediction and Update) based on time series modeling and prediction. Autoregressive moving average model (ARMA), a kind of time series modeling method, is employed for environmental map modeling and prediction, then predicted map and filtered observation are fused to fix the prediction error. The simulation and experiment results show that the proposed method improves long-term localization performance in dynamic environments.},
keywords = {autoregressive moving average processes;mobile robots;robot dynamics;time series;},
note = {prediction error;long-term localization performance;dynamic environments;time series Map Prediction;mobile robot;historical information;high-precision localization;long-term localization approach;ARMA-based Map Prediction;average model;time series modeling method;environmental map modeling;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9468884},
} 


@inproceedings{14718001 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Long-term topological localisation for service robots in dynamic environments using spectral maps},
journal = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014)},
author = {Krajnik, T. and Fentanes, J.P. and Mozos, O.M. and Duckett, T. and Ekekrantz, J. and Hanheide, M.},
year = {2014//},
pages = {4537 - 42},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a new approach for topological localisation of service robots in dynamic indoor environments. In contrast to typical localisation approaches that rely mainly on static parts of the environment, our approach makes explicit use of information about changes by learning and modelling the spatio-temporal dynamics of the environment where the robot is acting. The proposed spatio-temporal world model is able to predict environmental changes in time, allowing the robot to improve its localisation capabilities during long-term operations in populated environments. To investigate the proposed approach, we have enabled a mobile robot to autonomously patrol a populated environment over a period of one week while building the proposed model representation. We demonstrate that the experience learned during one week is applicable for topological localization even after a hiatus of three months by showing that the localization error rate is significantly lower compared to static environment representations.},
keywords = {indoor environment;learning (artificial intelligence);mobile robots;service robots;SLAM (robots);spatiotemporal phenomena;},
note = {long-term topological localisation;service robots;spectral maps;dynamic indoor environments;spatiotemporal dynamics learning;spatiotemporal dynamics modelling;environmental change prediction;mobile robot;model representation;localization error rate;},
URL = {http://dx.doi.org/10.1109/IROS.2014.6943205},
} 


@inproceedings{11010225 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Landmark rating and selection according to localization coverage: addressing the challenge of lifelong operation of SLAM in service robots},
journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2009)},
author = {Hochdorfer, S. and Schlegel, C.},
year = {2009//},
pages = {382 - 7},
address = {Piscataway, NJ, USA},
abstract = {Acting in everyday-life environments is still a great challenge in service robotics. Although algorithms and solutions already exist for many relevant subproblems, in particular the aspect of robustness and suitability for everyday use has been neglected so far very often. Robustness and suitability for everyday use are features affecting not only the overall system design but have impact on each single algorithm of each component.},
keywords = {service robots;},
note = {landmark rating;landmark selection;localization coverage;simultaneous localization and mapping;service robotics;P3DX-platform;},
URL = {http://dx.doi.org/10.1109/IROS.2009.5354433},
} 


@inproceedings{21407018 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {A General Framework for Lifelong Localization and Mapping in Changing Environment},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Min Zhao and Xin Guo and Le Song and Baoxing Qin and Xuesong Shi and Gim Hee Lee and Guanghui Sun},
year = {2021//},
pages = {3305 - 12},
address = {Piscataway, NJ, USA},
abstract = {The environment of most real-world scenarios such as malls and supermarkets changes at all times. A pre-built map that does not account for these changes becomes out-of-date easily. Therefore, it is necessary to have an up-to-date model of the environment to facilitate long-term operation of a robot. To this end, this paper presents a general lifelong simultaneous localization and mapping (SLAM) framework. Our framework uses a multiple session map representation, and exploits an efficient map updating strategy that includes map building, pose graph refinement and sparsification. To mitigate the unbounded increase of memory usage, we propose a map-trimming method based on the Chow-Liu maximum-mutual-information spanning tree. The proposed SLAM framework has been comprehensively validated by over a month of robot deployment in real supermarket environment. Furthermore, we release the dataset collected from the indoor and outdoor changing environment with the hope to accelerate lifelong SLAM research in the community. Our dataset is available at https://github.com/sanduan168/lifelong-SLAM-dataset.},
keywords = {graph theory;mobile robots;path planning;robot vision;SLAM (robots);trees (mathematics);},
note = {graph refinement;sparsification;unbounded increase;memory usage;map-trimming method;Chow-Liu maximum-mutual-information spanning tree;SLAM framework;robot deployment;supermarket environment;indoor changing environment;outdoor changing environment;SLAM research;lifelong localization;malls;supermarkets changes;pre-built map;out-of-date;up-to-date model;long-term operation;general lifelong simultaneous localization;multiple session map representation;efficient map updating strategy;map building;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9635985},
} 


@inproceedings{10363992 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {An adaptive appearance-based map for long-term topological localization of mobile robots},
journal = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
author = {Dayoub, F. and Duckett, T.},
year = {2008//},
pages = {3364 - 9},
address = {Piscataway, NJ, USA},
abstract = {This work considers a mobile service robot which uses an appearance-based representation of its workplace as a map, where the current view and the map are used to estimate the current position in the environment. Due to the nature of real-world environments such as houses and offices, where the appearance keeps changing, the internal representation may become out of date after some time. To solve this problem the robot needs to be able to adapt its internal representation continually to the changes in the environment. This paper presents a method for creating an adaptive map for long-term appearance-based localization of a mobile robot using long-term and short-term memory concepts, with omni-directional vision as the external sensor.},
keywords = {mobile robots;path planning;robot vision;service robots;},
note = {adaptive appearance-based map;long-term topological localization;mobile service robots;current position estimation;omni-directional vision;},
URL = {http://dx.doi.org/10.1109/IROS.2008.4650701},
} 


@inproceedings{19078696 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Lifelong Mapping using Adaptive Local Maps},
journal = {2019 European Conference on Mobile Robots (ECMR). Proceedings},
author = {Banerjee, N. and Lisin, D. and Briggs, J. and Llofriu, M. and Munich, M.E.},
year = {2019//},
pages = {8 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Occupancy mapping enables a mobile robot to make intelligent planning decisions to accomplish its tasks. Adaptive local maps is an algorithm which represents the occupancy information as a set of overlapping local maps anchored to poses in the robot's trajectory. At any time, a global occupancy map can be rendered from the local maps to be used for path planning. The advantage of this approach is that the occupancy information stays consistent despite the changes in the pose estimates resulting from loop closures and localization updates. The disadvantage, however, is that the number of local maps grows over time. For long robot runs, or for multiple runs in the same space, this growth will result in redundant occupancy information, which will in turn increase the time it takes to render the global map, as well as the memory footprint of the system. In this paper, we propose a novel approach for the maintenance of an adaptive local maps system, which intelligently prunes redundant local maps, ensuring the robustness and stability required for lifelong mapping.},
keywords = {mobile robots;path planning;pose estimation;SLAM (robots);},
note = {lifelong mapping;occupancy mapping;mobile robot;intelligent planning decisions;overlapping local maps;global occupancy map;localization updates;long robot runs;redundant occupancy information;adaptive local maps system;redundant local maps;robot trajectory;},
URL = {http://dx.doi.org/10.1109/ECMR.2019.8870347},
} 


@inproceedings{16503801 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Persistent localization and life-long mapping in changing environments using the Frequency Map Enhancement},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Krajnik, T. and Pulido Fentanes, J. and Hanheide, M. and Duckett, T.},
year = {2016//},
pages = {4558 - 63},
address = {Piscataway, NJ, USA},
abstract = {We present a lifelong mapping and localisation system for long-term autonomous operation of mobile robots in changing environments. The core of the system is a spatio-temporal occupancy grid that explicitly represents the persistence and periodicity of the individual cells and can predict the probability of their occupancy in the future. During navigation, our robot builds temporally local maps and integrates then into the global spatio-temporal grid. Through re-observation of the same locations, the spatio-temporal grid learns the long-term environment dynamics and gains the ability to predict the future environment states. This predictive ability allows to generate time-specific 2d maps used by the robot's localisation and planning modules. By analysing data from a long-term deployment of the robot in a human-populated environment, we show that the proposed representation improves localisation accuracy and the efficiency of path planning. We also show how to integrate the method into the ROS navigation stack for use by other roboticists.},
keywords = {mobile robots;navigation;operating systems (computers);path planning;robot dynamics;robot vision;SLAM (robots);},
note = {frequency map enhancement;lifelong mapping;localisation system;long-term autonomous operation;mobile robots;spatiotemporal occupancy grid;long-term environment dynamics;robot localisation;path planning;ROS navigation;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759671},
} 


@inproceedings{16055928 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Towards lifelong feature-based mapping in semi-static environments},
journal = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Rosen, D.M. and Mason, J. and Leonard, J.J.},
year = {2016//},
pages = {1063 - 70},
address = {Piscataway, NJ, USA},
abstract = {The feature-based graphical approach to robotic mapping provides a representationally rich and computationally efficient framework for an autonomous agent to learn a model of its environment. However, this formulation does not naturally support long-term autonomy because it lacks a notion of environmental change; in reality, &ldquo;everything changes and nothing stands still, &rdquo; and any mapping and localization system that aims to support truly persistent autonomy must be similarly adaptive. To that end, in this paper we propose a novel feature-based model of environmental evolution over time. Our approach is based upon the development of an expressive probabilistic generative feature persistence model that describes the survival of abstract semi-static environmental features over time. We show that this model admits a recursive Bayesian estimator, the persistence filter, that provides an exact online method for computing, at each moment in time, an explicit Bayesian belief over the persistence of each feature in the environment. By incorporating this feature persistence estimation into current state-of-the-art graphical mapping techniques, we obtain a flexible, computationally efficient, and information-theoretically rigorous framework for lifelong environmental modeling in an ever-changing world.},
keywords = {adaptive systems;Bayes methods;recursive estimation;robot vision;SLAM (robots);},
note = {lifelong feature-based mapping;feature-based graphical approach;robotic mapping;autonomous agent;localization system;adaptive system;expressive probabilistic generative feature persistence model;abstract semistatic environmental features;recursive Bayesian estimator;persistence filter;explicit Bayesian belief;feature persistence estimation;graphical mapping;lifelong environmental modeling;},
URL = {http://dx.doi.org/10.1109/ICRA.2016.7487237},
} 


@inproceedings{21487524 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Robust SLAM Systems: Are We There Yet?},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Bujanca, M. and Xuesong Shi and Spear, M. and Pengpeng Zhao and Lennox, B. and Lujan, M.},
year = {2021//},
pages = {5320 - 7},
address = {Piscataway, NJ, USA},
abstract = {Progress in the last decade has brought about significant improvements in the accuracy and speed of SLAM systems, broadening their mapping capabilities. Despite these advancements, long-term operation remains a major challenge, primarily due to the wide spectrum of perturbations robotic systems may encounter.Increasing the robustness of SLAM algorithms is an ongoing effort, however it usually addresses a specific perturbation. Generalisation of robustness across a large variety of challenging scenarios is not well-studied nor understood. This paper presents a systematic evaluation of the robustness of open-source state-of-the-art SLAM algorithms with respect to challenging conditions such as fast motion, non-uniform illumination, and dynamic scenes. The experiments are performed with perturbations present both independently of each other, as well as in combination in long-term deployment settings in unconstrained environments (lifelong operation).The detailed results (approx. 20,000 experiments) along with comprehensive documentation of the benchmarking tool for integrating new datasets and evaluating SLAM algorithms not studied in this work are available at https://robustslam.github.io/evaluation.},
keywords = {mobile robots;robot vision;SLAM (robots);},
note = {long-term deployment settings;robust SLAM systems;mapping capabilities;perturbations robotic systems;open-source SLAM algorithms;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9636814},
} 


@inproceedings{21504184 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Geometry-based Graph Pruning for Lifelong SLAM},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Kurz, G. and Holoch, M. and Biber, P.},
year = {2021//},
pages = {3313 - 20},
address = {Piscataway, NJ, USA},
abstract = {Lifelong SLAM considers long-term operation of a robot where already mapped locations are revisited many times in changing environments. As a result, traditional graph-based SLAM approaches eventually become extremely slow due to the continuous growth of the graph and the loss of sparsity. Both problems can be addressed by a graph pruning algorithm. It carefully removes vertices and edges to keep the graph size reasonable while preserving the information needed to provide good SLAM results. We propose a novel method that considers geometric criteria for choosing the vertices to be pruned. It is efficient, easy to implement, and leads to a graph with evenly spread vertices that remain part of the robot trajectory. Furthermore, we present a novel approach of marginalization that is more robust to wrong loop closures than existing methods. The proposed algorithm is evaluated on two publicly available real-world long-term datasets and compared to the unpruned case as well as ground truth. We show that even on a long dataset (25h), our approach manages to keep the graph sparse and the speed high while still providing good accuracy (40 times speed up, 6cm map error compared to unpruned case).},
keywords = {geometry;graph theory;SLAM (robots);trajectory control;},
note = {mapped locations;robot trajectory;real-world long-term datasets;geometry-based graph pruning;lifelong SLAM;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9636530},
} 


@article{21101958 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {A Life-Long SLAM Approach Using Adaptable Local Maps Based on Rasterized LIDAR Images},
journal = {IEEE Sensors Journal},
journal = {IEEE Sens. J. (USA)},
author = {Ali, W. and Peilin Liu and Rendong Ying and Zheng Gong},
volume = { 21},
number = { 19},
year = {2021//},
pages = {21740 - 9},
issn = {1530-437X},
address = {USA},
abstract = {Most real-time autonomous robot applications require a robot to traverse through a dynamic space for a long time. In some cases, a robot needs to work in the same environment. Such applications give rise to the problem of a life-long SLAM system. Life-long SLAM presents two main challenges i.e. the tracking should not fail in a dynamic environment and the need for a robust and efficient mapping strategy. The system should update maps with new information; while also keeping track of older observations. But, mapping for a long time can require higher computational requirements. In this paper, we propose a solution to the problem of life-long SLAM. We represent the global map as a set of rasterized images of local maps along with a map management system responsible for updating local maps and keeping track of older values. We also present an efficient approach of using the bag of visual words method for loop closure detection and relocalization. We evaluate the performance of our system on the KITTI dataset and an indoor dataset. Our loop closure system reported recall and precision of above 90 percent. The computational cost of our system is much lower as compared to state-of-the-art methods. Our method reports lower computational requirements even for long-term operation.},
keywords = {mobile robots;object detection;optical radar;path planning;robot vision;SLAM (robots);},
note = {life-long SLAM system;dynamic environment;robust mapping strategy;efficient mapping strategy;computational requirements;rasterized images;map management system;loop closure system;lower computational requirements;long-term operation;adaptable local maps;rasterized LIDAR;real-time autonomous robot applications;dynamic space;},
URL = {http://dx.doi.org/10.1109/JSEN.2021.3100882},
} 


@article{21589128 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Topographic SLAM Using a Single Terrain Altimeter in GNSS-Restricted Environment},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Junwoo Jang and Jinwhan Kim},
volume = { 10},
year = {2022//},
pages = {10806 - 10815},
issn = {2169-3536},
address = {USA},
abstract = {In a Global Navigation Satellite System (GNSS)-restricted area, a mobile robot navigation system exploits surrounding environment information. For an aerial or underwater vehicle, undulating terrain of a land or seabed surface is a valuable information resource that leads to the development of terrain-referenced navigation (TRN) algorithms. However, due to the vast amount of a vehicle's activity area, surveying all the regions to obtain a high-resolution terrain map is impractical and requires simultaneous localization and mapping (SLAM) as a highly desirable capability. This paper presents a topographic SLAM algorithm using only a single terrain altimeter, which is low-cost, computationally efficient, and sufficiently stable for long-term operation. The proposed rectangular panel map structure and update method enable robust and efficient SLAM. As terrain elevation changes are inherently nonlinear, an extended Kalman filter (EKF)-based SLAM filter is adopted. The feasibility and validity of the proposed algorithm are demonstrated through simulations using terrain elevation data from a real-world undersea environment.},
keywords = {altimeters;Kalman filters;mobile robots;navigation;nonlinear filters;satellite navigation;SLAM (robots);telecommunication control;terrain mapping;},
note = {single terrain altimeter;GNSS-restricted environment;mobile robot navigation system;environment information;aerial vehicle;underwater vehicle;undulating terrain;valuable information resource;terrain-referenced navigation algorithms;high-resolution terrain map;topographic SLAM algorithm;rectangular panel map structure;terrain elevation changes;extended Kalman filter-based SLAM filter;terrain elevation data;real-world undersea environment;global navigation satellite system-restricted area;TRN algorithms;vehicle activity area;simultaneous localization and mapping;EKF-based SLAM filter;},
URL = {http://dx.doi.org/10.1109/ACCESS.2022.3145978},
} 


@article{16299206 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Summary Maps for Lifelong Visual Localization},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Muhlfellner, P. and Burki, M. and Bosse, M. and Derendarz, W. and Philippsen, R. and Furgale, P.},
volume = { 33},
number = { 5},
year = {2016/08/},
pages = {561 - 90},
issn = {1556-4959},
address = {USA},
abstract = {Robots that use vision for localization need to handle environments that are subject to seasonal and structural change, and operate under changing lighting and weather conditions. We present a framework for lifelong localization and mapping designed to provide robust and metrically accurate online localization in these kinds of changing environments. Our system iterates between offline map building, map summary, and online localization. The offline mapping fuses data from multiple visually varied datasets, thus dealing with changing environments by incorporating new information. Before passing these data to the online localization system, the map is summarized, selecting only the landmarks that are deemed useful for localization. This <i>Summary Map</i> enables online localization that is accurate and robust to the variation of visual information in natural environments while still being computationally efficient. We present a number of summary policies for selecting useful features for localization from the multisession map, and we explore the tradeoff between localization performance and computational complexity. The system is evaluated on 77 recordings, with a total length of 30 kilometers, collected outdoors over 16 months. These datasets cover all seasons, various times of day, and changing weather such as sunshine, rain, fog, and snow. We show that it is possible to build consistent maps that span data collected over an entire year, and cover day-to-night transitions. Simple statistics computed on landmark observations are enough to produce a Summary Map that enables robust and accurate localization over a wide range of seasonal, lighting, and weather conditions.},
keywords = {robot vision;robust control;},
note = {summary maps;lifelong visual localization;robot vision;handle environments;weather conditions;lighting conditions;changing environments;online localization;offline map building;map summary;multiple visually varied datasets;natural environments;visual information;},
URL = {http://dx.doi.org/10.1002/rob.21595},
} 


@inproceedings{19298535 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Long-Term Visual Inertial SLAM based on Time Series Map Prediction},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Bowen Song and Weidong Chen and Jingchuan Wang and Hesheng Wang},
year = {2019//},
pages = {5364 - 9},
address = {Piscataway, NJ, USA},
abstract = {With the advance in the field of mobile robots, autonomous robots are required for long-term deployment in dynamic and complex environments. However, the performance of Visual Inertial SLAM systems in long-term operation is not satisfactory, and most long-term SLAM systems assumes periodic changes in the environment. This paper presents a novel solution for long-term monocular VI SLAM system in dynamic environment based on autoregression(AR) modeling and map prediction. Map points are first classified into static and semi-static map points according to a memory model. Modeling and prediction of the different states of semi-static map points are performed that are derived from time series models. The predicted map is then fused with the current map to achieve a better forecast for the next frame if the prediction is not satisfactory enough. Experiments are carried out on an embedded system. The results indicate that the map prediction is reliable and the proposed approach improves the performance of long-term localization and mapping in dynamic environments.},
keywords = {mobile robots;regression analysis;robot vision;SLAM (robots);},
note = {long-term monocular VI SLAM system;dynamic environment;semistatic map points;memory model;time series models;embedded system;visual inertial SLAM;time series map prediction;mobile robots;autonomous robots;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8968017},
} 


@inproceedings{20199512 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Lightweight SLAM with automatic orientation correction using 2D LiDAR scans},
journal = {2020 23rd International Symposium on Measurement and Control in Robotics (ISMCR)},
author = {Peter, G. and Kiss, B.},
year = {2020//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Simultaneous localization and mapping (SLAM) is about consistent maps in the long run. Loop closing is the most popular way for ensure long-term consistency in presence of multiple measurements by the same or multiple robots. Loop closure can be executed using raw odometrical data, but a more sophisticated, yet still light-weight method is presented in this paper: a landmark descriptor-based relative displacement calculation method for diminishing unwanted orientation errors that otherwise often lead to map inconsistency. Landmark descriptors are created using light detection and ranging (LiDAR) scans and the relation is calculated using scan-matching. The novelty of this research is a method providing long-term orientation and position correction without additional overhead between landmark detections, thus enabling simple agents to do the SLAM in a cooperative way.},
keywords = {feature extraction;mobile robots;optical radar;path planning;robot vision;SLAM (robots);},
note = {scan-matching;long-term orientation;position correction;landmark detections;lightweight SLAM;automatic orientation correction;2D LiDAR scans;consistent maps;loop closing;long-term consistency;same robots;multiple robots;loop closure;raw odometrical data;light-weight method;landmark descriptor-based relative displacement calculation method;unwanted orientation errors;map inconsistency;landmark descriptors;light detection;},
URL = {http://dx.doi.org/10.1109/ISMCR51255.2020.9263722},
} 


@article{16712506 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {SRAL: shared representative appearance learning for long-term visual place recognition},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Fei Han and Xue Yang and Yiming Deng and Rentschler, M. and Dejun Yang and Hao Zhang},
volume = { 2},
number = { 2},
year = {2017/04/},
pages = {1172 - 9},
issn = {2377-3774},
address = {USA},
abstract = {Place recognition, or loop closure detection, is an essential component to address the problem of visual simultaneous localization and mapping (SLAM). Long-term navigation of robots in outdoor environments introduces new challenges to enable life-long SLAM, including the strong appearance change resulting from vegetation, weather, and illumination variations across various times of the day, different days, months, or even seasons. In this paper, we propose a new shared representative appearance learning (SRAL) approach to address long-term visual place recognition. Different from previous methods using a single feature modality or a concatenation of multiple features, our SRAL method autonomously learns representative features that are shared in all scene scenarios, and then fuses the features together to represent the long-term appearance of environments observed by a robot during life-long navigation. By formulating SRAL as a regularized optimization problem, we use structured sparsity-inducing norms to model interrelationships of feature modalities. In addition, an optimization algorithm is developed to efficiently solve the formulated optimization problem, which holds a theoretical convergence guarantee. Extensive empirical study was performed to evaluate the SRAL method using large-scale benchmark datasets, including St Lucia, CMU-VL, and Nordland datasets. Experimental results have shown that our SRAL method obtains superior performance for life-long place recognition using individual images, outperforms previous single image-based methods, and is capable of estimating the importance of feature modalities.},
keywords = {feature extraction;image fusion;learning (artificial intelligence);mobile robots;object detection;object recognition;optimisation;path planning;robot vision;SLAM (robots);},
note = {large-scale benchmark datasets;Nordland dataset;CMU-VL dataset;St Lucia dataset;feature modalities;structured sparsity-inducing norms;regularized optimization problem;feature fusion;illumination variation;weather variation;vegetation variation;long-term robot navigation;visual SLAM;visual simultaneous localization-and-mapping;loop closure detection;long-term visual place recognition;shared representative appearance learning;SRAL;},
URL = {http://dx.doi.org/10.1109/LRA.2017.2662061},
} 


@inproceedings{20689891 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Map Updating Revisited for Navigation Map : A mathematical way to perform map updating for autonomous mobile robot},
journal = {2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)},
author = {Hongjian Chen and Zhiqiang Wang and Qing Zhu},
year = {2021//},
pages = {505 - 8},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous localization and mapping, SLAM can product a Map for autonomous robots and self-driving vehicle in navigation. In the actual environment, the scene changes frequently, which makes the old map no long reliable. Therefore, it is necessary to update such a map by an efficient and safely way. In this paper, we review the existing map updating, long-term localization methods and discuss about the challenges in this situation. We present a Map updating method in mathematical way which can update accurately. Our proposed method are tested in five indoor dataset and demonstrated feasibility.},
keywords = {mobile robots;path planning;robot vision;SLAM (robots);},
note = {navigation map;autonomous mobile robot;self-driving vehicle;long-term localization methods;map updating method;simultaneous localization and mapping;SLAM;},
URL = {http://dx.doi.org/10.1109/IPEC51340.2021.9421197},
} 


@article{21054716 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Loop-Closure Detection With a Multiresolution Point Cloud Histogram Mode in Lidar Odometry and Mapping for Intelligent Vehicles},
journal = {IEEE/ASME Transactions on Mechatronics},
journal = {IEEE/ASME Trans. Mechatron. (USA)},
author = {Qingyu Meng and Hongyan Guo and Xiaoming Zhao and Dongpu Cao and Hong Chen},
volume = { 26},
number = { 3},
year = {2021//},
pages = {1307 - 17},
issn = {1083-4435},
address = {USA},
abstract = {Precise positioning is the basic condition for intelligent vehicles to complete perception, decision making and control tasks. In response to this challenge, in this article, lidar simultaneous localization and mapping (SLAM) is taken as the research object, and a SLAM system is designed that integrates motion compensation and ground information removal functions, and can construct a real-time environment map and determine its own position on the map while the vehicle is driving. A loop-closure detection method with a multiresolution point cloud histogram mode is proposed, which can effectively detect whether the vehicle passes through the same position and perform optimization to obtain globally consistent pose and map information in the urban conditions with more driving loops. We conduct experiments on the well-known KITTI dataset and compare the results with those of state-of-the-art systems. The experiments confirm that the lidar SLAM system designed in this article can provide accurate and effective positioning information for intelligent vehicles. The proposed loop-closure detection algorithm has an excellent real-time performance and accuracy, which can guarantee the long-term driving operation of these vehicles.},
keywords = {distance measurement;mobile robots;motion compensation;object detection;optical radar;position control;robot vision;SLAM (robots);},
note = {control tasks;lidar simultaneous localization;integrates motion compensation;ground information removal functions;real-time environment map;loop-closure detection method;multiresolution point cloud histogram mode;map information;urban conditions;driving loops;lidar SLAM system;accurate positioning information;effective positioning information;intelligent vehicles;loop-closure detection algorithm;lidar odometry;precise positioning;basic condition;decision making;},
URL = {http://dx.doi.org/10.1109/TMECH.2021.3062647},
} 


@article{21203267 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Simultaneous Localization and Mapping for Inspection Robots in Water and Sewer Pipe Networks: A Review},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Aitken, J.M. and Evans, M.H. and Worley, R. and Edwards, S. and Rui Zhang and Dodd, T. and Mihaylova, L. and Anderson, S.R.},
volume = { 9},
year = {2021//},
pages = {140173 - 140198},
issn = {2169-3536},
address = {USA},
abstract = {At the present time, water and sewer pipe networks are predominantly inspected manually. In the near future, smart cities will perform intelligent autonomous monitoring of buried pipe networks, using teams of small robots. These robots, equipped with all necessary computational facilities and sensors (optical, acoustic, inertial, thermal, pressure and others) will be able to inspect pipes whilst navigating, self-localising and communicating information about the pipe condition and faults such as leaks or blockages to human operators for monitoring and decision support. The predominantly manual inspection of pipe networks will be replaced with teams of autonomous inspection robots that can operate for long periods of time over a large spatial scale. Reliable autonomous navigation and reporting of faults at this scale requires effective localization and mapping, which is the estimation of the robot's position and its surrounding environment. This survey presents an overview of state-of-the-art works on robot simultaneous localization and mapping (SLAM) with a focus on water and sewer pipe networks. It considers various aspects of the SLAM problem in pipes, from the motivation, to the water industry requirements, modern SLAM methods, map-types and sensors suited to pipes. Future challenges such as robustness for long term robot operation in pipes are discussed, including how making use of prior knowledge, e.g. geographic information systems (GIS) can be used to build map estimates, and improve multi-robot SLAM in the pipe environment.},
keywords = {inspection;mobile robots;multi-robot systems;navigation;path planning;pipes;robot vision;SLAM (robots);},
note = {map-types and sensors;water industry requirements;water pipe networks;multirobot SLAM;long term robot operation;robot simultaneous localization and mapping;autonomous inspection robots;predominantly manual inspection;intelligent autonomous monitoring;sewer pipe networks;},
URL = {http://dx.doi.org/10.1109/ACCESS.2021.3115981},
} 


@inproceedings{20256982 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Segmented Matching Method of Multi-Geophysics Field SLAM Data Based on LSTM},
journal = {2020 3rd International Conference on Unmanned Systems (ICUS)},
author = {ZiYuan Li and HuaPeng Yu and TongSheng Shen and ZhiHui Li},
year = {2020//},
pages = {147 - 51},
address = {Piscataway, NJ, USA},
abstract = {At present, simultaneous localization and mapping (SLAM) has become an important method for autonomous underwater vehicles (AUVs) to realize long-term navigation. However, using only bathymetric data in unknown environment has its own disadvantages, that are low precision and large computational load. To tackle with requirements of high-precision navigation under large-scale and long-term voyage condition, a SLAM method and corresponding matching algorithm for integrating multi-geophysical field data are proposed. By dividing the feature data and location data of geophysical field obtained into various submaps and sub-segments during AUV sailing, the dominant navigation data of each segment is identified using long short-term memory network. Validity of the proposed method is done by simulation experiments. During the simulation, the loop closure detection of each submap is used, and the matching counter is set to check the correct matching rate. Finally, the matching results with single geophysics field data under the same conditions are compared with multi-geophysics field data and analyzed. The experimental results have demonstrated the feasibility and correctness of the proposed method.},
keywords = {autonomous underwater vehicles;geophysics computing;image matching;mobile robots;path planning;recurrent neural nets;robot vision;SLAM (robots);},
note = {short-term memory network;matching counter;single geophysics field data;segmented matching method;multigeophysics field SLAM data;autonomous underwater vehicles;long-term navigation;bathymetric data;high-precision navigation;long-term voyage condition;dominant navigation data;LSTM;simultaneous localization and mapping;loop closure detection;},
URL = {http://dx.doi.org/10.1109/ICUS50048.2020.9274964},
} 


@inproceedings{19113227 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {On the Redundancy Detection in Keyframe-Based SLAM},
journal = {2019 International Conference on 3D Vision (3DV)},
author = {Schmuck, P. and Chli, M.},
year = {2019//},
pages = {594 - 603},
address = {Piscataway, NJ, USA},
abstract = {Egomotion and scene estimation is a key component in automating robot navigation, as well as in virtual reality applications for mobile phones or head-mounted displays. It is well known, however, that with long exploratory trajectories and multi-session mapping for long-term autonomy or collaborative applications, the maintenance of the ever-increasing size of these maps quickly becomes a bottleneck. With the explosion of data resulting in increasing runtime of the optimization algorithms ensuring the accuracy of the Simultaneous Localization And Mapping (SLAM) estimates, the large quantity of collected experiences is imposing hard limits on the scalability of such techniques. Considering the keyframe-based paradigm of SLAM techniques, this paper investigates the redundancy inherent in SLAM maps, by quantifying the information of different experiences of the scene as encoded in keyframes. Here we propose and evaluate different information-theoretic and heuristic metrics to remove dispensable scene measurements with minimal impact on the accuracy of the SLAM estimates. Evaluating the proposed metrics in two state-of-the-art centralized collaborative SLAM systems, we provide our key insights into how to identify redundancy in keyframe-based SLAM.},
keywords = {graph theory;information theory;mobile robots;optimisation;robot vision;SLAM (robots);virtual reality;},
note = {virtual reality;mobile phones;head-mounted displays;optimization algorithms;scene measurements;redundancy detection;automating robot navigation;collaborative SLAM systems;information-theoretic;simultaneous localization and mapping;},
URL = {http://dx.doi.org/10.1109/3DV.2019.00071},
} 


@article{14744624 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Lifelong localization in changing environments},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Tipaldi, G.D. and Meyer-Delius, D. and Burgard, W.},
volume = { 32},
number = { 14},
year = {2013/12/},
pages = {1662 - 78},
issn = {0278-3649},
address = {UK},
abstract = {Robot localization systems typically assume that the environment is static, ignoring the dynamics inherent in most realworld settings. Corresponding scenarios include households, offices, warehouses and parking lots, where the location of certain objects such as goods, furniture or cars can change over time. These changes typically lead to inconsistent observations with respect to previously learned maps and thus decrease the localization accuracy or even prevent the robot from globally localizing itself. In this paper we present a soundprobabilistic approach to lifelong localization in changing environments using a combination ofa Rao-Blackwellizedparticlefilter with a hidden Markov model. By exploiting several properties of this model, we obtain a highly efficient map management approach for dynamic environments, which makes it feasible to run our algorithm online. Extensive experiments with a real robot in a dynamically changing environment demonstrate that our algorithm reliably adapts to changes in the environment and also outperforms the popular MonteCarlo localization approach.},
keywords = {mobile robots;Monte Carlo methods;particle filtering (numerical methods);},
note = {lifelong localization;changing environments;robot localization systems;realworld settings;learned maps;sound probabilistic approach;Rao-Blackwellized particle filter;hidden Markov model;dynamic environments;MonteCarlo localization approach;mobile robots;},
URL = {http://dx.doi.org/10.1177/0278364913502830},
} 


@inproceedings{18976340 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Robust Loop-Closure Detection with a Learned Illumination Invariant Representation for Robot vSLAM},
journal = {2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM)},
author = {Shilang Chen and Junjun Wu and Yanran Wang and Lin Zhou and Qinghua Lu and Yunzhi Zhang},
year = {2019//},
pages = {342 - 7},
address = {Piscataway, NJ, USA},
abstract = {Robust loop-closure detection plays a key role for the long-term robot visual Simultaneous Localization and Mapping (SLAM) in indoor or outdoor environment, due to illumination changes can greatly affect the accuracy of online image matching, and keypoints may fail to match between images taken at the same location but different seasons. In this paper, we propose a robust loop-closure detection method for robot visual SLAM, which adopts invariant representation as image descriptors composed of learned features and adapts to changes in illumination and seasons. We evaluate our method on real datasets and demonstrate its excellent ability to handle illumination changes.},
keywords = {feature extraction;image matching;image representation;robot vision;robust control;SLAM (robots);},
note = {learned illumination invariant representation;illumination changes;online image matching;robust loop-closure detection method;robot visual SLAM;robot vSLAM;long-term robot visual simultaneous localization;},
URL = {http://dx.doi.org/10.1109/ICARM.2019.8833730},
} 


@inproceedings{11046421 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Lifelong localization of a mobile service-robot in everyday indoor environments using omnidirectional vision},
journal = {2009 IEEE International Conference on Technologies for Practical Robot Applications. TePRA 2009},
author = {Hochdorfer, S. and Lutz, M. and Schlegel, C.},
year = {2009//},
pages = {161 - 6},
address = {Piscataway, NJ, USA},
abstract = {SLAM (Simultaneous Localization and Mapping) mechanisms are a key component towards advanced service robotics applications. Currently, a major hurdle on the way to lifelong localization is the handling of the ever growing amount of landmarks over time. Therefore, the required resources in terms of memory and processing power are also growing over time.},
keywords = {mobile robots;pattern clustering;service robots;SLAM (robots);},
note = {lifelong localization;mobile service-robot;indoor environments;omnidirectional vision;simultaneous localization and mapping;processing power;memory resources;},
URL = {http://dx.doi.org/10.1109/TEPRA.2009.5339626},
} 


@inproceedings{21257530 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Lifelong Localization in Semi-Dynamic Environment},
journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Shifan Zhu and Xinyu Zhang and Shichun Guo and Jun Li and Huaping Liu},
year = {2021//},
pages = {14389 - 14395},
address = {Piscataway, NJ, USA},
abstract = {Mapping and localization in non-static environments are fundamental problems in robotics. Most of previous methods mainly focus on static and highly dynamic objects in the environment, which may suffer from localization failure in semi-dynamic scenarios without considering objects with lower dynamics, such as parked cars and stopped pedestrians. In this paper, we introduce semantic mapping and lifelong localization approaches to recognize semi-dynamic objects in non-static environments. We also propose a generic framework that can integrate mainstream object detection algorithms with mapping and localization algorithms. The mapping method combines an object detection algorithm and a SLAM algorithm to detect semi-dynamic objects and constructs a semantic map that only contains semi-dynamic objects in the environment. During navigation, the localization method can classify observation corresponding to static and non-static objects respectively and evaluate whether those semi-dynamic objects have moved, to reduce the weight of invalid observation and localization fluctuation. Real-world experiments show that the proposed method can improve the localization accuracy of mobile robots in non-static scenarios.},
keywords = {mobile robots;object detection;robot vision;SLAM (robots);},
note = {lifelong localization;semidynamic environment;nonstatic environments;static objects;highly dynamic objects;localization failure;semidynamic scenarios;lower dynamics;semantic mapping;semidynamic objects;mainstream object detection algorithms;localization algorithms;object detection algorithm;semantic map;localization method;nonstatic objects;invalid observation;localization fluctuation;nonstatic scenarios;},
URL = {http://dx.doi.org/10.1109/ICRA48506.2021.9561584},
} 


@article{20176309 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Augmenting visual SLAM with Wi-Fi sensing for indoor applications},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Hashemifar, Z.S. and Adhivarahan, C. and Balakrishnan, A. and Dantu, K.},
volume = { 43},
number = { 8},
year = {2019/12/},
pages = {2245 - 60},
issn = {0929-5593},
address = {Germany},
abstract = {Recent trends have accelerated the development of spatial applications on mobile devices and robots. These include navigation, augmented reality, human-robot interaction, and others. A key enabling technology for such applications is the understanding of the device's location and the map of the surrounding environment. This generic problem, referred to as Simultaneous Localization and Mapping (SLAM), is an extensively researched topic in robotics. However, visual SLAM algorithms face several challenges including perceptual aliasing and high computational cost. These challenges affect the accuracy, efficiency, and viability of visual SLAM algorithms, especially for long-term SLAM, and their use in resource-constrained mobile devices. A parallel trend is the ubiquity of Wi-Fi routers for quick Internet access in most urban environments. Most robots and mobile devices are equipped with a Wi-Fi radio as well. We propose a method to utilize Wi-Fi received signal strength to alleviate the challenges faced by visual SLAM algorithms. To demonstrate the utility of this idea, this work makes the following contributions: (i) We propose a generic way to integrate Wi-Fi sensing into visual SLAM algorithms, (ii) We integrate such sensing into three well-known SLAM algorithms, (iii) Using four distinct datasets, we demonstrate the performance of such augmentation in comparison to the original visual algorithms and (iv) We compare our work to Wi-Fi augmented FABMAP algorithm. Overall, we show that our approach can improve the accuracy of visual SLAM algorithms by 11% on average and reduce computation time on average by 15% to 25%.},
keywords = {augmented reality;indoor radio;Internet;mobile computing;mobile radio;mobile robots;path planning;robot vision;RSSI;SLAM (robots);telecommunication network routing;visual perception;wireless LAN;},
note = {Wi-Fi sensing;robotics;visual SLAM algorithms;resource-constrained mobile devices;Wi-Fi routers;Wi-Fi radio;indoor applications;simultaneous localization and mapping;perceptual aliasing;Wi-Fi received signal strength;Wi-Fi augmented FABMAP algorithm;},
URL = {http://dx.doi.org/10.1007/s10514-019-09874-z},
} 


@article{18803794 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Augmenting Visual SLAM with Wi-Fi Sensing For Indoor Applications [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Hashemifar, Z.S. and Adhivarahan, C. and Balakrishnan, A. and Dantu, K.},
year = {2019/03/15},
pages = {16 pp. - },
address = {USA},
abstract = {Recent trends have accelerated the development of spatial applications on mobile devices and robots. These include navigation, augmented reality, human-robot interaction, and others. A key enabling technology for such applications is the understanding of the device's location and the map of the surrounding environment. This generic problem, referred to as Simultaneous Localization and Mapping (SLAM), is an extensively researched topic in robotics. However, visual SLAM algorithms face several challenges including perceptual aliasing and high computational cost. These challenges affect the accuracy, efficiency and viability of visual SLAM algorithms, especially for long-term SLAM, and their use in resource-constrained mobile devices. A parallel trend is the ubiquity of Wi-Fi routers for quick Internet access in most urban environments. Most robots and mobile devices are equipped with a Wi-Fi radio as well. We propose a method to utilize Wi-Fi received signal strength to alleviate the challenges faced by visual SLAM algorithms. To demonstrate the utility of this idea, this work makes the following contributions: (i) We propose a generic way to integrate Wi-Fi sensing into visual SLAM algorithms, (ii) We integrate such sensing into three well-known SLAM algorithms, (iii) Using four distinct datasets, we demonstrate the performance of such augmentation in comparison to the original visual algorithms and (iv) We compare our work to Wi-Fi augmented FABMAP algorithm. Overall, we show that our approach can improve the accuracy of visual SLAM algorithms by 11% on average and reduce computation time on average by 15% to 25%.},
keywords = {augmented reality;human-robot interaction;Internet;mobile radio;mobile robots;path planning;robot vision;SLAM (robots);wireless LAN;},
note = {Wi-Fi radio;visual SLAM algorithms;Wi-Fi sensing;original visual algorithms;Wi-Fi Sensing;robots;robotics;long-term SLAM;resource-constrained mobile devices;Wi-Fi routers;},
} 


@article{18732638 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Labbe, M. and Michaud, F.},
volume = { 36},
number = { 2},
year = {2019/03/},
pages = {416 - 46},
issn = {1556-4959},
address = {USA},
abstract = {Distributed as an open-source library since 2013, real-time appearance-based mapping (RTAB-Map) started as an appearance-based loop closure detection approach with memory management to deal with large-scale and long-term online operation. It then grew to implement simultaneous localization and mapping (SLAM) on various robots and mobile platforms. As each application brings its own set of constraints on sensors, processing capabilities, and locomotion, it raises the question of which SLAM approach is the most appropriate to use in terms of cost, accuracy, computation power, and ease of integration. Since most of SLAM approaches are either visual- or lidar-based, comparison is difficult. Therefore, we decided to extend RTAB-Map to support both visual and lidar SLAM, providing in one package a tool allowing users to implement and compare a variety of 3D and 2D solutions for a wide range of applications with different robots and sensors. This paper presents this extended version of RTAB-Map and its use in comparing, both quantitatively and qualitatively, a large selection of popular real-world datasets (e.g., KITTI, EuRoC, TUM RGB-D, MIT Stata Center on PR2 robot), outlining strengths, and limitations of visual and lidar SLAM configurations from a practical perspective for autonomous navigation applications.},
keywords = {image colour analysis;mobile robots;object detection;optical radar;robot vision;SLAM (robots);},
note = {RTAB-Map;open-source lidar;visual simultaneous localization;mapping library;long-term online operation;open-source library;real-time appearance-based mapping;appearance-based loop closure detection approach;SLAM approach;SLAM approaches;visual- lidar-based;visual SLAM configurations;lidar SLAM configurations;},
URL = {http://dx.doi.org/10.1002/rob.21831},
} 


@inproceedings{21259668 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Preventing and Correcting Mistakes in Lifelong Mapping},
journal = {2021 European Conference on Mobile Robots (ECMR)},
author = {Banerjee, N. and Lisin, D. and Albanese, V. and Zhu, Z. and Lenser, S.R. and Shriver, J. and Ramaswamy, T. and Briggs, J. and Fong, P.},
year = {2021//},
pages = {8 pp. - },
address = {Piscataway, NJ, USA},
abstract = {A Graph SLAM system is only as good as the edges in its pose graph. Critical mistakes in the generation of these edges can instantly render a map inconsistent, misleading, and ultimately unusable. For a lifelong mapping system, where the map is updated continuously, avoiding these errors altogether is infeasible. Instead, we propose a system for detection of and recovery from severe errors in edge generation. Our system remedies both edges created by view observations and edges created by an odometry motion model. For observation edges, we pair a novel method for monitoring ambiguous views with an intelligent graph-merging algorithm capable of rejecting a relocalization in progress. For motion edges, we propose a qualitative geometric approach for detecting structural aberrations characteristic of odometry failures. We conclude with an analysis of our results based on an empirical study of thousands of robot runs.},
keywords = {computational complexity;distance measurement;graph theory;merging;mobile robots;particle filtering (numerical methods);robot kinematics;robot vision;SLAM (robots);},
note = {Graph SLAM system;pose graph;critical mistakes;map inconsistent;lifelong mapping system;severe errors;edge generation;system remedies;view observations;odometry motion model;observation edges;ambiguous views;intelligent graph-merging algorithm;motion edges;},
URL = {http://dx.doi.org/10.1109/ECMR50962.2021.9568826},
} 


@inproceedings{20414223 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Integration and evaluation of SLAM-based backpack mobile mapping system},
journal = {E3S Web of Conferences},
journal = {E3S Web Conf. (France)},
author = {Peidong Yu and Mengke Wang and Huanjian Chen},
volume = { 206},
year = {2020//},
pages = {03014 (6 pp.) - },
issn = {2267-1242},
address = {France},
abstract = {Mobile mapping is an efficient technology to acquire spatial data of the environment. As a supplement of vehicle-borne and air-borne methods, Backpack mobile mapping system (MMS) has a wide application prospect in indoor and underground space. High-precision positioning and attitude determination are the key to MMS. Usually, GNSS/INS integrated navigation system provides reliable pose information. However, in the GNSS-denied environments, there is no effective long-term positioning method. With the development of simultaneous localization and mapping (SLAM) algorithm, it provides a new solution for indoor mobile mapping. This paper develops a portable backpack mobile mapping system, which integrates multi-sensor such as LiDAR, IMU, GNSS and panoramic camera. The 3D laser SLAM algorithm is applied to the mobile mapping to realize the acquisition of geographic information data in various complex environments. The experimental results in typical indoor and outdoor scenes show that the system can achieve high-precision and efficient acquisition of 3D information, and the relative precision of point cloud is 2~4cm, which meets the requirements of scene mapping and reconstruction.},
keywords = {cameras;geographic information systems;image fusion;image reconstruction;indoor navigation;inertial navigation;mobile computing;optical radar;satellite navigation;SLAM (robots);},
note = {SLAM-based backpack mobile mapping system;spatial data;backpack MMS;indoor space;underground space;attitude determination;GNSS-denied environments;long-term positioning method;indoor mobile mapping;portable backpack mobile mapping system;3D laser SLAM algorithm;geographic information data;complex environments;typical indoor scenes;outdoor scenes;scene mapping;high-precision positioning;GNSS-INS integrated navigation system;simultaneous localization and mapping algorithm;LiDAR;IMU;panoramic camera;scene reconstruction;},
URL = {http://dx.doi.org/10.1051/e3sconf/202020603014},
} 


@inproceedings{18883620 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Network Uncertainty Informed Semantic Feature Selection for Visual SLAM},
journal = {2019 16th Conference on Computer and Robot Vision (CRV). Proceedings},
author = {Ganti, P. and Waslander, S.L.},
year = {2019//},
pages = {121 - 8},
address = {Piscataway, NJ, USA},
abstract = {In order to facilitate long-term localization using a visual simultaneous localization and mapping (SLAM) algorithm, careful feature selection can help ensure that reference points persist over long durations and the runtime and storage complexity of the algorithm remain consistent. We present SIVO (Semantically Informed Visual Odometry and Mapping), a novel information-theoretic feature selection method for visual SLAM which incorporates semantic segmentation and neural network uncertainty into the feature selection pipeline. Our algorithm selects points which provide the highest reduction in Shannon entropy between the entropy of the current state and the joint entropy of the state, given the addition of the new feature with the classification entropy of the feature from a Bayesian neural network. Each selected feature significantly reduces the uncertainty of the vehicle state and has been detected to be a static object (building, traffic sign, etc.) repeatedly with a high confidence. This selection strategy generates a sparse map which can facilitate long-term localization. The KITTI odometry dataset is used to evaluate our method, and we also compare our results against ORB_SLAM2. Overall, SIVO performs comparably to the baseline method while reducing the map size by almost 70%.},
keywords = {Bayes methods;data visualisation;feature extraction;image segmentation;mobile robots;neural nets;robot vision;SLAM (robots);},
note = {visual SLAM;semantic segmentation;neural network uncertainty;feature selection pipeline;joint entropy;classification entropy;Bayesian neural network;selection strategy;sparse map;ORB_SLAM2;map size;visual simultaneous localization and mapping;feature selection;information-theoretic feature selection method;Shannon entropy;semantically informed visual odometry and mapping;network uncertainty informed semantic feature selection;},
URL = {http://dx.doi.org/10.1109/CRV.2019.00024},
} 


@inproceedings{19060928 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Weighted Grid Partitioning for Panel-Based Bathymetric SLAM},
journal = {OCEANS 2019 - Marseille},
author = {Junwoo Jang and Jinwhan Kim},
year = {2019//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Bathymetric navigation enables the long-term operation of autonomous underwater vehicles by reducing navigation drift errors with no need for GPS position fixes. In the case that a bathymetric map is not available, the simultaneous localization and mapping (SLAM) algorithm is required, but this increases computational complexity and memory requirement. Panel-based bathymetric SLAM could considerably reduce the computational burden. However, it may suffers from incorrect update when the vehicle does not belong to the updated panel. This study proposes a new update method, called weighted grid partitioning, which considers the probability distribution of a vehicle's location, and is more effective in terms of the map accuracy, computational burden, and memory usage compared to standard update methods. The feasibility of the proposed algorithm is verified through simulations.},
keywords = {autonomous underwater vehicles;bathymetry;Global Positioning System;navigation;oceanographic techniques;SLAM (robots);},
note = {panel-based bathymetric SLAM;bathymetric navigation;long-term operation;autonomous underwater vehicles;navigation drift errors;GPS position fixes;bathymetric map;computational complexity;memory requirement;computational burden;updated panel;standard update methods;weighted grid partitioning;simultaneous localization and mapping algorithm;vehicle location;map accuracy;memory usage;},
URL = {http://dx.doi.org/10.1109/OCEANSE.2019.8867531},
} 


@inproceedings{18274256 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Sensors, SLAM and Long-term Autonomy: A Review},
journal = {2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)},
author = {Zaffar, M. and Ehsan, S. and Stolkin, R. and Maier, K.M.},
year = {2018//},
pages = {285 - 90},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous Localization and Mapping, commonly known as SLAM, has been an active research area in the field of Robotics over the past three decades. For solving the SLAM problem, every robot is equipped with either a single sensor or a combination of similar/different sensors. This paper attempts to review, discuss, evaluate and compare these sensors. Keeping an eye on future, this paper also assesses the characteristics of these sensors against factors critical to the long-term autonomy challenge.},
keywords = {robot vision;SLAM (robots);},
note = {SLAM problem;long-term autonomy challenge;simultaneous localization and mapping;SLAM;robotics;},
URL = {http://dx.doi.org/10.1109/AHS.2018.8541483},
} 


@inproceedings{18504194 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {An 879GOPS 243mW 80fps VGA Fully Visual CNN-SLAM Processor for Wide-Range Autonomous Exploration},
journal = {2019 IEEE International Solid- State Circuits Conference - (ISSCC)},
author = {Ziyun Li and Yu Chen and Luyao Gong and Lu Liu and Sylvester, D. and Blaauw, D. and Hun-Seok Kim},
year = {2019//},
pages = {134 - 6},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous localization and mapping (SLAM) estimates an agent's trajectory for all six degrees of freedom (6 DoF) and constructs a 3D map of an unknown surrounding. It is a fundamental kernel that enables head-mounted augmented/virtual reality devices and autonomous navigation of micro aerial vehicles. A noticeable recent trend in visual SLAM is to apply computationand memory-intensive convolutional neural networks (CNNs) that outperform traditional hand-designed feature-based methods [1]. For each video frame, CNN-extracted features are matched with stored keypoints to estimate the agent's 6-DoF pose by solving a perspective-n-points (PnP) non-linear optimization problem (Fig. 7.3.1, left). The agent's long-term trajectory over multiple frames is refined by a bundle adjustment process (BA, Fig. 7.3.1 right), which involves a large-scale (~120 variables) non-linear optimization. Visual SLAM requires massive computation (&gt;250GOP/s) in the CNN-based feature extraction and matching, as well as datadependent dynamic memory access and control flow with high-precision operations, creating significant low-power design challenges. Software implementations are impractical, resulting in 0.2s runtime with a ~3GHz CPU+ GPU system with &gt;100MB memory footprint and &gt;100W power consumption. Prior ASICs have implemented either an incomplete SLAM system [2,3] that lacks estimation of ego-motion or employed a simplified (non-CNN) feature extraction and tracking [2,4,5] that limits SLAM quality and range. A recent ASIC [5] augments visual SLAM with an off-chip high-precision inertial measurement unit (IMU), simplifying the computational complexity, but incurring additional power and cost overhead.},
keywords = {augmented reality;cameras;convolutional neural nets;feature extraction;mobile robots;path planning;pose estimation;power consumption;robot vision;SLAM (robots);},
note = {879GOPS 243mW 80fps;fully visual CNN-SLAM processor;autonomous navigation;microaerial vehicles;video frame;CNN-extracted features;perspective-n-points nonlinear optimization problem;long-term trajectory;bundle adjustment process;large-scale nonlinear optimization;high-precision operations;low-power design challenges;incomplete SLAM system;memory footprint;power consumption;visual SLAM;nonCNN feature extraction;CPU+ GPU system;head-mounted virtual reality devices;head-mounted augmented reality devices;data-dependent dynamic memory access;off-chip high-precision inertial measurement unit;computational complexity;power 243.0 mW;},
URL = {http://dx.doi.org/10.1109/ISSCC.2019.8662397},
} 


@inproceedings{21257451 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation},
journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Thomas, H. and Agro, B. and Gridseth, M. and Zhang, J. and Barfoot, T.D.},
year = {2021//},
pages = {14047 - 14053},
address = {Piscataway, NJ, USA},
abstract = {We present a self-supervised learning approach for the semantic segmentation of lidar frames. Our method is used to train a deep point cloud segmentation architecture without any human annotation. The annotation process is automated with the combination of simultaneous localization and mapping (SLAM) and ray-tracing algorithms. By performing multiple navigation sessions in the same environment, we are able to identify permanent structures, such as walls, and disentangle short-term and long-term movable objects, such as people and tables, respectively. New sessions can then be performed using a network trained to predict these semantic labels. We demonstrate the ability of our approach to improve itself over time, from one session to the next. With semantically filtered point clouds, our robot can navigate through more complex scenarios, which, when added to the training pool, help to improve our network predictions. We provide insights into our network predictions and show that our approach can also improve the performances of common localization techniques.},
keywords = {image segmentation;indoor communication;learning (artificial intelligence);mobile robots;optical radar;path planning;robot vision;SLAM (robots);},
note = {training pool;network predictions;common localization techniques;lidar segmentation;autonomous indoor navigation;self-supervised learning approach;semantic segmentation;lidar frames;deep point cloud segmentation architecture;human annotation;annotation process;SLAM;ray-tracing algorithms;multiple navigation sessions;permanent structures;disentangle short-term;long-term movable objects;new sessions;semantic labels;session;semantically filtered point clouds;},
URL = {http://dx.doi.org/10.1109/ICRA48506.2021.9561701},
} 


@inproceedings{19193983 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {How to Match Tracks of Visual Features for Automotive Long-Term SLAM},
journal = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
author = {Luthardt, S. and Ziegler, C. and Willert, V. and Adamy, J.},
year = {2019//},
pages = {934 - 41},
address = {Piscataway, NJ, USA},
abstract = {Accurate localization is a vital prerequisite for future assistance or autonomous driving functions in intelligent vehicles. To achieve the required localization accuracy and availability, long-term visual SLAM algorithms like LLama-SLAM are a promising option. In such algorithms visual feature tracks, i. e. landmark observations over several consecutive image frames, have to be matched to feature tracks recorded days, weeks or months earlier. This leads to a more challenging matching problem than in short-term visual localization and known descriptor matching methods cannot be applied directly. In this paper, we devise several approaches to compare and match feature tracks and evaluate their performance on a long-term data set. With the proposed descriptor combination and masking ("CoMa") method the best track matching performance is achieved with minor computational cost. This method creates a single combined descriptor for each feature track and furthermore increases the robustness by capturing the appearance variations of this track in a descriptor mask.},
keywords = {feature extraction;image matching;mobile robots;pose estimation;robot vision;SLAM (robots);},
note = {visual features;automotive long-term SLAM;vital prerequisite;future assistance;autonomous driving functions;LLama-SLAM;consecutive image frames;feature track;challenging matching problem;short-term visual localization;matching methods;long-term data;track matching performance;visual feature tracks;},
URL = {http://dx.doi.org/10.1109/ITSC.2019.8916895},
} 


@article{15592676 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Long-term RFID SLAM using Short-Range Sparse Tags},
journal = {International Journal of Automation and Smart Technology},
journal = {Int. J. Autom. Smart Technol. (Taiwan)},
author = {Jiun-Fu Chen and Chieh-Chih Wang},
volume = { 5},
number = { 1},
year = {2015/03/},
pages = {61 - 75},
issn = {2223-9766},
address = {Taiwan},
abstract = {While on the path forward to the long-term or lifelong robotics, one of the most important capabilities is to have a reliable localization and mapping module. Data association and loop detection play critical roles in the localization and mapping problem. By utilizing the radio frequency identification (RFID) technology, these problems can be solved using the extended Kalman filter (EKF) based simultaneous localization and mapping (SLAM) with the tag information. But one of the critical barriers to the long-term SLAM is the overconfidence issue. In this paper, we focus on solving the overconfidence issue, which is introduced by the linearization errors. An Unit Circle Representation (UCR) is proposed to diminish the error in the prediction stage and a Correlation Coefficient Preserved Inflation (CCPI) is developed to recover the overconfidence issue in the update stage. Based on only odometry and sparse short-range RFID data, the proposed method is capable to compensate the linearization errors in both simulation and real experiments.},
keywords = {Kalman filters;nonlinear filters;radiofrequency identification;sensor fusion;SLAM (robots);},
note = {long-term RFID SLAM;short-range sparse tags;data association;loop detection;localization and mapping problem;radio frequency identification;extended Kalman filter;EKF;unit circle representation;UCR;correlation coefficient preserved inflation;CCPI;},
URL = {http://dx.doi.org/10.5875/ausmt.v5i1.843},
} 


@inproceedings{20385429 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {A Robust Localization Method in Indoor Dynamic Environment},
journal = {IOP Conference Series: Materials Science and Engineering},
journal = {IOP Conf. Ser., Mater. Sci. Eng. (UK)},
author = {Shan Huang and Hong-Zhong Huang and Qi Zeng},
volume = { 1043},
year = {2021//},
pages = {052025 (17 pp.) - },
issn = {1757-8981},
address = {UK},
abstract = {Localization is one of the core technologies for mobile robots to achieve full autonomous movement, and is a prerequisite for other autonomous tasks. The robot working environment is dynamic in most cases, so the localization algorithm must overcome the effects of dynamic changes in the environment. The paper proposed a localization algorithm that allows the robot to perform robust and life-long localization in dynamic environment. The algorithm filter out high-dynamic objects and update semi-static object on the map at the same time, it can also use the information provided in semi-static objects to improve localization performance. In this paper, the processing of dynamic objects is divided into two parts: filtering of high-dynamic objects and updating of semi-static objects. For high dynamic object filtering, a dynamic object detection method combining a delay comparison method and a tracking method is proposed by observed the characteristics of localization system; For the update of semi-static objects, this paper uses the pose graph optimization and occupancy map to implement the dynamic update of the map. The combination of the two methods allows the robot to achieve long-term stable localization in a dynamic environment. The experimental results demonstrate that the proposed method allows the robot achieve long-term localization, overcome the effects of high-dynamic objects and keeping the map always consistent with the environment.},
keywords = {delays;distance measurement;graph theory;indoor navigation;mobile robots;object detection;optical radar;sensor fusion;SLAM (robots);},
note = {mobile robots;robot working environment;localization stability;robust localization;indoor dynamic environment;dynamic object detection;dynamic object filtering;autonomous movement;delay comparison;tracking method;semi static objects;LIDAR sensors;wheel odometer sensors;pose graph optimization;occupancy map;},
URL = {http://dx.doi.org/10.1088/1757-899X/1043/5/052025},
} 


@article{19350016 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Place Recognition for Stereo VisualOdometry using LiDAR Descriptors [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Jiawei Mo and Sattar, J.},
year = {16 Sept. 2019},
pages = {17 pp. - },
address = {USA},
abstract = {Place recognition is a core component in SLAM, and in most visual SLAM systems, it is based on the similarity between 2D images. However, the 3D points generated by visual odometry, and the structure information embedded within, are not exploited. In this paper, we adapt place recognition methods for 3D point clouds into stereo visual odometry. Stereo visual odometry generates 3D point clouds with a consistent scale. Thus, we are able to use global LiDAR descriptors for 3D point clouds to determine the similarity between places. 3D point clouds are more reliable than 2D visual cues (e.g., 2D features) against environmental changes such as varying illumination and can benefit visual SLAM systems in long-term deployment scenarios. Extensive evaluation on a public dataset (Oxford RobotCar) demonstrates the accuracy and efficiency of using 3D point clouds for place recognition over 2D methods.},
keywords = {distance measurement;feature extraction;object recognition;optical radar;SLAM (robots);stereo image processing;},
note = {global LiDAR descriptors;structure information;place recognition methods;visual SLAM systems;2D visual cues;3D point clouds;stereo visual odometry;},
} 


@inproceedings{20592293 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Towards life-long mapping of dynamic environments using temporal persistence modeling},
journal = {2020 25th International Conference on Pattern Recognition (ICPR)},
author = {Tsamis, G. and Kostavelis, I. and Giakoumis, D. and Tzovaras, D.},
year = {2020//},
pages = {10480 - 5},
address = {Piscataway, NJ, USA},
abstract = {The contemporary SLAM mapping systems assume a static environment and build a map that is then used for mobile robot navigation disregarding the dynamic changes in this environment. The paper at hand presents a novel solution for the problem of life-long mapping that continually updates a metric map represented as a 2D occupancy grid in large scale indoor environments with movable objects such as people, robots, objects etc. suitable for industrial applications. We formalize each cell's occupancy as a failure analysis problem and contribute temporal persistence modeling (TPM), an algorithm for probabilistic prediction of the time that a cell in an observed location is expected to be &ldquo;occupied&rdquo; or &ldquo;empty&rdquo; given sparse prior observations from a task specific mobile robot. Our work is evaluated in Gazebo simulation environment against the nominal occupancy of cells and the estimated obstacles persistence. We also show that robot navigation with life-long mapping demands less replans and leads to more efficient navigation in highly dynamic environments.},
keywords = {collision avoidance;control engineering computing;failure analysis;mobile robots;path planning;robot vision;SLAM (robots);},
note = {scale indoor environments;movable objects;failure analysis problem;temporal persistence modeling;task specific mobile robot;Gazebo simulation environment;estimated obstacles persistence;life-long mapping demands less replans;highly dynamic environments;contemporary SLAM mapping systems;static environment;mobile robot navigation;dynamic changes;metric map;2D occupancy grid;},
URL = {http://dx.doi.org/10.1109/ICPR48806.2021.9413161},
} 


@inproceedings{18392983 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Efficient Long-term Mapping in Dynamic Environments},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Lazaro, M.T. and Capobianco, R. and Grisetti, G.},
year = {2018//},
pages = {153 - 60},
address = {Piscataway, NJ, USA},
abstract = {As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.},
keywords = {graph theory;mobile robots;robot vision;SLAM (robots);},
note = {mapping problem;longterm SLAM datasets;graph coherency;intra-session loop closure detections;out-dated nodes;graph complexity;nonstatic entities;merging procedure;efficient ICP-based alignment;up-to-date state;2D point cloud data;local maps;graph SLAM paradigm;multiple mapping sessions;single mapping sessions;SLAM system;autonomous robots;dynamic environments;long-term robot operation;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8594310},
} 


@article{19387209 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Relocalization With Submaps: Multi-Session Mapping for Planetary Rovers Equipped With Stereo Cameras},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Giubilato, R. and Vayugundla, M. and Schuster, M.J. and Stuumlrzl, W. and Wedler, A. and Triebel, R. and Debei, S.},
volume = { 5},
number = { 2},
year = {2020/04/},
pages = {580 - 7},
issn = {2377-3774},
address = {USA},
abstract = {To enable long term exploration of extreme environments such as planetary surfaces, heterogeneous robotic teams need the ability to localize themselves on previously built maps. While the Localization and Mapping problem for single sessions can be efficiently solved with many state of the art solutions, place recognition in natural environments still poses great challenges for the perception system of a robotic agent. In this paper we propose a relocalization pipeline which exploits both 3D and visual information from stereo cameras to detect matches across local point clouds of multiple SLAM sessions. Our solution is based on a Bag of Binary Words scheme where binarized SHOT descriptors are enriched with visual cues to recall in a fast and efficient way previously visited places. The proposed relocalization scheme is validated on challenging datasets captured using a planetary rover prototype on Mount Etna, designated as a Moon analogue environment.},
keywords = {graph theory;mobile robots;multi-robot systems;path planning;planetary rovers;robot vision;SLAM (robots);},
note = {Binary Words scheme;binarized SHOT descriptors;visual cues;relocalization scheme;planetary rover prototype;Moon analogue environment;submaps;multisession Mapping;planetary rovers equipped;stereo cameras;long term exploration;extreme environments;planetary surfaces;heterogeneous robotic teams;built maps;single sessions;art solutions;place recognition;natural environments;perception system;robotic agent;relocalization pipeline;visual information;local point clouds;multiple SLAM sessions;},
URL = {http://dx.doi.org/10.1109/LRA.2020.2964157},
} 


@article{18001975 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Bescos, B. and Facil, J.M. and Civera, J. and Neira, J.},
volume = { 3},
number = { 4},
year = {2018/10/},
pages = {4076 - 83},
issn = {2377-3774},
address = {USA},
abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this letter we present DynaSLAM, a visual SLAM system that, building on ORB-SLAM2, adds the capabilities of dynamic object detection and background inpainting. DynaSLAM is robust in dynamic scenarios for monocular, stereo, and RGB-D configurations. We are capable of detecting the moving objects either by multiview geometry, deep learning, or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo, and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynaSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments.},
keywords = {image colour analysis;image restoration;learning (artificial intelligence);mobile robots;object detection;service robots;SLAM (robots);stereo image processing;},
note = {deep learning;multiview geometry;monocular datasets;stereo datasets;RGB-D datasets;frame background inpainting;scene rigidity;static map;dynamic object detection;ORB-SLAM2;DynaSLAM;autonomous vehicles;service robotics;visual SLAM system;SLAM algorithms;},
URL = {http://dx.doi.org/10.1109/LRA.2018.2860039},
} 


@article{20770100 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Improving Image Description with Auxiliary Modality for Visual Localization in Challenging Conditions},
journal = {International Journal of Computer Vision},
journal = {Int. J. Comput. Vis. (Germany)},
author = {Piasco, N. and Sidibe, D. and Gouet-Brunet, V. and Demonceaux, C.},
volume = { 129},
number = { 1},
year = {2021/01/},
pages = {185 - 202},
issn = {0920-5691},
address = {Germany},
abstract = {Image indexing for lifelong localization is a key component for a large panel of applications, including robot navigation, autonomous driving or cultural heritage valorization. The principal difficulty in long-term localization arises from the dynamic changes that affect outdoor environments. In this work, we propose a new approach for outdoor large scale image-based localization that can deal with challenging scenarios like cross-season, cross-weather and day/night localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We show through extensive evaluation that our method can improve localization performances, especially in challenging scenarios when the visual appearance of the scene has changed. Our method is able to leverage both visual and geometric clues from monocular images to create discriminative descriptors for cross-season localization and effective matching of images acquired at different time periods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images. Finally we extended our method to reflectance modality and we compare multi-modal descriptors respectively based on geometry, material reflectance and a combination of both.},
keywords = {computer vision;feature extraction;image classification;image colour analysis;image matching;image recognition;image representation;image retrieval;learning (artificial intelligence);mobile robots;object detection;object recognition;path planning;robot vision;},
note = {principal difficulty;long-term localization;dynamic changes;outdoor environments;outdoor large scale image-based localization;cross-weather;learned global image descriptor;scene geometry information;query image;localization accuracy;localization performances;visual appearance;visual clues;geometric clues;monocular images;cross-season localization;night images;daytime images;multimodal descriptors;image description;auxiliary modality;visual localization;image indexing;lifelong localization;robot navigation;autonomous driving heritage valorization;cultural heritage valorization;},
URL = {http://dx.doi.org/10.1007/s11263-020-01363-6},
} 


@article{20886414 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Thomas, H. and Agro, B. and Gridseth, M. and Jian Zhang and Barfoot, T.D.},
year = {2020/12/10},
pages = {7 pp. - },
address = {USA},
abstract = {We present a self-supervised learning approach for the semantic segmentation of lidar frames. Our method is used to train a deep point cloud segmentation architecture without any human annotation. The annotation process is automated with the combination of simultaneous localization and mapping (SLAM) and ray-tracing algorithms. By performing multiple navigation sessions in the same environment, we are able to identify permanent structures, such as walls, and disentangle short-term and long-term movable objects, such as people and tables, respectively. New sessions can then be performed using a network trained to predict these semantic labels. We demonstrate the ability of our approach to improve itself over time, from one session to the next. With semantically filtered point clouds, our robot can navigate through more complex scenarios, which, when added to the training pool, help to improve our network predictions. We provide insights into our network predictions and show that our approach can also improve the performances of common localization techniques.},
keywords = {image segmentation;mobile robots;optical radar;path planning;robot vision;SLAM (robots);supervised learning;},
note = {permanent structures;new sessions;semantic labels;semantically filtered point clouds;training pool;network predictions;common localization techniques;lidar segmentation;semantic segmentation;lidar frames;deep point cloud segmentation architecture;human annotation;annotation process;SLAM;multiple navigation sessions;},
} 


@article{18732636 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Distributed stereo vision-based 6D localization and mapping for multi-robot teams},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Schuster, M.J. and Schmid, K. and Brand, C. and Beetz, M.},
volume = { 36},
number = { 2},
year = {2019/03/},
pages = {305 - 32},
issn = {1556-4959},
address = {USA},
abstract = {Joint simultaneous localization and mapping (SLAM) constitutes the basis for cooperative action in multi-robot teams. We designed a stereo vision-based 6D SLAM system combining local and global methods to benefit from their particular advantages: (1) Decoupled local reference filters on each robot for real-time, long-term stable state estimation required for stabilization, control and fast obstacle avoidance; (2) Online graph optimization with a novel graph topology and intra- as well as inter-robot loop closures through an improved submap matching method to provide global multi-robot pose and map estimates; (3) Distribution of the processing of high-frequency and high-bandwidth measurements enabling the exchange of aggregated and thus compacted map data. As a result, we gain robustness with respect to communication losses between robots. We evaluated our improved map matcher on simulated and real-world datasets and present our full system in five real-world multi-robot experiments in areas of up 3,000 m<sup>2</sup> (bounding box), including visual robot detections and submap matches as loop-closure constraints. Further, we demonstrate its application to autonomous multi-robot exploration in a challenging rough-terrain environment at a Moon-analogue site located on a volcano.},
keywords = {collision avoidance;graph theory;mobile robots;multi-robot systems;path planning;robot vision;SLAM (robots);state estimation;stereo image processing;},
note = {autonomous multirobot exploration;visual robot detections;real-world multirobot experiments;improved map matcher;robots;thus compacted map data;aggregated compacted map data;high-bandwidth measurements;(3) Distribution;map estimates;inter-robot loop closures;intra;novel graph topology;graph optimization;fast obstacle avoidance;control;long-term stable state estimation;(1) Decoupled local reference filters;global methods;local methods;stereo vision-based 6D;SLAM;multirobot teams;mapping;},
URL = {http://dx.doi.org/10.1002/rob.21812},
} 


@article{18046291 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Exactly sparse delayed state filter on Lie groups for long-term pose graph SLAM},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Lenac, K. and Cesic, J. and Markovic, I. and Petrovic, I.},
volume = { 37},
number = { 6},
year = {2018/05/},
pages = {585 - 610},
issn = {0278-3649},
address = {UK},
abstract = {In this paper we propose a simultaneous localization and mapping (SLAM) back-end solution called the exactly sparse delayed state filter on Lie groups (LG-ESDSF). We derive LG-ESDSF and demonstrate that it retains all the good characteristics of the classic Euclidean ESDSF, the main advantage being the exact sparsity of the information matrix. The key advantage of LG-ESDSF in comparison with the classic ESDSF lies in the ability to respect the state space geometry by negotiating uncertainties and employing filtering equations directly on Lie groups. We also exploit the special structure of the information matrix in order to allow long-term operation while the robot is moving repeatedly through the same environment. To prove the effectiveness of the proposed SLAM solution, we conducted extensive experiments on two different publicly available datasets, namely the KITTI and EuRoC datasets, using two front-ends: one based on the stereo camera and the other on the 3D LIDAR. We compare LG-ESDSF with the general graph optimization framework (g<sup>2</sup>o) when coupled with the same front-ends. Similarly to g<sup>2</sup>o the proposed LG-ESDSF is front-end agnostic and the comparison demonstrates that our solution can match the accuracy of g<sup>2</sup>o, while maintaining faster computation times. Furthermore, the proposed back-end coupled with the stereo camera front-end forms a complete visual SLAM solution dubbed LG-SLAM. Finally, we evaluated LG-SLAM using the online KITTI protocol and at the time of writing it achieved the second best result among the stereo odometry solutions and the best result among the tested SLAM algorithms.},
keywords = {cameras;geometry;graph theory;Lie groups;matrix algebra;mobile robots;optimisation;SLAM (robots);},
note = {exactly sparse delayed state filter;LG-SLAM;Lie groups;uncertainties;state space geometry;LG-ESDSF;information matrix;stereo odometry solutions;online KITTI protocol;stereo camera front-end;graph optimization framework;3D LIDAR;EuRoC datasets;simultaneous localization and mapping back-end solution;long-term pose graph SLAM;visual SLAM solution;Euclidean ESDSF;filtering equations;},
URL = {http://dx.doi.org/10.1177/0278364918767756},
} 


@inproceedings{18308743 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {LLama-SLAM: Learning High-quality Visual Landmarks for Long-term Mapping and Localization},
journal = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
author = {Luthardt, S. and Willert, V. and Adamy, J.},
year = {2018//},
pages = {2645 - 52},
address = {Piscataway, NJ, USA},
abstract = {The precise localization of vehicles is an important requirement for autonomous driving or advanced driver assistance systems. Using common GNSS the ego position can be measured but not with the reliability and precision necessary. An alternative approach to achieve precise localization is the usage of visual landmarks observed by a camera mounted in the vehicle. However, this raises the necessity of reliable visual landmarks that are easily recognizable and persistent. We propose a novel SLAM algorithm that focuses on learning and mapping such visual long-term landmarks (LLamas). The algorithm therefore processes stereo image streams from several recording sessions in the same spatial area. The key part within LLama-SLAM is the assessment of the landmarks with quality values that are inferred as viewpoint dependent probabilities from observation statistics. By adding solely landmarks of high quality to the final LLama Map, it can be kept compact while still allowing reliable localization. Due to the long-term evaluation of the GNSS measurement during the sessions, the landmarks can be positioned precisely in a global referenced coordinate system. For a first assessment of the algorithm's capabilities, we present some experimental results from the mapping process combining three sessions recorded over two months on the same route.},
keywords = {control engineering computing;driver information systems;mobile robots;probability;road traffic control;robot vision;SLAM (robots);statistics;stereo image processing;},
note = {LLama-SLAM;autonomous driving;stereo image streams;GNSS measurement;statistics;high-quality visual landmark learning;long-term mapping and localization;GNSS;visual long-term landmarks;r advanced driver assistance systems;},
URL = {http://dx.doi.org/10.1109/ITSC.2018.8569323},
} 


@article{17961800 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {DynSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Bescos, B. and Facil, J.M. and Civera, J. and Neira, J.},
year = {2018/06/14},
pages = {8 pp. - },
address = {USA},
abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this paper we present DynSLAM, a visual SLAM system that, building over ORB-SLAM2 [1], adds the capabilities of dynamic object detection and background inpainting. DynSLAM is robust in dynamic scenarios for monocular, stereo and RGB-D configurations. We are capable of detecting the moving objects either by multi-view geometry, deep learning or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments.},
keywords = {image motion analysis;image restoration;object detection;object tracking;robot vision;service robots;SLAM (robots);stereo image processing;},
note = {DynSLAM;dynamic scenes;scene rigidity;SLAM algorithms;visual SLAM system;real-world environments;service robotics;autonomous vehicles;ORB-SLAM2;dynamic object detection;background inpainting;multiview geometry;static map;frame background;dynamic objects;standard visual SLAM baselines;moving object detection;RGB-D configurations;deep learning;stereo data set;public monocular dataset;},
} 


@article{20562768 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {SGC-VSLAM: A Semantic and Geometric Constraints VSLAM for Dynamic Indoor Environments},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Shiqiang Yang and Guohao Fan and Lele Bai and Cheng Zhao and Dexin Li},
volume = { 20},
number = { 8},
year = {2020//},
pages = {2432 (20 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {As one of the core technologies for autonomous mobile robots, Visual Simultaneous Localization and Mapping (VSLAM) has been widely researched in recent years. However, most state-of-the-art VSLAM adopts a strong scene rigidity assumption for analytical convenience, which limits the utility of these algorithms for real-world environments with independent dynamic objects. Hence, this paper presents a semantic and geometric constraints VSLAM (SGC-VSLAM), which is built on the RGB-D mode of ORB-SLAM2 with the addition of dynamic detection and static point cloud map construction modules. In detail, a novel improved quadtree-based method was adopted for SGC-VSLAM to enhance the performance of the feature extractor in ORB-SLAM (Oriented FAST and Rotated BRIEF-SLAM). Moreover, a new dynamic feature detection method called semantic and geometric constraints was proposed, which provided a robust and fast way to filter dynamic features. The semantic bounding box generated by YOLO v3 (You Only Look Once, v3) was used to calculate a more accurate fundamental matrix between adjacent frames, which was then used to filter all of the truly dynamic features. Finally, a static point cloud was estimated by using a new drawing key frame selection strategy. Experiments on the public TUM RGB-D (Red-Green-Blue Depth) dataset were conducted to evaluate the proposed approach. This evaluation revealed that the proposed SGC-VSLAM can effectively improve the positioning accuracy of the ORB-SLAM2 system in high-dynamic scenarios and was also able to build a map with the static parts of the real environment, which has long-term application value for autonomous mobile robots.},
keywords = {feature extraction;image colour analysis;mobile robots;quadtrees;robot vision;SLAM (robots);},
note = {SGC-VSLAM;dynamic indoor environments;autonomous mobile robots;state-of-the-art VSLAM;strong scene rigidity assumption;independent dynamic objects;semantic constraints VSLAM;geometric constraints VSLAM;dynamic detection;static point cloud map construction modules;dynamic feature detection method;semantic bounding box;truly dynamic features;ORB-SLAM2 system;high-dynamic scenarios;},
URL = {http://dx.doi.org/10.3390/s20082432},
} 


@inproceedings{18096155 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition},
journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Siva, S. and Hao Zhang},
year = {2018//},
pages = {9 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first large-scale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.},
keywords = {mobile robots;object recognition;robot vision;sensor fusion;SLAM (robots);},
note = {omnidirectional multisensory perception fusion;long-term place recognition;long-term autonomy;omnidirectional sensors;omnidirectional observation;multidirectional place recognition;omnidirectional multisensory data;appearance variations;Simultaneous Localization and Mapping;},
URL = {http://dx.doi.org/10.1109/ICRA.2018.8461042},
} 


@article{20885804 ,
language = {Chinese},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Simultaneous localization and tracking algorithm utilizing FastSLAM framework for autonomous underwater vehicles},
journal = {Control Theory &amp; Applications},
journal = {Control Theory Appl. (China)},
author = {Lu Jian and Chen Xu and Liu Tong and Ma Cheng-xian and He Jin-xin},
volume = { 37},
number = { 1},
year = {2020/01/},
pages = {89 - 97},
issn = {1000-8152},
address = {China},
abstract = {The cooperative localization is an important research question in the field of Tr-Co Robots study. The scheme of the cooperative localization algorithm depends on the ability of information interaction between the robots. To solve the problem that the cooperative localization accuracy is obviously reduced when the communication is interrupted for a long time between the autonomous underwater vehicles (AUV), the simultaneous localization and tracking (SLAT) algorithms based on the FastSLAM framework are developed in this research, borrowing the principle of the simultaneous localization and mapping (SLAM) algorithms. The master AUV is regarded as a non-cooperative target and a motion estimator used to track the master AUV is built in the slaver AUV. When the motion state of the master AUV is estimated, the improvement of the self localization accuracy of the slaver AUV is achieved, using the relative measurement information obtained from the sonar sensor on the slaver AUV in real time. The simulation experimental results show that the proposed SLATFI.0 and 2.0 algorithms can effectively reduce the localization errors compared to the conventional dead reckoning method under the condition of long-term communication interruption, and the 2.0 algorithm has better adaptability to the influence of the detection accuracy variety.},
keywords = {autonomous underwater vehicles;mobile robots;motion estimation;path planning;remotely operated vehicles;SLAM (robots);underwater vehicles;},
note = {tracking algorithm;FastSLAM framework;autonomous underwater vehicles;important research question;localization algorithm;mapping algorithms;master AUV;slaver AUV;self localization accuracy;localization errors;long-term communication interruption;},
URL = {http://dx.doi.org/10.7641/CTA.2019.80747},
} 


@article{17465036 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {AEKF-SLAM: a new algorithm for robotic underwater navigation},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Xin Yuan and Martinez-Ortega, J.-F. and Sanchez Fernandez, J.A. and Eckert, M.},
volume = { 17},
number = { 5},
year = {2017/05/},
pages = {1174 (30 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {In this work, we focus on key topics related to underwater Simultaneous Localization and Mapping (SLAM) applications. Moreover, a detailed review of major studies in the literature and our proposed solutions for addressing the problem are presented. The main goal of this paper is the enhancement of the accuracy and robustness of the SLAM-based navigation problem for underwater robotics with low computational costs. Therefore, we present a new method called AEKF-SLAM that employs an Augmented Extended Kalman Filter (AEKF)-based SLAM algorithm. The AEKF-based SLAM approach stores the robot poses and map landmarks in a single state vector, while estimating the state parameters via a recursive and iterative estimation-update process. Hereby, the prediction and update state (which exist as well in the conventional EKF) are complemented by a newly proposed augmentation stage. Applied to underwater robot navigation, the AEKF-SLAM has been compared with the classic and popular FastSLAM 2.0 algorithm. Concerning the dense loop mapping and line mapping experiments, it shows much better performances in map management with respect to landmark addition and removal, which avoid the long-term accumulation of errors and clutters in the created map. Additionally, the underwater robot achieves more precise and efficient self-localization and a mapping of the surrounding landmarks with much lower processing times. Altogether, the presented AEKF-SLAM method achieves reliably map revisiting, and consistent map upgrading on loop closure.},
keywords = {computational complexity;iterative methods;Kalman filters;mobile robots;nonlinear filters;parameter estimation;path planning;recursive estimation;SLAM (robots);state estimation;},
note = {recursive estimation-update process;iterative estimation-update process;underwater robot navigation;dense loop mapping;robotic underwater navigation;single state vector;AEKF-SLAM method;underwater simultaneous localization and mapping;computational costs;augmented extended Kalman filter;state parameter estimation;FastSLAM 2.0 algorithm;line mapping;},
URL = {http://dx.doi.org/10.3390/s17051174},
} 


@inproceedings{17300102 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Robust relocalization based on active loop closure for real-time monocular SLAM},
journal = {Computer Vision Systems. 11th International Conference, ICVS 2017. Revised Selected Papers: LNCS 10528},
author = {Xieyuanli Chen and Huimin Lu and Junhao Xiao and Hui Zhang and Pan Wang},
year = {2017//},
pages = {131 - 43},
address = {Cham, Switzerland},
abstract = {Remarkable performance has been achieved using the state-of-the-art monocular Simultaneous Localization and Mapping (SLAM) algorithms. However, tracking failure is still a challenging problem during the monocular SLAM process, and it seems to be even inevitable when carrying out long-term SLAM in large-scale environments. In this paper, we propose an <i>active loop closure</i> based relocalization system, which enables the monocular SLAM to detect and recover from tracking failures automatically even in previously unvisited areas where no keyframe exists. We test our system by extensive experiments including using the most popular KITTI dataset, and our own dataset acquired by a hand-held camera in outdoor large-scale and indoor small-scale real-world environments where man-made shakes and interruptions were added. The experimental results show that the least recovery time (within 5 ms) and the longest success distance (up to 46 m) were achieved comparing to other relocalization systems. Furthermore, our system is more robust than others, as it can be used in different kinds of situations, i.e., tracking failures caused by the blur, sudden motion and occlusion. Besides robots or autonomous vehicles, our system can also be employed in other applications, like mobile phones, drones, etc.},
keywords = {mobile robots;robot vision;SLAM (robots);},
note = {KITTI dataset;robust relocalization systems;simultaneous localization and mapping;monocular SLAM algorithms;large-scale environments;long-term SLAM;monocular SLAM process;real-time monocular SLAM;active loop closure;tracking failure;recovery time;indoor small-scale real-world environments;hand-held camera;time 5.0 ms;},
URL = {http://dx.doi.org/10.1007/978-3-319-68345-4_12},
} 


@inproceedings{18167959 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Map Management for Efficient Long-Term Visual Localization in Outdoor Environments},
journal = {2018 IEEE Intelligent Vehicles Symposium (IV)},
author = {Burki, M. and Dymczyk, M. and Gilitschenski, I. and Cadena, C. and Siegwart, R. and Nieto, J.},
year = {2018//},
pages = {682 - 8},
address = {Piscataway, NJ, USA},
abstract = {We present a complete map management process for a visual localization system designed for multi-vehicle long-term operations in resource constrained outdoor environments. Outdoor visual localization generates large amounts of data that need to be incorporated into a lifelong visual map in order to allow localization at all times and under all appearance conditions. Processing these large quantities of data is non-trivial, as it is subject to limited computational and storage capabilities both on the vehicle and on the mapping backend. We address this problem with a two-fold map update paradigm capable of, either, adding new visual cues to the map, or updating co-observation statistics. The former, in combination with offline map summarization techniques, allows enhancing the appearance coverage of the lifelong map while keeping the map size limited. On the other hand, the latter is able to significantly boost the appearance-based landmark selection for efficient online localization without incurring any additional computational or storage burden. Our evaluation in challenging outdoor conditions shows that our proposed map management process allows building and maintaining maps for precise visual localization over long time spans in a tractable and scalable fashion.},
keywords = {automobiles;mobile robots;path planning;robot vision;SLAM (robots);},
note = {appearance-based landmark selection;visual localization system;multivehicle long-term operations;resource constrained outdoor environments;outdoor visual localization;lifelong visual map;appearance conditions;mapping backend;two-fold map update paradigm;visual cues;offline map summarization techniques;appearance coverage;long-term visual localization;map management process;autonomous cars;},
URL = {http://dx.doi.org/10.1109/IVS.2018.8500432},
} 


@article{21360732 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Autonomous Vehicle Localization with Prior Visual Point Cloud Map Constraints in GNSS-Challenged Environments},
journal = {Remote Sensing},
journal = {Remote Sens. (Switzerland)},
author = {Xiaohu Lin and Fuhong Wang and Bisheng Yang and Wanwei Zhang},
volume = { 13},
number = { 3},
year = {2021//},
pages = {506 (18 pp.) - },
issn = {2072-4292},
address = {Switzerland},
abstract = {Accurate vehicle ego-localization is key for autonomous vehicles to complete high-level navigation tasks. The state-of-the-art localization methods adopt visual and light detection and ranging (LiDAR) simultaneous localization and mapping (SLAM) to estimate the position of the vehicle. However, both of them may suffer from error accumulation due to long-term running without loop optimization or prior constraints. Actually, the vehicle cannot always return to the revisited location, which will cause errors to accumulate in Global Navigation Satellite System (GNSS)-challenged environments. To solve this problem, we proposed a novel localization method with prior dense visual point cloud map constraints generated by a stereo camera. Firstly, the semi-global-block-matching (SGBM) algorithm is adopted to estimate the visual point cloud of each frame and stereo visual odometry is used to provide the initial position for the current visual point cloud. Secondly, multiple filtering and adaptive prior map segmentation are performed on the prior dense visual point cloud map for fast matching and localization. Then, the current visual point cloud is matched with the candidate sub-map by normal distribution transformation (NDT). Finally, the matching result is used to update pose prediction based on the last frame for accurate localization. Comprehensive experiments were undertaken to validate the proposed method, showing that the root mean square errors (RMSEs) of translation and rotation are less than 5.59 m and 0.08&deg;, respectively.},
keywords = {cameras;distance measurement;image matching;image reconstruction;mobile robots;object detection;optical radar;robot vision;satellite navigation;SLAM (robots);stereo image processing;traffic engineering computing;},
note = {autonomous vehicle localization;prior visual point cloud map constraints;GNSS-challenged environments;accurate vehicle ego-localization;autonomous vehicles;high-level navigation tasks;state-of-the-art localization methods;error accumulation;long-term running;prior constraints;Global Navigation Satellite System-challenged environments;novel localization method;prior dense visual point cloud map constraints;semiglobal-block-matching algorithm;stereo visual odometry;current visual point cloud;adaptive prior map segmentation;fast matching;candidate sub-map;size 5.59 m;},
URL = {http://dx.doi.org/10.3390/rs13030506},
} 


@article{20399324 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Deep Samplable Observation Model for Global Localization and Kidnapping},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Runjian Chen and Huan Yin and Yanmei Jiao and Dissanayake, G. and Yue Wang and Rong Xiong},
volume = { 6},
number = { 2},
year = {2021/04/},
pages = {2296 - 303},
issn = {2377-3766},
address = {USA},
abstract = {Global localization and kidnapping are two challenging problems in robot localization. The popular method, Monte Carlo Localization (MCL) addresses the problem by iteratively updating a set of particles with a &ldquo;sampling-weighting&rdquo; loop. Sampling is decisive to the performance of MCL [1]. However, traditional MCL can only sample from a uniform distribution over the state space. Although variants of MCL propose different sampling models, they fail to provide an accurate distribution or generalize across scenes. To better deal with these problems, we present a distribution proposal model named Deep Samplable Observation Model (DSOM). DSOM takes a map and a 2D laser scan as inputs and outputs a conditional multimodal probability distribution of the pose, making the samples more focusing on the regions with higher likelihood. With such samples, the convergence is expected to be more effective and efficient. Considering that the learning-based sampling model may fail to capture the accurate pose sometimes, we furthermore propose the Adaptive Mixture MCL (AdaM MCL), which deploys a trusty mechanism to adaptively select updating mode for each particle to tolerate this situation. Equipped with DSOM, AdaM MCL can achieve more accurate estimation, faster convergence and better scalability than previous methods in both synthetic and real scenes. Even in real environments with long-term changes, AdaM MCL is able to localize the robot using DSOM trained only by simulation observations from a SLAM map or a blueprint map. Source code for this paper is available here: https://github.com/Runjian-Chen/AdaM_MCL.},
keywords = {learning (artificial intelligence);learning systems;mobile robots;Monte Carlo methods;probability;SLAM (robots);},
note = {robot localization;sampling-weighting loop;uniform distribution;accurate distribution;distribution proposal model;DSOM;2D laser scan;conditional multimodal probability distribution;learning-based sampling;AdaM MCL;deep samplable observation model;global localization;adaptive mixture MCL;Monte Carlo localization;kidnapping;SLAM map;blueprint map;source code;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3061339},
} 


@article{20103294 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {A Samplable Multimodal Observation Model for Global Localization and Kidnapping [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Runjian Chen and Yue Wang and Huan Yin and Yanmei Jiao and Dissanayake, G. and Rong Xiong},
year = {2020/08/31},
pages = {15 pp. - },
address = {USA},
abstract = {Global localization and kidnapping are two challenging problems in robot localization. The popular method, Monte Carlo Localization (MCL) addresses the problem by sampling uniformly over the state space, which is unfortunately inefficient when the environment is large. To better deal with the the problems, we present a proposal model, named Deep Multimodal Observation Model (DMOM). DMOM takes a map and a 2D laser scan as inputs and outputs a conditional multimodal probability distribution of the pose, making the samples more focusing on the regions with higher likelihood. With such samples, the convergence is expected to be much efficient. Considering that learning based Samplable Observation Model may fail to capture the true pose sometimes, we furthermore propose the ADAPTIVE MIXTURE MCL, which adaptively selects updating mode for each particle to tolerate this situation. Equipped with DMOM, ADAPTIVE MIXTURE MCL can achieve more accurate estimation, faster convergence and better scalability compared with previous methods in both synthetic and real scenes. Even in real environment with long-term changing, ADAPTIVE MIXTURE MCL is able to localize the robot using DMON trained only on simulated observations from a SLAM map, or even a blueprint map.},
keywords = {learning (artificial intelligence);mobile robots;Monte Carlo methods;path planning;probability;SLAM (robots);statistical distributions;},
note = {samplable observation model;samplable multimodal observation model;Monte Carlo localization;named deep multimodal observation model;proposal model;state space;robot localization;kidnapping;global localization;simulated observations;ADAPTIVE MIXTURE MCL;conditional multimodal probability distribution;2D laser scan;DMOM;},
} 


@inproceedings{17300101 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {A Robust RGB-D Image-Based SLAM System},
journal = {Computer Vision Systems. 11th International Conference, ICVS 2017. Revised Selected Papers: LNCS 10528},
author = {Liangliang Pan and Jun Cheng and Wei Feng and Xiaopeng Ji},
year = {2017//},
pages = {120 - 30},
address = {Cham, Switzerland},
abstract = {Visual SLAM is widely used in robotics and computer vision. Although there have been many excellent achievements over the past few decades, there are still some challenges. 2D feature-based SLAM algorithm has been suffering from the inaccurate or insufficient correspondences while dealing with the case of textureless or frequently repeating regions. Furthermore, most of the SLAM systems cannot be used for long-term localization in a wide range of environment because of the heavy burden of calculating and memory. In this paper, we propose a robust RGB-D keyframe-based SLAM algorithm. The novelty of proposed approach lies in using both 2D and 3D features for tracking, pose estimation and bundle adjustment. By using 2D and 3D features, the SLAM system can achieve high accuracy and robustness in some challenging environments. The experimental results on TUM RGB-D dataset [1] and ICL-NUIM dataset [2] verify the effectiveness of our algorithm.},
keywords = {pose estimation;robot vision;SLAM (robots);},
note = {SLAM system;computer vision;RGB-D image;visual feature;},
URL = {http://dx.doi.org/10.1007/978-3-319-68345-4_11},
} 


@article{16518911 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age},
journal = {IEEE Transactions on Robotics},
journal = {IEEE Trans. Robot. (USA)},
author = {Cadena, C. and Carlone, L. and Carrillo, H. and Latif, Y. and Scaramuzza, D. and Neira, J. and Reid, I. and Leonard, J.J.},
volume = { 32},
number = { 6},
year = {2016/12/},
pages = {1309 - 32},
issn = {1552-3098},
address = {USA},
abstract = {Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
keywords = {SLAM (robots);},
note = {simultaneous-localization-and-mapping;SLAM community;long-term mapping;semantic representations;SLAM users;critical eye;},
URL = {http://dx.doi.org/10.1109/TRO.2016.2624754},
} 


@inproceedings{21407142 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Wen, B. and Bekris, K.},
year = {2021//},
pages = {8067 - 74},
address = {Piscataway, NJ, USA},
abstract = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method's reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack.},
keywords = {CAD;feature extraction;graph theory;image matching;image sequences;learning (artificial intelligence);object detection;path planning;pose estimation;robot vision;SLAM (robots);},
note = {6D pose tracking;category-level;video sequences;robot manipulation;prior efforts;target object;online template matching;robust feature extraction;low-drift tracking;significant occlusions;object motions;object instance CAD model;frequency 10.0 Hz;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9635991},
} 


@article{19469740 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Duckiefloat: a Collision-Tolerant Resource-Constrained Blimp for Long-Term Autonomy in Subterranean Environments [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Yi-Wei Huang and Chen-Lung Lu and Kuan-Lin Chen and Po-Sheng Ser and Jui-Te Huang and Yu-Chia Shen and Pin-Wei Chen and Po-Kai Chang and Sheng-Cheng Lee and Hsueh-Cheng Wang},
year = {2019/10/31},
pages = {7 pp. - },
address = {USA},
abstract = {There are several challenges for search and rescue robots: mobility, perception, autonomy, and communication. Inspired by the DARPA Subterranean (SubT) Challenge, we propose an autonomous blimp robot, which has the advantages of low power consumption and collision-tolerance compared to other aerial vehicles like drones. This is important for search and rescue tasks that usually last for one or more hours. However, the underground constrained passages limit the size of blimp envelope and its payload, making the proposed system resource-constrained. Therefore, a careful design consideration is needed to build a blimp system with on-board artifact search and SLAM. In order to reach long-term operation, a failure-aware algorithm with minimal communication to human supervisor to have situational awareness and send control signals to the blimp when needed.},
keywords = {aerospace robotics;rescue robots;SLAM (robots);},
note = {Duckiefloat;situational awareness;failure-aware algorithm;SLAM;blimp envelope;underground constrained passages;rescue tasks;aerial vehicles;collision-tolerance;low power consumption;autonomous blimp robot;DARPA Subterranean Challenge;rescue robots;subterranean environments;long-term autonomy;collision-tolerant resource-constrained blimp;minimal communication;long-term operation;on-board artifact search;blimp system;},
} 


@article{16989841 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Multisensory omni-directional long-term place recognition: Benchmark dataset and analysis [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Mathur, A. and Fei Han and Hao Zhang},
year = {2017/04/18},
pages = {15 pp. - },
address = {USA},
abstract = {Recognizing a previously visited place, also known as place recognition (or loop closure detection) is the key towards fully autonomous mobile robots and self-driving vehicle navigation. Augmented with various Simultaneous Localization and Mapping techniques (SLAM), loop closure detection allows for incremental pose correction and can bolster efficient and accurate map creation. However, repeated and similar scenes (perceptual aliasing) and long term appearance changes (e.g. weather variations) are major challenges for current place recognition algorithms. We introduce a new dataset Multisensory Omnidirectional Long-term Place recognition (MOLP) comprising omnidirectional intensity and disparity images. This dataset presents many of the challenges faced by outdoor mobile robots and current place recognition algorithms. Using MOLP dataset, we formulate the place recognition problem as a regularized sparse convex optimization problem. We conclude that information extracted from intensity image is superior to disparity image in isolating discriminative features for successful long term place recognition. Furthermore, when these discriminative features are extracted from an omnidirectional vision sensor, a robust bidirectional loop closure detection approach is established, allowing mobile robots to close the loop, regardless of the difference in the direction when revisiting a place.},
keywords = {convex programming;feature extraction;image sensors;object detection;object recognition;},
note = {multisensory omnidirectional long-term place recognition;place recognition;loop closure detection;fully autonomous mobile robots;self-driving vehicle navigation;simultaneous localization and mapping techniques;SLAM;perceptual aliasing;appearance change;MOLP;regularized sparse convex optimization problem;discriminative feature isolation;omnidirectional vision sensor;},
} 


@article{18877447 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Evolving Indoor Navigational Strategies Using Gated Recurrent Units In NEAT [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Butterworth, J. and Savani, R. and Tuyls, K.},
year = {2019/04/12},
pages = {15 pp. - },
address = {USA},
abstract = {Simultaneous Localisation and Mapping (SLAM) algorithms are expensive to run on smaller robotic platforms such as Micro-Aerial Vehicles. Bug algorithms are an alternative that use relatively little processing power, and avoid high memory consumption by not building an explicit map of the environment. Bug Algorithms achieve relatively good performance in simulated and robotic maze solving domains. However, because they are hand-designed, a natural question is whether they are globally optimal control policies. In this work we explore the performance of Neuroevolution - specifically NEAT - at evolving control policies for simulated differential drive robots carrying out generalised maze navigation. We extend NEAT to include Gated Recurrent Units (GRUs) to help deal with long term dependencies. We show that both NEAT and our NEAT-GRU can repeatably generate controllers that outperform I-Bug (an algorithm particularly well-suited for use in real robots) on a test set of 209 indoor maze like environments. We show that NEAT-GRU is superior to NEAT in this task but also that out of the 2 systems, only NEAT-GRU can continuously evolve successful controllers for a much harder task in which no bearing information about the target is provided to the agent.},
keywords = {mobile robots;optimal control;path planning;recurrent neural nets;robot vision;SLAM (robots);},
note = {evolving indoor navigational strategies;SLAM;smaller robotic platforms;relatively little processing power;high memory consumption;explicit map;relatively good performance;simulated maze solving domains;robotic maze solving domains;hand-designed;natural question;globally optimal control policies;evolving control policies;simulated differential drive robots;generalised maze navigation;long term dependencies;NEAT-GRU;I-Bug;successful controllers;simultaneous localisation and mapping algorithms;indoor maze;bug algorithms;microaerial vehicles;NEAT;gated recurrent units;},
} 


@article{20002222 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Improved and scalable online learning of spatial concepts and language models with mapping},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Taniguchi, A. and Hagiwara, Y. and Taniguchi, T. and Inamura, T.},
volume = { 44},
number = { 6},
year = {2020/07/},
pages = {927 - 46},
issn = {0929-5593},
address = {Germany},
abstract = {We propose a novel online learning algorithm, called SpCoSLAM 2.0, for spatial concepts and lexical acquisition with high accuracy and scalability. Previously, we proposed SpCoSLAM as an online learning algorithm based on unsupervised Bayesian probabilistic model that integrates multimodal place categorization, lexical acquisition, and SLAM. However, our original algorithm had limited estimation accuracy owing to the influence of the early stages of learning, and increased computational complexity with added training data. Therefore, we introduce techniques such as fixed-lag rejuvenation to reduce the calculation time while maintaining an accuracy higher than that of the original algorithm. The results show that, in terms of estimation accuracy, the proposed algorithm exceeds the original algorithm and is comparable to batch learning. In addition, the calculation time of the proposed algorithm does not depend on the amount of training data and becomes constant for each step of the scalable algorithm. Our approach will contribute to the realization of long-term spatial language interactions between humans and robots.},
keywords = {Bayes methods;estimation theory;human-robot interaction;knowledge acquisition;robot programming;SLAM (robots);spatial reasoning;unsupervised learning;},
note = {online learning;spatial concepts;language models;mapping;SpCoSLAM 2.0;lexical acquisition;fixed-lag rejuvenation;estimation accuracy;batch learning;calculation time;training data;long-term spatial language interactions;unsupervised Bayesian probabilistic model;multimodal place categorization integration;human-robot interaction;},
URL = {http://dx.doi.org/10.1007/s10514-020-09905-0},
} 


@inproceedings{15040650 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Review of underwater SLAM techniques},
journal = {2015 6th International Conference on Automation, Robotics and Applications (ICARA). Proceedings},
author = {Hidalgo, F. and Braunl, T.},
year = {2015//},
pages = {306 - 11},
address = {Piscataway, NJ, USA},
abstract = {SLAM (Simultaneous Localization and Mapping) for underwater vehicles is a challenging research topic due to the limitations of underwater localization sensors and error accumulation over long-term operations. Furthermore, acoustic sensors for mapping often provide noisy and distorted images or low-resolution ranging, while video images provide highly detailed images but are often limited due to turbidity and lighting. This paper presents a review of the approaches used in state-of-the-art SLAM techniques: Extended Kalman Filter SLAM (EKF-SLAM), FastSLAM, GraphSLAM and its application in underwater environments.},
keywords = {Kalman filters;SLAM (robots);underwater vehicles;video signal processing;},
note = {underwater SLAM techniques;simultaneous localization and mapping;underwater vehicles;underwater localization sensors;acoustic sensors;low-resolution ranging;video images;extended Kalman filter SLAM;EKF-SLAM;FastSLAM;GraphSLAM;underwater environments;},
URL = {http://dx.doi.org/10.1109/ICARA.2015.7081165},
} 


@inproceedings{17488948 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Research of large-scale offline map management in visual SLAM},
journal = {2017 4th International Conference on Systems and Informatics (ICSAI)},
author = {Qihui Shen and Hanxu Sun and Ping Ye},
year = {2017//},
pages = {215 - 19},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a novel method of visual simultaneous localization and mapping (SLAM), which is a method of real-time localization and mapping. It is important for a mobile robot to build a map while autonomously navigation. Due to the complexity of the robot work scene, the SLAM method proposed in this paper optimizes map management. It will cost a lot of time and space when a robot long-term works in a same large scene. Therefore, we propose a method in this paper to save a detail map as an offline map in advance. At the same time in order to facilitate the follow-up optimization, the offline map can be divided into several sub-graphs according to the similarity of the scene. Since the segmented offline map has been saved to local system, it can be loaded at any time to localization and obtain the pose of current frame.},
keywords = {mobile robots;robot vision;SLAM (robots);},
note = {large-scale offline map management;visual SLAM;mobile robot;robot work scene;SLAM method;robot long-term works;segmented offline map;local system;visual simultaneous localization and mapping;autonomous navigation;real-time localization and mapping;map management optimization;},
URL = {http://dx.doi.org/10.1109/ICSAI.2017.8248292},
} 


@article{19313930 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {LCPF: a particle filter lidar SLAM system with loop detection and correction},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Fuyu Nie and Weimin Zhang and Zhuo Yao and Yongliang Shi and Fangxing Li and Qiang Huang},
volume = { 8},
year = {2020//},
pages = {20401 - 12},
issn = {2169-3536},
address = {USA},
abstract = {A globally consistent map is the basis of indoor robot localization and navigation. However, map built by Rao-Blackwellized Particle Filter (RBPF) doesn't have high global consistency which is not suitable for long-term application in large scene. To address the problem, we present an improved RBPF Lidar SLAM system with loop detection and correction named LCPF. The efficiency and accuracy of loop detection depend on the segmentation of submaps. Instead of dividing the submap at fixed number of laser scan like existing method, Dynamic Submap Segmentation is proposed in LCPF. This segmentation algorithm decreases the error inside the submap by splitting the submap where there is high scan match error and later rectifies the error by an improved pose graph optimization between submaps. In order to segment the submap at appropriate point, when to create a new submap is determined by both the accumulation of scan match error and the particle distribution. Furthermore, LCPF uses branch and bound algorithm as basic detector for loop detection and multiple criteria to judge the reliability of a loop. In the criteria, a novel parameter called usable ratio was proposed to measure the useful information that a laser scan containing. Finally, comparisons to existing 2D-Lidar mapping algorithm are performed with a series of open dataset simulations and real robot experiments to demonstrate the effectiveness of LCPF.},
keywords = {graph theory;image segmentation;mobile robots;optical radar;particle filtering (numerical methods);path planning;robot vision;SLAM (robots);},
note = {loop detection;globally consistent map;indoor robot localization;navigation;Rao-Blackwellized particle filter;high global consistency;LCPF;laser scan;segmentation algorithm;high scan match error;particle filter lidar SLAM system;2D-lidar mapping algorithm;dynamic submap segmentation;improved RBPF lidar SLAM system;},
URL = {http://dx.doi.org/10.1109/ACCESS.2020.2968353},
} 


@inproceedings{21134296 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Using UHF-RFID Signals for Robot Localization Inside Pipelines},
journal = {2021 IEEE 16th Conference on Industrial Electronics and Applications (ICIEA)},
author = {Gunatilake, A. and Galea, M. and Thiyagarajan, K. and Kodagoda, S. and Piyathilaka, L. and Darji, P.},
year = {2021//},
pages = {1109 - 14},
address = {Piscataway, NJ, USA},
abstract = {Underground water pipes are important to any country's infrastructure. Overtime, the metallic pipes are prone to corrosion, which can lead to water leakage and pipe bursts. In order to prolong the service life of those assets, water utilities in Australia apply protective pipe linings. Long-term monitoring and timely intervention are crucial for maintaining those lining assets. However, the water utilities do not possess the comprehensive technology to achieve it. The main reasons for lacking such technology are the unavailability of sensors and accurate robot localization technologies. Feature based localization methods such as SLAM has limited use as the application of liners alters the features and the environment. Encoder based localization is not accurate enough to observe the evolution of defects over a long period of time requiring unique defect correspondence. This motivates us to explore accurate contact-less and wireless based localization methods. We propose a cost-effective localization method using UHF-RFID signals for robot localization inside pipelines based on Gaussian process combined particle filter. Experiments carried out in field extracted pipe samples from the Sydney water pipe network show that using the RSSI and Phase data together in the measurement model with particle filter algorithm improves the localization accuracy up to 15 centimeters precision.},
keywords = {condition monitoring;flaw detection;Gaussian processes;particle filtering (numerical methods);pipelines;radiofrequency identification;SLAM (robots);},
note = {Sydney water pipe network;localization accuracy;pipe samples;Gaussian process combined particle filter;cost-effective localization method;wireless based localization methods;unique defect correspondence;encoder based localization;accurate robot localization technologies;lining assets;long-term monitoring;protective pipe linings;water utilities;service life;pipe bursts;water leakage;metallic pipes;underground water pipes;pipelines;UHF-RFID signals;},
URL = {http://dx.doi.org/10.1109/ICIEA51954.2021.9516284},
} 


@article{16065531 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Long-term mapping techniques for ship hull inspection and surveillance using an autonomous underwater vehicle},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Ozog, P. and Carlevaris-Bianco, N. and Ayoung Kim and Eustice, R.M.},
volume = { 33},
number = { 3},
year = {2016/05/},
pages = {265 - 89},
issn = {1556-4959},
address = {USA},
abstract = {This paper reports on a system for an autonomous underwater vehicle to perform <i>in situ</i>, multiple session hull inspection using long-term simultaneous localization and mapping (SLAM). Our method assumes very little <i>a priori</i> knowledge, and it does not require the aid of acoustic beacons for navigation, which is a typical mode of navigation in this type of application. Our system combines recent techniques in underwater saliency-informed visual SLAM and a method for representing the ship hull surface as a collection of many locally planar surface features. This methodology produces accurate maps that can be constructed in real-time on consumer-grade computing hardware. A single-session SLAM result is initially used as a prior map for later sessions, where the robot automatically merges the multiple surveys into a common hull-relative reference frame. To perform the relocalization step, we use a particle filter that leverages the locally planar representation of the ship hull surface, and a fast visual descriptor matching algorithm. Finally, we apply the recently developed graph sparsification tool, generic linear constraints, as a way to manage the computational complexity of the SLAM system as the robot accumulates information across multiple sessions. We show results for 20 SLAM sessions for two large vessels over the course of days, months, and even up to three years, with a total path length of approximately 10.2 km.},
keywords = {autonomous underwater vehicles;graph theory;mobile robots;particle filtering (numerical methods);ships;SLAM (robots);telerobotics;video surveillance;},
note = {long term mapping techniques;ship hull inspection;ship hull surveillance;autonomous underwater vehicle;multiple session hull inspection;simultaneous localization and mapping;SLAM;acoustic beacons;consumer grade computing hardware;particle filter;locally planar representation;visual descriptor matching algorithm;generic linear constraints;graph sparsification tool;computational complexity;SLAM system;},
URL = {http://dx.doi.org/10.1002/rob.21582},
} 


@article{20520244 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {AUV Bathymetric Simultaneous Localisation and Mapping Using Graph Method},
journal = {Journal of Navigation},
journal = {J. Navig. (UK)},
author = {Teng Ma and Ye Li and Yusen Gong and Rupeng Wang and Mingwei Sheng and Qiang Zhang},
volume = { 72},
number = { 6},
year = {2019/11/},
pages = {1602 - 22},
issn = {0373-4633},
address = {UK},
abstract = {Although topographic mapping missions and geological surveys carried out by Autonomous Underwater Vehicles (AUVs) are becoming increasingly prevalent, the lack of precise navigation in these scenarios still limits their application. This paper deals with the problems of long-term underwater navigation for AUVs and provides new mapping techniques by developing a Bathymetric Simultaneous Localisation And Mapping (BSLAM) method based on graph SLAM technology. To considerably reduce the calculation cost, the trajectory of the AUV is divided into various submaps based on Differences of Normals (DoN). Loop closures between submaps are obtained by terrain matching; meanwhile, maximum likelihood terrain estimation is also introduced to build weak data association within the submap. Assisted by one weight voting method for loop closures, the global and local trajectory corrections work together to provide an accurate navigation solution for AUVs with weak data association and inaccurate loop closures. The viability, accuracy and real-time performance of the proposed algorithm are verified with data collected onboard, including an 8 km planned track recorded at a speed of 4 knots in Qingdao, China.},
keywords = {autonomous underwater vehicles;Global Positioning System;mobile robots;navigation;robot vision;SLAM (robots);terrain mapping;},
note = {global trajectory corrections;weight voting method;weak data association;maximum likelihood terrain estimation;submap;graph SLAM technology;mapping techniques;long-term underwater navigation;autonomous underwater vehicles;geological surveys;topographic mapping missions;graph method;terrain matching;DoN;BSLAM;differences of normals;AUV bathymetric simultaneous localisation and mapping;navigation solution;local trajectory corrections;},
URL = {http://dx.doi.org/10.1017/S0373463319000286},
} 


@inproceedings{20500853 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Localizing backscatters by a single robot with zero start-up cost},
journal = {2019 IEEE Global Communications Conference (GLOBECOM)},
author = {Shengkai Zhang and Wei Wang and Sheyang Tang and Shi Jin and Tao Jiang},
year = {2019//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Recent years have witnessed the rapid proliferation of low- power backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such low-power backscatter tags is crucial for IoT-based smart services. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, increasing the deployment cost. To empower universal localization service, this paper presents Rover, an indoor localization system that simultaneously localizes multiple backscatter tags with zero start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing WiFi-based positioning measurements with inertial measurements to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues such as the interference among multiple tags and the real- time processing for solving the SLAM problem. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords = {backscatter;indoor radio;Internet of Things;radionavigation;SLAM (robots);wireless LAN;},
note = {low- power backscatter technologies;long-term connectivity;smart cities;smart homes;low-power backscatter tags;IoT-based smart services;current backscatter localization systems;deployment cost;universal localization service;indoor localization system;multiple backscatter tags;zero start-up cost;inertial sensors;joint optimization framework;WiFi-based positioning measurements;inertial measurements;connected tags;prototype Rover;localization accuracies;SLAM problem;WiFi chips;},
URL = {http://dx.doi.org/10.1109/GLOBECOM38437.2019.9013768},
} 


@article{19204254 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Localizing Backscatters by a Single Robot With Zero Start-up Cost [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Shengkai Zhang and Wei Wang and Sheyang Tang and Shi Jin and Tao Jiang},
year = {2019/08/08},
pages = {6 pp. - },
address = {USA},
abstract = {Recent years have witnessed the rapid proliferation of low-power backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such low-power backscatter tags is crucial for IoT-based smart services. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, increasing the deployment cost. To empower universal localization service, this paper presents Rover, an indoor localization system that simultaneously localizes multiple backscatter tags with zero start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing WiFi-based positioning measurements with inertial measurements to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues such as the interference among multiple tags and the real-time processing for solving the SLAM problem. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords = {indoor radio;Internet of Things;mobile robots;position measurement;radiofrequency identification;SLAM (robots);wireless LAN;wireless sensor networks;},
note = {backscatters;zero start-up cost;universal localization service;indoor localization system;multiple backscatter tags;inertial sensors;joint optimization framework;WiFi-based positioning measurements;inertial measurements;connected tags;design addresses practical issues;multiple tags;prototype Rover;localization accuracies;deployment cost;known positions;map;current backscatter localization systems;IoT-based smart services;low-power backscatter tags;smart homes;smart cities;long-term connectivity;ubiquitous term connectivity;low-power backscatter technologies;rapid proliferation;size 74.6 cm;size 39.3 cm;},
} 


@inproceedings{17210977 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Dynamic map update of non-static facility logistics environment with a multi-robot system},
journal = {KI 2017: Advances in Artificial Intelligence. 40th Annual German Conference on AI. Proceedings: LNAI 10505},
author = {Shaik, N. and Liebig, T. and Kirsch, C. and Muller, H.},
year = {2017//},
pages = {249 - 61},
address = {Cham, Switzerland},
abstract = {Autonomous robots need to perceive and represent their environments and act accordingly. Using simultaneous localization and mapping (SLAM) methods, robots can build maps of the environment which are efficient for localization and path planning as long as the environment remains unchanged. However, facility logistics environments are not static because pallets and other obstacles are stored temporarily. This paper proposes a novel solution for updating maps of changing environments (i.e. environments with low-dynamic or semi-static objects) in real-time with multiple robots. Each robot is equipped with a laser range sensor and runs localization to estimate its position. Each robot senses the change in the environment with respect to a current map, initially built with a SLAM method, and constructs a temporary map which will be merged into the current map using localization information and line features of the map. This procedure enables the creation of long-term mapping robot systems for facility logistics.},
keywords = {cartography;estimation theory;laser ranging;logistics;multi-robot systems;path planning;sensors;SLAM (robots);},
note = {dynamic map update;nonstatic facility logistics environment;mult-robot system;autonomous robots;simultaneous localization and mapping;SLAM methods;map building;path planning;facility logistics environments;multiple robots;laser range sensor;position estimation;long-term mapping robot systems;},
URL = {http://dx.doi.org/10.1007/978-3-319-67190-1_19},
} 


@article{15658832 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Long-Term Simultaneous Localization and Mapping in Dynamic Environments},
author = {Carlevaris-Bianco, N.D.},
year = {2015//},
pages = {148 - },
address = {Ann Arbor, MI, USA},
abstract = {One of the core competencies required for autonomous mobile robotics is the ability to use sensors to perceive the environment. From this noisy sensor data, the robot must build a representation of the environment and localize itself within this representation. This process, known as simultaneous localization and mapping (SLAM), is a prerequisite for almost all higher-level autonomous behavior in mobile robotics. By associating the robot's sensory observations as it moves through the environment, and by observing the robot's ego-motion through proprioceptive sensors, constraints are placed on the trajectory of the robot and the configuration of the environment. This results in a probabilistic optimization problem to find the most likely robot trajectory and environment configuration given all of the robot's previous sensory experience. SLAM has been well studied under the assumptions that the robot operates for a relatively short time period and that the environment is essentially static during operation. However, performing SLAM over long time periods while modeling the dynamic changes in the environment remains a challenge. The goal of this thesis is to extend the capabilities of SLAM to enable long-term autonomous operation in dynamic environments. The contribution of this thesis has three main components: First, we propose a framework for controlling the computational complexity of the SLAM optimization problem so that it does not grow unbounded with exploration time. Second, we present a method to learn visual feature descriptors that are more robust to changes in lighting, allowing for improved data association in dynamic environments. Finally, we use the proposed tools in SLAM systems that explicitly models the dynamics of the environment in the map by representing each location as a set of example views that capture how the location changes with time. We experimentally demonstrate that the proposed methods enable long-term SLAM in dynamic environments using a large, realworld vision and LIDAR dataset collected over the course of more than a year. This dataset captures a wide variety of dynamics: from short-term scene changes including moving people, cars, changing lighting, and weather conditions; to long-term dynamics including seasonal conditions and structural changes caused by construction.},
keywords = {mobile robots;optical radar;sensor fusion;},
note = {autonomous mobile robotics;LIDAR dataset;real-world vision;data association;visual feature descriptors;computational complexity;long-term autonomous operation;dynamic environments;long-term simultaneous localization and mapping;},
} 


@article{18517446 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {AUV robust bathymetric simultaneous localization and mapping},
journal = {Ocean Engineering},
journal = {Ocean Eng. (Netherlands)},
author = {Teng Ma and Ye Li and Rupeng Wang and Zheng Cong and Yusen Gong},
volume = { 166},
year = {2018/10/15},
pages = {336 - 49},
issn = {0029-8018},
address = {Netherlands},
abstract = {Bathymetric simultaneous localization and mapping (BSLAM) technique could provide long-term underwater navigation results for autonomous underwater vehicles (AUVs) and produce a self-consistent bathymetric map simultaneously. However, the inter-frame motion inside BSLAM is still difficult to estimate, and BSLAM might fail catastrophically with invalid loop closures caused by the measurement errors of vehicle states and bathymetric data. To deal with these problems, an AUV robust BSLAM algorithm is proposed based on graph SLAM. In this algorithm, weak data association is constructed via sparse pseudo-input Gaussian process (SPGP) regression to predict inter-frame motion, and a multi-window consistency method (MCM) is introduced to identify invalid loop closures. Various simulation experiments are conducted under different environments. Comparisons are made between more standard approaches, and our proposed algorithm is shown to be viable, accurate, and could robustly handle invalid loop closures. [All rights reserved Elsevier].},
keywords = {autonomous underwater vehicles;filtering theory;Gaussian processes;navigation;SLAM (robots);terrain mapping;},
note = {multiwindow consistency method;sparse pseudoinput Gaussian process regression;weak data association;AUV robust BSLAM algorithm;bathymetric data;vehicle states;measurement errors;inter-frame motion;self-consistent bathymetric map;autonomous underwater vehicles;long-term underwater navigation results;AUV robust bathymetric simultaneous localization;invalid loop closures;},
URL = {http://dx.doi.org/10.1016/j.oceaneng.2018.08.029},
} 


@inproceedings{17428227 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Model-aided monocular visual-inertial state estimation and dense mapping},
journal = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Qiu, K. and Shen, S.},
year = {2017//},
pages = {1783 - 9},
address = {Piscataway, NJ, USA},
abstract = {Robust state estimation and real-time dense mapping are two core capabilities for autonomous navigation of mobile robots. Global Navigation Satellite System (GNSS) and visual odometry/SLAM are popular methods for state estimation. However, when working between tall buildings or in indoor environments, GNSS fails due to limited sky view or obstruction from buildings. Visual odometry/SLAM are prone to long-term drifting in the absence of reliable loop closure detection. A state estimation method with global-consistent guarantee is desirable for navigation applications. As for real-time mapping, SLAM methods usually get a sparse map that is not good enough for obstacle avoidance and path-planning, and high-quality dense mapping is often computationally too demanding for mobile devices. Realizing the availability of city-scale 3D models, in this work, we improve our previous work on model-based global localization, and propose a model-aided monocular visual-inertial state estimation and dense mapping solution. We first develop a global-consistent state estimator by fusing visual-inertial odometry with the model-based localization results. Utilizing depth prior from the model, we perform motion stereo with semi-global disparity smoothing. Our dense mapping pipeline is capable of online detection of obstacles that are originally not included in the offline 3D model. Our method runs onboard an embedded computer in real-time. We validate both the state estimation and mapping accuracy in real-world experiments.},
keywords = {collision avoidance;image motion analysis;mobile robots;robot vision;SLAM (robots);state estimation;stereo image processing;},
note = {visual-inertial odometry;model-based localization results;semiglobal disparity smoothing;dense mapping pipeline;visual-inertial state estimation;robust state estimation;real-time dense mapping;autonomous navigation;Global Navigation Satellite System;GNSS;visual odometry/SLAM;reliable loop closure detection;state estimation method;navigation applications;real-time mapping;SLAM methods;high-quality dense mapping;city-scale 3D models;global localization;dense mapping solution;global-consistent state estimator;},
URL = {http://dx.doi.org/10.1109/IROS.2017.8205992},
} 


@inproceedings{16554819 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {OpenABLE: An Open-source Toolbox for Application in Life-Long Visual Localization of Autonomous Vehicles},
journal = {2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)},
author = {Arroyo, R. and Alcantarilla, P.F. and Bergasa, L.M. and Romera, E.},
year = {2016//},
pages = {965 - 70},
address = {Piscataway, NJ, USA},
abstract = {Visual information is a valuable asset in any perception scheme designed for an intelligent transportation system. In this regard, the camera-based recognition of locations provides a higher situational awareness of the environment, which is very useful for varied localization solutions typically needed in long-term autonomous navigation, such as loop closure detection and visual odometry or SLAM correction. In this paper we present OpenABLE, an open-source toolbox contributed to the community with the aim of helping researchers in the application of these kinds of life-long localization algorithms. The implementation follows the philosophy of the topological place recognition method named ABLE, including several new features and improvements. These functionalities allow to match locations using different global image description methods and several configuration options, which enable the users to control varied parameters in order to improve the performance of place recognition depending on their specific problem requisites. The applicability of our toolbox in visual localization purposes for intelligent vehicles is validated in the presented results, jointly with comparisons to the main state-of-the-art methods.},
keywords = {cameras;image matching;intelligent transportation systems;mobile robots;public domain software;robot vision;SLAM (robots);topology;},
note = {OpenABLE;open-source toolbox;visual localization;autonomous vehicle;visual information;intelligent transportation system;ITS;camera-based location recognition;situational awareness;topological place recognition;location matching;simultaneous localization and mapping;SLAM;},
URL = {http://dx.doi.org/10.1109/ITSC.2016.7795672},
} 


@inproceedings{18372916 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A B-Spline Mapping Framework for Long-Term Autonomous Operations},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Rodrigues, R.T. and Aguiar, A.P. and Pascoal, A.},
year = {2018//},
pages = {3204 - 9},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a 2D B-spline mapping framework for representing unstructured environments in a compact manner. While occupancy-grid and landmark-based maps have been successfully employed by the robotics community in indoor scenarios, outdoor long-term autonomous operations require a more compact representation of the environment. This work tackles this problem by interpolating the data of a high frequency sensor using B-spline curves. Compared to lines and circles, splines are more powerful in the sense that they allow for the description of more complex shapes in the scene. In this work, spline curves are continuously tracked and aligned across multiple sensor readings using lightweight methods, making the proposed framework suitable for robot navigation in outdoor missions. In particular, a Simultaneous Localization and Mapping (SLAM) algorithm specifically tailored for B-spline maps is presented here. The efficacy of the proposed framework is demonstrated by Software-in-the-Loop (SiL) simulations in different scenarios.},
keywords = {image representation;image sensors;mobile robots;navigation;path planning;robot vision;SLAM (robots);splines (mathematics);},
note = {landmark-based maps;robotics community;high frequency sensor;B-spline curves;B-spline maps;mapping algorithm;2D B-spline mapping framework;outdoor long-term autonomous operations;simultaneous localization and mapping;SLAM algorithm;software-in-the-loop simulations;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8594456},
} 


@inproceedings{18112483 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data},
journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Li Sun and Zhi Yan and Mellado, S.M. and Hanheide, M. and Duckett, T.},
year = {2018//},
pages = {5942 - 8},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.},
keywords = {cameras;codecs;image annotation;image coding;learning (artificial intelligence);mobile robots;object detection;pedestrians;pose estimation;robot vision;service robots;SLAM (robots);},
note = {long-term temporal information;sequence-to-sequence LSTM encoder-decoder;on-the-fly prediction;global coordinate system;T-Pose-LSTM model;human trajectory prediction;long-term mobile robot deployments;3DOF pedestrian trajectory prediction learned;Long-Term autonomous mobile robot deployment data;autonomous mobile service robots;monocular camera images;range-finder sensors;3DOF pedestrian trajectory prediction approach;temporal 3DOF-pose long-short-term memory;robust human detection;},
URL = {http://dx.doi.org/10.1109/ICRA.2018.8461228},
} 


@article{18999326 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A realtime autonomous robot navigation framework for human like high-level interaction and task planning in global dynamic environment [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Sung-Hyeon Joo and Manzoor, S. and Rocha, Y.G. and Hyun-Uk Lee and Tae-Yong Kuc},
year = {2019/05/30},
pages = {4 pp. - },
address = {USA},
abstract = {In this paper, we present a framework for real-time autonomous robot navigation based on cloud and on-demand databases to address two major issues of human-like robot interaction and task planning in global dynamic environment, which is not known a priori. Our framework contributes to make human-like brain GPS mapping system for robot using spatial information and performs 3D visual semantic SLAM for independent robot navigation. We accomplish the feat by separating robot's memory system into Long-Term Memory (LTM) and Short-Term Memory (STM). We also form robot's behavior and knowledge system by linking these memories to Autonomous Navigation Module (ANM), Learning Module (LM), and Behavior Planner Module (BPM). The proposed framework is assessed through simulation using ROS-based Gazebo-simulated mobile robot, RGB-D camera (3D sensor) and a laser range finder (2D sensor) in 3D model of realistic indoor environment. Simulation corroborates the substantial practical merit of our proposed framework.},
keywords = {control engineering computing;laser ranging;mobile robots;navigation;path planning;robot vision;robots;SLAM (robots);},
note = {real-time autonomous robot navigation;on-demand databases;robot interaction;task planning;global dynamic environment;brain GPS mapping system;performs 3D visual semantic SLAM;independent robot navigation;memory system;knowledge system;Autonomous Navigation Module;Behavior Planner Module;ROS-based Gazebo-simulated mobile robot;3D sensor;2D sensor;realistic indoor environment;realtime autonomous robot navigation framework;high-level interaction;},
} 


@article{18829246 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Topological local-metric framework for mobile robots navigation: a long term perspective},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Li Tang and Yue Wang and Xiaqing Ding and Huan Yin and Rong Xiong and Shoudong Huang},
volume = { 43},
number = { 1},
year = {2019/01/31},
pages = {197 - 211},
issn = {0929-5593},
address = {Germany},
abstract = {Long term mapping and localization are the primary components for mobile robots in real world application deployment, of which the crucial challenge is the robustness and stability. In this paper, we introduce a topological local-metric framework (TLF), aiming at dealing with environmental changes, erroneous measurements and achieving constant complexity. TLF organizes the sensor data collected by the robot in a topological graph, of which the geometry is only encoded in the edge, i.e. the relative poses between adjacent nodes, relaxing the global consistency to local consistency. Therefore the TLF is more robust to unavoidable erroneous measurements from sensor information matching since the error is constrained in the local. Based on TLF, as there is no global coordinate, we further propose the localization and navigation algorithms by switching across multiple local metric coordinates. Besides, a lifelong memorizing mechanism is presented to memorize the environmental changes in the TLF with constant complexity, as no global optimization is required. In experiments, the framework and algorithms are evaluated on 21-session data collected by stereo cameras, which are sensitive to illumination, and compared with the state-of-art global consistent framework. The results demonstrate that TLF can achieve similar localization accuracy with that from global consistent framework, but brings higher robustness with lower cost. The localization performance can also be improved from sessions because of the memorizing mechanism. Finally, equipped with TLF, the robot navigates itself in a 1 km session autonomously.},
keywords = {graph theory;mobile robots;navigation;path planning;},
note = {global consistent framework;topological local-metric framework;TLF;constant complexity;sensor data;stability;robustness;world application deployment;long term perspective;mobile robots navigation;local-metric framework;localization performance;similar localization accuracy;global optimization;environmental changes;multiple local metric coordinates;navigation algorithms;sensor information matching;unavoidable erroneous measurements;local consistency;global consistency;topological graph;size 1.0 km;},
URL = {http://dx.doi.org/10.1007/s10514-018-9724-7},
} 


@article{15404195 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {LiDAR scan matching aided inertial navigation system in GNSS-denied environments},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Jian Tang and Yuwei Chen and Xiaoji Niu and Li Wang and Liang Chen and Jingbin Liu and Chuang Shi and Hyyppa, J.},
volume = { 15},
number = { 7},
year = {2015/07/},
pages = {16710 - 28},
issn = {1424-8220},
address = {Switzerland},
abstract = {A new scan that matches an aided Inertial Navigation System (INS) with a low-cost LiDAR is proposed as an alternative to GNSS-based navigation systems in GNSS-degraded or -denied environments such as indoor areas, dense forests, or urban canyons. In these areas, INS-based Dead Reckoning (DR) and Simultaneous Localization and Mapping (SLAM) technologies are normally used to estimate positions as separate tools. However, there are critical implementation problems with each standalone system. The drift errors of velocity, position, and heading angles in an INS will accumulate over time, and on-line calibration is a must for sustaining positioning accuracy. SLAM performance is poor in featureless environments where the matching errors can significantly increase. Each standalone positioning method cannot offer a sustainable navigation solution with acceptable accuracy. This paper integrates two complementary technologies-INS and LiDAR SLAM-into one navigation frame with a loosely coupled Extended Kalman Filter (EKF) to use the advantages and overcome the drawbacks of each system to establish a stable long-term navigation process. Static and dynamic field tests were carried out with a self-developed Unmanned Ground Vehicle (UGV) platform-NAVIS. The results prove that the proposed approach can provide positioning accuracy at the centimetre level for long-term operations, even in a featureless indoor environment.},
keywords = {calibration;inertial navigation;Kalman filters;nonlinear filters;optical radar;position control;remotely operated vehicles;satellite navigation;SLAM (robots);},
note = {LiDAR scan matching aided inertial navigation system;INS;GNSS-degraded environment;GNSS-denied environment;DR;dead reckoning;simultaneous localization and mapping technology;SLAM technology;position estimation;calibration;loosely coupled extended Kalman filter;EKF;unmanned ground vehicle;UGV;NAVIS;},
URL = {http://dx.doi.org/10.3390/s150716710},
} 


@inproceedings{20076245 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Self Localization Based On Neighborhood Probability Mapping for Humanoid Robot},
journal = {2020 4th International Conference on Vocational Education and Training (ICOVET)},
author = {Jiono, M. and Mahandi, Y.D. and Norma Mustika, S. and Sendari, S. and Dzikri, A.M.},
year = {2020//},
pages = {355 - 9},
address = {Piscataway, NJ, USA},
abstract = {The humanoid robot competition is an autonomous robot with a human-like body platform with a single camera as a vision sensor and balancing sensor to support them to play soccer in the specific field. The technical challenges in this competition such following the ball, running during search the ball, dynamic walking, kicking while maintaining the balance body condition, decision making with other robot, localization and mapping as research issues investigated in the Humanoid competition. Localization and mapping still big challenges in humanoid competition, it was only single camera is used in competition rule and no others sensor to support the position and orientation during playing the game. The proposed system was developed is neighborhood probability mapping. The long-term goal of this research is to realize an ideal system to accelerate the redesign field condition and implementation process in a humanoid robot that can be monitored in real-time. The aim of this research is to take the opportunities: (a) increasing the robot's performance of vision and intelligence on the humanoid robot; (b) with this SLAM method the robot can distinguish between the balls that are in the field and outside the field; (c) able to distinguish the enemy goal from the goal itself based on goal detection and line detection; (d) the goal keeper robot capable of acting as an attacker and scanning the kick towards the enemy goal. The testing condition was implemented between simulation testing and real testing in same times. Based on the data experimental result, the robot can estimate their position and orientation during searching the ball position, goal position and obstacle coordinate with high real time accuracy. The result shows that the proposed system can be applied to the humanoid soccer robot in the real time directly and it worked with less error.},
keywords = {decision making;humanoid robots;legged locomotion;path planning;robot vision;SLAM (robots);},
note = {neighborhood probability mapping;humanoid robot competition;autonomous robot;single camera;vision sensor;balancing sensor;balance body condition;humanoid competition;goal detection;line detection;robot capable;ball position;goal position;humanoid soccer robot;SLAM method;},
URL = {http://dx.doi.org/10.1109/ICOVET50258.2020.9230237},
} 


@inproceedings{15475821 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {An Adaptive Gaussian Particle Filter based Simultaneous Localization and Mapping with dynamic process model noise bias compensation},
journal = {2015 IEEE 7th International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM). Proceedings},
author = {Rao, A. and Wang Han},
year = {2015//},
pages = {210 - 15},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous Localization and Mapping (SLAM) is a fundamental component of all autonomous robotics systems, which probabilisticaly fuses information from an exteroceptive sensor and a proprioceptive sensor to simultaneously estimate the robot's trajectory and the map. Inputs from the pro-prioceptive sensor are fed into the estimation algorithm via a process model corresponding with the vehicle kinematics, while a measurement model is used to process inputs from the exteroceptive sensor. Most SLAM algorithms assume known, fixed model estimate bias. This assumption does not hold true for systems with wrongly modeled estimate bias, or those affected by component fatigue due to applications requiring long term autonomy. This paper will display the adverse effects of mismodeled process model bias using a simulation. An adaptive algorithm employing Adaptive Gaussian Particle Filter based process model bias compensation will be deployed in tandem with a particle filter based FastSLAM algorithm. The algorithm will be compared favourably with existing state of the art SLAM algorithms in controlled simulations. Experimental data from a marine environment will be used to validate the efficacy of the algorithm.},
keywords = {adaptive filters;Gaussian processes;mobile robots;particle filtering (numerical methods);sensor fusion;trajectory control;},
note = {adaptive Gaussian particle filter;simultaneous localization and mapping;dynamic process model noise bias compensation;autonomous robotic system;probabilistic information fusion;exteroceptive sensor;proprioceptive sensor;robot trajectory estimation;vehicle kinematics;model estimate bias;adaptive algorithm;FastSLAM algorithm;marine environment;},
URL = {http://dx.doi.org/10.1109/ICCIS.2015.7274622},
} 


@article{15929729 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {An Open-source Bio-inspired Solution to Underwater SLAM},
journal = {IFAC - Papers Online},
journal = {IFAC, Pap. Online (Netherlands)},
author = {Silveira, L. and Guth, F. and Drews-Jr, P. and Ballester, P. and Machado, M. and Codevilla, F. and Duarte-Filho, N. and Botelho, S.},
volume = { 48},
number = { 2},
year = {2015//},
pages = {212 - 17},
issn = {2405-8963},
address = {Netherlands},
abstract = {We present a bio-inspired approach to deal with the localization and spatial mapping problem, extending the successful previous RatSLAM approach from 2D ground vehicles to the 3D underwater environments. Our approach, called DolphinSLAM, is a SLAM system based on mammals navigation. Experiments in simulation and real environments were conducted involving long-term navigation tasks with different robots and sensors. Our proposal is open- source, being integrated with the Robot Operating System (ROS). [All rights reserved Elsevier].},
keywords = {autonomous underwater vehicles;marine navigation;public domain software;sensors;SLAM (robots);},
note = {RatSLAM approach;2D ground vehicles;mammal navigation;robot operating system;ROS;open-source bio-inspired solution;underwater SLAM;3D underwater environments;DolphinSLAM;long-term navigation;sensors;simultaneous localization and mapping;},
URL = {http://dx.doi.org/10.1016/j.ifacol.2015.06.035},
} 


@inproceedings{18417691 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A real-time visual-inertial mapping and localization method by fusing unstable GPS},
journal = {2018 13th World Congress on Intelligent Control and Automation (WCICA). Proceedings},
author = {Zhongyuan Zhang and Hesheng Wang and Weidong Chen},
year = {2018//},
pages = {1397 - 402},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a novel method which fuse visual, IMU and GPS tightly to realize high-precision real-time localization and mapping simultaneously (SLAM). Our method is based on the bundle adjustment (BA). The confidence of the GPS signal is used to determine the window size in the local mapping thread and judge whether the keyframe is reliable. The long-term unreliable keyframe linking with large uncertainty of GPS which called GPS-restricted or GPS-denied situation will cause the drift when mapping. To eliminate the drift, in contrast to use the closed-loop detection and global optimization which will increase the computational burden extremely with the size of the map enlarged, a semi-global optimization method is proposed to relieve the burden, which make the localization estimated by this method possible to be used to navigate for unmanned vehicles. In our method, the confidence of the GPS signal is significantly important, however, the covariance supplied by the GPS receiver may not be trustworthy sometimes, which cause some unnecessary mistake when optimizing, thus a semi-supervised clustering method taking the information of GPS and IMU into account synthetically is introduced to get that confidence more robustly.},
keywords = {distance measurement;Global Positioning System;mobile robots;path planning;pattern clustering;SLAM (robots);},
note = {GPS signal;window size;local mapping thread;GPS-denied situation;closed-loop detection;global optimization;semiglobal optimization method;GPS receiver;semisupervised clustering method;IMU;real-time visual-inertial mapping;localization method;unstable GPS;high-precision real-time localization;bundle adjustment;GPS-restricted situation;},
URL = {http://dx.doi.org/10.1109/WCICA.2018.8630513},
} 


@article{20322850 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {A Geodetic Normal Distribution Map for Long-Term LiDAR Localization on Earth},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Chansoo Kim and Sungjin Cho and Myoungho Sunwoo and Resende, P. and Bradai, B. and Kichun Jo},
volume = { 9},
year = {2021//},
pages = {470 - 84},
issn = {2169-3536},
address = {USA},
abstract = {Light detection and ranging (LiDAR) sensors enable a vehicle to estimate a pose by matching their measurements with a point cloud (PCD) map. However, the PCD map structure, widely used in robot fields, has some problems to be applied for mass production in automotive fields. First, the PCD map is too big to store all map data at in-vehicle units or download the map data from a wireless network according to the vehicle location. Second, the PCD map, represented by a single origin in the Cartesian coordinates, causes coordinate conversion errors due to an inaccurate plane-orb projection, when the vehicle estimate the geodetic pose on Earth. To solve two problems, this paper presents a geodetic normal distribution (GND) map structure. The GND map structure supports a geodetic quad-tree tiling system with multiple origins to minimize the coordinate conversion errors. The map data managed by the GND map structure are compressed by using Cartesian probabilistic distributions of points as map features. The truncation errors by heterogeneous coordinates between the geodetic tiling system and Cartesian distributions are compensated by the Cartesian voxelization rule. In order to match the LiDAR measurements with the GND map structure, the paper proposes map-matching approaches based on Monte-Carlo and optimization. The paper performed some experiments to evaluate the map size compression and the long-term localization on Earth: comparison with the PCD map structure, localization in various continents, and long-term localization.},
keywords = {cartography;Monte Carlo methods;optical radar;optimisation;pose estimation;},
note = {map size compression;PCD map structure;long-term LiDAR localization;point cloud map;map data;geodetic normal distribution map structure;GND map structure;geodetic quad-tree tiling system;map-matching approaches;light detection and ranging sensors;LiDAR sensors;Cartesian voxelization rule;},
URL = {http://dx.doi.org/10.1109/ACCESS.2020.3047421},
} 


@inproceedings{16559180 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Information-based Active SLAM via topological feature graphs},
journal = {2016 IEEE 55th Conference on Decision and Control (CDC)},
author = {Beipeng Mu and Giamou, M. and Paull, L. and Agha-mohammadi, A.-A. and Leonard, J. and How, J.},
year = {2016//},
pages = {5583 - 90},
address = {Piscataway, NJ, USA},
abstract = {Exploring an unknown space and building maps is a fundamental capability for mobile robots. For fully autonomous systems, the robot would further need to actively plan its paths during exploration. The problem of designing robot trajectories to actively explore an unknown environment and minimize the map error is referred to as active simultaneous localization and mapping (active SLAM). Existing work has focused on planning paths with occupancy grid maps, which do not scale well and suffer from long term drift. This work proposes a Topological Feature Graph (TFG) representation that scales well and develops an active SLAM algorithm with it. The TFG uses graphical models, which utilize independences between variables, and enables a unified quantification of exploration and exploitation gains with a single entropy metric. Hence, it facilitates a natural and principled balance between map exploration and refinement. A probabilistic roadmap path-planner is used to generate robot paths in real time. Experimental results demonstrate that the proposed approach achieves better accuracy than a standard grid-map based approach while requiring orders of magnitude less computation and memory resources.},
keywords = {entropy;graph theory;mobile robots;path planning;probability;SLAM (robots);trajectory control;},
note = {information-based active SLAM;mobile robots;fully autonomous systems;path planning;robot trajectories;occupancy grid maps;topological feature graph representation;TFG representation;entropy metric;probabilistic roadmap path-planner;robot path generation;standard grid-map based approach;magnitude less computation;memory resources;active simultaneous localization and mapping;},
URL = {http://dx.doi.org/10.1109/CDC.2016.7799127},
} 


@inproceedings{19078690 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from 3-D Lidar Scans},
journal = {2019 European Conference on Mobile Robots (ECMR). Proceedings},
author = {Schaefer, A. and Buscher, D. and Vertens, J. and Luft, L. and Burgard, W.},
year = {2019//},
pages = {7 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation [1]. In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15 months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy.},
keywords = {mobile robots;optical radar;},
note = {long-term urban vehicle localization;long-term stability;pole-like objects;urban environments;complete mapping;long-term localization system;3D lidar data;novel pole detector;mapping module;online localization module;open-source implementation;long-term reliability;urban area;initial map;3D lidar scans;pole landmarks extraction;mobile robot;time 15.0 month;},
URL = {http://dx.doi.org/10.1109/ECMR.2019.8870928},
} 


@inproceedings{20842379 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {ExMaps: Long-Term Localization in Dynamic Scenes using Exponential Decay},
journal = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
author = {Rotsidis, A. and Lutteroth, C. and Hall, P. and Richardt, C.},
year = {2021//},
pages = {2866 - 75},
address = {Los Alamitos, CA, USA},
abstract = {Visual camera localization using offline maps is widespread in robotics and mobile applications. Most state-of-the-art localization approaches assume static scenes, so maps are often reconstructed once and then kept constant. However, many scenes are dynamic and as changes in the scene happen, future localization attempts may struggle or fail entirely. Therefore, it is important for successful long-term localization to update and maintain maps as new observations of the scene, and changes in it, arrive. We propose a novel method for automatically discovering which points in a map remain stable over time, and which are due to transient changes. To this end, we calculate a stability store for each point based on its visibility over time, weighted by an exponential decay over time. This allows us to consider the impact of time when scoring points, and to distinguish which points are useful for long-term localization. We evaluate our method on the CMU Extended Seasons dataset (outdoors) and a new indoor dataset of a retail shop, and show the benefit of maintaining a `live map' that integrates updates over time using our exponential decay based method over a static `base map'.},
keywords = {cameras;geophysical image processing;indoor radio;mobile computing;mobile robots;robot vision;SLAM (robots);terrain mapping;},
note = {static scenes;long-term localization;live map;exponential decay based method;static base map;dynamic scenes;visual camera localization;offline maps;CMU Extended Seasons dataset;},
URL = {http://dx.doi.org/10.1109/WACV48630.2021.00291},
} 


@article{18926823 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Learning by Inertia: self-supervised monocular visual odometry for road vehicles [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Chengze Wang and Yuan Yuan and Qi Wang},
year = {2019/05/05},
pages = {5 pp. - },
address = {USA},
abstract = {In this paper, we present iDVO (inertia-embedded deep visual odometry), a self-supervised learning based monocular visual odometry (VO) for road vehicles. When modelling the geometric consistency within adjacent frames, most deep VO methods ignore the temporal continuity of the camera pose, which results in a very severe jagged fluctuation in the velocity curves. With the observation that road vehicles tend to perform smooth dynamic characteristics in most of the time, we design the inertia loss function to describe the abnormal motion variation, which assists the model to learn the consecutiveness from long-term camera ego-motion. Based on the recurrent convolutional neural network (RCNN) architecture, our method implicitly models the dynamics of road vehicles and the temporal consecutiveness by the extended Long Short-Term Memory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to handle the non-consistency in fast camera motion by blocking the boundary part and which generates more efficiency in the whole non-consistency mask. The proposed method is evaluated on the KITTI dataset, and the results demonstrate state-of-the-art performance with respect to other monocular deep VO and SLAM approaches.},
keywords = {cameras;convolutional neural nets;distance measurement;image motion analysis;image sequences;learning (artificial intelligence);mobile robots;motion estimation;pose estimation;recurrent neural nets;road vehicles;robot vision;SLAM (robots);traffic engineering computing;},
note = {extended long short-term memory block;RCNN;LSTM;KITTI dataset;SLAM approaches;monocular deep VO;fast camera motion;recurrent convolutional neural network architecture;long-term camera ego-motion;abnormal motion variation;inertia loss function;smooth dynamic characteristics;deep VO methods;self-supervised learning;inertia-embedded deep visual odometry;road vehicles;self-supervised monocular visual odometry;},
} 


@inproceedings{18778770 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Learning by Inertia: Self-supervised Monocular Visual Odometry for Road Vehicles},
journal = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings},
author = {Chengze Wang and Yuan Yuan and Qi Wang},
year = {2019//},
pages = {2252 - 6},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we present iDVO (inertia-embedded deep visual odometry), a self-supervised learning based monocular visual odometry (VO) for road vehicles. When modelling the geometric consistency within adjacent frames, most deep VO methods ignore the temporal continuity of the camera pose, which results in a very severe jagged fluctuation in the velocity curves. With the observation that road vehicles tend to perform smooth dynamic characteristics in most of the time, we design the inertia loss function to describe the abnormal motion variation, which assists the model to learn the consecutiveness from long-term camera ego-motion. Based on the recurrent convolutional neural network (RCNN) architecture, our method implicitly models the dynamics of road vehicles and the temporal consecutiveness by the extended Long Short-Term Memory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to handle the non-consistency in fast camera motion by blocking the boundary part and which generates more efficiency in the whole non-consistency mask. The proposed method is evaluated on the KITTI dataset, and the results demonstrate state-of-the-art performance with respect to other monocular deep VO and SLAM approaches.},
keywords = {cameras;convolutional neural nets;distance measurement;image motion analysis;pose estimation;recurrent neural nets;road vehicles;robot vision;SLAM (robots);supervised learning;},
note = {smooth dynamic characteristics;long-term camera ego-motion;recurrent convolutional neural network architecture;road vehicles;extended Long Short-Term Memory block;self-supervised monocular visual odometry;inertia-embedded deep visual odometry;self-supervised learning;camera motion;},
URL = {http://dx.doi.org/10.1109/ICASSP.2019.8683446},
} 


@article{20882056 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from 3-D Lidar Scans [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Schaefer, A. and Buscher, D. and Vertens, J. and Luft, L. and Burgard, W.},
year = {2019/10/23},
pages = {9 pp. - },
address = {USA},
abstract = {Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation at www.github.com/acschaefer/polex. In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15~months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy. [European Conference on Mobile Robots, Prague, Czech Republic, 2019, pp. 1-7 doi:10.1109/ECMR.2019.8870928].},
keywords = {mobile robots;optical radar;traffic engineering computing;},
note = {pole landmarks extracted;urban environments;complete mapping;novel pole detector;mapping module;online localization module;mobile robot;urban area;initial map;Mobile Robots;},
} 


@inproceedings{14718027 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Online global loop closure detection for large-scale multi-session graph-based SLAM},
journal = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014)},
author = {Labbe, M. and Michaud, F.},
year = {2014//},
pages = {2661 - 6},
address = {Piscataway, NJ, USA},
abstract = {For large-scale and long-term simultaneous localization and mapping (SLAM), a robot has to deal with unknown initial positioning caused by either the kidnapped robot problem or multi-session mapping. This paper addresses these problems by tying the SLAM system with a global loop closure detection approach, which intrinsically handles these situations. However, online processing for global loop closure detection approaches is generally influenced by the size of the environment. The proposed graph-based SLAM system uses a memory management approach that only consider portions of the map to satisfy online processing requirements. The approach is tested and demonstrated using five indoor mapping sessions of a building using a robot equipped with a laser rangefinder and a Kinect.},
keywords = {graph theory;laser ranging;mobile robots;position control;robot vision;SLAM (robots);},
note = {Kinect;laser rangefinder;indoor mapping sessions;online processing requirements;memory management approach;multisession mapping;kidnapped robot problem;initial positioning;simultaneous localization and mapping;large-scale multisession graph-based SLAM;online global loop closure detection;},
URL = {http://dx.doi.org/10.1109/IROS.2014.6942926},
} 


@article{16594137 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Opportunistic sampling-based active visual SLAM for underwater inspection},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Chaves, S.M. and Kim, A. and Galceran, E. and Eustice, R.M.},
volume = { 40},
number = { 7},
year = {2016/10/},
pages = {1245 - 65},
issn = {0929-5593},
address = {Germany},
abstract = {This paper reports on an active SLAM framework for performing large-scale inspections with an underwater robot. We propose a path planning algorithm integrated with visual SLAM that plans loop-closure paths in order to decrease navigation uncertainty. While loop-closing revisit actions bound the robot's uncertainty, they also lead to redundant area coverage and increased path length. Our proposed opportunistic framework leverages sampling-based techniques and information filtering to plan revisit paths that are coverage efficient. We employ Gaussian process regression for modeling the prediction of camera registrations and use a two-step optimization procedure for selecting revisit actions. We show that the proposed method offers many benefits over existing solutions and good performance for bounding navigation uncertainty in long-term autonomous operations with hybrid simulation experiments and real-world field trials performed by an underwater inspection robot.},
keywords = {Gaussian processes;inspection;mobile robots;path planning;regression analysis;robot vision;sampling methods;SLAM (robots);underwater vehicles;},
note = {hybrid simulation experiments;autonomous operations;bounding navigation uncertainty;optimization procedure;camera registrations;Gaussian process regression;information filtering;sampling-based techniques;loop-closure paths;path planning algorithm;large-scale inspections;underwater inspection robot;active visual SLAM;opportunistic sampling;},
URL = {http://dx.doi.org/10.1007/s10514-016-9597-6},
} 


@inproceedings{19987356 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships},
journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Peng Gao and Hao Zhang},
year = {2020//},
pages = {1070 - 6},
address = {Piscataway, NJ, USA},
abstract = {Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.},
keywords = {graph theory;image matching;mobile robots;robot vision;SLAM (robots);},
note = {robotics applications;simultaneously localization and mapping;spatial relationship similarities;spatial cues;visual cues;old landmarks;long-term environment changes;landmark information;integrate landmark appearances;worst-case graph matching;place recognition performance;long-term place recognition;worst appearance similarity;similar appearances;worst-case scenario;graph matching problem;visual appearances;angular spatial relationships;graph representation;},
URL = {http://dx.doi.org/10.1109/ICRA40945.2020.9196906},
} 


@inproceedings{16776669 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Consistent Cuboid Detection for Semantic Mapping},
journal = {2017 IEEE 11th International Conference on Semantic Computing (ICSC)},
author = {Hashemifar, Z.S. and Kyung Won Lee and Napp, N. and Dantu, K.},
year = {2017//},
pages = {526 - 31},
address = {Los Alamitos, CA, USA},
abstract = {Building and storing efficient maps is an essential feature for long-term autonomy of robots. Modern sensors (such as Kinect) tend to produce a lot of data. However, long-term autonomy requires us to store this information in a succinct manner. One way to reduce dimensionality of information is to attribute semantics. Most indoor objects are cuboidal in nature. We conjecture that cuboids are a suitable semantic feature to attribute to indoor objects for efficient mapping. We adapt a cuboid fitting algorithm previously proposedfor object recognition, for indoor mapping. Our work stems from the observation that landmark detection for mappingrequires consistent detection of those landmarks. We implement several modifications to this cuboid detection algorithm that lead to consistent detection such as emptiness, orientation, surface coverage, distance from edges, and others. We incorporate these in the identification of the cuboid candidates in a scene, as well as an optimization algorithm for finding the best set of consistent cubes to cover a given scene. Our experiments show that in comparison, the set of cuboids detected by our algorithm are at least 50% more consistent based on our metrics.SLAM.},
keywords = {object recognition;optimisation;SLAM (robots);},
note = {semantic mapping;cuboid fitting algorithm;landmark detection;cuboid detection algorithm;optimization algorithm;SLAM;},
URL = {http://dx.doi.org/10.1109/ICSC.2017.78},
} 


@inproceedings{15667018 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Multi-robot 6D graph SLAM connecting decoupled local reference filters},
journal = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Schuster, M.J. and Brand, C. and Hirschmuller, H. and Suppa, M. and Beetz, M.},
year = {2015//},
pages = {5093 - 100},
address = {Piscataway, NJ, USA},
abstract = {Teams of mobile robots can be deployed in search and rescue missions to explore previously unknown environments. Methods for joint localization and mapping constitute the basis for (semi-)autonomous cooperative action, in particular when navigating in GPS-denied areas. As communication losses may occur, a decentralized solution is required. With these challenges in mind, we designed a submap-based SLAM system that relies on inertial measurements and stereo-vision to create multi-robot dense 3D maps. For online pose and map estimation, we integrate the results of keyframe-based local reference filters through incremental graph SLAM. To the best of our knowledge, we are the first to combine these two methods to benefit from their particular advantages for 6D multi-robot localization and mapping: Local reference filters on each robot provide real-time, long-term stable state estimates that are required for stabilization, control and fast obstacle avoidance, whereas online graph optimization provides global multi-robot pose and map estimates needed for cooperative planning. We propose a novel graph topology for a decoupled integration of local filter estimates from multiple robots into a SLAM graph according to the filters' uncertainty estimates and independence assumptions and evaluated its benefits on two different robots in indoor, outdoor and mixed scenarios. Further, we performed two extended experiments in a multi-robot setup to evaluate the full SLAM system, including visual robot detections and submap matches as inter-robot loop closure constraints.},
keywords = {collision avoidance;graph theory;image filtering;multi-robot systems;optimisation;pose estimation;rescue robots;robot vision;SLAM (robots);stereo image processing;},
note = {SLAM graph;visual robot detections;interrobot loop closure constraints;local filter;decoupled integration;graph topology;cooperative planning;global multirobot pose estimation;online graph optimization;obstacle avoidance;6D multirobot localization and mapping;incremental graph SLAM;keyframe-based local reference filters;map estimation;online pose estimation;multirobot dense 3D maps;stereo-vision;inertial measurements;submap-based SLAM system;decentralized solution;semiautonomous cooperative action;joint mapping;joint localization;search and rescue missions;mobile robot teams;decoupled local reference filters;multirobot 6D graph SLAM;},
URL = {http://dx.doi.org/10.1109/IROS.2015.7354094},
} 


@inproceedings{19298876 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Predictive and adaptive maps for long-term visual navigation in changing environments},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Halodova, L. and Dvorrakova, E. and Majer, F. and Vintr, T. and Martinez Mozos, O. and Dayoub, F. and Krajnik, T.},
year = {2019//},
pages = {7033 - 9},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we compare different map management techniques for long-term visual navigation in changing environments. In this scenario, the navigation system needs to continuously update and refine its feature map in order to adapt to the environment appearance change. To achieve reliable long-term navigation, the map management techniques have to (i) select features useful for the current navigation task, (ii) remove features that are obsolete, (iii) and add new features from the current camera view to the map. We propose several map management strategies and evaluate their performance with regard to the robot localisation accuracy in long-term teach-and-repeat navigation. Our experiments, performed over three months, indicate that strategies which model cyclic changes of the environment appearance and predict which features are going to be visible at a particular time and location, outperform strategies which do not explicitly model the temporal evolution of the changes.},
keywords = {cameras;feature selection;mobile robots;navigation;robot vision;SLAM (robots);},
note = {predictive maps;adaptive maps;long-term visual navigation;navigation system;feature map;environment appearance change;long-term navigation;map management strategies;teach- repeat navigation;model cyclic changes;long-term teach-and-repeat navigation;feature selection;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8967994},
} 


@inproceedings{15291608 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Optical flow localisation and appearance mapping (OFLAAM) for long-term navigation},
journal = {2015 International Conference on Unmanned Aircraft Systems (ICUAS). Proceedings},
author = {Pastor-Moreno, D. and Hyo-Sang Shin and Waldock, A.},
year = {2015//},
pages = {980 - 8},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a novel method to use optical flow navigation for long term navigation. Unlike standard SLAM approaches for augmented reality, OFLAAM is designed for Micro Air Vehicles (MAV). It uses a optical flow camera pointing downwards, a IMU and a monocular camera pointing frontwards. That configuration avoids the computational expensive mapping and tracking of the 3D features. It only maps these features in a vocabulary list by a localization module to tackle the optical flow drift and the lose of the navigation estimation. That module, based on the well established algorithm DBoW2, will be also used to close the loop and allow long-term navigation in previously visited areas. The combination of high speed optical flow navigation with a low rate localization algorithm allows fully autonomous navigation for MAV, at the same time it reduces the overall computational load. This framework is implemented in ROS (Robot Operating System) and tested attached to a laptop. A representative scenario is used to validate and analyze the performance of the system.},
keywords = {aircraft navigation;augmented reality;autonomous aerial vehicles;cameras;control engineering computing;image sequences;object tracking;SLAM (robots);},
note = {optical flow localisation and appearance mapping;OFLAAM;long-term navigation;standard SLAM approach;augmented reality;micro air vehicle;MAV;optical flow camera;IMU;monocular camera pointing frontward;computational expensive mapping and tracking;3D feature;localization module;optical flow drift;navigation estimation;algorithm DBoW2;high speed optical flow navigation;low rate localization algorithm;autonomous navigation;ROS;robot operating system;laptop;},
URL = {http://dx.doi.org/10.1109/ICUAS.2015.7152387},
} 


@inproceedings{13488746 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {A framework for RF-visual SLAM},
journal = {2013 10th International Bhurban Conference on Applied Sciences and Technology (IBCAST 2013). Proceedings},
author = {Anwar, S. and Qingjie Zhao and Qadeer, N. and Khan, S.I.},
year = {2013//},
pages = {103 - 8},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous Localization and Mapping, SLAM, is an important topic in the field of robotics and autonomous navigation. The metric SLAM suffers from sensor inaccuracies and thus cannot be used for long-term navigation. In such case, Visual SLAM or a Hybrid SLAM based on both metric and visual approach is a good alternative. In this paper, in order to speed up a Visual SLAM, we propose a novel concept of dynamic dictionary generated on the results of triangulation done on RF, radio frequency, signals from nearest cell towers of a cellular network. This dynamic dictionary efficiently manages the scalability of a Visual SLAM and make it possible to work in a large-scale environment. A framework is proposed along with triangulation data of a city and with simulations to support the concept.},
keywords = {cellular radio;mobile robots;navigation;path planning;SLAM (robots);},
note = {RF-visual SLAM;simultaneous localization and mapping;robotics;autonomous navigation;metric SLAM;long-term navigation;hybrid SLAM;dynamic dictionary;radio frequency;cellular network;large-scale environment;triangulation data;},
URL = {http://dx.doi.org/10.1109/IBCAST.2013.6512139},
} 


@inproceedings{14630798 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Toward long-term, automated ship hull inspection with visual SLAM, explicit surface optimization, and generic graph-sparsification},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Ozog, P. and Eustice, R.M.},
year = {2014//},
pages = {3832 - 9},
address = {Piscataway, NJ, USA},
abstract = {This paper reports on a method for an autonomous underwater vehicle to perform real-time visual simultaneous localization and mapping (SLAM) on large ship hulls over multiple sessions. Along with a monocular camera, our method uses a piecewise-planar model to explicitly optimize the ship hull surface in our factor-graph framework, and anchor nodes to co-register multiple surveys. To enable realtime performance for long-term SLAM, we use the recent Generic Linear Constraints (GLC) framework to sparsify our factor-graph. This paper analyzes how our single-session SLAM techniques can be used in the GLC framework, and describes a particle filter reacquisition algorithm so that an underwater session can be automatically re-localized to a previously built SLAM graph. We provide real-world experimental results involving automated ship hull inspection, and show that our localization filter out-performs Fast Appearance-Based Mapping (FAB-MAP), a popular place-recognition system. Using our approach, we can automatically align surveys that were taken days, months, and even years apart.},
keywords = {autonomous underwater vehicles;graph theory;inspection;mobile robots;optimisation;particle filtering (numerical methods);robot vision;ships;SLAM (robots);},
note = {place-recognition system;FAB-MAP;fast appearance-based mapping;localization filter;SLAM graph;particle filter reacquisition algorithm;single-session SLAM techniques;GLC framework;generic linear constraint framework;factor-graph framework;piecewise-planar model;monocular camera;visual simultaneous localization and mapping;autonomous underwater vehicle;generic graph-sparsification;explicit ship hull surface optimization;visual SLAM;automated ship hull inspection;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907415},
} 


@article{15980555 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Ceiling vision-based active SLAM framework for dynamic and wide-open environments},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Su-Yong An and Lae-Kyoung Lee and Se-Young Oh},
volume = { 40},
number = { 2},
year = {2016/02/},
pages = {291 - 324},
issn = {0929-5593},
address = {Germany},
abstract = {A typical indoor environment can be divided into three categories; office (or room), hallway, and wide-open space such as lobby and hall. There have been numerous approaches for solving simultaneous localization and mapping (SLAM) problem in office (or room) and hallway. However, direct application of the existing approaches to wide-open space may be failed, because it has some distinguished features compared to other indoor places. To solve this problem, this paper proposes a new ceiling vision-based active SLAM framework, with an emphasis on practical deployment of service robot for commercial use in dynamically changing and wide-open environments by adopting the ceiling vision. First, for defining ceiling feature which can be extracted regardless of complexity of ceiling pattern we introduce a model-free landmark, i.e., visual node descriptor, which consists of edge points and their orientations in image space. Second, a recursive `explore and exploit' is proposed for autonomous mapping. It is recursively performed by spreading out mapped area gradually while the robot is actively localized in the map. It can improve map accuracy due to frequent small loop closing. Third, a dynamic edge link (DEL) is proposed to cope with environmental changes in the map. Owing to DEL, we do not need to filter out corrupted sensor data and to distinguish moving object from static one. Also, a self-repairing map mechanism is introduced to deal with unexpected installation or removal of inner structures. We therefore achieve long-term navigation. Several simulations and real experiments in various places show that the proposed active SLAM framework could build a topologically consistent map, and demonstrated that it can be applied well to real environments such as wide-open space in a city hall and railway station.},
keywords = {mobile robots;navigation;robot vision;service robots;SLAM (robots);},
note = {long-term navigation;DEL;dynamic edge link;image space;visual node descriptor;model-free landmark;service robot;simultaneous localization and mapping;indoor environment;wide-open environments;dynamic environments;ceiling vision-based active SLAM framework;},
URL = {http://dx.doi.org/10.1007/s10514-015-9453-0},
} 


@inproceedings{17058118 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {SLAMinDB: Centralized graph databases for mobile robotics},
journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Fourie, D. and Claassens, S. and Pillai, S. and Mata, R. and Leonard, J.},
year = {2017//},
pages = {6331 - 7},
address = {Piscataway, NJ, USA},
abstract = {Robotic systems typically require memory recall mechanisms for a variety of tasks including localization, mapping, planning, visualization etc. We argue for a novel memory recall framework that enables more complex inference schemas by separating the computation from its associated data. In this work we propose a shared, centralized data persistence layer that maintains an ensemble of online, situationally-aware robot states. This is realized through a queryable graph-database with an accompanying key-value store for larger data. In turn, this approach is scalable and enables a multitude of capabilities such as experience-based learning and long-term autonomy. Using multi-modal simultaneous localization and mapping and a few example use-cases, we demonstrate the versatility and extensible nature that centralized persistence and SLAMinDB can provide. In order to support the notion of life-long autonomy, we envision robots to be endowed with such a persistence model, enabling them to revisit previous experiences and improve upon their existing task-specific capabilities.},
keywords = {control engineering computing;database management systems;graph theory;mobile robots;SLAM (robots);},
note = {SLAMinDB;centralized graph databases;mobile robotics;memory recall mechanisms;complex inference schemas;shared centralized data persistence layer;online situationally-aware robot states;queryable graph-database;key-value store;experience-based learning;long-term autonomy;multimodal simultaneous localization and mapping;persistence model;},
URL = {http://dx.doi.org/10.1109/ICRA.2017.7989749},
} 


@inproceedings{17058365 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {SemanticFusion: Dense 3D semantic mapping with convolutional neural networks},
journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
author = {McCormac, J. and Handa, A. and Davison, A. and Leutenegger, S.},
year = {2017//},
pages = {4628 - 35},
address = {Piscataway, NJ, USA},
abstract = {Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need to extend beyond geometry and appearance - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state-of-the-art dense Simultaneous Localization and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondences between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of &ap;25Hz.},
keywords = {feedforward neural nets;image fusion;image segmentation;indoor navigation;mobile robots;robot vision;SLAM (robots);video signal processing;},
note = {SemanticFusion;dense 3D semantic mapping;convolutional neural networks;visual sensing;mobile robots;robot intelligence;intuitive user interaction;CNN;simultaneous localisation and mapping system;SLAM system;ElasticFusion;long-term dense correspondences;indoor RGB-D video frames;loopy scanning trajectories;semantic predictions;NYUv2 dataset;2D semantic labelling;},
URL = {http://dx.doi.org/10.1109/ICRA.2017.7989538},
} 


@inproceedings{19986802 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Map as the hidden sensor: fast odometry-based global localization},
journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Cheng Peng and Weikersdorfer, D.},
year = {2020//},
pages = {2317 - 23},
address = {Piscataway, NJ, USA},
abstract = {Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in the long-term, our method using only odometry and the map converges in long-term. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.},
keywords = {distance measurement;mobile robots;path planning;sensors;tensors;},
note = {odometry-based global localization;ambiguous observations;odometry drift;blind robots;robot state;belief tensor;map-corrected odometry localization;map traversability;robotics applications;hidden sensor;},
URL = {http://dx.doi.org/10.1109/ICRA40945.2020.9197225},
} 


@article{20655845 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {OpenCSI: An Open-Source Dataset for Indoor Localization Using CSI-Based Fingerprinting [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Gassner, A. and Musat, C. and Rusu, A. and Burg, A.},
year = {2021/04/16},
pages = {9 pp. - },
address = {USA},
abstract = {Many applications require accurate indoor localization. Fingerprint-based localization methods propose a solution to this problem, but rely on a radio map that is effort-intensive to acquire. We automate the radio map acquisition phase using a software-defined radio (SDR) and a wheeled robot. Furthermore, we open-source a radio map acquired with our automated tool for a 3GPP Long-Term Evolution (LTE) wireless link. To the best of our knowledge, this is the first publicly available radio map containing channel state information (CSI). Finally, we describe first localization experiments on this radio map using a convolutional neural network to regress for location coordinates.},
keywords = {fingerprint identification;indoor communication;indoor radio;Long Term Evolution;mobile robots;neural nets;software radio;wireless LAN;},
note = {accurate indoor localization;radio map acquisition phase;automated tool;publicly available radio map;localization experiments;},
} 


@article{20399279 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {R-LOAM: improving LiDAR odometry and mapping with point-to-mesh features of a known 3D reference object},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Oelsch, M. and Karimi, M. and Steinbach, E.},
volume = { 6},
number = { 2},
year = {2021/04/},
pages = {2068 - 75},
issn = {2377-3766},
address = {USA},
abstract = {LiDAR-based odometry and mapping is used in many robotic applications to retrieve the robot's position in an unknown environment and allows for autonomous operation in GPS-denied (e.g., indoor) environments. With a 3D LiDAR sensor, highly accurate localization becomes possible, which enables high quality 3D reconstruction of the environment. In this letter we extend the well-known LOAM framework by leveraging prior knowledge about a reference object in the environment to further improve the localization accuracy. This requires a known 3D model of the reference object and its known position in a global coordinate frame. Instead of only relying on the point features in the mapping module of LOAM, we also include mesh features extracted from the 3D triangular mesh of the reference object in the optimization problem. For fast correspondence computation of mesh features, we use the Axis-Aligned-Bounding-Box-Tree (AABB) structure. Essentially, our approach not only makes use of the previously built map for absolute localization in the environment, but also takes the relative position to the reference object into account, effectively reducing long-term drift. To validate the proposed concept, we generated datasets using the Gazebo simulation environment in exemplary visual inspection scenarios of an airplane inside a hangar and the Eiffel Tower. An actuated 3D LiDAR sensor is mounted via a 1-DoF gimbal on a UAV capturing 360&deg; scans. We benchmark our approach against the state-of-the-art open-source LOAM framework. The results show that the proposed joint optimization using both point and mesh features yields a significant reduction in Absolute Pose Error (APE) and therefore improves the map and 3D reconstruction quality during long-term operations.},
keywords = {autonomous aerial vehicles;distance measurement;feature extraction;Global Positioning System;image reconstruction;mesh generation;mobile robots;optical radar;pose estimation;position control;robot vision;SLAM (robots);solid modelling;stereo image processing;trees (mathematics);},
note = {high quality 3D reconstruction;3D model;3D triangular mesh;Gazebo simulation environment;actuated 3D LiDAR sensor;point-to-mesh features;robot position;R-LOAM;LiDAR odometry and mapping;3D reference object;axis-aligned-bounding-box-tree structure;open-source LOAM;AABB structure;mesh feature extraction;UAV 360&deg; scan capturing;absolute pose error;GPS;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3060413},
} 


@inproceedings{17737446 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Efficient Map Compression for Collaborative Visual SLAM},
journal = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings},
author = {Van Opdenbosch, D. and Aykut, T. and Alt, N. and Steinbach, E.},
year = {2018//},
pages = {992 - 1000},
address = {Los Alamitos, CA, USA},
abstract = {Swarm robotics is receiving increasing interest, because the collaborative completion of tasks, such as the exploration of unknown environments, leads to improved performance and reduced effort. The ability to exchange map information is an essential requirement for collaborative exploration. When moving to large-scale environments, where the communication data rate between the swarm participants is typically limited, efficient compression algorithms and an approach for discarding less informative parts of the map are key for a successful long-term operation. In this paper, we present a novel compression approach for environment maps obtained from a visual SLAM system. We apply feature coding to the visual information to compress the map efficiently. We make use of a minimum spanning tree to connect all features that serve as observations of a single map point. Thereby, we can exploit inter-feature dependencies and obtain an optimal coding order. Additionally, we add a map sparsification step to keep only useful map points by solving a linear integer programming problem, which preserves the map points that exhibit both good compression properties and high observability. We evaluate the proposed method on a standard dataset and show that our approach outperforms state-of-the-art techniques.},
keywords = {data compression;image coding;integer programming;linear programming;mobile robots;multi-robot systems;robot vision;SLAM (robots);trees (mathematics);},
note = {swarm robotics;map information;collaborative exploration;communication data rate;swarm participants;environment maps;visual SLAM system;visual information;minimum spanning tree;single map point;inter-feature dependencies;optimal coding order;map sparsification step;map compression;collaborative visual SLAM;linear integer programming problem;},
URL = {http://dx.doi.org/10.1109/WACV.2018.00114},
} 


@inproceedings{18955842 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Identifying robust landmarks in feature-based maps},
journal = {2019 IEEE Intelligent Vehicles Symposium (IV)},
author = {Berrio, J.S. and Ward, J. and Worrall, S. and Nebot, E.},
year = {2019//},
pages = {1166 - 72},
address = {Piscataway, NJ, USA},
abstract = {To operate in an urban environment, an automated vehicle must be capable of accurately estimating its position within a global map reference frame. This is necessary for optimal path planning and safe navigation. To accomplish this over an extended period of time, the global map requires long term maintenance. This includes the addition of newly observable features and the removal of transient features belonging to dynamic objects. The latter is especially important for the long-term use of the map as matching against a map with features that no longer exist can result in incorrect data associations, and consequently erroneous localisation. This paper addresses the problem of removing features from the map that correspond to objects that are no longer observable/present in the environment. This is achieved by assigning a single score which depends on the geometric distribution and characteristics when the features are re-detected (or not) on different occasions. Our approach not only eliminates ephemeral features, but can also be used as a reduction algorithm for highly dense maps. We tested our approach using half a year of weekly drives over the same 500 metre section of road in an urban environment. The results presented demonstrate the validity of the long term approach to map maintenance.},
keywords = {feature extraction;geometry;mobile robots;object detection;optimisation;path planning;robot vision;SLAM (robots);},
note = {feature-based maps;urban environment;automated vehicle;global map reference frame;optimal path planning;safe navigation;dynamic objects;geometric distribution;map maintenance;landmarks identification;autonomous vehicle;},
URL = {http://dx.doi.org/10.1109/IVS.2019.8814289},
} 


@inproceedings{21406958 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Evaluation of Long-term LiDAR Place Recognition},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Peltomaki, J. and Alijani, F. and Puura, J. and Huttunen, H. and Rahtu, E. and Kamarainen, J.-K.},
year = {2021//},
pages = {4487 - 92},
address = {Piscataway, NJ, USA},
abstract = {We compare a state-of-the-art deep image retrieval and a deep place recognition method for place recognition using LiDAR data. Place recognition aims to detect previously visited locations and thus provides an important tool for navigation, mapping, and localisation. Experimental comparisons are conducted using challenging outdoor and indoor datasets, Oxford Radar RobotCar and COLD, in the "long-term" setting where the test conditions differ substantially from the training and gallery data. Based on our results the image retrieval methods using LiDAR depth images can achieve accurate localization (the single best match recall 80%) within 5.00 m in urban outdoors. In office indoors the comparable accuracy is 50 cm but is more sensitive to changes in the environment.},
keywords = {feature extraction;image classification;image matching;image recognition;image retrieval;object recognition;optical radar;remote sensing by laser beam;robot vision;SLAM (robots);visual databases;},
note = {long-term LiDAR place recognition;state-of-the-art deep image retrieval;deep place recognition method;LiDAR data;place recognition aims;visited locations;experimental comparisons;challenging outdoor datasets;indoor datasets;Oxford Radar RobotCar;gallery data;image retrieval methods;LiDAR depth images;size 5.0 m;size 50.0 cm;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9636320},
} 


@inproceedings{16503977 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Checkout my map: version control for fleetwide visual localisation},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Gadd, M. and Newman, P.},
year = {2016//},
pages = {5729 - 36},
address = {Piscataway, NJ, USA},
abstract = {This paper is about underpinning long-term operations of fleets of vehicles using visual localisation. In particular it examines ways in which vehicles, considered as independent agents, can share, update and leverage each others' visual experiences in a mutually beneficial way. We draw on our previous work in Experience-based Navigation (EBN) [1], in which a visual map supporting multiple representations of the same place is built, yielding real-time localisation capability for a solitary vehicle. We now consider how any number of such agents might operate in concert via data sharing policies that are germane to the shared task of lifelong localisation. We rapidly construct considerable maps by the conjoining of work distributed to asynchronous processes, and share expertise amongst the team by the selective dispensing of mission-specific map contents. We demonstrate and evaluate our system against 100km of data collected in North Oxford over a period of a month featuring diverse deviation in appearance due to atmospheric, lighting, and structural dynamics. We show that our framework is capable of creating maps in a fraction of the time required by single-agent EBN, with no significant loss in localisation robustness, and is able to furnish robots on real-world forays with maps which require much less storage.},
keywords = {configuration management;mobile robots;navigation;robot dynamics;robot vision;SLAM (robots);vehicle dynamics;},
note = {robots;autonomous vehicle fleets;structural dynamics;visual map supporting;EBN;experience-based navigation;visual localisation;version control;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759843},
} 


@article{18626899 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using Scan Context Image},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Giseop Kim and Byungjae Park and Ayoung Kim},
volume = { 4},
number = { 2},
year = {2019/04/},
pages = {1948 - 55},
issn = {2377-3774},
address = {USA},
abstract = {In this letter, we present a long-term localization method that effectively exploits the structural information of an environment via an image format. The proposed method presents a robust year-round localization performance even when learned in just a single day. The proposed localizer learns a point cloud descriptor, named Scan Context Image (SCI), and performs robot localization on a grid map by formulating the place recognition problem as place classification using a convolutional neural network. Our method is faster than existing methods proposed for place recognition because it avoids a pairwise comparison between a query and scans in a database. In addition, we provide thorough validations using publicly available long-term datasets, the NCLT dataset and the Oxford RobotCar dataset, and show that the Scan Context Image (SCI) localization attains consistent performance over a year and outperforms existing methods.},
keywords = {image classification;learning (artificial intelligence);mobile robots;neural nets;optical radar;path planning;robot vision;},
note = {long-term LiDAR localization;long-term localization method;structural information;image format;point cloud descriptor;place recognition problem;place classification;convolutional neural network;robot localization;grid map;Oxford RobotCar dataset;scan context image localization;time 1 year;time 1 d;},
URL = {http://dx.doi.org/10.1109/LRA.2019.2897340},
} 


@article{15062845 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {A SLAM based on auxiliary marginalised particle filter and differential evolution},
journal = {International Journal of Systems Science},
journal = {Int. J. Syst. Sci. (UK)},
author = {Havangi, R. and Nekoui, M.A. and Teshnehlab, M. and Taghirad, H.D.},
volume = { 45},
number = { 9},
year = {2014//},
pages = {1913 - 26},
issn = {0020-7721},
address = {UK},
abstract = {FastSLAM is a framework for simultaneous localisation and mapping (SLAM) using a Rao-Blackwellised particle filter. In FastSLAM, particle filter is used for the robot pose (position and orientation) estimation, and parametric filter (i.e. EKF and UKF) is used for the feature location's estimation. However, in the long term, FastSLAM is an inconsistent algorithm. In this paper, a new approach to SLAM based on hybrid auxiliary marginalised particle filter and differential evolution (DE) is proposed. In the proposed algorithm, the robot pose is estimated based on auxiliary marginal particle filter that operates directly on the marginal distribution, and hence avoids performing importance sampling on a space of growing dimension. In addition, static map is considered as a set of parameters that are learned using DE. Compared to other algorithms, the proposed algorithm can improve consistency for longer time periods and also, improve the estimation accuracy. Simulations and experimental results indicate that the proposed algorithm is effective.},
keywords = {evolutionary computation;Kalman filters;mobile robots;nonlinear filters;particle filtering (numerical methods);pose estimation;robot vision;SLAM (robots);},
note = {simultaneous localisation-and-mapping;hybrid auxiliary marginalised particle filter;differential evolution;Rao-Blackwellised particle filter;robot pose estimation;position estimation;orientation estimation;parametric filter;EKF;UKF;feature location estimation;marginal distribution;static map;FastSLAM;},
URL = {http://dx.doi.org/10.1080/00207721.2012.759299},
} 


@inproceedings{18994624 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Obstacle Persistent Adaptive Map Maintenance for Autonomous Mobile Robots using Spatio-temporal Reasoning},
journal = {2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)},
author = {Pitschl, M.L. and Pryor, M.W.},
year = {2019//},
pages = {1023 - 8},
address = {Piscataway, NJ, USA},
abstract = {Mobile robotic systems operate in increasingly realistic scenarios even as users have increased expectations for the duration of autonomous tasks. Mobile robots face unique challenges when operating in environments that change over time, where systems must maintain an accurate representation of the environment with respect to both spatial and temporal dimensions. This paper describes a spatio-temporal technique for extending the autonomy of a mobile robot in a changing environment. This new technique called Obstacle Persistent Adaptive Map Maintenance (OPAMM) uses navigation data collected during normal operations to perform periodic self-maintenance of its environment model. OPAMM implements a probabilistic feature persistence model to predict the survival state of obstacles and update the world model. Maintaining an accurate world model is necessary for extending the long-term autonomy of robots in realistic scenarios. Results show that robots using OPAMM had localizations scores higher than other methods, thus reducing long-term localization degradation.},
keywords = {mobile robots;temporal reasoning;},
note = {changing environment;OPAMM;periodic self-maintenance;environment model;probabilistic feature persistence model;autonomous mobile robots;spatio-temporal reasoning;mobile robotic systems;autonomous tasks;spatial dimensions;temporal dimensions;spatio-temporal technique;obstacle persistent adaptive map maintenance;long-term localization degradation;},
URL = {http://dx.doi.org/10.1109/COASE.2019.8843095},
} 



