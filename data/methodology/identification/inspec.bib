@inproceedings{3471189 ,
language = {English},
copyright = {Copyright 1989, IEE},
title = {Characterising an indoor environment with a mobile robot and uncalibrated stereo},
journal = {Proceedings. 1989 IEEE International Conference on Robotics and Automation (Cat. No.89CH2750-8)},
author = {Sarachik, K.B.},
year = {1989//},
pages = {984 - 9},
address = {Washington, DC, USA},
abstract = {The author shows how it is possible for a mobile robot to exploit the visual information obtained by scanning a room to determine its size and shape, and to orient itself continually within it. The equipment used is a very simple camera setup whose detailed initial configuration is not known but can be deduced as the algorithm runs. The approach does not require any special environment, nor is it sensitive to changes in the physical aspect of the room being inspected such as moved furniture or roaming people. The long-term goal of the project is for the robot to use the information thus acquired in order to build maps of its environment, presumed to be a single floor of an office building, and to localize itself within this framework.},
keywords = {computer vision;computerised pattern recognition;computerised picture processing;mobile robots;},
note = {computer vision;unstructured environment;map-building;indoor environment;mobile robot;uncalibrated stereo;room;camera setup;},
URL = {http://dx.doi.org/10.1109/ROBOT.1989.100109},
} 


@inproceedings{18372740 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Laser Map Aided Visual Inertial Localization in Changing Environment},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Xiaqing Ding and Yue Wang and Dongxuan Li and Li Tang and Huan Yin and Rong Xiong},
year = {2018//},
pages = {4794 - 801},
address = {Piscataway, NJ, USA},
abstract = {Long-term visual localization in outdoor environment is a challenging problem, especially faced with the cross-seasonal, bi-directional tasks and changing environment. In this paper we propose a novel visual inertial localization framework that localizes against the LiDAR-built map. Based on the geometry information of the laser map, a hybrid bundle adjustment framework is proposed, which estimates the poses of the cameras with respect to the prior laser map as well as optimizes the state variables of the online visual inertial odometry system simultaneously. For more accurate crossmodal data association, the laser map is optimized using multisession laser and visual data to extract the salient and stable subset for visual localization. To validate the efficiency of the proposed method, we collect data in south part of our campus in different seasons, along the same and opposite-direction route. In all sessions of localization data, our proposed method gives satisfactory results, and shows the superiority of the hybrid bundle adjustment and map optimization<sup>1</sup>.},
keywords = {cameras;geometry;optical radar;optimisation;robot vision;SLAM (robots);},
note = {map optimization;changing environment;bi-directional tasks;LiDAR-built map;online visual inertial odometry system;laser map aided visual inertial localization;geometry information;crossmodal data association;multisession laser;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8593846},
} 


@inproceedings{21550585 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Vehicle Navigation by Visual Navigational Aids for Automatic Lunar Mission},
journal = {2021 IEEE 6th International Conference on Actual Problems of Unmanned Aerial Vehicles Development (APUAVD)},
author = {Ostroumov, I. and Kuzmenko, N.},
year = {2021//},
pages = {71 - 5},
address = {Piscataway, NJ, USA},
abstract = {Nowadays the question of Moon exploration is one of the key priorities. Many Lunar robotics missions are planned in near future by different space agencies around the world. Moon has considered to be the best place for a research station with long-term human presence for finding answers on fundamental questions about the universe. Automatic navigation of starship during a landing phase on Lunar surface is already solved with a help of inertial reference system aided visual algorithms. However, questions of automatic navigation of moving and flying vehicles on the Lunar surface are still open. Inertial navigation is limited by time, self-localization and mapping algorithms require multiple unique features of relief to guarantee required accuracy for successful automatic mission complication. In the current study, we propose the deployment of a network of visual navigational aids on the Lunar surface to support ground automatic missions. A weak atmosphere of the Moon makes effective visual beacons navigation system for long areas. A network of navigational aids includes primary and secondary ground stations which are blinking synchronously. Synchronization is supported by radio waves from the primary ground station. We consider the nature of crater relief to increase operational area of the system. The Time Difference of Arrival method is used to detect vehicle position by blinking network of visual navigational aids. In the numerical application, we consider different scenarios of network configuration to support automatic vehicle navigation inside of Tycho crater. Also, deployment of visual navigational aids network will increase the number of optical features which improve performance of already used positioning methods.},
keywords = {aerospace robotics;inertial navigation;lunar surface;mobile robots;path planning;position control;robot vision;space vehicle navigation;synchronisation;time-of-arrival estimation;},
note = {lunar surface;inertial reference system;visual algorithms;automatic navigation;inertial navigation;ground automatic missions;Moon exploration;automatic vehicle navigation;visual navigational aids network;long-term human presence;lunar robotics missions;automatic lunar mission;visual beacons navigation system;space agencies;starship;landing phase;flying vehicles;moving vehicles;self-localization and mapping;ground stations;synchronization;radio waves;time difference of arrival;vehicle position detection;network configuration;Tycho crater;optical features;positioning methods;},
URL = {http://dx.doi.org/10.1109/APUAVD53804.2021.9615417},
} 


@article{19744290 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Voxgraph: Globally Consistent, Volumetric Mapping using Signed Distance Function Submaps [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Reijgwart, V. and Millane, A. and Oleynikova, H. and Siegwart, R. and Cadena, C. and Nieto, J.},
year = {2020/04/27},
pages = {8 pp. - },
address = {USA},
abstract = {Globally consistent dense maps are a key requirement for long-term robot navigation in complex environments. While previous works have addressed the challenges of dense mapping and global consistency, most require more computational resources than may be available on-board small robots. We propose a framework that creates globally consistent volumetric maps on a CPU and is lightweight enough to run on computationally constrained platforms. Our approach represents the environment as a collection of overlapping Signed Distance Function (SDF) submaps, and maintains global consistency by computing an optimal alignment of the submap collection. By exploiting the underlying SDF representation, we generate correspondence free constraints between submap pairs that are computationally efficient enough to optimize the global problem each time a new submap is added. We deploy the proposed system on a hexacopter Micro Aerial Vehicle (MAV) with an Intel i7-8650U CPU in two realistic scenarios: mapping a large-scale area using a 3D LiDAR, and mapping an industrial space using an RGB-D camera. In the large-scale outdoor experiments, the system optimizes a 120x80m map in less than 4s and produces absolute trajectory RMSEs of less than 1m over 400m trajectories. Our complete system, called voxgraph, is available as open source. [IEEE Robotics and Automation Letters 5.1 (2019): 227-234 doi:10.1109/LRA.2019.2953859].},
keywords = {autonomous aerial vehicles;cameras;graph theory;microprocessor chips;microrobots;mobile robots;optical radar;path planning;robot vision;SLAM (robots);},
note = {voxgraph;absolute trajectory RMSE;large-scale outdoor experiments;RGB-D camera;industrial space;3D LiDAR;large-scale area;Intel i7-8650U CPU;hexacopter micro aerial vehicle;SDF representation;signed distance function submaps;submap pairs;submap collection;computationally constrained platforms;globally consistent volumetric maps;computational resources;global consistency;dense mapping;long-term robot navigation;globally consistent dense maps;volumetric mapping;},
} 


@inproceedings{16503977 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Checkout my map: version control for fleetwide visual localisation},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Gadd, M. and Newman, P.},
year = {2016//},
pages = {5729 - 36},
address = {Piscataway, NJ, USA},
abstract = {This paper is about underpinning long-term operations of fleets of vehicles using visual localisation. In particular it examines ways in which vehicles, considered as independent agents, can share, update and leverage each others' visual experiences in a mutually beneficial way. We draw on our previous work in Experience-based Navigation (EBN) [1], in which a visual map supporting multiple representations of the same place is built, yielding real-time localisation capability for a solitary vehicle. We now consider how any number of such agents might operate in concert via data sharing policies that are germane to the shared task of lifelong localisation. We rapidly construct considerable maps by the conjoining of work distributed to asynchronous processes, and share expertise amongst the team by the selective dispensing of mission-specific map contents. We demonstrate and evaluate our system against 100km of data collected in North Oxford over a period of a month featuring diverse deviation in appearance due to atmospheric, lighting, and structural dynamics. We show that our framework is capable of creating maps in a fraction of the time required by single-agent EBN, with no significant loss in localisation robustness, and is able to furnish robots on real-world forays with maps which require much less storage.},
keywords = {configuration management;mobile robots;navigation;robot dynamics;robot vision;SLAM (robots);vehicle dynamics;},
note = {robots;autonomous vehicle fleets;structural dynamics;visual map supporting;EBN;experience-based navigation;visual localisation;version control;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759843},
} 


@inproceedings{10014880 ,
language = {English},
copyright = {Copyright 2008, The Institution of Engineering and Technology},
title = {Active SLAM in structured environments},
journal = {2008 IEEE International Conference on Robotics and Automation. The Half-Day Workshop on: Towards Autonomous Agriculture of Tomorrow},
author = {Leung, C. and Shoudong Huang and Dissanayake, G.},
year = {2008//},
pages = {1898 - 903},
address = {Piscataway, NJ, USA},
abstract = {This paper considers the trajectory planning problem for line-feature based SLAM in structured indoor environments. The robot poses and line features are estimated using smooth and mapping (SAM) which is found to provide more consistent estimates than the extended Kalman filter (EKF) The objective of trajectory planning is to minimise the uncertainty of the estimates and to maximise coverage. Trajectory planning is performed using model predictive control (MPC) with an attractor incorporating long term goals. This planning is demonstrated both in simulation and in a real-time experiment with a Pioneer2DX robot.},
keywords = {Kalman filters;path planning;predictive control;SLAM (robots);},
note = {active SLAM;structured indoor environments;trajectory planning problem;extended Kalman filter;model predictive control;Pioneer2DX robot;},
} 


@inproceedings{17357794 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {A hybrid topological mapping and navigation method for large area robot mapping},
journal = {2017 56th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE). Proceedings},
author = {Ravankar, A.A. and Ravankar, A. and Emaru, T. and Kobayashi, Y.},
year = {Sept. 2017},
pages = {1104 - 7},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we present a hybrid topological mapping and navigation method for mobile robots. The proposed method combines metric and topological information to create map and generate navigation plan for the robot. As compared to traditional approaches of robot mapping, the method is lightweight and can be used for mapping and navigation in large areas which is particularly useful for service robots operating in large buildings. The method only uses local information for navigation while maintaining the global topological graph nodes. The topological nodes are used effectively for navigation and can also be used to store semantic information of the scene such as robot poses, scans and scene properties for complete long term robot autonomy. By combining the information from the two maps (topological and grid map), autonomous navigation and mapping in large areas for robots is possible.},
keywords = {graph theory;mobile robots;path planning;robot vision;service robots;SLAM (robots);},
note = {hybrid topological mapping;mobile robots;topological information;navigation plan;service robots;topological grid map;large area robot mapping;navigation method;topological graph nodes;long term robot autonomy;map creation;},
URL = {http://dx.doi.org/10.23919/SICE.2017.8105770},
} 


@article{16660801 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Exploration and mapping technique suited for visual-features based localization of MAVs},
journal = {Journal of Intelligent &amp; Robotic Systems},
journal = {J. Intell. Robot. Syst. (Germany)},
author = {Chudoba, J. and Kulich, M. and Saska, M. and Baa, T. and Preuil, L.},
volume = { 84},
number = { 1-4},
year = {2016/12/},
pages = {351 - 69},
issn = {0921-0296},
address = {Germany},
abstract = {An approach for long term localization, stabilization, and navigation of micro-aerial vehicles (MAVs) in unknown environment is presented in this paper. The proposed method relies strictly on onboard sensors of employed MAVs and does not require any external positioning system. The core of the method consists in extraction of information from pictures consequently captured using a camera carried by the particular MAV. Visual features are obtained from images of the surface under the MAV, and stored into a map that is represented by these features. The position of the MAV is then obtained through matching with previously stored features. An important part of the proposed system is a novel approach for exploration and mapping of the workspace of robots. This method enables efficient exploring of the unknown environment, while keeping the iteratively built map of features consistent. The proposed algorithm is suitable for mapping of surfaces, both outdoor and indoor, with various density of the image features. The sufficient precision and long term persistence of the method allows its utilization for stabilization of large MAV groups that work in formations with small relative distances between particular vehicles. Numerous experiments with quadrotor helicopters and various numerical simulations have been realized for verification of the entire system and its components.},
keywords = {autonomous aerial vehicles;feature extraction;microrobots;SLAM (robots);},
note = {robot workspace;image features;microaerial vehicles;MAVs;visual-features based localization;mapping technique;exploration technique;},
URL = {http://dx.doi.org/10.1007/s10846-016-0358-8},
} 


@article{21226521 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Direct RGB-D Visual Odometry Based on Hybrid Strategy},
journal = {IEEE Sensors Journal},
journal = {IEEE Sens. J. (USA)},
author = {Jiyuan Cai and Lingkun Luo and Qiuyu Yu and Bing Liu and Shiqiang Hu},
volume = { 21},
number = { 20},
year = {2021//},
pages = {23278 - 23288},
issn = {1530-437X},
address = {USA},
abstract = {Edge-based Direct Visual Odometry (E-DVO) plays a crucial role in robot navigation across the low-texture and rich-texture scenes, however, two essential factors are overlooked in the traditional E-DVO methods. (1) Traditional E-DVO methods seriously rely on photometric or geometric cost, thereby generating the non-robust performance under the light changing or structure-less conditions; (2) EDVO methods generally suffer drift issue mainly derived from inaccurate rotation estimation for the long term visual odometry task. In this article, a novel hybrid cost function leveraging the photometric and geometric cost within a bi-direction framework is proposed to facilitate the addressed the former issue. While the latter issue is approached through hybridization of a simple yet effective switching strategy which can guarantee both robustness and accuracy by combining the global Manhattan model and direct edge alignment. We carry out various experiments on TUM RGB-D and ICL-NUIM datasets for performance evaluation. Results show that our method has the advantage of strong robustness and high accuracy compared with state-of-the-art methods, e.g., Canny-VO and ORB-SLAM2.},
keywords = {C++ language;cameras;distance measurement;edge detection;image colour analysis;image sequences;image texture;mobile robots;public domain software;robot vision;SLAM (robots);},
note = {hybrid strategy;robot navigation;low-texture;rich-texture scenes;photometric cost;geometric cost;nonrobust performance;drift issue;rotation estimation;long term visual odometry task;novel hybrid cost function;bi-direction framework;hybridization;simple yet effective switching strategy;robustness;direct edge alignment;edge-based direct visual odometry;Direct RGB-D visual odometry;},
URL = {http://dx.doi.org/10.1109/JSEN.2021.3109413},
} 


@inproceedings{21413695 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {MP3: A Unified Model to Map, Perceive, Predict and Plan},
journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
author = {Casas, S. and Sadat, A. and Urtasun, R.},
year = {2021//},
pages = {14398 - 14407},
address = {Piscataway, NJ, USA},
abstract = {High-definition maps (HD maps) are a key component of most modern self-driving systems due to their valuable semantic and geometric information. Unfortunately, building HD maps has proven hard to scale due to their cost as well as the requirements they impose in the localization system that has to work everywhere with centimeter-level accuracy. Being able to drive without an HD map would be very beneficial to scale self-driving solutions as well as to increase the failure tolerance of existing ones (e.g., if localization fails or the map is not up-to-date). Towards this goal, we propose MP3, an end-to-end approach to mapless<sup>1</sup> driving where the input is raw sensor data and a high-level command (e.g., turn left at the intersection). MP3 predicts intermediate representations in the form of an online map and the current and future state of dynamic agents, and exploits them in a novel neural motion planner to make interpretable decisions taking into account uncertainty. We show that our approach is significantly safer, more comfortable, and can follow commands better than the baselines in challenging long-term closed-loop simulations, as well as when compared to an expert driver in a large-scale real-world dataset.},
keywords = {closed loop systems;collision avoidance;intelligent transportation systems;mobile robots;neural nets;robot vision;SLAM (robots);traffic engineering computing;},
note = {MP3;high-definition maps;HD map;modern self-driving systems;valuable semantic information;geometric information;localization system;centimeter-level accuracy;self-driving solutions;end-to-end approach;high-level command;online map;mapless driving;raw sensor data;dynamic agents;neural motion planner;},
URL = {http://dx.doi.org/10.1109/CVPR46437.2021.01417},
} 


@article{20399279 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {R-LOAM: improving LiDAR odometry and mapping with point-to-mesh features of a known 3D reference object},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Oelsch, M. and Karimi, M. and Steinbach, E.},
volume = { 6},
number = { 2},
year = {2021/04/},
pages = {2068 - 75},
issn = {2377-3766},
address = {USA},
abstract = {LiDAR-based odometry and mapping is used in many robotic applications to retrieve the robot's position in an unknown environment and allows for autonomous operation in GPS-denied (e.g., indoor) environments. With a 3D LiDAR sensor, highly accurate localization becomes possible, which enables high quality 3D reconstruction of the environment. In this letter we extend the well-known LOAM framework by leveraging prior knowledge about a reference object in the environment to further improve the localization accuracy. This requires a known 3D model of the reference object and its known position in a global coordinate frame. Instead of only relying on the point features in the mapping module of LOAM, we also include mesh features extracted from the 3D triangular mesh of the reference object in the optimization problem. For fast correspondence computation of mesh features, we use the Axis-Aligned-Bounding-Box-Tree (AABB) structure. Essentially, our approach not only makes use of the previously built map for absolute localization in the environment, but also takes the relative position to the reference object into account, effectively reducing long-term drift. To validate the proposed concept, we generated datasets using the Gazebo simulation environment in exemplary visual inspection scenarios of an airplane inside a hangar and the Eiffel Tower. An actuated 3D LiDAR sensor is mounted via a 1-DoF gimbal on a UAV capturing 360&deg; scans. We benchmark our approach against the state-of-the-art open-source LOAM framework. The results show that the proposed joint optimization using both point and mesh features yields a significant reduction in Absolute Pose Error (APE) and therefore improves the map and 3D reconstruction quality during long-term operations.},
keywords = {autonomous aerial vehicles;distance measurement;feature extraction;Global Positioning System;image reconstruction;mesh generation;mobile robots;optical radar;pose estimation;position control;robot vision;SLAM (robots);solid modelling;stereo image processing;trees (mathematics);},
note = {high quality 3D reconstruction;3D model;3D triangular mesh;Gazebo simulation environment;actuated 3D LiDAR sensor;point-to-mesh features;robot position;R-LOAM;LiDAR odometry and mapping;3D reference object;axis-aligned-bounding-box-tree structure;open-source LOAM;AABB structure;mesh feature extraction;UAV 360&deg; scan capturing;absolute pose error;GPS;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3060413},
} 


@article{18943010 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Visual localization using sparse semantic 3D map [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Tianxin Shi and Shuhan Shen and Xiang Gao and Lingjie Zhu},
year = {2019/04/07},
pages = {5 pp. - },
address = {USA},
abstract = {Accurate and robust visual localization under a wide range of viewing condition variations including season and illumination changes, as well as weather and day-night variations, is the key component for many computer vision and robotics applications. Under these conditions, most traditional methods would fail to locate the camera. In this paper we present a visual localization algorithm that combines structure-based method and image-based method with semantic information. Given semantic information about the query and database images, the retrieved images are scored according to the semantic consistency of the 3D model and the query image. Then the semantic matching score is used as weight for RANSAC's sampling and the pose is solved by a standard PnP solver. Experiments on the challenging long-term visual localization benchmark dataset demonstrate that our method has significant improvement compared with the state-of-the-arts.},
keywords = {cameras;computer vision;feature extraction;image matching;image retrieval;image sensors;mobile robots;pose estimation;robot vision;},
note = {robust visual localization;sparse semantic 3D map;long-term visual localization benchmark dataset;semantic matching score;query image;semantic consistency;retrieved images;database images;given semantic information;image-based method;structure-based method;visual localization algorithm;robotics applications;computer vision;day-night variations;weather;illumination changes;condition variations;},
} 


@article{20109021 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Season-Invariant and Viewpoint-Tolerant LiDAR Place Recognition in GPS-Denied Environments},
journal = {IEEE Transactions on Industrial Electronics},
journal = {IEEE Trans. Ind. Electron. (USA)},
author = {Fengkui Cao and Fei Yan and Sen Wang and Yan Zhuang and Wei Wang},
volume = { 68},
number = { 1},
year = {2021/01/},
pages = {563 - 74},
issn = {0278-0046},
address = {USA},
abstract = {Place recognition remains a challenging problem under various perceptual conditions, e.g., all weather, times of day, seasons, and viewpoint shifts. Different from most of the existing place recognition methods using pure vision, this article studies light detection and ranging (LiDAR) based approaches. Point clouds have some benefits for place recognition since they do not suffer from illumination changes. On the other hand, they are dramatically affected by structural changes from different viewpoints or across seasons. In this article, a novel LiDAR-based place recognition system is proposed to achieve long-term robust localization, even under severe seasonal changes and viewpoint shifts. To improve the efficiency, a compact cylindrical image model is designed to convert three-dimensional point clouds to two-dimensional images representing the prominent geometric relationships of scenes. The contexts (buildings, trees, road structures, etc.) of scenes are utilized for efficient place recognition. A sequence-based temporal consistency check is also introduced for postverification. Extensive real experiments on three datasets (Oxford RobotCar [1], NCLT [2], and DUT-AS) show that the proposed system outperforms both state-of-the-art visual and LiDAR-based methods, verifying its robust performance in challenging scenarios.},
keywords = {feature extraction;image classification;image sensors;mobile robots;object recognition;optical radar;robot vision;SLAM (robots);visual databases;},
note = {LiDAR-based methods;sequence-based temporal consistency check;efficient place recognition;three-dimensional point clouds;compact cylindrical image model;severe seasonal changes;long-term robust localization;novel LiDAR-based place recognition system;structural changes;illumination changes;article studies light detection;existing place recognition methods;viewpoint shifts;GPS-denied environments;viewpoint-tolerant LiDAR place recognition;season-invariant;},
URL = {http://dx.doi.org/10.1109/TIE.2019.2962416},
} 


@article{21472174 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {High-precision and robust localization system for mobile robots in complex and large-scale indoor scenes},
journal = {International Journal of Advanced Robotic Systems},
journal = {Int. J. Adv. Robot. Syst. (USA)},
author = {Jibo Wang and Chengpeng Li and Bangyu Li and Chenglin Pang and Zheng Fang},
volume = { 18},
number = { 5},
year = {2021//},
pages = {17298814211047690 (16 pp.) - },
issn = {1729-8814},
address = {USA},
abstract = {High-precision and robust localization is the key issue for long-term and autonomous navigation of mobile robots in industrial scenes. In this article, we propose a high-precision and robust localization system based on laser and artificial landmarks. The proposed localization system is mainly composed of three modules, namely scoring mechanism-based global localization module, laser and artificial landmark-based localization module, and relocalization trigger module. Global localization module processes the global map to obtain the map pyramid, thus improve the global localization speed and accuracy when robots are powered on or kidnapped. Laser and artificial landmark-based localization module is employed to achieve robust localization in highly dynamic scenes and high-precision localization in target areas. The relocalization trigger module is used to monitor the current localization quality in real time by matching the current laser scan with the global map and feeds it back to the global localization module to improve the robustness of the system. Experimental results show that our method can achieve robust robot localization and real-time detection of the current localization quality in indoor scenes and industrial environment. In the target area, the position error is less than 0.004 m and the angle error is less than 0.01 rad.},
keywords = {automatic guided vehicles;mobile robots;navigation;position control;robust control;velocity control;},
note = {localization quality;robust localization system;mobile robots;large-scale indoor scenes;mechanism-based global localization module;artificial landmark-based localization module;relocalization trigger module;global localization speed;highly dynamic scenes;high-precision localization;autonomous navigation;map pyramid;industrial environment;target area;AGV;automated guided vehicle;},
URL = {http://dx.doi.org/10.1177/17298814211047690},
} 


@article{20685236 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {kRadar++: Coarse-to-Fine FMCW Scanning Radar Localisation},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {De Martini, D. and Gadd, M. and Newman, P.},
volume = { 20},
number = { 21},
year = {2020/11/},
pages = {6002 (23 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {This paper presents a novel two-stage system which integrates topological localisation candidates from a radar-only place recognition system with precise pose estimation using spectral landmark-based techniques. We prove that the-recently available-seminal radar place recognition (RPR) and scan matching sub-systems are complementary in a style reminiscent of the mapping and localisation systems underpinning visual teach-and-repeat (VTR) systems which have been exhibited robustly in the last decade. Offline experiments are conducted on the most extensive radar-focused urban autonomy dataset available to the community with performance comparing favourably with and even rivalling alternative state-of-the-art radar localisation systems. Specifically, we show the long-term durability of the approach and of the sensing technology itself to autonomous navigation. We suggest a range of sensible methods of tuning the system, all of which are suitable for online operation. For both tuning regimes, we achieve, over the course of a month of localisation trials against a single static map, high recalls at high precision, and much reduced variance in erroneous metric pose estimation. As such, this work is a necessary first step towards a radar teach-and-repeat (RTR) system and the enablement of autonomy across extreme changes in appearance or inclement conditions.},
keywords = {mobile robots;path planning;pose estimation;robot vision;SLAM (robots);},
note = {erroneous metric pose estimation;coarse-to-fine FMCW;two-stage system;topological localisation candidates;radar-only place recognition system;precise pose estimation;spectral landmark-based techniques;scan matching sub-systems;teach-and-repeat systems;extensive radar-focused urban autonomy dataset;rivalling alternative state-of-the-art radar localisation systems;localisation trials;},
URL = {http://dx.doi.org/10.3390/s20216002},
} 


@inproceedings{18955842 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Identifying robust landmarks in feature-based maps},
journal = {2019 IEEE Intelligent Vehicles Symposium (IV)},
author = {Berrio, J.S. and Ward, J. and Worrall, S. and Nebot, E.},
year = {2019//},
pages = {1166 - 72},
address = {Piscataway, NJ, USA},
abstract = {To operate in an urban environment, an automated vehicle must be capable of accurately estimating its position within a global map reference frame. This is necessary for optimal path planning and safe navigation. To accomplish this over an extended period of time, the global map requires long term maintenance. This includes the addition of newly observable features and the removal of transient features belonging to dynamic objects. The latter is especially important for the long-term use of the map as matching against a map with features that no longer exist can result in incorrect data associations, and consequently erroneous localisation. This paper addresses the problem of removing features from the map that correspond to objects that are no longer observable/present in the environment. This is achieved by assigning a single score which depends on the geometric distribution and characteristics when the features are re-detected (or not) on different occasions. Our approach not only eliminates ephemeral features, but can also be used as a reduction algorithm for highly dense maps. We tested our approach using half a year of weekly drives over the same 500 metre section of road in an urban environment. The results presented demonstrate the validity of the long term approach to map maintenance.},
keywords = {feature extraction;geometry;mobile robots;object detection;optimisation;path planning;robot vision;SLAM (robots);},
note = {feature-based maps;urban environment;automated vehicle;global map reference frame;optimal path planning;safe navigation;dynamic objects;geometric distribution;map maintenance;landmarks identification;autonomous vehicle;},
URL = {http://dx.doi.org/10.1109/IVS.2019.8814289},
} 


@inproceedings{21130575 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Robust Method for Static 3D Point Cloud Map Building using Multi-View Images with Multi-Resolution},
journal = {2021 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
author = {Chen Yao and Hu Zhu and Shipeng Lv and Dingyuan Zhang and Zhenzhong Jia},
year = {2021//},
pages = {782 - 7},
address = {Piscataway, NJ, USA},
abstract = {Robots can perform various missions in multiple changing environments. The dynamic objects have significant influence on the long-term autonomy and 3D map construction, because &ldquo;ghost tracks&rdquo; inevitably exist due to the continuous error-accumulation of the input data. So it is critical to keep only static subsets and exclude noisy obstacles to mitigate the influence on mapping and navigation. We propose a robust static map building method, which compares the discrepancies between single scan data against the noisy map. This method focuses on the advantages of most dynamic objects of different views with unique attribution and will be easily detected in these views. Accordingly, we present the novel &ldquo;Multi-View and Multi-Resolution&rdquo; image-based method with BEV-RV (Bird's Eye View-Range View) modules to discriminate static/dynamic point clouds. Through two stages of iteration with different image window sizes of point level, we first collect more static points of some inevitably wrong judgments and then remove such completely unreliable dynamic points at a later stage. Experimental evaluations are conducted by using the KITTI dataset as ground truth. Qualitative analysis indicates that the proposed method is robust and reliable against state-of-the-art methodsin some dynamic regions.},
keywords = {image denoising;image resolution;iterative methods;mobile robots;navigation;object detection;robot vision;SLAM (robots);stereo image processing;},
note = {noisy map;dynamic objects;dynamic point clouds;dynamic regions;robust method;static 3D point cloud map building;MultiView images;multiple changing environments;long-term autonomy;3D map construction;continuous error-accumulation;static subsets;noisy obstacles;navigation;robust static map building method;single scan data;mage window sizes;Bird's Eye View-Range View;multiresolution image-based method;ghost tracks;BEV-RV;KITTI dataset;},
URL = {http://dx.doi.org/10.1109/RCAR52367.2021.9517646},
} 


@inproceedings{15686682 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Loop closure detection by compressed sensing for exploration of mobile robots in outdoor environments},
journal = {2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM). Proceedings},
author = {Ravari, A.N. and Taghirad, H.D.},
year = {2015//},
pages = {511 - 16},
address = {Piscataway, NJ, USA},
abstract = {In the problem of simultaneously localization and mapping (SLAM) for a mobile robot, it is required to detect previously visited locations so the estimation error shall be reduced. Sensor observations are compared by a similarity metric to detect loops. In long term navigation or exploration, the number of observations increases and so the complexity of the loop closure detection. Several techniques are proposed in order to reduce the complexity of loop closure detection. Few algorithms have considered the loop closure detection from a subset of sensor observations. In this paper, the compressed sensing approach is exploited to detect loops from few sensor measurements. In the basic compressed sensing it is assumed that a signal has a sparse representation is a basis which means that only a few elements of the signal are non-zero. Based on the compressed sensing approach a sparse signal can be recovered from few linear noisy projections by l<sub>1</sub> minimization. The difference matrix which is widely used for loop detection has a sparse structure, where similar observations are shown by zero distance and different locations are indicated by ones. Based on the multiple measurement vector technique which is an extension of the basic compressed sensing, the loop closure detection is performed by comparison of few sensor observations. The applicability of the proposed algorithm is investigated in some outdoor environments through some publicly available data sets. It has been shown by some experiments that the proposed method can detect loops effectively.},
keywords = {compressed sensing;matrix algebra;mobile robots;object detection;robot vision;SLAM (robots);},
note = {loop closure detection;compressed sensing;mobile robots;SLAM;simultaneously localization and mapping;sensor observation;sensor measurement;signal representation;difference matrix;l<sub>1</sub> minimization;},
URL = {http://dx.doi.org/10.1109/ICRoM.2015.7367836},
} 


@article{19687591 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Onboard detection-tracking-localization},
journal = {IEEE/ASME Transactions on Mechatronics},
journal = {IEEE/ASME Trans. Mechatron. (USA)},
author = {Dengqing Tang and Qiang Fang and Lincheng Shen and Tianjiang Hu},
volume = { 25},
number = { 3},
year = {2020/06/},
pages = {1555 - 65},
issn = {1083-4435},
address = {USA},
abstract = {This article investigates long-term positioning of moving objects by monocular vision of a miniature fixed-wing unmanned aerial vehicle. It is challenging to perform a real-time onboard vision processing task, due to the strict payload capacity and power budget limitations of microflying vehicles. We propose a parallel onboard architecture that explicitly decouples the long-term positioning task into iteratively operated detection, tracking, and localization. The proposed approach is eventually called onboard detection-tracking-localization, namely oDTL. The detector automatically extracts and identifies the object from image frames captured at in-flight durations. A learning-based network is constructed to improve detection accuracy and robustness against ever-changing outdoor illumination conditions and flying viewpoints. The tracker follows the object within specified region-of-interest from frame to frame with lower computing consumption. To further reduce target-losing rate, a concept of blind zone is proposed and applied, and its boundaries in sequential images are also theoretically inferred. The position estimator maps the flying vehicle pose, the image coordinates, and calibration specifications into real-world positions of the moving target. An extended Kalman filter is developed for rough position estimation, and a smooth module is introduced for the refinement of the position. Three offline comparative experiments and three online experiments have been conducted respectively to testify the real-time capability of our approach. The collected experimental results also demonstrate the feasible accuracy and robustness of the overall solution within the specified flying onboard scenarios.},
keywords = {aerospace components;aircraft control;autonomous aerial vehicles;calibration;feature extraction;image capture;image sensors;image sequences;Kalman filters;learning (artificial intelligence);mobile robots;nonlinear filters;object detection;position control;robot vision;target tracking;},
note = {onboard detection-tracking-localization;monocular vision;miniature fixed-wing unmanned aerial vehicle;vision processing task;strict payload capacity;power budget limitations;microflying vehicles;parallel onboard architecture;long-term positioning task;iteratively operated detection;image frames;robustness;outdoor illumination conditions;position estimator maps;flying vehicle;rough position estimation;learning-based network;image sequences;extended Kalman filter;region-of-interest;},
URL = {http://dx.doi.org/10.1109/TMECH.2020.2976794},
} 


@article{20562768 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {SGC-VSLAM: A Semantic and Geometric Constraints VSLAM for Dynamic Indoor Environments},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Shiqiang Yang and Guohao Fan and Lele Bai and Cheng Zhao and Dexin Li},
volume = { 20},
number = { 8},
year = {2020//},
pages = {2432 (20 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {As one of the core technologies for autonomous mobile robots, Visual Simultaneous Localization and Mapping (VSLAM) has been widely researched in recent years. However, most state-of-the-art VSLAM adopts a strong scene rigidity assumption for analytical convenience, which limits the utility of these algorithms for real-world environments with independent dynamic objects. Hence, this paper presents a semantic and geometric constraints VSLAM (SGC-VSLAM), which is built on the RGB-D mode of ORB-SLAM2 with the addition of dynamic detection and static point cloud map construction modules. In detail, a novel improved quadtree-based method was adopted for SGC-VSLAM to enhance the performance of the feature extractor in ORB-SLAM (Oriented FAST and Rotated BRIEF-SLAM). Moreover, a new dynamic feature detection method called semantic and geometric constraints was proposed, which provided a robust and fast way to filter dynamic features. The semantic bounding box generated by YOLO v3 (You Only Look Once, v3) was used to calculate a more accurate fundamental matrix between adjacent frames, which was then used to filter all of the truly dynamic features. Finally, a static point cloud was estimated by using a new drawing key frame selection strategy. Experiments on the public TUM RGB-D (Red-Green-Blue Depth) dataset were conducted to evaluate the proposed approach. This evaluation revealed that the proposed SGC-VSLAM can effectively improve the positioning accuracy of the ORB-SLAM2 system in high-dynamic scenarios and was also able to build a map with the static parts of the real environment, which has long-term application value for autonomous mobile robots.},
keywords = {feature extraction;image colour analysis;mobile robots;quadtrees;robot vision;SLAM (robots);},
note = {SGC-VSLAM;dynamic indoor environments;autonomous mobile robots;state-of-the-art VSLAM;strong scene rigidity assumption;independent dynamic objects;semantic constraints VSLAM;geometric constraints VSLAM;dynamic detection;static point cloud map construction modules;dynamic feature detection method;semantic bounding box;truly dynamic features;ORB-SLAM2 system;high-dynamic scenarios;},
URL = {http://dx.doi.org/10.3390/s20082432},
} 


@inproceedings{19315104 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {FLAME: Feature-Likelihood Based Mapping and Localization for Autonomous Vehicles},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Su Pang and Kent, D. and Morris, D. and Radha, H.},
year = {2019//},
pages = {5312 - 19},
address = {Piscataway, NJ, USA},
abstract = {Accurate vehicle localization is arguably the most critical and fundamental task for autonomous vehicle navigation. While dense 3D point-cloud-based maps enable precise localization, they impose significant storage and transmission burdens when used in city-scale environments. In this paper, we propose a highly compressed representation for LiDAR maps, along with an efficient and robust real-time alignment algorithm for on-vehicle LiDAR scans. The proposed mapping framework, which we refer to as Feature Likelihood Acquisition Map Emulation (FLAME), requires less than 0.1% of the storage space of the original 3D point cloud map. In essence, FLAME emulates an original map through feature likelihood functions. In particular, FLAME models planar, pole and curb features. These three feature classes are long-term stable, distinct and common among vehicular roadways. Multiclass feature points are extracted from LiDAR scans through feature detection. A new multiclass-based point-to-distribution alignment method is proposed to find the association and alignment between the multiclass feature points and the FLAME map. The experimental results show that the proposed framework can achieve the same level of accuracy (less than 10cm) as the 3D point cloud based localization.},
keywords = {feature extraction;mobile robots;object detection;optical radar;path planning;road vehicles;robot vision;},
note = {feature likelihood acquisition map emulation;3D point-cloud-based maps;feature-likelihood based mapping;3D point cloud based localization;FLAME map;multiclass-based point-to-distribution alignment method;feature detection;multiclass feature points;feature likelihood functions;storage space;on-vehicle LiDAR scans;real-time alignment algorithm;LiDAR maps;city-scale environments;transmission burdens;autonomous vehicle navigation;vehicle localization;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8968082},
} 


@article{19487997 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {3D LiDAR-based global localization using siamese neural network},
journal = {IEEE Transactions on Intelligent Transportation Systems},
journal = {IEEE Trans. Intell. Transp. Syst. (USA)},
author = {Huan Yin and Yue Wang and Xiaqing Ding and Li Tang and Shoudong Huang and Rong Xiong},
volume = { 21},
number = { 4},
year = {2020/04/},
pages = {1380 - 92},
issn = {1524-9050},
address = {USA},
abstract = {Global localization in 3D point clouds is a challenging task for mobile vehicles in outdoor scenarios, which requires the vehicle to localize itself correctly in a given map without prior knowledge of its pose. This is a critical component of autonomous vehicles or robots on the road for handling localization failures. In this paper, based on reduced dimension scan representations learned from neural networks, a solution to global localization is proposed by achieving place recognition first and then metric pose estimation in the global prior map. Specifically, we present a semi-handcrafted feature learning method for 3D Light detection and ranging (LiDAR) point clouds using artificial statistics and siamese network, which transforms the place recognition problem into a similarity modeling problem. Additionally, the sensor data using dimension reduced representations require less storage space and make the searching easier. With the learned representations by networks and the global poses, a prior map is built and used in the localization framework. In the localization step, position only observations obtained by place recognition are used in a particle filter algorithm to achieve precise pose estimation. To demonstrate the effectiveness of our place recognition and localization approach, KITTI benchmark and our multi-session datasets are employed for comparison with other geometric-based algorithms. The results show that our system can achieve both high accuracy and efficiency for long-term autonomy.},
keywords = {feature extraction;image filtering;image representation;learning (artificial intelligence);mobile robots;neural nets;optical radar;particle filtering (numerical methods);pose estimation;robot vision;SLAM (robots);},
note = {3D point clouds;mobile vehicles;outdoor scenarios;autonomous vehicles;localization failures;reduced dimension scan representations;neural networks;global prior map;feature learning method;3D light detection;siamese network;place recognition problem;similarity modeling problem;dimension reduced representations;learned representations;global poses;localization framework;localization step;localization approach;3D LiDAR-based global localization;siamese neural network;},
URL = {http://dx.doi.org/10.1109/TITS.2019.2905046},
} 


@inproceedings{18903966 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A Linear-Complexity EKF for Visual-Inertial Navigation with Loop Closures},
journal = {2019 International Conference on Robotics and Automation (ICRA)},
author = {Geneva, P. and Eckenhoff, K. and Guoquan Huang},
year = {2019//},
pages = {3535 - 41},
address = {Piscataway, NJ, USA},
abstract = {Enabling real-time visual-inertial navigation in unknown environments while achieving bounded-error performance holds great potentials in robotic applications. To this end, in this paper, we propose a novel linear-complexity EKF for visual-inertial localization, which can efficiently utilize loop closure constraints, thus allowing for long-term persistent navigation. The key idea is to adapt the Schmidt-Kalman formulation within the multi-state constraint Kalman filter (MSCKF) framework, in which we selectively include keyframes as nuisance parameters in the state vector for loop closures but do not update their estimates and covariance in order to save computations while still tracking their cross-correlations with the current navigation states. As a result, the proposed Schmidt-MSCKF has only O(n) computational complexity while still incorporating loop closures into the system. The proposed approach is validated extensively on large-scale real-world experiments, showing significant performance improvements when compared to the standard MSCKF, while only incurring marginal computational overhead.},
keywords = {computational complexity;correlation methods;inertial navigation;Kalman filters;mobile robots;nonlinear filters;robot vision;SLAM (robots);},
note = {Schmidt-MSCKF;real-time visual-inertial navigation;bounded-error performance;robotic applications;visual-inertial localization;loop closure constraints;long-term persistent navigation;Schmidt-Kalman formulation;multistate constraint Kalman filter framework;state vector;linear-complexity EKF;cross-correlations;navigation states;computational complexity;performance improvement;},
URL = {http://dx.doi.org/10.1109/ICRA.2019.8793836},
} 


@inproceedings{15798733 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Underwater Robot Visual Place Recognition in the Presence of Dramatic Appearance Change},
journal = {OCEANS 2015 - MTS/IEEE Washington},
author = {Jie Li and Eustice, R.M. and Johnson-Roberson, M.},
year = {2015//},
pages = {385 - 90},
address = {Piscataway, NJ, USA},
abstract = {This paper reports on an algorithm for underwater visual place recognition in the presence of dramatic appearance change. Long-term visual place recognition is challenging underwater due to biofouling, corrosion, and other effects that lead to dramatic visual appearance change, which often causes traditional point-based feature methods to perform poorly. Building upon the authors' earlier work, this paper presents an algorithm for underwater vehicle place recognition and relocalization that enables an autonomous underwater vehicle (AUV) to relocalize itself to a previously-built simultaneous localization and mapping (SLAM) graph. High-level structural features are learned using a supervised learning framework that retains features that have a high potential to persist in the underwater environment. Combined with a particle filtering framework, these features are used to provide a probabilistic representation of localization confidence. The algorithm is evaluated on real data, from multiple years, collected by a Hovering Autonomous Underwater Vehicle (HAUV) for ship hull inspection.},
keywords = {autonomous underwater vehicles;geophysical image processing;image recognition;learning (artificial intelligence);mobile robots;oceanographic techniques;particle filtering (numerical methods);probability;SLAM (robots);},
note = {ship hull inspection;HAUV;hovering autonomous underwater vehicle;probabilistic representation;particle filtering framework;supervised learning framework;high-level structural feature;SLAM graph;simultaneous localization and mapping graph;point-based feature method;biofouling;underwater robot visual place recognition;},
} 


@article{19376748 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {A fast, complete, point cloud based loop closure for LiDAR odometry and mapping [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Jiarong Lin and Fu Zhang},
year = {25 Sept. 2019},
pages = {7 pp. - },
address = {USA},
abstract = {This paper presents a loop closure method to correct the long-term drift in LiDAR odometry and mapping (LOAM). Our proposed method computes the 2D histogram of keyframes, a local map patch, and uses the normalized cross-correlation of the 2D histograms as the similarity metric between the current keyframe and those in the map. We show that this method is fast, invariant to rotation, and produces reliable and accurate loop detection. The proposed method is implemented with careful engineering and integrated into the LOAM algorithm, forming a complete and practical system ready to use. To benefit the community by serving a benchmark for loop closure, the entire system is made open source on Github.},
keywords = {distance measurement;image matching;image registration;mobile robots;optical radar;path planning;robot vision;SLAM (robots);},
note = {complete system;fast point cloud based loop closure;complete point cloud based loop closure;loop closure method;long-term drift;local map patch;normalized cross-correlation;current keyframe;reliable loop detection;accurate loop detection;},
} 


@article{16406345 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {SemanticFusion: dense 3D semantic mapping with convolutional neural networks [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {McCormac, J. and Handa, A. and Davison, A. and Leutenegger, S.},
year = {16 Sept. 2016},
pages = {7 pp. - },
address = {USA},
abstract = {Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need extend beyond geometry and appearence - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state of the art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondence between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of approximately 25Hz.},
keywords = {human-robot interaction;image fusion;image segmentation;mobile robots;neural nets;robot vision;SLAM (robots);user interfaces;video signal processing;},
note = {SemanticFusion;dense 3D semantic mapping;convolutional neural networks;visual sensing mapping;mobile robots;robot intelligence;intuitive user interaction;simultaneous localisation and mapping system;SLAM system;ElasticFusion;indoor RGB-D video;loopy scanning trajectory;CNN semantic predictions;reconstruction dataset;single frame segmentation;baseline single frame predictions;2D semantic labelling;multiple prediction fusion;NYUv2 dataset;multiple view points;},
} 


@inproceedings{19283699 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {A Dense Segmentation Network for Fine Semantic Mapping},
journal = {2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
author = {Guoyu Zuo and Tao Zheng and Zichen Xu and Daoxiong Gong},
year = {2019//},
pages = {1969 - 74},
address = {Piscataway, NJ, USA},
abstract = {This paper proposes a fine semantic mapping method using dense segmentation network (DS-Net) to obtain good performance of semantic mapping fusion, in which the semantic segmentation network (DS-Net) is constructed based on the idea of DenseNet's dense connection. First, the RGB image and the depth image are used to generate a dense indoor scene map via the state-of-the-art dense SLAM (ElasticFusion). Then, semantic segmentation are precisely performed on the input RGB image via DS-Net. Finally, the long-term correspondence between the landmarks and the indoor scene map is established using the continuous frames both in the visual odometer and loop detection, and the final fused semantic map is obtained by integrating semantic predictions of the RGB-D video frames of multiple angles with the indoor scene map. Experiments were performed on the NYUv2 and CIFAR10 datasets and our laboratory environments. Results show shows that our method can reduce the error of dense map construction and obtain good semantic segmentation performance.},
keywords = {distance measurement;image colour analysis;image fusion;image segmentation;mobile robots;robot vision;SLAM (robots);video signal processing;},
note = {CIFAR10 datasets;NYUv2 datasets;loop detection;visual odometer;ElasticFusion;dense connection;DenseNet;dense SLAM;fused semantic map;semantic segmentation performance;dense map construction;RGB-D video frames;semantic predictions;RGB image;dense indoor scene map;depth image;semantic segmentation network;semantic mapping fusion;DS-Net;fine semantic mapping method;dense segmentation network;},
URL = {http://dx.doi.org/10.1109/ROBIO49542.2019.8961709},
} 


@inproceedings{19410544 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Real-Time, Environmentally-Robust 3D LiDAR Localization},
journal = {2019 IEEE International Conference on Imaging Systems and Techniques (IST). Proceedings},
author = {Yilong Zhu and Bohuan Xue and Linwei Zheng and Huaiyang Huang and Ming Liu and Rui Fan},
year = {2019//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Localization, or position fixing, is an important problem in robotics research. In this paper, we propose a novel approach for long-term localization in a changing environment using 3D LiDAR. We first create the map of a real environment using GPS and LiDAR. Then, we divide the map into several small parts as the targets for cloud registration, which can not only improve the robustness but also reduce the registration time. We proposed a localization method called PointLocalization. PointLocalization allows us to fuse different kinds of odometers, which can optimize the accuracy and frequency of localization results. We evaluate our algorithm on an unmanned ground vehicle (UGV) using LiDAR and a wheel encoder, and obtain the localization results at more than 20 Hz after fusion. The algorithm can also localize the UGV in a 180-degree field of view (FOV). Using an outdated map captured six months ago, this algorithm shows great robustness, and the test results show that it can achieve an accuracy of 10 cm. PointLocalization has been tested for a period of more than six months in a crowded factory and has operated successfully over a distance of more than 2000 km.},
keywords = {cloud computing;optical radar;remotely operated vehicles;},
note = {environmentally-robust 3D LiDAR localization;position fixing;long-term localization;cloud registration;registration time;localization method;PointLocalization;unmanned ground vehicle;UGV;outdated map;wheel encoder;field of view;distance 2000.0 km;frequency 20.0 Hz;},
URL = {http://dx.doi.org/10.1109/IST48021.2019.9010305},
} 


@article{20636724 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {DiSCO: Differentiable Scan Context With Orientation},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Xuecheng Xu and Huan Yin and Zexi Chen and Yuehua Li and Yue Wang and Rong Xiong},
volume = { 6},
number = { 2},
year = {2021/04/},
pages = {2791 - 8},
issn = {2377-3766},
address = {USA},
abstract = {Global localization is essential for robot navigation, of which the first step is to retrieve a query from the map database. This problem is called place recognition. In recent years, LiDAR scan based place recognition has drawn attention as it is robust against the appearance change. In this letter, we propose a LiDAR-based place recognition method, named Differentiable Scan Context with Orientation (DiSCO), which simultaneously finds the scan at a similar place and estimates their relative orientation. The orientation can further be used as the initial value for the down-stream local optimal metric pose estimation, improving the pose estimation especially when a large orientation between the current scan and retrieved scan exists. Our key idea is to transform the feature into the frequency domain. We utilize the magnitude of the spectrum as the place descriptor, which is theoretically rotation-invariant. In addition, based on the differentiable phase correlation, we can efficiently estimate the global optimal relative orientation using the spectrum. With such structural constraints, the network can be learned in an end-to-end manner, and the backbone is fully shared by the two tasks, achieving better interpretability and lightweight. Finally, DiSCO is validated on three datasets with long-term outdoor conditions, showing better performance than the compared methods. Codes are released at https://github.com/MaverickPeter/DiSCO-pytorch.},
keywords = {feature extraction;image recognition;mobile robots;object recognition;optical radar;pose estimation;robot vision;SLAM (robots);},
note = {map database;robot navigation;global localization;global optimal relative orientation;differentiable phase correlation;place descriptor;current scan;down-stream local optimal metric pose estimation;similar place;named Differentiable Scan Context;LiDAR-based place recognition method;appearance change;LiDAR scan;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3060741},
} 


@article{15404195 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {LiDAR scan matching aided inertial navigation system in GNSS-denied environments},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Jian Tang and Yuwei Chen and Xiaoji Niu and Li Wang and Liang Chen and Jingbin Liu and Chuang Shi and Hyyppa, J.},
volume = { 15},
number = { 7},
year = {2015/07/},
pages = {16710 - 28},
issn = {1424-8220},
address = {Switzerland},
abstract = {A new scan that matches an aided Inertial Navigation System (INS) with a low-cost LiDAR is proposed as an alternative to GNSS-based navigation systems in GNSS-degraded or -denied environments such as indoor areas, dense forests, or urban canyons. In these areas, INS-based Dead Reckoning (DR) and Simultaneous Localization and Mapping (SLAM) technologies are normally used to estimate positions as separate tools. However, there are critical implementation problems with each standalone system. The drift errors of velocity, position, and heading angles in an INS will accumulate over time, and on-line calibration is a must for sustaining positioning accuracy. SLAM performance is poor in featureless environments where the matching errors can significantly increase. Each standalone positioning method cannot offer a sustainable navigation solution with acceptable accuracy. This paper integrates two complementary technologies-INS and LiDAR SLAM-into one navigation frame with a loosely coupled Extended Kalman Filter (EKF) to use the advantages and overcome the drawbacks of each system to establish a stable long-term navigation process. Static and dynamic field tests were carried out with a self-developed Unmanned Ground Vehicle (UGV) platform-NAVIS. The results prove that the proposed approach can provide positioning accuracy at the centimetre level for long-term operations, even in a featureless indoor environment.},
keywords = {calibration;inertial navigation;Kalman filters;nonlinear filters;optical radar;position control;remotely operated vehicles;satellite navigation;SLAM (robots);},
note = {LiDAR scan matching aided inertial navigation system;INS;GNSS-degraded environment;GNSS-denied environment;DR;dead reckoning;simultaneous localization and mapping technology;SLAM technology;position estimation;calibration;loosely coupled extended Kalman filter;EKF;unmanned ground vehicle;UGV;NAVIS;},
URL = {http://dx.doi.org/10.3390/s150716710},
} 


@inproceedings{19450733 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Magneto-visual-inertial Dead-reckoning: Improving Estimation Consistency by Invariance},
journal = {2019 IEEE 58th Conference on Decision and Control (CDC)},
author = {Caruso, D. and Eudes, A. and Sanfourche, M. and Vissiere, D. and Le Besnerais, G.},
year = {2019//},
pages = {7923 - 30},
address = {Piscataway, NJ, USA},
abstract = {Tractable algorithms used for 6DOF visual-inertial odometry have decades-long history of estimation consistency issues. Those arise in particular in two well-studied filters: namely the EKF-SLAM and MSCKF. Recently, strong theoretical works linked the error-state of these filters with their consistency properties; these results led to the synthesis of far more consistent filters. In previous works, we have shown that using similar filter for the fusion of magneto-inertial sensors with optical ones improved classical visual-inertial navigation systems. The consistency of those novel magneto-visual-inertial filters were, however, not addressed until now. This work does. We apply invariance theory findings to the specific case of magneto-inertial odometry and magneto-visual-inertial odometry for the synthesis of a filter with interesting consistency properties. We describe thoroughly such an invariant filter, implement it and conduct experiments on carefully captured data from real sensor. By comparing the results of non-invariant, observability-constrained and invariant versions of the filter, we find that the invariant version (i) shows an error estimate that is consistent with observability of the system, (ii) is applicable in case of unknown heading at initialization, (iii) improves long-term behavior of the filter and (iv) exhibits a lower normalized estimation error. We experiment on challenging scenarios for regular visual-inertial pedestrian navigation systems.},
keywords = {distance measurement;filtering theory;inertial navigation;Kalman filters;mobile robots;nonlinear filters;pedestrians;robot vision;SLAM (robots);},
note = {consistent filters;magneto-inertial sensors;visual-inertial navigation systems;magneto-visual-inertial filters;invariance theory findings;magneto-inertial odometry;magneto-visual-inertial odometry;invariant filter;error estimate;lower normalized estimation error;visual-inertial pedestrian navigation systems;magneto-visual-inertial dead-reckoning;improving estimation consistency;tractable algorithms;6DOF visual-inertial odometry;estimation consistency issues;EKF-SLAM;strong theoretical works;noninvariant filter;},
URL = {http://dx.doi.org/10.1109/CDC40024.2019.9029283},
} 


@inproceedings{18903892 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Learning Scene Geometry for Visual Localization in Challenging Conditions},
journal = {2019 International Conference on Robotics and Automation (ICRA)},
author = {Piasco, N. and Sidibe, D. and Gouet-Brunet, V. and Demonceaux, C.},
year = {2019//},
pages = {9094 - 100},
address = {Piscataway, NJ, USA},
abstract = {We propose a new approach for outdoor large scale image based localization that can deal with challenging scenarios like cross-season, cross-weather, day/night and long-term localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We are able to increase recall@1 performances by 2.15% on cross-weather and long-term localization scenario and by 4.24% points on a challenging winter/summer localization sequence versus state-of-the-art methods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images.},
keywords = {feature extraction;image colour analysis;image retrieval;learning (artificial intelligence);object detection;robot vision;},
note = {daytime images;learning scene geometry;visual localization;outdoor large scale image;cross-season;learned global image descriptor;scene geometry information;depth map;query image;localization accuracy;cross-weather;long-term localization scenario;night images;winter localization sequence;summer localization sequence;},
URL = {http://dx.doi.org/10.1109/ICRA.2019.8794221},
} 


@inproceedings{19986802 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Map as the hidden sensor: fast odometry-based global localization},
journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Cheng Peng and Weikersdorfer, D.},
year = {2020//},
pages = {2317 - 23},
address = {Piscataway, NJ, USA},
abstract = {Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in the long-term, our method using only odometry and the map converges in long-term. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.},
keywords = {distance measurement;mobile robots;path planning;sensors;tensors;},
note = {odometry-based global localization;ambiguous observations;odometry drift;blind robots;robot state;belief tensor;map-corrected odometry localization;map traversability;robotics applications;hidden sensor;},
URL = {http://dx.doi.org/10.1109/ICRA40945.2020.9197225},
} 


@article{21441083 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {NASA Space Robotics Challenge 2 Qualification Round: An Approach to Autonomous Lunar Rover Operations},
journal = {IEEE Aerospace and Electronic Systems Magazine},
journal = {IEEE Aerosp. Electron. Syst. Mag. (USA)},
author = {Kilic, C. and Martinez R, B., Jr. and Tatsch, C.A. and Beard, J. and Strader, J. and Das, S. and Ross, D. and Yu Gu and Pereira, G.A.S. and Gross, J.N.},
volume = { 36},
number = { 12},
year = {2021//},
pages = {24 - 41},
issn = {0885-8985},
address = {USA},
abstract = {Plans for establishing a long-term human presence on the Moon will require substantial increases in robot autonomy and multirobot coordination to support establishing a lunar outpost. To achieve these objectives, algorithm design choices for the software developments need to be tested and validated for expected scenarios such as autonomous in situ resource utilization, localization in challenging environments, and multirobot coordination. However, real-world experiments are extremely challenging and limited for extraterrestrial environment. Also, realistic simulation demonstrations in these environments are still rare and demanded for initial algorithm testing capabilities. To help some of these needs, the NASA Centennial Challenges program established the Space Robotics Challenge Phase 2 (SRC2), which consist of virtual robotic systems in a realistic lunar simulation environment, where a group of mobile robots were tasked with reporting volatile locations within a global map, excavating and transporting these resources, and detecting and localizing a target of interest. The main goal of this article is to share our team's experiences on the design tradeoffs to perform autonomous robotic operations in a virtual lunar environment and to share strategies to complete the mission requirements posed by NASA SRC2 competition during the qualification round. Of the 114 teams that registered for participation in the NASA SRC2, team Mountaineers finished as one of only six teams to receive the top qualification round prize.},
keywords = {aerospace robotics;mobile robots;Moon;multi-robot systems;planetary rovers;},
note = {NASA Centennial Challenges program;virtual robotic systems;realistic lunar simulation environment;mobile robots;autonomous robotic operations;virtual lunar environment;NASA SRC2 competition;NASA Space Robotics Challenge 2 qualification round;autonomous lunar rover operations;long-term human presence;robot autonomy;multirobot coordination;lunar outpost;software developments;resource utilization;real-world experiments;extraterrestrial environment;realistic simulation demonstrations;algorithm testing capabilities;},
URL = {http://dx.doi.org/10.1109/MAES.2021.3115897},
} 


@inproceedings{21623298 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Performance Analysis of Feature Detectors and Descriptors in Underwater and Polar Environments},
journal = {OCEANS 2021: San Diego - Porto},
author = {Shah, V. and Nir, J. and Kaveti, P. and Singh, H.},
year = {2021//},
pages = {7 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Many scientific mapping surveys that deploy robotic platforms in underwater and polar environments perform Visual Simultaneous Localization and Mapping (VSLAM), Structure for Motion (SfM), and Image Mosaicking. These techniques heavily rely on robust and reliable feature-based vision front ends. The job of a vision front end is to provide correspondence information between different camera views which is then directly fed into a bundle adjustment step. Although the atomic steps involved in constructing a vision front end are well-known, many of the popular choices of the features and their parameters do not perform reliably in a variety of visually degraded underwater and polar environments that are characterized by low texture and contrast, and by unevenly lit and low-overlap imagery. In this paper, we develop novel metrics and quantitative analysis methods which can measure the impact of image pre-processing steps such as Contrast Limited Adaptive Histogram Equalization (CLAHE) on the improvement of vision front-end outputs. Our metrics and quantitative analysis can guide the selection between different feature detectors and descriptors to develop a reliable and robust vision front end that can operate in a wider range of underwater and polar environments. We showcase how CLAHE improves the saliency of features and feature track length on a visually degraded dataset underwater, resulting in a substantial increase in correspondence information for the SfM solution. Finally, we perform an end-to-end SfM analysis that shows reduced accumulated drift over the long term and improved accuracy.},
keywords = {cameras;computer vision;feature extraction;image enhancement;image motion analysis;image segmentation;image sensors;mobile robots;robot vision;SLAM (robots);},
note = {performance analysis;polar environments;scientific mapping surveys;deploy robotic platforms;Visual Simultaneous Localization;Image Mosaicking;robust feature-based vision front ends;reliable feature-based vision front ends;correspondence information;bundle adjustment step;atomic steps;underwater environments;low texture;low-overlap imagery;quantitative analysis methods;image pre-processing steps;Contrast Limited Adaptive Histogram Equalization;vision front-end outputs;different feature detectors;reliable vision front end;robust vision front end;feature track length;visually degraded dataset;end-to-end SfM analysis;},
URL = {http://dx.doi.org/10.23919/OCEANS44145.2021.9705975},
} 


@inproceedings{16503885 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Pole-based Localization for Autonomous Vehicles in Urban Scenarios},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Spangenberg, R. and Goehring, D. and Rojas, R.},
year = {2016//},
pages = {2161 - 6},
address = {Piscataway, NJ, USA},
abstract = {Localization is a key capability for autonomous vehicles especially in urban scenarios. We propose the use of pole-like landmarks as primary features in these environments, as they are distinct, long-term stable and can be detected reliably with a stereo camera system. Furthermore, the resulting map representation is memory efficient, allowing for easy storage and on-line updates. The localization is performed in real-time by a stereo camera system as a main sensor, using vehicle odometry and an off-the-shelf GPS as secondary information sources. Localization is performed by a particle filter approach, coupled with an Kalman filter for robustness and sensor fusion. This leads to a lateral accuracy below 20 cm in various urban test areas. The system has been included in our autonomous test vehicle and successfully demonstrated the full loop from mapping to autonomous driving.},
keywords = {cameras;Global Positioning System;Kalman filters;mobile robots;particle filtering (numerical methods);road vehicles;robot vision;sensor fusion;SLAM (robots);stereo image processing;},
note = {autonomous driving;sensor fusion;Kalman filter;particle filter approach;off-the-shelf GPS;vehicle odometry;stereo camera system;map representation;pole-like landmark;urban scenario;autonomous vehicle;pole-based localization;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759339},
} 


@inproceedings{19987656 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments},
journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Li Sun and Adolfsson, D. and Magnusson, M. and Andreasson, H. and Posner, I. and Duckett, T.},
year = {2020//},
pages = {4386 - 92},
address = {Piscataway, NJ, USA},
abstract = {This paper proposes a novel approach for global localisation of mobile robots in large-scale environments. Our method leverages learning-based localisation and filtering-based localisation, to localise the robot efficiently and precisely through seeding Monte Carlo Localisation (MCL) with a deeplearned distribution. In particular, a fast localisation system rapidly estimates the 6-DOF pose through a deep-probabilistic model (Gaussian Process Regression with a deep kernel), then a precise recursive estimator refines the estimated robot pose according to the geometric alignment. More importantly, the Gaussian method (i.e. deep probabilistic localisation) and nonGaussian method (i.e. MCL) can be integrated naturally via importance sampling. Consequently, the two systems can be integrated seamlessly and mutually benefit from each other. To verify the proposed framework, we provide a case study in large-scale localisation with a 3D lidar sensor. Our experiments on the Michigan NCLT long-term dataset show that the proposed method is able to localise the robot in 1.94 s on average (median of 0.8 s) with precision 0.75 m in a largescale environment of approximately 0.5 km<sup>2</sup>.},
keywords = {Gaussian processes;learning (artificial intelligence);mobile robots;Monte Carlo methods;neural nets;optical radar;path planning;recursive estimation;robot vision;SLAM (robots);},
note = {precise lidar-based robot localisation;large-scale environments;global localisation;mobile robots;Monte Carlo Localisation;MCL;fast localisation system;deep-probabilistic model;Gaussian process regression;deep kernel;precise recursive estimator;Gaussian method;deep probabilistic localisation;large-scale localisation;largescale environment;time 0.8 s;size 0.75 m;},
URL = {http://dx.doi.org/10.1109/ICRA40945.2020.9196708},
} 


@inproceedings{16545318 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {3D localization, mapping and path planning for search and rescue operations},
journal = {2016 IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR). Proceedings},
author = {Dube, R. and Gawel, A. and Cadena, C. and Siegwart, R. and Freda, L. and Gianni, M.},
year = {2016//},
pages = {272 - 3},
address = {Piscataway, NJ, USA},
abstract = {This work presents our results on 3D robot localization, mapping and path planning for the latest joint exercise of the European project &ldquo;Long-Term Human-Robot Teaming for Robots Assisted Disaster Response&rdquo; (TRADR)<sup>1</sup>. The full system is operated and evaluated by firemen end-users in real-world search and rescue experiments. We demonstrate that the system is able to plan a path to a goal position desired by the fireman operator in the TRADR Operational Control Unit (OCU), using a persistent 3D map created by the robot during previous sorties.},
keywords = {disasters;mobile robots;path planning;service robots;},
note = {path planning;3D mapping;rescue operations;search operations;3D robot localization;long term human robot teaming for robots assisted disaster response;TRADR;fireman operator;TRADR operational control unit;OCU;autonomous mobile robots;},
URL = {http://dx.doi.org/10.1109/SSRR.2016.7784311},
} 


@article{20636668 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Hough2Map - Iterative Event-Based Hough Transform for High-Speed Railway Mapping},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Tschopp, F. and von Einem, C. and Cramariuc, A. and Hug, D. and Palmer, A.W. and Siegwart, R. and Chli, M. and Nieto, J.},
volume = { 6},
number = { 2},
year = {2021/04/},
pages = {2745 - 52},
issn = {2377-3766},
address = {USA},
abstract = {To cope with the growing demand for transportation on the railway system, accurate, robust, and high-frequency positioning is required to enable a safe and efficient utilization of the existing railway infrastructure. As a basis for a localization system we propose a complete on-board mapping pipeline able to map robust meaningful landmarks, such as poles from power lines, in the vicinity of the vehicle. Such poles are good candidates for reliable and long term landmarks even through difficult weather conditions or seasonal changes. To address the challenges of motion blur and illumination changes in railway scenarios we employ a Dynamic Vision Sensor, a novel event-based camera. Using a sideways oriented on-board camera, poles appear as vertical lines. To map such lines in a real-time event stream, we introduce Hough<sup><b>2</b></sup>Map, a novel consecutive iterative event-based Hough transform framework capable of detecting, tracking, and triangulating close-by structures. We demonstrate the mapping reliability and accuracy of Hough<sup><b>2</b></sup>Map on real-world data in typical usage scenarios and evaluate using surveyed infrastructure ground truth maps. Hough<sup><b>2</b></sup>Map achieves a detection reliability of up to $92\,\%$ and a mapping root mean square error accuracy of 1.1518 m.<sup>1</sup><sup>1</sup>The code is available at https://github.com/ethz-asl/Hough2Map.},
keywords = {cameras;computer vision;geographic information systems;Hough transforms;image sensors;mean square error methods;railways;},
note = {high-speed railway mapping;iterative event-based Hough transform;Hough<sup>2</sup>Map;railway scenarios;long term landmarks;reliable term landmarks;power lines;map robust meaningful landmarks;on-board mapping pipeline;localization system;railway infrastructure;safe utilization;high-frequency positioning;railway system;mapping root;surveyed infrastructure ground truth maps;mapping reliability;consecutive iterative event-based Hough;real-time event stream;on-board camera;event-based camera;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3061404},
} 


@inproceedings{21407142 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Wen, B. and Bekris, K.},
year = {2021//},
pages = {8067 - 74},
address = {Piscataway, NJ, USA},
abstract = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method's reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack.},
keywords = {CAD;feature extraction;graph theory;image matching;image sequences;learning (artificial intelligence);object detection;path planning;pose estimation;robot vision;SLAM (robots);},
note = {6D pose tracking;category-level;video sequences;robot manipulation;prior efforts;target object;online template matching;robust feature extraction;low-drift tracking;significant occlusions;object motions;object instance CAD model;frequency 10.0 Hz;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9635991},
} 


@inproceedings{18324342 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Scalable Change Detection from 3D Point Cloud Maps: Invariant Map Coordinate for Joint Viewpoint-change Localization},
journal = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
author = {Yoshiki, T. and Kanji, T. and Naiming, Y.},
year = {2018//},
pages = {1115 - 21},
address = {Piscataway, NJ, USA},
abstract = {This study addresses the problem of visual change detection using a 3D point cloud (PC) map acquired by a car-like robot. With recent advances in long-term autonomous navigation, change detection under global viewpoint uncertainty has become a topic of considerable interest. In our study, we extend the traditional two-level pipeline of change detection: (1) scene registration and (2) scene comparison, to enable scalable and efficient change detection. In the traditional pipeline, the registration stage is required to align a given scene pair (i.e., query and reference PC maps) that are taken at different times into the same coordinate system, before comparing the two PCs. However, the registration stage is a time-consuming step, which makes it harder to realize a scalable change detection. Our key concept is to transform every query or reference PC beforehand into an invariant coordinate system, which should be predefined and invariant to environment changes (e.g., dynamic objects, clutters, the mapper vehicle's trajectories), so as to enable a direct comparison of spatial layout between the two different maps. The proposed framework employs an efficient bag-of-local-features (BoLF) scene model and realizes a scalable joint viewpoint-change detection. Change detection experiments using a publicly available cross-season NCLT dataset validate the efficacy of the approach.},
keywords = {image registration;object detection;},
note = {long-term autonomous navigation;global viewpoint uncertainty;scalable change detection;efficient change detection;traditional pipeline;registration stage;coordinate system;environment changes;efficient bag-of-local-features scene model;scalable joint viewpoint-change detection;change detection experiments;3D point cloud map;invariant map coordinate;joint viewpoint-change localization;visual change detection;scene registration;},
URL = {http://dx.doi.org/10.1109/ITSC.2018.8569294},
} 


@article{16246630 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Training a Convolutional Neural Network for Appearance-Invariant Place Recognition [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Gomez-Ojeda, R. and Lopez-Antequera, M. and Petkov, N. and Gonzalez-Jimenez, J.},
year = {2015/05/27},
pages = {9 pp. - },
address = {USA},
abstract = {Place recognition is one of the most challenging problems in computer vision, and has become a key part in mobile robotics and autonomous driving applications for performing loop closure in visual SLAM systems. Moreover, the difficulty of recognizing a revisited location increases with appearance changes caused, for instance, by weather or illumination variations, which hinders the long-term application of such algorithms in real environments. In this paper we present a convolutional neural network (CNN), trained for the first time with the purpose of recognizing revisited locations under severe appearance changes, which maps images to a low dimensional space where Euclidean distances represent place dissimilarity. In order for the network to learn the desired invariances, we train it with triplets of images selected from datasets which present a challenging variability in visual appearance. The triplets are selected in such way that two samples are from the same location and the third one is taken from a different place. We validate our system through extensive experimentation, where we demonstrate better performance than state-of-art algorithms in a number of popular datasets.},
keywords = {computer vision;image recognition;mobile robots;neural nets;SLAM (robots);},
note = {convolutional neural network;appearance-invariant place recognition;computer vision;mobile robotics;autonomous driving applications;visual SLAM systems;CNN;place dissimilarity;Euclidean distances;},
} 


@inproceedings{12510393 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Hierarchical loop detection for mobile outdoor robots},
journal = {Proceedings of the SPIE - The International Society for Optical Engineering},
journal = {Proc. SPIE - Int. Soc. Opt. Eng. (USA)},
author = {Lang, D. and Winkens, C. and Haselich, M. and Paulus, D.},
volume = { 8301},
year = {2012//},
pages = {83010P (11 pp.) - },
issn = {0277-786X},
address = {USA},
abstract = {Loop closing is a fundamental part of 3D simultaneous localization and mapping (SLAM) that can greatly enhance the quality of long-term mapping. It is essential for the creation of globally consistent maps. Conceptually, loop closing is divided into detection and optimization. Recent approaches depend on a single sensor to recognize previously visited places in the loop detection stage. In this study, we combine data of multiple sensors such as GPS, vision, and laser range data to enhance detection results in repetitively changing environments that are not sufficiently explained by a single sensor. We present a fast and robust hierarchical loop detection algorithm for outdoor robots to achieve a reliable environment representation even if one or more sensors fail.},
keywords = {mobile robots;sensors;SLAM (robots);},
note = {mobile outdoor robots;loop closing;3D simultaneous localization and mapping;SLAM;globally consistent maps;single sensor;multiple sensors;GPS;laser range data;vision;hierarchical loop detection algorithm;},
URL = {http://dx.doi.org/10.1117/12.908277},
} 


@inproceedings{20425099 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Voxel-based Representation Learning for Place Recognition Based on 3D Point Clouds},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Siva, S. and Nahman, Z. and Hao Zhang},
year = {2020//},
pages = {8351 - 7},
address = {Piscataway, NJ, USA},
abstract = {Place recognition is a critical component towards addressing the key problem of Simultaneous Localization and Mapping (SLAM). Most existing methods use visual images; whereas, place recognition using 3D point clouds, especially based on the voxel representations, has not been well addressed yet. In this paper, we introduce the novel approach of voxel-based representation learning (VBRL) that uses 3D point clouds to recognize places with long-term environment variations. VBRL splits a 3D point cloud input into voxels and uses multi-modal features extracted from these voxels to perform place recognition. Additionally, VBRL uses structured sparsity-inducing norms to learn representative voxels and feature modalities that are important to match places under long-term changes. Both place recognition, and voxel and feature learning are integrated into a unified regularized optimization formulation. As the sparsity-inducing norms are non-smooth, it is hard to solve the formulated optimization problem. Thus, we design a new iterative optimization algorithm, which has a theoretical convergence guarantee. Experimental results have shown that VBRL performs place recognition well using 3D point cloud data and is capable of learning the importance of voxels and feature modalities.},
keywords = {feature extraction;image representation;image sensors;iterative methods;learning (artificial intelligence);mobile robots;optimisation;robot vision;SLAM (robots);},
note = {VBRL;place recognition;3D point cloud data;voxel-based representation learning;3D point clouds;voxel representations;3D point cloud input;feature learning;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9340992},
} 


@inproceedings{15583143 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {An Optimization Technique for Positioning Multiple Maps for Self-Driving Car's Autonomous Navigation},
journal = {2015 IEEE 18th International Conference on Intelligent Transportation Systems (ITSC). Proceedings},
author = {Dominguez, S. and Khomutenko, B. and Garcia, G. and Martinet, P.},
year = {2015//},
pages = {2694 - 9},
address = {Los Alamitos, CA, USA},
abstract = {Self-driving car's navigation requires a very precise localization covering wide areas and long distances. Moreover, they have to do it at faster speeds than conventional mobile robots. This paper reports on an efficient technique to optimize the position of a sequence of maps along a journey. We take advantage of the short-term precision and reduced space on disk of the localization using 2D occupancy grid maps, from now on called sub-maps, as well as, the long-term global consistency of a Kalman filter that fuses odometry and GPS measurements. In our approach, horizontal planar LiDARs and odometry measurements are used to perform 2D-SLAM generating the sub-maps, and the EKF to generate the trajectory followed by the car in global coordinates. During the trip, after finishing each sub-map, a relaxation process is applied to a set of the last sub-maps to position them globally using both, global and map's local path. The importance of this method lies on its performance, expending low computing resources, so it can work in real time on a computer with conventional characteristics and on its robustness which makes it suitable for being used on a self-driving car as it doesn't depend excessively on the availability of GPS signal or the eventual appearance of moving objects around the car. Extensive testing has been performed in the suburbs and in the down-town of Nantes (France) covering a distance of 25 kilometers with different traffic conditions obtaining satisfactory results for autonomous driving.},
keywords = {cartography;control engineering computing;distance measurement;Global Positioning System;Kalman filters;mobile robots;optical radar;optimisation;road vehicles;},
note = {autonomous driving;GPS signal;computing resource;relaxation process;EKF;2D-SLAM;odometry measurement;horizontal planar LiDAR;GPS measurement;Kalman filter;long-term global consistency;2D occupancy grid map;reduced space;short-term precision;mobile robot;self-driving car autonomous navigation;positioning multiple map;optimization technique;},
URL = {http://dx.doi.org/10.1109/ITSC.2015.433},
} 


@article{17992502 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Rao-Blackwellized Gaussian Sum Particle Filtering for Multipath Assisted Positioning},
journal = {Journal of Electrical and Computer Engineering},
journal = {J. Electr. Comput. Eng. (UK)},
author = {Ulmschneider, M. and Gentner, C. and Jost, T. and Dammann, A.},
volume = { 2018},
year = {2018//},
pages = {4761601 (15 pp.) - },
issn = {2090-0147},
address = {UK},
abstract = {In multipath assisted positioning, multipath components arriving at a receiver are regarded as being transmitted by a virtual transmitter in a line-of-sight condition. As the locations and clock offsets of the virtual and physical transmitters are in general unknown, simultaneous localization and mapping (SLAM) schemes can be applied to simultaneously localize a user and estimate the states of physical and virtual transmitters as landmarks. Hence, multipath assisted positioning enables localizing a user with only one physical transmitter depending on the scenario. In this paper, we present and derive a novel filtering approach for our multipath assisted positioning algorithm called Channel-SLAM. Making use of Rao-Blackwellization, the location of a user is tracked by a particle filter, and each landmark is represented by a sum of Gaussian probability density functions, whose parameters are estimated by unscented Kalman filters. Since data association, that is, finding correspondences among landmarks, is essential for robust long-term SLAM, we also derive a data association scheme. We evaluate our filtering approach for multipath assisted positioning by simulations in an urban scenario and by outdoor measurements.},
keywords = {indoor radio;Kalman filters;nonlinear filters;particle filtering (numerical methods);probability;SLAM (robots);},
note = {multipath components;blackwellized Gaussian sum particle filtering;multipath assisted positioning algorithm;physical transmitter;simultaneous localization;physical transmitters;virtual transmitters;virtual transmitter;},
URL = {http://dx.doi.org/10.1155/2018/4761601},
} 


@inproceedings{17058262 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Semantics-aware visual localization under challenging perceptual conditions},
journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Naseer, T. and Oliveira, G.L. and Brox, T. and Burgard, W.},
year = {2017//},
pages = {2614 - 20},
address = {Piscataway, NJ, USA},
abstract = {Visual place recognition under difficult perceptual conditions remains a challenging problem due to changing weather conditions, illumination and seasons. Long-term visual navigation approaches for robot localization should be robust to these dynamics of the environment. Existing methods typically leverage feature descriptions of whole images or image regions from Deep Convolutional Neural Networks. Some approaches also exploit sequential information to alleviate the problem of spatially inconsistent and non-perfect image matches. In this paper, we propose a novel approach for learning a discriminative holistic image representation which exploits the image content to create a dense and salient scene description. These salient descriptions are learnt over a variety of datasets under large perceptual changes. Such an approach enables us to precisely segment the regions of an image which are geometrically stable over large time lags. We combine features from these salient regions and an off-the-shelf holistic representation to form a more robust scene descriptor. We also introduce a semantically labeled dataset which captures extreme perceptual and structural scene dynamics over the course of 3 years. We evaluated our approach with extensive experiments on data collected over several kilometers in Freiburg and show that our learnt image representation outperforms off-the-shelf features from the deep networks and hand-crafted features.},
keywords = {convolution;image matching;image representation;image segmentation;neural nets;robot vision;SLAM (robots);},
note = {semantics-aware visual localization;visual place recognition;long-term visual navigation;robot localization;image feature descriptions;deep convolutional neural networks;spatially inconsistent image matches;nonperfect image matches;discriminative holistic image representation;dense scene description;salient scene description;image segmentation;perceptual scene dynamics;structural scene dynamics;Freiburg;},
URL = {http://dx.doi.org/10.1109/ICRA.2017.7989305},
} 


@inproceedings{11963094 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Global localization using multiple hypothesis tracking: A real-world approach},
journal = {2011 IEEE Conference on Technologies for Practical Robot Applications (TePRA)},
author = {Lutz, M. and Hochdorfer, S. and Schlegel, C.},
year = {2011//},
pages = {127 - 32},
address = {Piscataway, NJ, USA},
abstract = {Life-long and robust operation are important challenges to be solved towards everyday usability of service robots. Global localization is of particular interest for real-world applications. If a robot would not be able to relocalize itself within a known map, all positions stored by the robot (rooms, objects, etc.) would become obsolete. Although Simultaneous Localization and Mapping (SLAM) allows to initially map new and unknown environments and to keep track of environmental changes, it does not solve the global localization problem. Each time SLAM is restarted at different locations, it introduces a new map and a new frame of reference. In this paper, we propose a solution to the global localization problem which uses a SLAM generated feature map. The approach is demonstrated with an omnicam and bearing-only features. A new way to weight hypotheses and to sort out false hypotheses results in fast convergence even with arbitrary relocalization paths. The combined approach is a further step towards life-long operation of service robots and covers every part of a robot lifecycle, ranging from a setup via SLAM to efficient global localization for reuse of maps and object poses after restart.},
keywords = {mobile robots;path planning;robot vision;service robots;SLAM (robots);},
note = {multiple hypothesis tracking;service robots;simultaneous localization and mapping;global localization problem;SLAM;feature map;},
URL = {http://dx.doi.org/10.1109/TEPRA.2011.5753494},
} 


@article{12647728 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Appearance-based mapping and localization for mobile robots using a feature stability histogram},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Bacca, B. and Salvi, J. and Cufi, X.},
volume = { 59},
number = { 10},
year = {2011/10/},
pages = {840 - 57},
issn = {0921-8890},
address = {Netherlands},
abstract = {The strength of appearance-based mapping models for mobile robots lies in their ability to represent the environment through high-level image features and to provide human-readable information. However, developing a mapping and a localization method using these kinds of models is very challenging, especially if robots must deal with long-term mapping, localization, navigation, occlusions, and dynamic environments. In other words, the mobile robot has to deal with environmental appearance change, which modifies its representation of the environment. This paper proposes an indoor appearance-based mapping and a localization method for mobile robots based on the human memory model, which was used to build a Feature Stability Histogram (FSH) at each node in the robot topological map. This FSH registers local feature stability over time through a voting scheme, and the most stable features were considered for mapping, for Bayesian localization and for incrementally updating the current appearance reference view in the topological map. The experimental results are presented using an omnidirectional images dataset acquired over the long-term and considering: illumination changes (time of day, different seasons), occlusions, random removal of features, and perceptual aliasing. The results include a comparison with the approach proposed by Dayoub and Duckett (2008) [19] and the popular Bag-of-Words (Bazeille and Filliat, 2010) [35] approach. The obtained results confirm the viability of our method and indicate that it can adapt the internal map representation over time to localize the robot both globally and locally. [All rights reserved Elsevier].},
keywords = {Bayes methods;mobile robots;path planning;robot vision;SLAM (robots);},
note = {indoor appearance-based mapping model;mobile robots;feature stability histogram;high-level image features;human-readable information;localization method;long-term mapping;navigation;occlusions;dynamic environments;environmental appearance change;human memory model;robot topological map;voting scheme;Bayesian localization;omnidirectional images dataset;illumination changes;feature random removal;perceptual aliasing;bag-of-words approach;},
URL = {http://dx.doi.org/10.1016/j.robot.2011.06.008},
} 


@inproceedings{17617874 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Visual place recognition with CNNs: from global to partial},
journal = {2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)},
author = {Zhe Xin and Xiaoguang Cui and Jixiang Zhang and Yiping Yang and Yanqing Wang},
year = {2017//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Visual place recognition is one of the most challenging problems in computer vision, due to the large diversities that real-world places can represent. Recently, visual place recognition has become a key part of loop closure detection and topological localization in long-term mobile robot autonomy. In this work, we build up a novel visual place recognition pipeline composed of a first filtering stage followed by a partial reranking process. In the filtering stage, image-wise features are utilized to find a small set of potential places. Afterwards, stable region-wise landmarks are extracted for more accurate matching in the partial reranking process. All global and partial image representations are derived from pre-trained Convolutional Neural Networks (CNNs), and the landmarks are extracted by object proposal techniques. Moreover, a new similarity measurement is provided by considering both spatial and scale distribution of landmarks. Compared with current methods only considering scale distribution, the presented similarity measurement can benefit recognition precision and robustness effectively. Experiments with varied viewpoints and environmental conditions demonstrate that the proposed method achieves superior performance against state-of-the-art methods.},
keywords = {computer vision;feature extraction;feedforward neural nets;image matching;image recognition;image representation;learning (artificial intelligence);mobile robots;object detection;object recognition;robot vision;SLAM (robots);},
note = {long-term mobile robot autonomy;partial reranking process;potential places;global image representations;partial image representations;recognition precision;pre-trained convolutional neural networks;visual place recognition pipeline;CNN;},
URL = {http://dx.doi.org/10.1109/IPTA.2017.8310121},
} 


@article{19387351 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Exploring Performance Bounds of Visual Place Recognition Using Extended Precision},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Ferrarini, B. and Waheed, M. and Waheed, S. and Ehsan, S. and Milford, M.J. and McDonald-Maier, K.D.},
volume = { 5},
number = { 2},
year = {2020/04/},
pages = {1688 - 95},
issn = {2377-3774},
address = {USA},
abstract = {Recent advances in image description and matching allowed significant improvements in Visual Place Recognition (VPR). The wide variety of methods proposed so far and the increase of the interest in the field have rendered the problem of evaluating VPR methods an important task. As part of the localization process, VPR is a critical stage for many robotic applications and it is expected to perform reliably in any location of the operating environment. To design more reliable and effective localization systems this letter presents a generic evaluation framework based on the new Extended Precision performance metric for VPR. The proposed framework allows assessment of the upper and lower bounds of VPR performance and finds statistically significant performance differences between VPR methods. The proposed evaluation method is used to assess several state-of-the-art techniques with a variety of imaging conditions that an autonomous navigation system commonly encounters on long term runs. The results provide new insights into the behaviour of different VPR methods under varying conditions and help to decide which technique is more appropriate to the nature of the venture or the task assigned to an autonomous robot.},
keywords = {image recognition;mobile robots;navigation;path planning;robot vision;SLAM (robots);},
note = {performance bounds;Visual Place Recognition;image description;robotic applications;localization systems;generic evaluation framework;Extended Precision performance metric;VPR performance;autonomous navigation system;autonomous robot;},
URL = {http://dx.doi.org/10.1109/LRA.2020.2969197},
} 


@inproceedings{19211254 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Mapping and Localization Using Semantic Road Marking with Centimeter-level Accuracy in Indoor Parking Lots},
journal = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
author = {Jiaxin Hu and Ming Yang and Hanqing Xu and Yuesheng He and Chunxiang Wang},
year = {2019//},
pages = {4068 - 73},
address = {Piscataway, NJ, USA},
abstract = {Accurate localization is one of the fundamental tasks of vehicles visual navigation in parking lots. In this paper, we propose a practical and novel solution, which exploits road marking semantic segmentation to attack the problem of long-term and high-precision visual localization. Based on the semantic data association derived from road markings segmentation, point cloud fusion and loop detection strategies are designed to improve the performance of semantic map building. Applying the generated map, we present a point cloud registration algorithm combining semantic and geometric inference to improve the localization precision. Experiments on real-world indoor parking lots prove that the semantic map created by the proposed method reveals more accurate and consistent performance. Moreover, localization error is no more than 10cm, while running in real-time performance.},
keywords = {feature extraction;image registration;image segmentation;mobile robots;object detection;robot vision;},
note = {loop detection strategies;semantic map building;point cloud registration;semantic inference;geometric inference;localization precision;real-world indoor parking lots;localization error;semantic road marking;centimeter-level accuracy;semantic segmentation;semantic data association;road marking segmentation;point cloud fusion;vehicle visual navigation;},
URL = {http://dx.doi.org/10.1109/ITSC.2019.8917529},
} 


@article{16846617 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {1 year, 1000 km: the Oxford RobotCar dataset},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Maddern, W. and Pascoe, G. and Linegar, C. and Newman, P.},
volume = { 36},
number = { 1},
year = {2017/01/},
pages = {3 - 15},
issn = {0278-3649},
address = {UK},
abstract = {We present a challenging new dataset for autonomous driving: the Oxford RobotCar Dataset. Over the period of May 2014 to December 2015 we traversed a route through central Oxford twice a week on average using the Oxford RobotCar platform, an autonomous Nissan LEAF. This resulted in over 1000 km of recorded driving with almost 20 million images collected from 6 cameras mounted to the vehicle, along with LIDAR, GPS and INS ground truth. Data was collected in all weather conditions, including heavy rain, night, direct sunlight and snow. Road and building works over the period of a year significantly changed sections of the route from the beginning to the end of data collection. By frequently traversing the same route over the period of a year we enable research investigating long-term localization and mapping for autonomous vehicles in real-world, dynamic urban environments. The full dataset is available for download at: http://robotcar-dataset.robots.ox.ac.uk.},
keywords = {Global Positioning System;image processing;inertial navigation;mobile robots;optical radar;rain;SLAM (robots);traffic engineering computing;},
note = {Oxford RobotCar dataset;Oxford RobotCar Dataset;size = 1000.0 km;INS;autonomous driving;Nissan LEAF;LIDAR;GPS;autonomous vehicles;size 1000.0 km;},
URL = {http://dx.doi.org/10.1177/0278364916679498},
} 


@article{17077179 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Sparse optimization for robust and efficient loop closing},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Latif, Y. and Guoquan Huang and Leonard, J. and Neira, J.},
volume = { 93},
year = {2017/07/},
pages = {13 - 26},
issn = {0921-8890},
address = {Netherlands},
abstract = {It is essential for a robot to be able to detect revisits or <i>loop closures</i> for long-term visual navigation. A key insight explored in this work is that the loop-closing event inherently occurs sparsely, i.e., the image currently being taken matches with only a small subset (if any) of previous images. Based on this observation, we formulate the problem of loop-closure detection as a <i>sparse, convex</i>&lscr;<sub>1</sub>-minimization problem. By leveraging fast convex optimization techniques, we are able to efficiently find loop closures, thus enabling real-time robot navigation. This novel formulation requires no offline dictionary learning, as required by most existing approaches, and thus allows <i>online incremental</i> operation. Our approach ensures a <i>unique</i> hypothesis by choosing only a single globally optimal match when making a loop-closure decision. Furthermore, the proposed formulation enjoys a <i>flexible</i> representation with <i>no</i> restriction imposed on how images should be represented, while requiring only that the representations are &ldquo;close&rdquo; to each other when the corresponding images are visually similar. The proposed algorithm is validated extensively using real-world datasets. [All rights reserved Elsevier].},
keywords = {convex programming;image representation;learning (artificial intelligence);minimisation;mobile robots;optimisation;path planning;robot vision;SLAM (robots);},
note = {fast convex optimization techniques;-minimization problem;convex &lscr;;loop-closure detection;previous images;loop-closing event;key insight;long-term visual navigation;revisits;efficient loop closing;robust loop closing;sparse optimization;corresponding images;loop-closure decision;single globally optimal match;offline dictionary learning;real-time robot navigation;loop closures;},
URL = {http://dx.doi.org/10.1016/j.robot.2017.03.016},
} 


@inproceedings{18903460 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Detection-by-localization: maintenance-free change object detector},
journal = {2019 International Conference on Robotics and Automation (ICRA)},
author = {Kanji, T.},
year = {2019//},
pages = {4348 - 55},
address = {Piscataway, NJ, USA},
abstract = {Recent researches demonstrate that selflocalization performance is a very useful measure of likelihood-of-change (LoC) for change detection. In this paper, this ldquodetection-by-localizationrdquo scheme is studied in a novel generalized task of object-level change detection. In our framework, a given query image is segmented into object-level subimages (termed ldquoscene partsrdquo), which are then converted to subimagelevel pixel-wise LoC maps via the detection-by-localization scheme. Our approach models a self-localization system as a ranking function, outputting a ranked list of reference images, without requiring relevance score. Thanks to this new setting, we can generalize our approach to a broad class of selflocalization systems. We further propose an aggregation of different self-localization results from different queries so as to achieve higher precision. Our ranking based self-localization model allows to fuse self-localization results from different modalities via an unsupervised rank fusion derived from a field of multi-modal information retrieval (MMR). Our framework does not rely on the raw-score-merging hypothesis. Challenging experiments of cross-season change detection using the publicly available North Campus Long-Term (NCLT) dataset validates the efficacy of our proposed method.},
keywords = {feature extraction;image fusion;image retrieval;image segmentation;object detection;query processing;robot vision;},
note = {object-level subimages;pixel-wise LoC maps;detection-by-localization scheme;ranking function;ranked list;ranking based self-localization model;unsupervised rank fusion;cross-season change detection;maintenance-free change object detector;likelihood-of-change;object-level change detection;generalized task;query image;subimagelevel pixel-wise LoC maps;publicly available North Campus Long-Term dataset;publicly available NCLT dataset;multimodal information retrieval;MMR;},
URL = {http://dx.doi.org/10.1109/ICRA.2019.8793482},
} 


@article{14728298 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {The MIT Stata Center dataset},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Fallon, M. and Johannsson, H. and Kaess, M. and Leonard, J.J.},
volume = { 32},
number = { 14},
year = {2013/12/},
pages = {1695 - 9},
issn = {0278-3649},
address = {USA},
abstract = {This paper presents a large scale dataset of vision (stereo and RGB-D), laser and proprioceptive data collected over an extended duration by a Willow Garage PR2 robot in the 10 story MIT Stata Center. As of September 2012 the dataset comprises over 2.3 TB, 38 h and 42 km (the length of a marathon). The dataset is ofparticular interest to robotics and computer vision researchers interested in long-term autonomy. It is expected to be useful in a variety of research areas- robotic mapping (long-term, visual, RGB-D or laser), change detection in indoor environments, human pattern analysis, long-term path planning. For ease of use the original ROS `bag' log files are provided and also a derivative version combining human readable data and imagery in standard formats. Ofparticular importance, this dataset also includes ground-truth position estimates of the robot at every instance (to typical accuracy of 2 cm) using as-built floor-plans- which were carefully extracted using our software tools. The provision of ground-truth for such a large dataset enables more meaningful comparison between algorithms than has previously been possible.},
keywords = {collision avoidance;mobile robots;object detection;robot vision;SLAM (robots);software tools;},
note = {simultaneous localization and mapping;software tools;as-built floor-plans;ground truth position estimation;human readable data;ROS bag log files;long term path planning;human pattern analysis;indoor environments;change detection;long term autonomy;computer vision;Willow Garage PR2 robot;MIT Stata Center dataset;},
URL = {http://dx.doi.org/10.1177/0278364913509035},
} 


@inproceedings{15286147 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Distributed real-time cooperative localization and mapping using an uncertainty-aware expectation maximization approach},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Jing Dong and Nelson, E. and Indelman, V. and Michael, N. and Dellaert, F.},
year = {2015//},
pages = {5807 - 14},
address = {Piscataway, NJ, USA},
abstract = {We demonstrate distributed, online, and real-time cooperative localization and mapping between multiple robots operating throughout an unknown environment using indirect measurements. We present a novel Expectation Maximization (EM) based approach to efficiently identify inlier multi-robot loop closures by incorporating robot pose uncertainty, which significantly improves the trajectory accuracy over long-term navigation. An EM and hypothesis based method is used to determine a common reference frame. We detail a 2D laser scan correspondence method to form robust correspondences between laser scans shared amongst robots. The implementation is experimentally validated using teams of aerial vehicles, and analyzed to determine its accuracy, computational efficiency, scalability to many robots, and robustness to varying environments. We demonstrate through multiple experiments that our method can efficiently build maps of large indoor and outdoor environments in a distributed, online, and real-time setting.},
keywords = {distributed control;expectation-maximisation algorithm;multi-robot systems;uncertain systems;},
note = {distributed real-time cooperative localization;cooperative mapping;uncertainty-aware expectation maximization approach;EM based approach;multiple robots;indirect measurements;inlier multirobot loop closures;robot pose uncertainty;trajectory accuracy;long-term navigation;hypothesis based method;common reference frame;2D laser scan correspondence method;aerial vehicles;computational efficiency;robot scalability;indoor environments;outdoor environments;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7140012},
} 


@inproceedings{14023840 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Consistent sparsification for graph optimization},
journal = {2013 European Conference on Mobile Robots. Proceedings},
author = {Guoquan Huang and Kaess, M. and Leonard, J.J.},
year = {2013//},
pages = {150 - 7},
address = {Piscataway, NJ, USA},
abstract = {In a standard pose-graph formulation of simultaneous localization and mapping (SLAM), due to the continuously increasing numbers of nodes (states) and edges (measurements), the graph may grow prohibitively too large for long-term navigation. This motivates us to systematically reduce the pose graph amenable to available processing and memory resources. In particular, in this paper we introduce a consistent graph sparsification scheme: (i) sparsifying nodes via marginalization of old nodes, while retaining all the information (consistent relative constraints) - which is conveyed in the discarded measurements - about the remaining nodes after marginalization; and (ii) sparsifying edges by formulating and solving a consistent &lscr;1-regularized minimization problem, which automatically promotes the sparsity of the graph. The proposed approach is validated on both synthetic and real data.},
keywords = {graph theory;minimisation;SLAM (robots);},
note = {graph optimization sparsification;standard pose-graph formulation;simultaneous localization and mapping;SLAM;memory resources;consistent graph sparsification scheme;graph node marginalization;graph node sparsification;consistent relative constraints;consistent l<sub>1</sub>-regularized minimization problem;synthetic data;real data;graph edge sparsification;},
URL = {http://dx.doi.org/10.1109/ECMR.2013.6698835},
} 


@article{19623621 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Robust Visual Place Recognition Based on Context Information},
journal = {IFAC - Papers Online},
journal = {IFAC, Pap. Online (Netherlands)},
author = {Deyun Dai and Zonghai Chen and Jikai Wang and Peng Bao and Hao Zhao},
volume = { 52},
number = { 22},
year = {2019//},
pages = {49 - 54},
issn = {2405-8963},
address = {Netherlands},
abstract = {In large-scale and long-term visual SLAM, robust place recognition is essential for building a global consistent map. However, sensor viewpoints and environmental condition changes, including lighting, weather, and seasons, bring a huge challenge to place recognition. We propose a place recognition algorithm based on CNN features and graph model. Firstly, CNN features of images are extracted though an AlexNet network with migration characteristics, and N-nearest neighbor image descriptors of the current image descriptor are found by approximate nearest neighbor searching. Then, according to the difference between descriptors, a weighted directed acyclic graph (weighted DAG) model which describes a cost of context matching between images is established. Finally, a candidate matching sequence with minimum cost on this model is achieved by using Dijkstra algorithm. Compared with SeqCNNSLAM and Fast-SeqSLAM, the experimental results demonstrate higher recognition accuracy and robustness of our algorithm. [All rights reserved Elsevier].},
keywords = {convolutional neural nets;directed graphs;feature extraction;image matching;nearest neighbour methods;object recognition;SLAM (robots);},
note = {approximate nearest neighbor searching;weighted directed acyclic graph model;context matching;robust visual place recognition;context information;global consistent map;sensor viewpoints;environmental condition changes;place recognition algorithm;CNN features;AlexNet network;migration characteristics;neighbor image descriptors;current image descriptor;candidate matching sequence;},
URL = {http://dx.doi.org/10.1016/j.ifacol.2019.11.046},
} 


@inproceedings{9109904 ,
language = {English},
copyright = {Copyright 2006, The Institution of Engineering and Technology},
copyright = {CD-ROM},
title = {Active airborne localisation and exploration in unknown environments using inertial SLAM},
journal = {2006 IEEE Aerospace Conference (IEEE Cat. No. 05TH8853C)},
author = {Bryson, M. and Salah Sukkarieh},
year = {2006//},
pages = {13 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Future unmanned aerial vehicle (UAV) applications will require high-accuracy localisation in environments in which navigation infrastructure such as the Global Positioning System (GPS) and prior terrain maps may be unavailable or unreliable. In these applications, long-term operation requires the vehicle to build up a spatial map of the environment while simultaneously localising itself within the map, a task known as simultaneous localisation and mapping (SLAM). In the first part of this paper we present an architecture for performing inertial-sensor based SLAM on an aerial vehicle. We demonstrate an on-line path planning scheme that intelligently plans the vehicle's trajectory while exploring unknown terrain in order to maximise the quality of both the resulting SLAM map and localisation estimates necessary for the autonomous control of the UAV. Two important performance properties and their relationship to the dynamic motion and path planning systems on-board the UAV are analysed. Firstly we analyse information-based measures such as entropy. Secondly we perform an observability analysis of inertial SLAM by recasting the algorithms into an indirect error model form. Qualitative knowledge gained from the observability analysis is used to assist in the design of an information-based trajectory planner for the UAV. Results of the online path planning algorithm are presented using a high-fidelity 6-DoF simulation of a UAV during a simulated navigation and mapping task.},
keywords = {aircraft instrumentation;aircraft navigation;Global Positioning System;path planning;remotely operated vehicles;sensors;terrain mapping;},
note = {active airborne localisation;active airborne exploration;unmanned aerial vehicle applications;navigation infrastructure;spatial map;Simultaneous Localisation And Mapping;inertial sensor;on-line path planning scheme;vehicle trajectory;autonomous control;dynamic motion;path planning systems;entropy;observability analysis;indirect error model form;information-based trajectory planner;online path planning algorithm;high-fidelity 6-DoF simulation;mapping task;},
} 


@inproceedings{13844045 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {How to learn an illumination robust image feature for place recognition},
journal = {2013 IEEE Intelligent Vehicles Symposium (IV)},
author = {Lategahn, H. and Beck, J. and Kitt, B. and Stiller, C.},
year = {2013//},
pages = {285 - 91},
address = {Piscataway, NJ, USA},
abstract = {Place recognition for loop closure detection lies at the heart of every Simultaneous Localization and Mapping (SLAM) method. Recently methods that use cameras and describe the entire image by one holistic feature vector have experienced a resurgence. Despite the success of these methods, it remains unclear how a descriptor should be constructed for this particular purpose. The problem of choosing the right descriptor becomes even more pronounced in the context of life long mapping. The appearance of a place may vary considerably under different illumination conditions and over the course of a day. None of the handcrafted descriptors published in literature are particularly designed for this purpose. Herein, we propose to use a set of elementary building blocks from which millions of different descriptors can be constructed automatically. Moreover, we present an evaluation function which evaluates the performance of a given image descriptor for place recognition under severe lighting changes. Finally we present an algorithm to efficiently search the space of descriptors to find the best suited one. Evaluating the trained descriptor on a test set shows a clear superiority over its hand crafted counter parts like BRIEF and U-SURF. Finally we show how loop closures can be reliably detected using the automatically learned descriptor. Two overlapping image sequences from two different days and times are merged into one pose graph. The resulting merged pose graph is optimized and does not contain a single false link while at the same time all true loop closures were detected correctly. The descriptor and the place recognizer source code is published with datasets on http://www.mrt.kit.edu/libDird.php.},
keywords = {image sequences;mobile robots;SLAM (robots);},
note = {illumination robust image feature;place recognition;loop closure detection;simultaneous localization and mapping;SLAM method;cameras;feature vector;evaluation function;image descriptor;image sequences;pose graph;place recognizer source code;},
URL = {http://dx.doi.org/10.1109/IVS.2013.6629483},
} 


@article{16740705 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Episodic non-Markov localization},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Biswas, J. and Veloso, M.M.},
volume = { 87},
year = {2017/01/},
pages = {162 - 76},
issn = {0921-8890},
address = {Netherlands},
abstract = {Markov localization and its variants are widely used for mobile robot localization. These methods assume Markov independence of observations, implying that the observations can be entirely explained by a map. However, in real human environments, robots frequently make unexpected observations due to unmapped static objects like chairs and tables, and dynamic objects like humans. We therefore introduce Episodic non-Markov Localization (EnML), which reasons about the world as consisting of three classes of objects: long-term features corresponding to permanent mapped objects, short-term features corresponding to unmapped static objects, and dynamic features corresponding to unmapped moving objects. Long-term features are represented by a static map, while short-term features are detected and tracked in real-time. To reason about unexpected observations and their correlations across poses, we augment the Dynamic Bayesian Network for Markov localization to include varying edges and nodes, resulting in a novel Varying Graphical Network representation. The maximum likelihood estimate of the belief is incrementally computed by non-linear functional optimization. By detecting timesteps along the robot's trajectory where unmapped observations prior to such time steps are unrelated to those afterwards, EnML limits the history of observations and pose estimates to &ldquo;episodes&rdquo; over which the belief is computed. We demonstrate EnML using different types of sensors including laser rangefinders and depth cameras, and over multiple datasets, comparing it with alternative approaches. We further include results of a team of indoor autonomous service mobile robots traversing hundreds of kilometers using EnML. [All rights reserved Elsevier].},
keywords = {belief networks;laser ranging;Markov processes;maximum likelihood estimation;mobile robots;optimisation;sensor placement;trajectory control;},
note = {episodic nonMarkov localization;EnML;mobile robot localization;autonomous service mobile robots;depth cameras;laser rangefinders;episodes;pose estimate limiting;robot trajectory;timestep detection;nonlinear functional optimization;maximum likelihood estimation;varying graphical network representation;dynamic Bayesian network;static map;unmapped moving objects;dynamic features;short-term features;permanent mapped objects;long-term features;unmapped static objects;Markov observation independence;},
URL = {http://dx.doi.org/10.1016/j.robot.2016.09.005},
} 


@inproceedings{15475821 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {An Adaptive Gaussian Particle Filter based Simultaneous Localization and Mapping with dynamic process model noise bias compensation},
journal = {2015 IEEE 7th International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM). Proceedings},
author = {Rao, A. and Wang Han},
year = {2015//},
pages = {210 - 15},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous Localization and Mapping (SLAM) is a fundamental component of all autonomous robotics systems, which probabilisticaly fuses information from an exteroceptive sensor and a proprioceptive sensor to simultaneously estimate the robot's trajectory and the map. Inputs from the pro-prioceptive sensor are fed into the estimation algorithm via a process model corresponding with the vehicle kinematics, while a measurement model is used to process inputs from the exteroceptive sensor. Most SLAM algorithms assume known, fixed model estimate bias. This assumption does not hold true for systems with wrongly modeled estimate bias, or those affected by component fatigue due to applications requiring long term autonomy. This paper will display the adverse effects of mismodeled process model bias using a simulation. An adaptive algorithm employing Adaptive Gaussian Particle Filter based process model bias compensation will be deployed in tandem with a particle filter based FastSLAM algorithm. The algorithm will be compared favourably with existing state of the art SLAM algorithms in controlled simulations. Experimental data from a marine environment will be used to validate the efficacy of the algorithm.},
keywords = {adaptive filters;Gaussian processes;mobile robots;particle filtering (numerical methods);sensor fusion;trajectory control;},
note = {adaptive Gaussian particle filter;simultaneous localization and mapping;dynamic process model noise bias compensation;autonomous robotic system;probabilistic information fusion;exteroceptive sensor;proprioceptive sensor;robot trajectory estimation;vehicle kinematics;model estimate bias;adaptive algorithm;FastSLAM algorithm;marine environment;},
URL = {http://dx.doi.org/10.1109/ICCIS.2015.7274622},
} 


@article{20379909 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Range-focused fusion of Camera-IMU-UWB for accurate and drift-reduced localization},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Thien Hoang Nguyen and Thien-Minh Nguyen and Lihua Xie},
volume = { 6},
number = { 2},
year = {2021/04/},
pages = {1678 - 85},
issn = {2377-3766},
address = {USA},
abstract = {In this work, we present a tightly-coupled fusion scheme of a monocular camera, a 6-DoF IMU, and a single unknown Ultra-wideband (UWB) anchor to achieve accurate and drift-reduced localization. Specifically, this letter focuses on incorporating the UWB sensor into an existing state-of-the-art visual-inertial system. Previous works toward this goal use a single nearest UWB range data to update robot positions in the sliding window (&ldquo;position-focused&rdquo;) and have demonstrated encouraging results. However, these approaches ignore 1) the time-offset between UWB and camera sensors, and 2) all other ranges between two consecutive keyframes. Our approach shifts the perspective to the UWB measurements (&ldquo;range-focused&rdquo;) by leveraging the propagated information readily available from the visual-inertial odometry pipeline. This allows the UWB data to be used in a more effective manner: the time-offset of each range data is addressed and all available measurements can be utilized. Experimental results show that the proposed method consistently outperforms previous methods in both estimating the anchor position and reducing the drift in long-term trajectories.},
keywords = {cameras;distance measurement;image fusion;inertial navigation;inertial systems;mobile robots;pose estimation;position control;radiofrequency measurement;robot vision;SLAM (robots);ultra wideband technology;},
note = {robot positions;camera-IMU-UWB;state-of-the-art visual-inertial system;position-focused sliding window;tightly-coupled fusion scheme;UWB sensor;ultra-wideband sensor;6-DoF IMU;monocular camera;drift-reduced localization;range-focused fusion;anchor position estimation;UWB data;visual-inertial odometry pipeline;UWB measurements;camera sensors;time-offset;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3057838},
} 


@inproceedings{19506958 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Monocular visual odometry based on hybrid parameterization},
journal = {Proceedings of the SPIE},
journal = {Proc. SPIE (USA)},
author = {Mohamed, S.A.S. and Haghbayan, M.-H. and Heikkonen, J. and Tenhunen, H. and Plosila, J.},
volume = { 11433},
year = {2019//},
pages = {114332A (6 pp.) - },
issn = {0277-786X},
address = {USA},
abstract = {Visual odometry (VO) is one of the most challenging techniques in computer vision for autonomous vehicle/vessels. In VO, the camera pose that also represents the robot pose in ego-motion is estimated analyzing the features and pixels extracted from the camera images. Different VO techniques mainly provide different trade-offs among the resources that are being considered for odometry, such as camera resolution, computation/communication capacity, power/energy consumption, and accuracy. In this paper, a hybrid technique is proposed for camera pose estimation by combining odometry based on triangulation using the long-term period of direct-based odometry and the short-term period of inverse depth mapping. Experimental results based on the EuRoC data set shows that the proposed technique significantly outperforms the traditional direct-based pose estimation method for Micro Aerial Vehicle (MAV), keeping its potential negative effect on performance negligible.},
keywords = {computer vision;distance measurement;image sensors;motion estimation;pose estimation;robot vision;SLAM (robots);},
note = {EuRoC data set;computation-communication capacity;autonomous vehicle-vessels;microaerial vehicle;estimation method;inverse depth mapping;direct-based odometry;camera resolution;camera images;ego-motion;computer vision;hybrid parameterization;monocular visual odometry;},
URL = {http://dx.doi.org/10.1117/12.2556718},
} 


@article{21053400 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Globally Optimal Fetoscopic Mosaicking Based on Pose Graph Optimisation With Affine Constraints},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Liang Li and Bano, S. and Deprest, J. and David, A. and Stoyanov, D. and Vasconcelos, F.},
volume = { 6},
number = { 4},
year = {2021//},
pages = {7831 - 8},
issn = {2377-3766},
address = {USA},
abstract = {Fetoscopic laser ablation surgery could be guided using a high-quality panorama of the operating site, representing a map of the placental vasculature. This can be achieved during the initial inspection phase of the procedure using image mosaicking techniques. Due to the lack of camera calibration in the operating room, it has been mostly modelled as an affine registration problem. While previous work mostly focuses on image feature extraction for visual odometry, the challenges related to large-scale reconstruction (re-localisation, loop closure, drift correction) remain largely unaddressed in this context. This letter proposes using pose graph optimisation to produce globally optimal image mosaics of placental vessels. Our approach follows the SLAM framework with a front-end for visual odometry and a back-end for long-term refinement. Our front-end uses a recent state-of-the-art odometry approach based on vessel segmentation, which is then managed by a key-frame structure and the bag-of-words (BoW) scheme to retrieve loop closures. The back-end, which is our key contribution, models odometry and loop closure constraints as a pose graph with affine warpings between states. This problem in the special Euclidean space cannot be solved by existing pose graph algorithms and available libraries such as G2O. We model states on affine Lie group with local linearisation in its Lie algebra. The cost function is established using Mahalanobis distance with the vectorisation of the Lie algebra. Finally, an iterative optimisation algorithm is adopted to minimise the cost function. The proposed pose graph optimisation is first validated on simulation data with a synthetic trajectory that has different levels of noise and different numbers of loop closures. Then the whole system is validated using real fetoscopic data that has three sequences with different numbers of frames and loop closures. Experimental results validate the advantage of the proposed method compared with baselines.},
keywords = {cameras;distance measurement;feature extraction;graph theory;image registration;image segmentation;image sensors;iterative methods;Lie algebras;medical image processing;mobile robots;object tracking;optimisation;pose estimation;robot vision;SLAM (robots);surgery;},
note = {globally optimal fetoscopic mosaicking;pose graph optimisation;affine constraints;fetoscopic laser ablation surgery;high-quality panorama;operating site;placental vasculature;initial inspection phase;image mosaicking techniques;operating room;affine registration problem;image feature extraction;visual odometry;loop closure;globally optimal image mosaics;placental vessels;recent state-of-the-art odometry approach;models odometry;affine warpings;existing pose graph;model states;affine Lie group;Lie algebra;iterative optimisation algorithm;fetoscopic data;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3100938},
} 


@inproceedings{12194882 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Research on object recognition using bag of word model for mobile robot navigation},
journal = {Proceedings of the 2011 IEEE International Conference on Mechatronics and Automation (ICMA 2011)},
author = {Jin-fu Yang and Kai Wang and Ming-ai Li and Lu Liu},
year = {2011//},
pages = {1735 - 40},
address = {Piscataway, NJ, USA},
abstract = {Robust long term positioning for autonomous mobile robots is essential for many applications. Key to a successful visual SLAM system is correctly recognizing the objects and labeling where the robot is. Local image features are popular with constructing object recognition system, which are invariant to image scaling, translation, rotation, and partially invariant to illumination changes and affine. In this paper, we proposed an object recognition method based on the bag of word model, mainly idea includes three steps as follows: firstly, a set of local image patches are sampled using a key point detector, and each patch is a descriptor based on scale invariant feature transform. Then outliers are removed by RANSAC algorithm, and the resulting distribution of descriptors is quantified by using vector quantization against a pre-specified codebook to convert it to a histogram of votes for codebook centers. Finally, a KNN algorithm is used to classify images through the resulting global descriptor vector. The experimental results show that our proposed method has a better performance against the previous methods.},
keywords = {image classification;image coding;mobile robots;object recognition;path planning;random processes;robot vision;SLAM (robots);vector quantisation;},
note = {object recognition;bag of word model;mobile robot navigation;long term positioning;autonomous mobile robot;visual SLAM system;local image feature;image scaling;image translation;image rotation;local image patch;key point detector;scale invariant feature transform;RANSAC algorithm;vector quantization;codebook center;KNN algorithm is;image classification;},
URL = {http://dx.doi.org/10.1109/ICMA.2011.5986295},
} 


@inproceedings{15589670 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Vision-based Markov localization across large perceptual changes},
journal = {2015 European Conference on Mobile Robots (ECMR). Proceedings},
author = {Naseer, T. and Suger, B. and Ruhnke, M. and Burgard, W.},
year = {2015//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Recently, there has been significant progress towards lifelong, autonomous operation of mobile robots, especially in the field of localization and mapping. One important challenge in this context is visual localization under substantial perceptual changes, for example, coming from different seasons. In this paper, we present an approach to localize a mobile robot with a low frequency camera with respect to an image sequence, recorded previously within a different season. Our approach uses a discrete Bayes filter and a sensor model based on whole image descriptors. Thereby it exploits sequential information to model the dynamics of the system. Since we compute a probability distribution over the whole state space, our approach can handle more complex trajectories that may include same season loop-closures as well as fragmented sub-sequences. Throughout an extensive experimental evaluation on challenging datasets, we demonstrate that our approach outperforms state-of-the-art techniques.},
keywords = {Bayes methods;image filtering;image sequences;Markov processes;mobile robots;robot vision;SLAM (robots);},
note = {vision-based Markov localization;mobile robot autonomous operation;substantial perceptual changes;low frequency camera;image sequence;discrete Bayes filter;sensor model;probability distribution;},
URL = {http://dx.doi.org/10.1109/ECMR.2015.7324181},
} 


@inproceedings{16639425 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Migratory birds-inspired navigation system for unmanned aerial vehicles},
journal = {2016 IEEE International Conference on Information and Automation (ICIA). Proceedings},
author = {Yu Zhang and Ainong Chao and Boxin Zhao and Huawei Liu and Xiaolin Zhao},
year = {2016//},
pages = {276 - 81},
address = {Piscataway, NJ, USA},
abstract = {Migration birds are able to navigate themselves during a long-distance journey without getting lost. They actually achieve just what is being sought for in the field of Unmanned Aerial Vehicles (UAVs): long-term autonomous navigation. This paper proposes an approach that combines the migration birds' sense principles with Micro-Electro-Mechanical System (MEMS) sensors to estimate UAVs position within GPS-denied environments. Camera, orientation and web-based maps (such as Google/Baidu Maps) are chosen to simulate the birds' localization cues: vision, earth magnetic field and mental maps. The visual odometry, Particle Filter theories are used in the proposed approach to integrate multiple sensor measurements. Real flying experiments are conducted both in indoor and outdoor environments. The results validate that the proposed migration-inspired visual odometry system can estimate the UAV localization effectively.},
keywords = {aerospace navigation;autonomous aerial vehicles;cameras;distance measurement;geomagnetism;Internet;microsensors;particle filtering (numerical methods);robot vision;},
note = {long-distance journey;unmanned aerial vehicles;long-term autonomous navigation;migration bird sense principles;microelectromechanical system sensors;MEMS sensors;UAV position estimation;GPS-denied environments;camera;Web-based maps;bird localization cues;bird vision;earth magnetic field;mental maps;visual odometry;particle filter theories;multiple sensor measurements;real flying experiments;migration-inspired visual odometry system;UAV localization;},
URL = {http://dx.doi.org/10.1109/ICInfA.2016.7831835},
} 


@article{18059467 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Vision-aided Multi-UAV Autonomous Flocking in GPS-denied Environment},
journal = {IEEE Transactions on Industrial Electronics},
journal = {IEEE Trans. Ind. Electron. (USA)},
author = {Yazhe Tang and Yuchao Hu and Jinqiang Cui and Fang Liao and Mingjie Lao and 1 and 1},
volume = { 66},
number = { 1},
year = {2019/01/},
pages = {616 - 26},
issn = {0278-0046},
address = {USA},
abstract = {This paper presents a sophisticated vision-aided flocking system for unmanned aerial vehicles (UAVs), which is able to operate in GPS-denied unknown environments for exploring and searching missions, and also able to adopt two types of vision sensors, day and thermal cameras, to measure relative motion between UAVs in different lighting conditions without using wireless communication. In order to realize robust vision-aided flocking, an integrated framework of tracking-learning-detection on the basis of multifeature coded correlation filter has been developed. To achieve long-term tracking, a redetector is trained online to adaptively reinitialize target for global sensing. An advanced flocking strategy is developed to address the autonomous multi-UAVs' cooperative flight. Light detection and ranging (LiDAR)-based navigation modules are developed for autonomous localization, mapping, and obstacle avoidance. Flight experiments of a team of UAVs have been conducted to verify the performance of this flocking system in a GPS-denied environment. The extensive experiments validate the robustness of the proposed vision algorithms in challenging scenarios.},
keywords = {autonomous aerial vehicles;collision avoidance;Global Positioning System;image sensors;mobile robots;multi-robot systems;optical radar;robot vision;},
note = {GPS-denied environment;vision algorithms;vision-aided multiUAV autonomous flocking;unmanned aerial vehicles;UAVs;GPS-denied unknown environments;vision sensors;tracking-learning-detection;multifeature coded correlation filter;long-term tracking;advanced flocking strategy;autonomous localization;lighting conditions;autonomous multiUAVs cooperative flight;vision-aided flocking system;searching missions;thermal cameras;wireless communication;global sensing;light detection and ranging-based navigation modules;obstacle avoidance;autonomous mapping;LIDAR;},
URL = {http://dx.doi.org/10.1109/TIE.2018.2824766},
} 


@inproceedings{18904336 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Tightly Coupled 3D Lidar Inertial Odometry and Mapping},
journal = {2019 International Conference on Robotics and Automation (ICRA)},
author = {Haoyang Ye and Yuying Chen and Ming Liu},
year = {2019//},
pages = {3144 - 50},
address = {Piscataway, NJ, USA},
abstract = {Ego-motion estimation is a fundamental requirement for most mobile robotic applications. By sensor fusion, we can compensate the deficiencies of stand-alone sensors and provide more reliable estimations. We introduce a tightly coupled lidar-IMU fusion method in this paper. By jointly minimizing the cost derived from lidar and IMU measurements, the lidarIMU odometry (LIO) can perform well with considerable drifts after long-term experiment, even in challenging cases where the lidar measurement can be degraded. Besides, to obtain more reliable estimations of the lidar poses, a rotation-constrained refinement algorithm (LIO-mapping) is proposed to further align the lidar poses with the global map. The experiment results demonstrate that the proposed method can estimate the poses of the sensor pair at the IMU update rate with high precision, even under fast motion conditions or with insufficient features.},
keywords = {distance measurement;image fusion;mobile robots;motion estimation;optical radar;pose estimation;robot vision;SLAM (robots);},
note = {fast motion conditions;ego-motion estimation;mobile robotic applications;sensor fusion;stand-alone sensors;tightly coupled lidar-IMU fusion method;IMU measurements;lidarIMU odometry;lidar measurement;rotation-constrained refinement algorithm;LIO-mapping;sensor pair;IMU update rate;lidar pose estimation;},
URL = {http://dx.doi.org/10.1109/ICRA.2019.8793511},
} 


@inproceedings{19299099 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Robust Outdoor Self-localization In Changing Environments},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Haris, M. and Franzius, M. and Bauer-Wersing, U.},
year = {2019//},
pages = {714 - 19},
address = {Piscataway, NJ, USA},
abstract = {In outdoor scenarios changing conditions (e.g., seasonal, weather and lighting effects) have a substantial impact on the appearance of a scene, which often prevents successful visual localization. The application of an unsupervised Slow Feature Analysis (SFA) on the images captured by an autonomous robot enables self-localization from a single image. However, changes occurring during the training phase or over a more extended period can affect the learned representations. To address the problem, we propose to join long-term recordings from an outdoor environment based on their position correspondences. The established hierarchical model trained on raw images performs well, but as an extension, we extract Fourier components of the views and use that for learning of spatial representations, which reduces the computation time and makes it adequate to run on an ARM embedded system. We present the experimental results from a simulated environment and real-world outdoor recordings collected over a full year, which has effects like different day time, weather, seasons and dynamic objects. Results show an increasing invariance w.r.t. changing conditions over time, thus an outdoor robot can improve its localization performance during operation.},
keywords = {control engineering computing;embedded systems;Fourier analysis;mobile robots;robot vision;SLAM (robots);unsupervised learning;},
note = {robust outdoor self-localization;outdoor scenarios;visual localization;unsupervised Slow Feature Analysis;unsupervised SFA;autonomous robot;representation learning;outdoor environment;Fourier components;ARM embedded system;outdoor robot;localization performance;changing environments;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8967549},
} 


@inproceedings{19212361 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Visual Localization Using Sparse Semantic 3D Map},
journal = {2019 IEEE International Conference on Image Processing (ICIP). Proceedings},
author = {Tianxin Shi and Shuhan Shen and Xiang Gao and Lingjie Zhu},
year = {2019//},
pages = {315 - 19},
address = {Piscataway, NJ, USA},
abstract = {Accurate and robust visual localization under a wide range of viewing condition variations including season and illumination changes, as well as weather and day-night variations, is the key component for many computer vision and robotics applications. Under these conditions, most traditional methods would fail to locate the camera. In this paper we present a visual localization algorithm that combines structure-based method and image-based method with semantic information. Given semantic information about the query and database images, the retrieved images are scored according to the semantic consistency of the 3D model and the query image. Then the semantic matching score is used as weight for RANSAC's sampling and the pose is solved by a standard PnP solver. Experiments on the challenging long-term visual localization benchmark dataset demonstrate that our method has significant improvement compared with the state-of-the-arts.},
keywords = {image matching;image retrieval;iterative methods;pose estimation;sampling methods;solid modelling;visual databases;},
note = {structure-based method;image-based method;database images;images retrieval;query image;semantic matching score;sparse semantic 3D map;illumination changes;visual localization;season changes;3D model;RANSAC sampling;standard PnP solver;},
URL = {http://dx.doi.org/10.1109/ICIP.2019.8802957},
} 


@article{19925589 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Learning to calibrate: Reinforcement learning for guided calibration of visual-inertial rigs},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Nobre, F. and Heckman, C.},
volume = { 38},
number = { 12-13},
year = {2019/10/},
pages = {1388 - 402},
issn = {0278-3649},
address = {USA},
abstract = {We present a new approach to assisted intrinsic and extrinsic calibration with an observability-aware visual-inertial calibration system that guides the user through the calibration procedure by suggesting easy-to-perform motions that render the calibration parameters observable. This is done by identifying which subset of the parameter space is rendered observable with a rank-revealing decomposition of the Fisher information matrix, modeling calibration as a Markov decision process and using reinforcement learning to establish which discrete sequence of motions optimizes for the regression of the desired parameters. The goal is to address the assumption common to most calibration solutions: that sufficiently informative motions are provided by the operator. We do not make use of a process model and instead leverage an experience-based approach that is broadly applicable to any platform in the context of simultaneous localization and mapping. This is a step in the direction of long-term autonomy and ldquopower-on-and-gordquo robotic systems, making repeatable and reliable calibration accessible to the non-expert operator.},
keywords = {calibration;cameras;inertial navigation;learning (artificial intelligence);Markov processes;mobile robots;nonlinear filters;SLAM (robots);},
note = {experience-based approach;repeatable calibration;reliable calibration;guided calibration;visual-inertial rigs;assisted intrinsic calibration;extrinsic calibration;observability-aware visual-inertial calibration system;calibration procedure;render;calibration parameters;parameter space;rank-revealing decomposition;Fisher information matrix;Markov decision process;using reinforcement learning;motions optimizes;desired parameters;calibration solutions;sufficiently informative motions;process model;},
URL = {http://dx.doi.org/10.1177/0278364919844824},
} 


@inproceedings{19298782 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Learning Local Feature Descriptor with Motion Attribute For Vision-based Localization},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Yafei Song and Di Zhu and Jia Li and Yonghong Tian and Mingyang Li},
year = {2019//},
pages = {3794 - 801},
address = {Piscataway, NJ, USA},
abstract = {In recent years, camera-based localization has been widely used for robotic applications, and most proposed algorithms rely on local features extracted from recorded images. For better performance, the features used for open-loop localization are required to be short-term globally static, and the ones used for re-localization or loop closure detection need to be long-term static. Therefore, the motion attribute of a local feature point could be exploited to improve localization performance, e.g., the feature points extracted from moving persons or vehicles can be excluded from these systems due to their unsteadiness. In this paper, we design a fully convolutional network (FCN), named MD-Net, to perform motion attribute estimation and feature description simultaneously. MD-Net has a shared backbone network to extract features from the input image and two network branches to complete each sub-task. With MD-Net, we can obtain the motion attribute while avoiding increasing much more computation. Experimental results demonstrate that the proposed method can learn distinct local feature descriptor along with motion attribute only using an FCN, by outperforming competing methods by a wide margin. We also show that the proposed algorithm can be integrated into a vision-based localization algorithm to improve estimation accuracy significantly.},
keywords = {cameras;convolutional neural nets;feature extraction;learning (artificial intelligence);robot vision;SLAM (robots);},
note = {local feature point;fully convolutional network;MD-Net;motion attribute estimation;feature description;local feature descriptor;vision-based localization algorithm;camera-based localization;open-loop localization;re-localization;loop closure detection;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8967749},
} 


@inproceedings{15286217 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Work smart, not hard: recalling relevant experiences for vast-scale but time-constrained localisation},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Linegar, C. and Churchill, W. and Newman, P.},
year = {2015//},
pages = {90 - 7},
address = {Piscataway, NJ, USA},
abstract = {This paper is about life-long vast-scale localisation in spite of changes in weather, lighting and scene structure. Building upon our previous work in Experience-based Navigation [1], we continually grow and curate a visual map of the world that explicitly supports multiple representations of the same place. We refer to these representations as experiences, where a single experience captures the appearance of an environment under certain conditions. Pedagogically, an experience can be thought of as a visual memory. By accumulating experiences we are able to handle cyclic appearance change (diurnal lighting, seasonal changes, and extreme weather conditions) and also adapt to slow structural change. This strategy, although elegant and effective, poses a new challenge: In a region with many stored representations - which one(s) should we try to localise against given finite computational resources? By learning from our previous use of the experience-map, we can make predictions about which memories we should consider next, conditioned on how the robot is currently localised in the experience-map. During localisation, we prioritise the loading of past experiences in order to minimise the expected computation required. We do this in a probabilistic way and show that this memory policy significantly improves localisation efficiency, enabling long-term autonomy on robots with limited computational resources. We demonstrate and evaluate our system over three challenging datasets, totalling 206km of outdoor travel. We demonstrate the system in a diverse range of lighting and weather conditions, scene clutter, camera occlusions, and permanent structural change in the environment.},
keywords = {environmental factors;probability;robots;},
note = {time-constrained localisation;vast-scale localisation;scene structure;lighting structure;weather structure;visual map;experience-based navigation;visual memory;cyclic appearance change;diurnal lighting;seasonal changes;extreme weather conditions;experience-map;memory policy;localisation efficiency;long-term autonomy;robots;probabilistic way;computational resources;scene clutter;camera occlusions;permanent structural change;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7138985},
} 


@inproceedings{19951507 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Graph Optimization Methods for Large-Scale Crowdsourced Mapping},
journal = {2020 IEEE 23rd International Conference on Information Fusion (FUSION). Proceedings},
author = {Stoven-Dubois, A. and Dziri, A. and Leroy, B. and Chapuis, R.},
year = {2020//},
pages = {8 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Automotive players have recently shown an increasing interest in high-precision mapping, with the aim of enhancing vehicles safety and autonomy. Nevertheless, the acquisition, processing, and updates of accurate maps remains an economic challenge. Collaborative mapping through vehicles crowdsourcing represents a promising solution to tackle this problem. However, the potential scalability and accuracy provided by such an approach have yet to be studied and assessed. In this paper, we study the use of graph optimization in the scope of collaborative mapping. We build a map of geo-localized landmarks by crowdsourcing observations from multiple vehicles, and applying several successive map updates. We present different strategies to adapt graph optimization to the crowdsourced approach, and compare their performances in terms of map quality and scalability on simulation data. We show the critical requirement, in a long-term context, to ensure consistency of the map updates, and we propose a scalable solution which is able to build an accurate map of geolocalized landmarks.},
keywords = {cartography;graph theory;optimisation;outsourcing;regression analysis;road safety;road vehicles;},
note = {collaborative mapping;vehicles crowdsourcing;potential scalability;geo-localized landmarks;multiple vehicles;successive map updates;crowdsourced approach;map quality;scalable solution;accurate map;graph optimization methods;automotive players;high-precision mapping;vehicles safety;economic challenge;large-scale crowdsourced mapping;},
URL = {http://dx.doi.org/10.23919/FUSION45008.2020.9190292},
} 


@inproceedings{20256927 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Structured feature enhanced deep visual relocalization},
journal = {2020 3rd International Conference on Unmanned Systems (ICUS)},
author = {Qihao Peng and Zhiyu Xiang and Tianle Wang},
year = {2020//},
pages = {632 - 7},
address = {Piscataway, NJ, USA},
abstract = {Relocalization is a mandatary task for robots or unmanned vehicles which need to work under changing environments over the long term. Recently, deep learning has been successfully applied to visual relocalization, where the absolute camera pose can be regressed from monocular RGB images. However, the resulting accuracy is still sub-optimal. In this paper, we propose a deep relocalization network which regresses the global poses of the images in the scenario. Our model takes in tuples of images and enforces constraints between pose predictions for pairs as an additional loss term in training. The features coded from convolutional layers are further enhanced by two bidirectional LSTM in a structured way to boost the features' inner correlation, which is the main contribution of the work. Then the we use unlabeled data to further fine-tune the network and during inference we perform pose graph optimization (PGO) to get smoother and globally consistent pose predictions. Experiments on public indoor and outdoor dataset demonstrate that our model achieved better relocalization performance than the baseline and is more robust to illumination changes, texture-less areas and repetitive structures in the scenario.},
keywords = {cameras;feature extraction;graph theory;image colour analysis;image matching;image segmentation;learning (artificial intelligence);mobile robots;optimisation;pose estimation;recurrent neural nets;robot vision;SLAM (robots);},
note = {deep visual relocalization;mandatary task;unmanned vehicles;deep learning;absolute camera;monocular RGB images;resulting accuracy;deep relocalization network;global poses;enforces constraints;additional loss term;convolutional layers;bidirectional LSTM;features;graph optimization;smoother pose predictions;globally consistent pose predictions;relocalization performance;illumination changes;repetitive structures;structured feature;},
URL = {http://dx.doi.org/10.1109/ICUS50048.2020.9274860},
} 


@inproceedings{8634390 ,
language = {English},
copyright = {Copyright 2005, IEE},
title = {Use of classification and segmentation of sidescan sonar images for long term registration},
journal = {Oceans 2005 - Europe (IEEE Cat. No. 05EX1042)},
author = {Leblond, I. and Legris, M. and Solaiman, B.},
volume = {Vol. 1},
year = {2005//},
pages = {322 - 7},
address = {Piscataway, NJ, USA},
abstract = {This article handles the possibility of using classification and segmentation of sidescan sonar images for long term registration. In our case, long term registration means to find the displacement between two images which can have been mapped with many weeks or many months between them. The aim of this study is to help AUVs (autonomous underwater vehicles) to navigate, in particular to correct the drift of navigation sensors. This type of positioning raises two sorts of problems, which come from image properties: spatial variability and temporal variability. The first one is caused above all by the sonar geometry and appears for example like a modification of the shape or the position of the shadow according to the point of view. This effect can also be seen in textures, for example on megaripples of sand, which can be more or less visible depending on the point of view of the sonar. The second one is more the consequence of the seafloor physics: between two images, mapped at different times, some elements may have changed. An obvious example is the presence of evanescent "objects" like fishes but this variability can also be seen on sediments, which borders can move due to local bottom dynamics. With the aim to solve these problems and to provide reliable landmarks for matching, we decided to classify and segment the images. The data have first been corrected from TVG (time varying gain) effects and despeckelised in order to process images which are more representative of the seafloor. The basis idea is to use a supervised method of classification. To do that, we consider some parameters which are coming from a decomposition by Gabor filters, in order to segment with linear discriminant analysis and use of the nearest neighbour method. Registration needs accurately localised landmarks: so, this operation is split in several stages, refining step by step the classification, in order to obtain a map which describes the seafloor with the most possible detailed frontiers. Then, we present the obtained results considering five texture classes: rocks, megaripples, sand, mud and shadow. These several areas and their frontiers are the basis landmarks to match the images. However, before using the segmentation, we must check its reliability. So, it appears that the frontiers, though they are realistic, are not accurate enough to make a registration precise to few pixels, especially in rock areas. Similarly, according to the orientation of the ripples, they may be seen as ripples or sand. These observations are due to the directivity of the sonar, which caused these effects on the segmentation. To do registration, we must take into account these problems. So, the results of the segmentation will be used only for a coarse registration, in order to find quickly the area of interest but also to assess the reliability of our registration (matching on ripples areas is a priori less reliable than on rocks areas). The results of registration are shown, proving the good adequacy between reference image and test image. Others methods, more quantitative, will be able to be tested, to refine the results.},
keywords = {image classification;image registration;image segmentation;rocks;sand;sonar imaging;underwater vehicles;},
note = {sidescan sonar image classification;sidescan sonar image segmentation;sonar image registration;AUV;autonomous underwater vehicles;navigation sensor drift correction;image property;spatial variability;temporal variability;sonar geometry;sand megaripples;seafloor physics;evanescent objects;fish;sediments;seabottom dynamics;TVG effects;time varying gain effects;data despeckelised;image processing;supervised method;Gabor filters;linear discriminant analysis;nearest neighbour method;texture class;rocks;mud;shadow position;reference image;test image;},
} 


@article{20423689 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {2-entity random sample consensus for robust visual localization: framework, methods, and verifications},
journal = {IEEE Transactions on Industrial Electronics},
journal = {IEEE Trans. Ind. Electron. (USA)},
author = {Yanmei Jiao and Yue Wang and Xiaqing Ding and Bo Fu and Shoudong Huang and Rong Xiong},
volume = { 68},
number = { 5},
year = {2021/05/},
pages = {4519 - 28},
issn = {0278-0046},
address = {USA},
abstract = {Robust and efficient visual localization is essential for numerous robotic applications. However, it remains a challenging problem especially when significant environmental or perspective changes are present, as there are high percentage of outliers, i.e., incorrect feature matches between the query image and the map. In this article, we propose a novel 2-entity random sample consensus (RANSAC) framework using three-dimensional-two-dimensional point and line feature matches for visual localization with the aid of inertial measurements and derive minimal closed-form solutions using only 1 point 1 line or 2 point matches for both monocular and multi-camera system. The proposed 2-entity RANSAC can achieve higher robustness against outliers as multiple types of features are utilized and the number of matches needed to compute a pose is reduced. Furthermore, we propose a learning-based sampling strategy selection mechanism and a feature scoring network to be adaptive to different environmental characteristics such as structured and unstructured. Finally, both simulation and real-world experiments are performed to validate the robustness and effectiveness of the proposed method in scenarios with long-term and perspective changes.},
keywords = {cameras;computer vision;feature extraction;image matching;learning (artificial intelligence);path planning;pose estimation;robot vision;},
note = {different environmental characteristics;learning-based sampling strategy selection mechanism;higher robustness;2-entity RANSAC;multicamera system;1 point 1 line;closed-form solutions;line feature;three-dimensional-two-dimensional point;novel 2-entity random sample consensus framework;query image;incorrect feature;significant environmental perspective changes;numerous robotic applications;efficient visual localization;robust localization;robust visual localization;},
URL = {http://dx.doi.org/10.1109/TIE.2020.2984970},
} 


@inproceedings{13919296 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Robust Loop Closing Over Time},
journal = {2012 Robotics: Science and Systems},
author = {Latif, Y. and Cadena, C. and Neira, J.},
volume = {vol.8},
year = {2013//},
pages = {233 - 40},
address = {Cambridge, MA, USA},
abstract = {Long term autonomy in robots requires the ability to reconsider previously taken decisions when new evidence becomes available. Loop closing links generated by a place recognition system may become inconsistent as additional evidence arrives. This paper is concerned with the detection and exclusion of such contradictory information from the map being built, in order to recover the correct map estimate. We propose a novel consistency based method to extract the loop closure regions that agree both among themselves and with the robot trajectory over time. We also assume that the contradictory loop closures are inconsistent among themselves and with the robot trajectory. We support our proposal, the RRR algorithm, on well-known odometry systems, e.g. visual or laser, using the very efficient graph optimization framework g2o as back-end. We back our claims with several experiments carried out on real data.},
keywords = {feature extraction;graph theory;optimisation;robot vision;SLAM (robots);trajectory control;},
note = {long term robot autonomy;loop closing links;contradictory information;map estimation;consistency based method;loop closure regions extraction;robot trajectory;contradictory loop closures;RRR algorithm;odometry systems;graph optimization framework;},
} 


@inproceedings{14616887 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Multiple map hypotheses for planning and navigating in non-stationary environments},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Morris, T. and Dayoub, F. and Corke, P. and Wyeth, G. and Upcroft, B.},
year = {2014//},
pages = {2765 - 70},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a method to enable a mobile robot working in non-stationary environments to plan its path and localize within multiple map hypotheses simultaneously. The maps are generated using a long-term and short-term memory mechanism that ensures only persistent configurations in the environment are selected to create the maps. In order to evaluate the proposed method, experimentation is conducted in an office environment. Compared to navigation systems that use only one map, our system produces superior path planning and navigation in a non-stationary environment where paths can be blocked periodically, a common scenario which poses significant challenges for typical planners.},
keywords = {mobile robots;path planning;},
note = {multiple map hypotheses;nonstationary environments;mobile robot working;short-term memory mechanism;long-term memory mechanism;path planning;path navigation;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907255},
} 


@article{11959739 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Comparison of Two Image and Inertial Sensor Fusion Techniques for Navigation in Unmapped Environments},
journal = {IEEE Transactions on Aerospace and Electronic Systems},
journal = {IEEE Trans. Aerosp. Electron. Syst. (USA)},
author = {Taylor, C.N. and Veth, M.J. and Raquet, J.F. and Miller, M.M.},
volume = { 47},
number = { 2},
year = {2011/04/},
pages = {946 - 58},
issn = {0018-9251},
address = {USA},
abstract = {To enable navigation of miniature aerial vehicles (MAVs) with a low-quality inertial measurement unit (IMU), external sensors are typically fused with the information generated by the low-quality IMU. Most commercial systems for MAVs currently fuse GPS measurements with IMU information to navigate the MAV. However there are many scenarios in which an MAV might prove useful, but GPS is not available (e.g., indoors, urban terrain, etc.). Therefore several approaches have recently been introduced that couple information from an IMU with visual information (usually captured by an electro-optical camera). In general the methods for fusing visual information with an IMU utilizes one of two techniques: 1) applying rigid body constraints on where landmarks should appear in a set of two images (constraint-based fusion) or 2) simultaneously estimating the location of features that are observed by the camera (mapping) and the location of the camera (simultaneous localization and mapping-SLAM-based fusion). While each technique has some nuances associated with its implementation in a true MAV environment (i.e., computational requirements, real-time implementation, feature tracking, etc.), this paper focuses solely on answering the question "Which fusion technique (constraint- or SLAM-based) enables more accurate long-term MAV navigation?" To answer this question, specific implementations of a constraint- and SLAM-based fusion technique, with novel modifications for improved results on MAVs, are described. A basic simulation environment is used to perform a comparison of the constraint- and SLAM-based fusion methods. We demonstrate the superiority of SLAM-based techniques in specific MAV flight scenarios and discuss the relative weaknesses and strengths of each fusion approach.},
keywords = {aircraft;cameras;data visualisation;Global Positioning System;image fusion;mobile robots;remotely operated vehicles;SLAM (robots);},
note = {image comparison;inertial sensor fusion technique;unmapped environment;miniature aerial vehicle;inertial measurement unit;commercial system;GPS measurement;IMU information;visual information;constraint based fusion;simultaneous localization and mapping;SLAM based fusion;},
URL = {http://dx.doi.org/10.1109/TAES.2011.5751236},
} 


@inproceedings{12318092 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Memory management for real-time appearance-based loop closure detection},
journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
author = {Labbe, M. and Michaud, F.},
year = {2011//},
pages = {1271 - 6},
address = {Piscataway, NJ, USA},
abstract = {Loop closure detection is the process involved when trying to find a match between the current and a previously visited locations in SLAM. Over time, the amount of time required to process new observations increases with the size of the internal map, which may influence real-time processing. In this paper, we present a novel real-time loop closure detection approach for large-scale and long-term SLAM. Our approach is based on a memory management method that keeps computation time for each new observation under a fixed limit. Results demonstrate the approach's adaptability and scalability using four standard data sets.},
keywords = {image matching;mobile robots;robot vision;SLAM (robots);storage management;},
note = {memory management;real-time appearance;location matching;SLAM;internal map;real-time loop closure detection approach;},
URL = {http://dx.doi.org/10.1109/IROS.2011.6048225},
} 


@article{11955798 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Indoor navigation to support the blind person using true pathway within the map},
journal = {Journal of Computer Sciences},
journal = {J. Comput. Sci. (USA)},
author = {Ali, A.M. and Nordin, M.J.},
volume = { 6},
number = { 7},
year = {2010//},
pages = {740 - 7},
issn = {1549-3636},
address = {USA},
abstract = {Problem statement: Map creation remains a very active field in the robotics and AI community, however it contains some challenge points like data association and the high degree of accuracy of localization which are seems to be difficult in some cases, more than that, most of these study focus on the robot navigation, without any consideration for the semantic of the environment, to serve human like blind persons. Approach: This study introduced a monocular SLAM method, which uses the Scale Invariant Features Transform (SIFT) representation for the scene. The scene represented as clouds of sift features within the map; this hierarchical representation of space, serving to estimate the current direction in the environment within the current session. The system exploited the tracking of the same features of successive frames to calculate scalar weights for these features, to build a map of the environment indicating the camera movement, then by comparing the camera movement of the current moving with the true pathway within the same session the system can help and advice the blind person to navigate more confidently, through auditory information for the path way in the surroundings. Extended Kalman Filter (EKF) used to estimate the camera movement within the successive frames. Results: The experimental work tested using the proposed method with a hand-held camera walking in indoor environment. The results show a good estimation on the spatial locations of the camera within few milliseconds. Tracking of the true pathway in addition to semantic environment within the session can give a good support to the blind person for navigation. Conclusion: The study presented new semantic features model helping the blind person for navigation environment using these clouds of features, for long-term appearance-based localization of a cane with web camera vision as the external sensor.},
keywords = {handicapped aids;Kalman filters;sensor fusion;SLAM (robots);},
note = {indoor navigation;blind person support;true pathway;map creation;AI community;robotic community;data association;SLAM method;scale invariant features transform;SIFT;extended Kalman filter;EKF;},
URL = {http://dx.doi.org/10.3844/jcssp.2010.740.747},
} 


@article{16393824 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Deformable map matching for uncertain loop-less maps [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Kanji, T.},
year = {8 Sept. 2016},
pages = {7 pp. - },
address = {USA},
abstract = {In the classical context of robotic mapping and localization, map matching is typically defined as the task of finding a rigid transformation (i.e., 3DOF rotation/translation on the 2D moving plane) that aligns the query and reference maps built by mobile robots. This definition is valid in loop-rich trajectories that enable a mapper robot to close many loops, for which precise maps can be assumed. The same cannot be said about the newly emerging autonomous navigation and driving systems, which typically operate in loop-less trajectories that have no large loop (e.g., straight paths). In this paper, we propose a solution that overcomes this limitation by merging the two maps. Our study is motivated by the observation that even when there is no large loop in either the query or reference map, many loops can often be obtained in the merged map. We add two new aspects to map matching: (1) image retrieval with discriminative deep convolutional neural network (DCNN) features, which efficiently generates a small number of good initial alignment hypotheses; and (2) map merge, which jointly deforms the two maps to minimize differences in shape between them. To realize practical computation time, we also present a preemption scheme that avoids excessive evaluation of useless map-matching hypotheses. To verify our approach experimentally, we created a novel collection of uncertain loop-less maps by utilizing the recently published North Campus Long-Term (NCLT) dataset and its ground-truth GPS data. The results obtained using these map collections confirm that our approach improves on previous map-matching approaches.},
keywords = {convolution;image matching;image retrieval;mobile robots;neural nets;path planning;robot vision;},
note = {deformable map matching;uncertain loop-less maps;robotic mapping;robotic localization;2D moving plane;mobile robots;loop-rich trajectories;mapper robot;autonomous navigation and driving systems;loop-less trajectories;image retrieval;discriminative deep convolutional neural network features;DCNN features;map merge;shape difference minimization;uncertain loopless maps;North Campus Long-Term dataset;NCLT dataset;GPS data;map collections;},
} 


@article{20411073 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {MP3: A Unified Model to Map, Perceive, Predict and Plan [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Casas, S. and Sadat, A. and Urtasun, R.},
year = {2021/01/17},
pages = {22 pp. - },
address = {USA},
abstract = {High-definition maps (HD maps) are a key component of most modern self-driving systems due to their valuable semantic and geometric information. Unfortunately, building HD maps has proven hard to scale due to their cost as well as the requirements they impose in the localization system that has to work everywhere with centimeter-level accuracy. Being able to drive without an HD map would be very beneficial to scale self-driving solutions as well as to increase the failure tolerance of existing ones (e.g., if localization fails or the map is not up-to-date). Towards this goal, we propose MP3, an end-to-end approach to mapless driving where the input is raw sensor data and a high-level command (e.g., turn left at the intersection). MP3 predicts intermediate representations in the form of an online map and the current and future state of dynamic agents, and exploits them in a novel neural motion planner to make interpretable decisions taking into account uncertainty. We show that our approach is significantly safer, more comfortable, and can follow commands better than the baselines in challenging long-term closed-loop simulations, as well as when compared to an expert driver in a large-scale real-world dataset.},
keywords = {closed loop systems;collision avoidance;mobile robots;motion control;neural nets;path planning;robot vision;traffic engineering computing;},
note = {MP3;HD map;valuable semantic information;geometric information;localization system;mapless driving;online map;},
} 


@inproceedings{15617832 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Improved SeqSLAM for real-time place recognition and navigation error correction},
journal = {2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC). Proceedings},
author = {Yujie Wang and Xiaoping Hu and Junxiang Lian and Lilian Zhang and Xianglong Kong},
volume = {vol.1},
year = {2015//},
pages = {260 - 4},
address = {Los Alamitos, CA, USA},
abstract = {Place recognition plays an important role in long term navigation in challenging environment and Seq SLAM has achieved quite remarkable results. In this paper, we mainly adopt three strategies to improve the original Seq SLAM algorithm: integrating Seq SLAM with odometry, optimizing sequence searching strategy and multi-scale sequence matching. The improved algorithm is evaluated using the KITTI dataset. The template library is created online using navigation information from the sliding-window visual-inertial odometer. When a place is recognized, the corresponding information is used as observation of the filter. The result shows the superiority of the proposed method in real-time place recognition. The optimized sequence searching strategy performs much better in minor deviations. Meanwhile, the advantages of longer sequence match (higher recall rate) and short sequence match (precise location) are combined together. At last, the navigation errors are greatly reduced by close-loop detection. The overall position error of odometer with Seq SLAM is 20.3m (0.55% of the trajectory), which is much smaller than the navigation errors of the single odometer (32.0m, 0.86%).},
keywords = {image matching;object recognition;path planning;robot vision;SLAM (robots);},
note = {SeqSLAM algorithm;realtime place recognition;navigation error correction;odometry;sequence searching strategy;multiscale sequence matching;KITTI dataset;sliding-window visual-inertial odometer;},
URL = {http://dx.doi.org/10.1109/IHMSC.2015.23},
} 


@inproceedings{12317981 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Dense multi-planar scene estimation from a sparse set of images},
journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
author = {Argiles, A. and Civera, J. and Montesano, L.},
year = {2011//},
pages = {4448 - 54},
address = {Piscataway, NJ, USA},
abstract = {Ego-motion estimation and 3D scene reconstruction from image data has been a long term aim both in the Robotics and Computer Vision communities. Nevertheless, while both visual SLAM and Structure from Motion already provide an accurate ego-motion estimation, visual scene estimation does not offer yet such a satisfactory result; being in most cases limited to a sparse set of salient points. In this paper we propose an algorithm to densify a sparse point-based reconstruction into a dense multi-plane based one, from the only input of a set of sparse images.},
keywords = {estimation theory;image reconstruction;robot vision;set theory;SLAM (robots);},
note = {dense multiplanar scene estimation;image sparse set;egomotion estimation;3D scene reconstruction;computer vision;robotic vision;SLAM;sparse point based reconstruction;},
URL = {http://dx.doi.org/10.1109/IROS.2011.6048114},
} 


@article{17659950 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Error Modelling for Multi-Sensor Measurements in Infrastructure-Free Indoor Navigation},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Ruotsalainen, L. and Kirkko-Jaakkola, M. and Rantanen, J. and Makela, M.},
volume = { 18},
number = { 2},
year = {2018/02/},
pages = {590 (17 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {The long-term objective of our research is to develop a method for infrastructure-free simultaneous localization and mapping (SLAM) and context recognition for tactical situational awareness. Localization will be realized by propagating motion measurements obtained using a monocular camera, a foot-mounted Inertial Measurement Unit (IMU), sonar, and a barometer. Due to the size and weight requirements set by tactical applications, Micro-Electro-Mechanical (MEMS) sensors will be used. However, MEMS sensors suffer from biases and drift errors that may substantially decrease the position accuracy. Therefore, sophisticated error modelling and implementation of integration algorithms are key for providing a viable result. Algorithms used for multi-sensor fusion have traditionally been different versions of Kalman filters. However, Kalman filters are based on the assumptions that the state propagation and measurement models are linear with additive Gaussian noise. Neither of the assumptions is correct for tactical applications, especially for dismounted soldiers, or rescue personnel. Therefore, error modelling and implementation of advanced fusion algorithms are essential for providing a viable result. Our approach is to use particle filtering (PF), which is a sophisticated option for integrating measurements emerging from pedestrian motion having non-Gaussian error characteristics. This paper discusses the statistical modelling of the measurement errors from inertial sensors and vision based heading and translation measurements to include the correct error probability density functions (pdf) in the particle filter implementation. Then, model fitting is used to verify the pdfs of the measurement errors. Based on the deduced error models of the measurements, particle filtering method is developed to fuse all this information, where the weights of each particle are computed based on the specific models derived. The performance of the developed method is tested via two experiments, one at a university's premises and another in realistic tactical conditions. The results show significant improvement on the horizontal localization when the measurement errors are carefully modelled and their inclusion into the particle filtering implementation correctly realized.},
keywords = {barometers;Gaussian noise;inertial navigation;Kalman filters;measurement errors;microsensors;mobile robots;motion measurement;particle filtering (numerical methods);probability;sensor fusion;sensors;SLAM (robots);},
note = {tactical situational awareness;context recognition;infrastructure-free simultaneous localization;infrastructure-free indoor navigation;multisensor measurements;particle filtering method;deduced error models;particle filter implementation;correct error probability density functions;translation measurements;vision based heading;inertial sensors;statistical modelling;nonGaussian error characteristics;advanced fusion algorithms;Kalman filters;multisensor fusion;integration algorithms;sophisticated error modelling;MEMS sensors;MicroElectro-Mechanical sensors;sonar;},
URL = {http://dx.doi.org/10.3390/s18020590},
} 


@article{12773099 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Visual Odometry : Part II: Matching, Robustness, Optimization, and Applications},
journal = {IEEE Robotics &amp; Automation Magazine},
journal = {IEEE Robot. Autom. Mag. (USA)},
author = {Fraundorfer, F. and Scaramuzza, D.},
volume = { 19},
number = { 2},
year = {2012/06/},
pages = {78 - 90},
issn = {1070-9932},
address = {USA},
abstract = {Part II of the tutorial has summarized the remaining building blocks of the VO pipeline: specifically, how to detect and match salient and repeatable features across frames and robust estimation in the presence of outliers and bundle adjustment. In addition, error propagation, applications, and links to publicly available code are included. VO is a well understood and established part of robotics. VO has reached a maturity that has allowed us to successfully use it for certain classes of applications: space, ground, aerial, and underwater. In the presence of loop closures, VO can be used as a building block for a complete SLAM algorithm to reduce motion drift. Challenges that still remain are to develop and demonstrate large-scale and long-term implementations, such as driving autonomous cars for hundreds of miles. Such systems have recently been demonstrated using Lidar and Radar sensors [86]. However, for VO to be used in such systems, technical issues regarding robustness and, especially, long-term stability have to be resolved. Eventually, VO has the potential to replace Lidar-based systems for egomotion estimation, which are currently leading the state of the art in accuracy, robustness, and reliability. VO offers a cheaper and mechanically easier-to-manufacture solution for egomotion estimation, while, additionally, being fully passive. Furthermore, the ongoing miniaturization of digital cameras offers the possibility to develop smaller and smaller robotic systems capable of ego-motion estimation.},
keywords = {cameras;distance measurement;motion estimation;robot vision;SLAM (robots);},
note = {visual odometry;VO pipeline;robust estimation;error propagation;space application;ground application;aerial application;underwater application;loop closures;SLAM algorithm;motion drift;driving autonomous car;radar sensor;Lidar-based system;egomotion estimation;easier-to-manufacture solution;digital camera;robotic system;},
URL = {http://dx.doi.org/10.1109/MRA.2012.2182810},
} 


@article{16848819 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Unsupervised Place Discovery for Visual Place Classification [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Xiaoxiao, F. and Kanji, T. and Kouya, I.},
year = {2016/12/20},
pages = {5 pp. - },
address = {USA},
abstract = {In this study, we explore the use of deep convolutional neural networks (DCNNs) in visual place classification for robotic mapping and localization. An open question is how to partition the robot's workspace into places to maximize the performance (e.g., accuracy, precision, recall) of potential DCNN classifiers. This is a chicken and egg problem: If we had a well-trained DCNN classifier, it is rather easy to partition the robot's workspace into places, but the training of a DCNN classifier requires a set of pre-defined place classes. In this study, we address this problem and present several strategies for unsupervised discovery of place classes ("time cue," "location cue," "time-appearance cue," and "location-appearance cue"). We also evaluate the efficacy of the proposed methods using the publicly available University of Michigan North Campus Long-Term (NCLT) Dataset.},
keywords = {feedforward neural nets;image classification;mobile robots;motion control;robot vision;},
note = {unsupervised place discovery;visual place classification;deep convolutional neural networks;DCNN;robotic mapping;robotic localization;predefined place classes;University of Michigan north campus long-term dataset;NCLT;},
} 


@inproceedings{10749152 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {The autonomous city explorer (ACE) project - mobile robot navigation in highly populated urban environments},
journal = {2009 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Lidoris, G. and Rohrmuller, F. and Wollherr, D. and Buss, M.},
year = {2009//},
pages = {1416 - 22},
address = {Piscataway, NJ, USA},
abstract = {One of the greatest challenges nowadays in robotics is the advancement of robots from industrial tools to companions and helpers of humans, operating in natural, populated environments. In this respect, the Autonomous City Explorer (ACE) project aims to combine the research fields of autonomous mobile robot navigation and human robot interaction. A robot has been created that is capable of navigating in an unknown, highly populated, urban environment, based only on information extracted through interaction with passers-by and its local perception capabilities. This paper describes the algorithms and architecture that make up the navigation subsystem of ACE. More specifically, the algorithms used for Simultaneous Localization and Mapping (SLAM), path planning in dynamic environments and behavior selection are presented, as well as the system architecture that integrates them to a complete working system. Results from an extended field experiment, where the robot navigated autonomously through the downtown city area of Munich, are analyzed and show that the robot is capable of long-term, safe navigation in real-world settings.},
keywords = {human-robot interaction;mobile robots;path planning;SLAM (robots);},
note = {autonomous city explorer project;autonomous mobile robot navigation;highly populated urban environment;human robot interaction;simultaneous localization-and-mapping;SLAM;path planning;dynamic environment;},
URL = {http://dx.doi.org/10.1109/ROBOT.2009.5152534},
} 


@inproceedings{7169545 ,
language = {English},
copyright = {Copyright 2002, IEE},
title = {Mobile robotics in the long term-exploring the fourth dimension},
journal = {Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180)},
author = {Austin, D. and Fletcher, L. and Zelinsky, A.},
volume = {vol.2},
year = {2001//},
pages = {613 - 18},
address = {Piscataway, NJ, USA},
abstract = {Explores the issues involved in deployment of mobile robots in real-world situations and presents solutions and approaches under development at the Australian National University. For deployment of mobile robots outside of the laboratory, long-term operation is required. Hence, we have developed an automatic recharging system. In addition, a Web-based teleoperation system is used to provide missions to test the long-term reliability of the robot. The final aspect of real-world operation that is explored here is operations in dynamic environments. To date, researchers have assumed static environments for mapping and localisation. We propose methods to avoid this restriction.},
keywords = {active vision;Internet;mobile robots;reliability;robot vision;telerobotics;},
note = {mobile robotics;real-world situations;Australian National University;long-term operation;automatic recharging system;Web-based teleoperation system;long-term reliability;dynamic environments;},
URL = {http://dx.doi.org/10.1109/IROS.2001.976237},
} 


@inproceedings{12315802 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Self Help: Seeking Out Perplexing Images for Ever Improving Navigation},
journal = {IEEE International Conference on Robotics and Automation},
author = {Paul, R. and Newman, P.},
year = {2011//},
pages = {445 - 51},
address = {Piscataway, NJ, USA},
abstract = {This paper is a demonstration of how a robot can, through introspection and then targeted data retrieval, improve its own performance. It is a step in the direction of lifelong learning and adaptation and is motivated by the desire to build robots that have plastic competencies which are not baked in. They should react to and benefit from use. We consider a particular instantiation of this problem in the context of place recognition. Based on a topic based probabilistic model of images, we use a measure of perplexity to evaluate how well a working set of background images explain the robot's online view of the world. Offline, the robot then searches an external resource to seek out additional background images that bolster its ability to localise in its environment when used next. In this way the robot adapts and improves performance through use.},
keywords = {image retrieval;mobile robots;path planning;probability;robot vision;SLAM (robots);},
note = {self help;perplexing image seeking out;navigation improvement;lifelong learning;place recognition;image probabilistic model;FAB-MAP algorithm;introspection;data retrieval;},
} 


@inproceedings{16503990 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Robust sound source mapping using three-layered selective audio rays for mobile robots},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Daobilige Su and Nakamura, K. and Nakadai, K. and Miro, J.V.},
year = {2016//},
pages = {2771 - 7},
address = {Piscataway, NJ, USA},
abstract = {This paper investigates sound source mapping in a real environment using a mobile robot. Our approach is based on audio ray tracing which integrates occupancy grids and sound source localization using a laser range finder and a microphone array. Previous audio ray tracing approaches rely on all observed rays and grids. As such observation errors caused by sound reflection, sound occlusion, wall occlusion, sounds at misdetected grids, etc. can significantly degrade the ability to locate sound sources in a map. A three-layered selective audio ray tracing mechanism is proposed in this work. The first layer conducts frame-based unreliable ray rejection (sensory rejection) considering sound reflection and wall occlusion. The second layer introduces triangulation and audio tracing to detect falsely detected sound sources, rejecting audio rays associated to these misdetected sounds sources (short-term rejection). A third layer is tasked with rejecting rays using the whole history (long-term rejection) to disambiguate sound occlusion. Experimental results under various situations are presented, which proves the effectiveness of our method.},
keywords = {acoustic signal detection;acoustic signal processing;audio signal processing;mobile robots;ray tracing;},
note = {robust sound source mapping;mobile robots;occupancy grids;sound source localization;laser range finder;microphone array;observation errors;sound reflection;sound occlusion;wall occlusion;three-layered selective audio ray tracing mechanism;frame-based unreliable ray rejection;triangulation;sound source detection;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759430},
} 


@article{19820626 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Multi-domain airflow modeling and ventilation characterization using mobile robots, stationary sensors and machine learning},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Hernandez Bennetts, V. and Kamarudin, K. and Wiedemann, T. and Kucner, T.P. and Somisetty, S.L. and Lilienthal, A.J.},
volume = { 19},
number = { 5},
year = {2019/03/},
pages = {1119 (21 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {Ventilation systems are critically important components of many public buildings and workspaces. Proper ventilation is often crucial for preventing accidents, such as explosions in mines and avoiding health issues, for example, through long-term exposure to harmful respirable matter. Validation and maintenance of ventilation systems is thus of key interest for plant operators and authorities. However, methods for ventilation characterization, which allow us to monitor whether the ventilation system in place works as desired, hardly exist. This article addresses the critical challenge of ventilation characterization-measuring and modelling air flow at micro-scales-that is, creating a high-resolution model of wind speed and direction from airflow measurements. Models of the near-surface micro-scale flow fields are not only useful for ventilation characterization, but they also provide critical information for planning energy-efficient paths for aerial robots and many applications in mobile robot olfaction. In this article we propose a heterogeneous measurement system composed of static, continuously sampling sensing nodes, complemented by localized measurements, collected during occasional sensing missions with a mobile robot. We introduce a novel, data-driven, multi-domain airflow modelling algorithm that estimates (1) fields of posterior distributions over wind direction and speed (ldquoventilation mapsrdquo, spatial domain); (2) sets of ventilation calendars that capture the evolution of important airflow characteristics at measurement positions (temporal domain); and (3) a frequency domain analysis that can reveal periodic changes of airflow in the environment. The ventilation map and the ventilation calendars make use of an improved estimation pipeline that incorporates a wind sensor model and a transition model to better filter out sporadic, noisy airflow changes. These sudden changes may originate from turbulence or irregular activity in the surveyed environment and can, therefore, disturb modelling of the relevant airflow patterns. We tested the proposed multi-domain airflow modelling approach with simulated data and with experiments in a semi-controlled environment and present results that verify the accuracy of our approach and its sensitivity to different turbulence levels and other disturbances. Finally, we deployed the proposed system in two different real-world industrial environments (foundry halls) with different ventilation regimes for three weeks during full operation. Since airflow ground truth cannot be obtained, we present a qualitative discussion of the generated airflow models with plant operators, who concluded that the computed models accurately depicted the expected airflow patterns and are useful to understand how pollutants spread in the work environment. This analysis may then provide the basis for decisions about corrective actions to avoid long-term exposure of workers to harmful respirable matter.},
keywords = {chemioception;computerised instrumentation;flow measurement;flow sensors;learning (artificial intelligence);mobile robots;ventilation;},
note = {high-resolution model;airflow measurements;microscale flow fields;mobile robot olfaction;heterogeneous measurement system;localized measurements;ventilation map;spatial domain analysis;ventilation calendars;measurement positions;temporal domain;frequency domain analysis;wind sensor model;transition model;sporadic airflow changes;noisy airflow changes;generated airflow models;plant operators;computed models;expected airflow patterns;harmful respirable matter;multidomain airflow modeling;stationary sensors;ventilation system characterization;airflow ground truth patterns;air flow modelling;mobile robot;machine learning;},
URL = {http://dx.doi.org/10.3390/s19051119},
} 


@article{11266231 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Keypoint design and evaluation for place recognition in 2D lidar maps},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Bosse, M. and Zlot, R.},
volume = { 57},
number = { 12},
year = {2009/12/31},
pages = {1211 - 24},
issn = {0921-8890},
address = {Netherlands},
abstract = {We address the place recognition problem, which we define as the problem of establishing whether an observed location has been previously seen, and if so, determining the transformation aligning the current observations to an existing map. In the contexts of robot navigation and mapping, place recognition amounts to globally localizing a robot or map segment without being given any prior estimate. An efficient method of solving this problem involves first selecting a set of keypoints in the scene which store an encoding of their local region, and then utilizing a sublinear-time search into a database of keypoints previously generated from the global map to identify places with common features. We present an algorithm to embed arbitrary keypoint descriptors in a reduced-dimension metric space, in order to frame the problem as an efficient nearest neighbor search. Given that there are a multitude of possibilities for keypoint design, we propose a general methodology for comparing keypoint location selection heuristics and descriptor models that describe the region around the keypoint. With respect to selecting keypoint locations, we introduce a metric that encodes how likely it is that the keypoint will be found in the presence of noise and occlusions during mapping passes. Metrics for keypoint descriptors are used to assess the distinguishability between the distributions of matches and non-matches and the probability the correct match will be found in an approximate k-nearest neighbors search. Verification of the test outcomes is done by comparing the various keypoint designs on a kilometers-scale place recognition problem. We apply our design evaluation methodology to three keypoint selection heuristics and six keypoint descriptor models. A full place recognition system is presented, including a series of match verification algorithms which effectively filter out false positives. Results from city-scale and long-term mapping problems illustrate our approach for both offline and online SLAM, map merging, and global localization and demonstrate that our algorithm is able to produce accurate maps over trajectories of hundreds of kilometers. [All rights reserved Elsevier].},
keywords = {optical radar;pattern recognition;SLAM (robots);},
note = {keypoint design;place recognition;2D lidar maps;robot navigation;sublinear time search;arbitrary keypoint descriptors;k-nearest neighbors search;match verification algorithms;mapping problems;SLAM;},
URL = {http://dx.doi.org/10.1016/j.robot.2009.07.009},
} 


@inproceedings{13998964 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {A metric approach for environments mapping},
journal = {2013 International Conference on Control, Decision and Information Technologies (CoDIT)},
author = {Slimane, N. and Khireddine, M.S. and Chafaa, K.},
year = {2013//},
pages = {647 - 52},
address = {Piscataway, NJ, USA},
abstract = {One of the main issues in mobile robotics is the autonomous navigation of a mobile robot in an unknown environment. Concurrent mapping and localisation or simultaneous localisation and mapping is a stochastic map building method which permits consistent robot navigation without requiring an a priori map. The governing idea which guides autonomous robotics consists in saying that the vehicle builds its chart progressively during exploration enabling it to evolve in the long term in unknown places in advance. When the robot environment chart is not known a priori, a generation module of incremental chart must obligatorily be integrated into the navigation system. The map is built incrementally as the robot observes the environment with its on-board sensors and, at the same time, is used to localise the robot. Unfortunately, the inaccuracy of the odometric sensors does not allow a sufficiently correct positioning of the robot. In this paper, simultaneous localisation and map building is performed with a metric approach which permits both precision and robustness. The most important innovation of the approach is the way how errors in the robot localisation control are handled by map building using the landmarks localisation information. The method uses data from a laser scanner to extract distances and orientations of landmarks and combines control localisation and metric paradigm. The metric approach, based on the Kalman filter, uses a new concept to avoid the problem of the drift in odometry. The simulation section will validate the maps representation approach and presents different aspect of environments.},
keywords = {distance measurement;Kalman filters;mobile robots;navigation;object detection;position control;robot vision;SLAM (robots);stochastic processes;},
note = {metric approach;environment mapping;mobile robotics;autonomous navigation;unknown environment;concurrent mapping and localisation;simultaneous localisation and mapping;stochastic map building method;robot navigation;autonomous robotics;vehicle;exploration;robot environment chart;incremental chart;navigation system;robot on-board sensors;odometric sensors;robot positioning;robustness;robot localisation control;landmark localisation information;laser scanner data;landmark distance extraction;landmark orientation extraction;control localisation;Kalman filter;odometry drift;},
URL = {http://dx.doi.org/10.1109/CoDIT.2013.6689619},
} 


@inproceedings{13195014 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Laser-only road-vehicle localization with dual 2D push-broom LIDARS and 3D priors},
journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2012)},
author = {Baldwin, I. and Newman, P.},
year = {2012//},
pages = {2490 - 7},
address = {Piscataway, NJ, USA},
abstract = {We demonstrate the viability of using 2D LIDAR data as the sole means for accurate, robust, long-term road-vehicle localization within a prior map in a complex, dynamic real-world setting. We utilize a dual-LIDAR system - one oriented horizontally, in order to infer vehicle linear and rotational velocity, and one declined to capture a dense view of the surrounds - that allows us to estimate both velocity and position within a prior map. We show how probabilistically modelling the noisy local velocity estimates from the horizontal laser feed, fusing these estimates with data from the declined LIDAR to form a dense 3D swathe and matching this swathe statistically within a map will allow for robust, long-term position estimation. We accommodate estimation errors induced by passing vehicles, pedestrians, ground-strike etc., by learning a positional-dependent sensor model - that is, a sensor-model that varies spatially - and show that learning such a model for LIDAR data allows us to deal gracefully with the complexities of realworld data. We validate the concept over more than 9 kilometres of driven distance in and around the town of Woodstock, Oxfordshire.},
keywords = {navigation;optical radar;optical sensors;probability;road vehicles;},
note = {laser-only road-vehicle localization;dual 2D push-broom LIDARS;3D priors;dynamic real-world setting;rotational velocity;vehicle linear velocity;probabilistic modelling;estimation errors;pedestrians;ground-strike;sensor-model;horizontal laser feed;long-term road-vehicle localization;},
URL = {http://dx.doi.org/10.1109/IROS.2012.6385677},
} 


@article{15947956 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Localizing ground penetrating RADAR: a step toward robust autonomous ground vehicle localization},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Cornick, M. and Koechling, J. and Stanley, B. and Beijia Zhang},
volume = { 33},
number = { 1},
year = {2016/01/},
pages = {82 - 102},
issn = {1556-4959},
address = {USA},
abstract = {Autonomous ground vehicles navigating on road networks require robust and accurate localization over long-term operation and in a wide range of adverse weather and environmental conditions. GPS/INS (inertial navigation system) solutions, which are insufficient alone to maintain a vehicle within a lane, can fail because of significant radio frequency noise or jamming, tall buildings, trees, and other blockage or multipath scenarios. LIDAR and camera map-based vehicle localization can fail when optical features become obscured, such as with snow or dust, or with changes to gravel or dirt road surfaces. Localizing ground penetrating radar (LGPR) is a new mode of a priori map-based vehicle localization designed to complement existing approaches with a low sensitivity to failure modes of LIDAR, camera, and GPS/INS sensors due to its low-frequency RF energy, which couples deep into the ground. Most subsurface features detected are inherently stable over time. Significant research, discussed herein, remains to prove general utility. We have developed a novel low-profile ultra-low power LGPR system and demonstrated real-time operation underneath a passenger vehicle. A correlation maximizing optimization technique was developed to allow real-time localization at 126 Hz. Here we present the detailed design and results from highway testing, which uses a simple heuristic for fusing LGPR estimates with a GPS/INS system. Cross-track localization accuracies of 4.3 cm RMS relative to a &ldquo;truth&rdquo; RTK GPS/INS unit at speeds up to 100 km/h (60 mph) are demonstrated. These results, if generalizable, introduce a widely scalable real-time localization method with cross-track accuracy as good as or better than current localization methods.},
keywords = {Global Positioning System;ground penetrating radar;inertial navigation;position control;remotely operated vehicles;road vehicles;},
note = {low-profile ultra-low power LGPR system;camera map-based vehicle localization;light detection and ranging;LIDAR;inertial navigation system;Global Positioning System;INS solution;GPS solution;road networks;autonomous ground vehicle localization;ground penetrating radar localization;frequency 126 Hz;},
URL = {http://dx.doi.org/10.1002/rob.21605},
} 


@inproceedings{12375648 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Natural landmark-based monocular localization for MAVs},
journal = {2011 IEEE International Conference on Robotics and Automation (ICRA 2011)},
author = {Wendel, A. and Irschara, A. and Bischof, H.},
year = {2011//},
pages = {5792 - 9},
address = {Piscataway, NJ, USA},
abstract = {Highly accurate localization of a micro aerial vehicle (MAV) with respect to a scene is important for a wide range of applications, in particular surveillance and inspection. Most existing approaches to visual localization focus on indoor environments, while such tasks require outdoor navigation. Within this work, we introduce a novel algorithm for monocular visual localization for MAVs based on the concept of virtual views in 3D space. Under the assumption that significant parts of the scene do not alter their geometry and serve as natural landmarks, the accuracy of our visual approach outperforms consumer grade GPS systems. In an experimental setup we compare our approach to a state-of-the-art visual SLAM algorithm and evaluate the performance by geometric validation from an observer's view. As our method directly allows global registration, it is neither prone to drift nor bias. This makes it well suited for long-term autonomous navigation.},
keywords = {aircraft;geometry;image reconstruction;microrobots;navigation;robot vision;SLAM (robots);solid modelling;surveillance;},
note = {micro aerial vehicle;surveillance;inspection;outdoor navigation;3D space;visual SLAM algorithm;geometric algorithm;global registration;autonomous navigation;natural landmark based monocular visual localization;MAV;},
URL = {http://dx.doi.org/10.1109/ICRA.2011.5980317},
} 


@inproceedings{18401110 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Stabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Peng Yin and Lingyun Xu and Zhe Liu and Lu Li and Salman, H. and Yuqing He and Weiliang Xu and Hesheng Wang and Choset, H.},
year = {2018//},
pages = {1162 - 7},
address = {Piscataway, NJ, USA},
abstract = {Place recognition is one of the major challenges for the LiDAR-based effective localization and mapping task. Traditional methods are usually relying on geometry matching to achieve place recognition, where a global geometry map need to be restored. In this paper, we accomplish the place recognition task based on an end-to-end feature learning framework with the LiDAR inputs. This method consists of two core modules, a dynamic octree mapping module that generates local 2D maps with the consideration of the robot's motion; and an unsupervised place feature learning module which is an improved adversarial feature learning network with additional assistance for the long-term place recognition requirement. More specially, in place feature learning, we present an additional Generative Adversarial Network with a designed Conditional Entropy Reduction module to stabilize the feature learning process in an unsupervised manner. We evaluate the proposed method on the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental results show that the proposed method outperforms state-of-the-art in place recognition tasks under long-term applications. What's more, the feature size and inference efficiency in the proposed method are applicable in real-time performance on practical robotic platforms.},
keywords = {entropy;feature extraction;geometry;image matching;image recognition;learning (artificial intelligence);mobile robots;octrees;optical radar;robot vision;unsupervised learning;},
note = {Generative Adversarial Network;adversarial feature;place recognition;global geometry map;Conditional Entropy Reduction module;unsupervised place feature;local 2D maps;dynamic octree mapping module;core modules;LiDAR inputs;end-to-end feature;geometry matching;traditional methods;LiDAR-based place recognition;unsupervised feature learning;feature size;place recognition task;North Campus Long-Term LiDAR dataset;feature learning process;place feature learning;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8593562},
} 


@article{18880425 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving Study of Driver Behavior and Interaction With Automation},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Fridman, L. and Brown, D.E. and Glazer, M. and Angell, W. and Dodd, S. and Jenik, B. and Terwilliger, J. and Patsekin, A. and Kindelsberger, J. and Li Ding and Seaman, S. and Mehler, A. and Sipperley, A. and Pettinato, A. and Seppelt, B.D. and Angell, L. and Mehler, B. and Reimer, B.},
volume = { 7},
year = {2019//},
pages = {102021 - 38},
issn = {2169-3536},
address = {USA},
abstract = {Today, and possibly for a long time to come, the full driving task is too complex an activity to be fully formalized as a sensing-acting robotics system that can be explicitly solved through model-based and learning-based approaches in order to achieve full unconstrained vehicle autonomy. Localization, mapping, scene perception, vehicle control, trajectory optimization, and higher-level planning decisions associated with autonomous vehicle development remain full of open challenges. This is especially true for unconstrained, real-world operation where the margin of allowable error is extremely small and the number of edge-cases is extremely large. Until these problems are solved, human beings will remain an integral part of the driving task, monitoring the AI system as it performs anywhere from just over 0% to just under 100% of the driving. The governing objectives of the MIT Advanced Vehicle Technology (MIT-AVT) study are to 1) undertake large-scale real-world driving data collection that includes high-definition video to fuel the development of deep learning-based internal and external perception systems; 2) gain a holistic understanding of how human beings interact with vehicle automation technology by integrating video data with vehicle state data, driver characteristics, mental models, and self-reported experiences with technology; and 3) identify how technology and other factors related to automation adoption and use can be improved in ways that save lives. In pursuing these objectives, we have instrumented 23 Tesla Model S and Model X vehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6 vehicles for both long-term (over a year per driver) and medium-term (one month per driver) naturalistic driving data collection. Furthermore, we are continually developing new methods for the analysis of the massive-scale dataset collected from the instrumented vehicle fleet. The recorded data streams include IMU, GPS, and CAN messages, and high-definition video streams of the driver's face, the driver cabin, the forward roadway, and the instrument cluster (on select vehicles). The study is on-going and growing. To date, we have 122 participants, 15610 days of participation, 511638 mi, and 7.1 billion video frames. This paper presents the design of the study, the data collection hardware, the processing of the data, and the computer vision algorithms currently being used to extract actionable knowledge from the data.},
keywords = {computer vision;data analysis;driver information systems;learning (artificial intelligence);mobile robots;remotely operated vehicles;road safety;road vehicles;},
note = {MIT Advanced Vehicle Technology study;large-scale naturalistic driving study;driver behavior;driving task;sensing-acting robotics system;unconstrained vehicle autonomy;scene perception;vehicle control;higher-level planning decisions;autonomous vehicle development;real-world operation;AI system;MIT-AVT;real-world driving data collection;external perception systems;human beings interact;vehicle automation technology;integrating video data;vehicle state data;driver characteristics;mental models;automation adoption;Model X vehicles;massive-scale dataset;instrumented vehicle fleet;recorded data streams;high-definition video streams;driver cabin;select vehicles;data collection hardware;deep learning;Tesla Model S;Cadillac CT6 vehicles;Range Rover Evoque;Volvo S90 vehicles;time 15610.0 d;},
URL = {http://dx.doi.org/10.1109/ACCESS.2019.2926040},
} 


@inproceedings{10646060 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {Uncalibrated monocular based simultaneous localization and mapping for indoor autonomous mobile robot navigation},
journal = {2009 International Conference on Networking, Sensing and Control},
author = {Siyao Fu and Guosheng Yang},
year = {2009//},
pages = {663 - 8},
address = {Piscataway, NJ, USA},
abstract = {This paper describes a an SLAM algorithm for the navigation for an indoor autonomous mobile robot. The main emphasis of this paper is on the ability of line extraction. A recognition method based on straight line extraction is proposed for extracting the key features on the office ceiling, in an effort to estimate the pose of mobile robot. Random sample consensus (RANSAC) paradigm is used to group the line segments. During the navigation, onboard odometry is used at the beginning stage to estimate the information of environment for visual reckoning, while lamps on the ceiling act as beacons for positioning to eliminate accumulation of errors after a long-term run. The data captured from infrared sensors is used for constructing a map. The proposed method scales well with respect to the size of the input image and the number and size of the shapes within the data. Moreover the algorithm is conceptually simple and easy to implement. Simulation and experimental results show that good recognition and localization can be achieved using the proposed method, allowing for the interested region correspondence matching and mapping between images from different sensors or the same sensor indifferent time phrase.},
keywords = {distance measurement;feature extraction;image fusion;image matching;mobile robots;navigation;pose estimation;random processes;robot vision;SLAM (robots);},
note = {simultaneous localization and mapping algorithm;indoor autonomous mobile robot navigation;recognition method;straight line extraction;key feature extraction;pose estimation;random sample consensus paradigm;onboard odometry;visual reckoning;infrared sensors;image matching;},
URL = {http://dx.doi.org/10.1109/ICNSC.2009.4919356},
} 


@article{11447252 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Appearance-based mapping and localisation using feature stability histograms},
journal = {Electronics Letters},
journal = {Electron. Lett. (UK)},
author = {Bacca, B. and Salvi, J. and Batlle, J. and Cufi, X.},
volume = { 46},
number = { 16},
year = {2010/08/05},
pages = {1120 - 1},
issn = {0013-5194},
address = {UK},
abstract = {Proposed is an appearance-based mapping and localisation method based on the human memory model, which is used to build a feature stability histogram (FSH) at each node in the robot topological map. FSH registers local feature stability over time through a voting scheme, and most stable features are considered for mapping and Bayesian localisation. Experimental results are presented using omnidirectional images acquired through long-term acquisition considering: illumination changes, occlusions, random removal of features and perceptual aliasing. This method is able to adapt the internal node's representation through time to achieve global and local robot localisation.},
keywords = {Bayes methods;mobile robots;path planning;robot vision;SLAM (robots);stability;},
note = {appearance based mapping;localisation;feature stability histograms;human memory model;robot topological map;voting scheme;Bayesian localisation;omnidirectional images;},
URL = {http://dx.doi.org/10.1049/el.2010.1599},
} 


@article{14728299 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Self-help: Seeking out perplexing images for ever improving topological mapping},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Paul, R. and Newman, P.},
volume = { 32},
number = { 14},
year = {2013/12/},
pages = {1742 - 66},
issn = {0278-3649},
address = {USA},
abstract = {In this work, we present a novel approach that allows a robot to improve its own navigation performance through introspection and then targeted data retrieval. It is a step in the direction of life-long learning and adaptation and is motivated by the desire to build robots that have plastic competencies which are not baked in. They should react to and benefitfrom use. We consider a particular instantiation of this problem in the context of place recognition. Based on a topic-based probabilistic representation for images, we use a measure of perplexity to evaluate how well a working set of background images explain the robot's online view of the world. Offline, the robot then searches an external resource to seek out additional background images that bolster its ability to localize in its environment when used next. In this way the robot adapts and improves performance through use. We demonstrate this approach using data collected from a mobile robot operating in outdoor workspaces.},
keywords = {image recognition;image representation;mobile robots;navigation;path planning;probability;robot vision;},
note = {perplexing images;topological mapping;robot;navigation performance;targeted data retrieval;introspection;life-long learning;place recognition;topic-based probabilistic representation;background images;mobile robot;outdoor workspaces;data collection;},
URL = {http://dx.doi.org/10.1177/0278364913509859},
} 


@inproceedings{14616670 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Incremental unsupervised topological place discovery},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Murphy, L. and Sibley, G.},
year = {2014//},
pages = {1312 - 18},
address = {Piscataway, NJ, USA},
abstract = {This paper describes an online place discovery and recognition engine that fuses information over time to create topologically distinct places. A key motivation is the recognition that a single image may be a poor exemplar of what constitutes a place. Images are not `places' nor are they `documents'. Instead, by treating image-sequences as a multimodal distribution over topics - and by discovering topics incrementally and online - it is possible to both reduce the memory footprint of place recognition systems, and to improve precision and recall. Distinctive key-places are represented by a cluster topics found from the covisibility graph of a relative simultaneous localization and mapping engine - key-places inherently span many images. A dynamic vocabulary of visual words and density based clustering is used to continually estimate a set of visual topics, changes in which drive the place-recognition process. The system is evaluated using an indoor robot sequence, a standard outdoor robot sequence and a long-term sequence from a static camera. Experiments demonstrate qualitatively distinct themes associated with discovered places - from common place types such as `hallway', or `desk-area', to temporal concepts such as `dusk', `dawn' or `mid-day'. Compared to traditional image-based place-recognition, this reduces the information that must be stored without reducing place-recognition performance.},
keywords = {graph theory;image sensors;image sequences;object recognition;pattern clustering;robot vision;SLAM (robots);},
note = {incremental unsupervised topological place discovery;online place discovery-and-recognition engine;memory footprint reduction;place recognition systems;precision improvement;recall improvement;cluster topics;covisibility graph;relative simultaneous localization-and-mapping engine;dynamic visual words vocabulary;density based clustering;visual topics estimation;indoor robot sequence;standard outdoor robot sequence;static camera;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907022},
} 


@inproceedings{15285925 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Efficient and effective matching of image sequences under substantial appearance changes exploiting GPS priors},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Vysotska, O. and Naseer, T. and Spinello, L. and Burgard, W. and Stachniss, C.},
year = {2015//},
pages = {2774 - 9},
address = {Piscataway, NJ, USA},
abstract = {The ability to localize a robot is an important capability and matching of observations under substantial changes is a prerequisite for robust long-term operation. This paper investigates the problem of efficiently coping with seasonal changes in image data. We present an extension of a recent approach [15] to visual image matching using sequence information. Our extension allows for exploiting GPS priors in the matching process to overcome the main computational bottleneck of the previous method and to handle loops within the image sequences. We present an experimental evaluation using real world data containing substantial seasonal changes and show that our approach outperforms the previous method in case a noisy GPS pose prior is available.},
keywords = {Global Positioning System;image matching;image sequences;SLAM (robots);},
note = {image sequences matching;GPS priors;image data;visual image matching;sequence information;seasonal changes;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7139576},
} 


@article{12015512 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Hierarchical mine production scheduling using discrete-event simulation},
journal = {International Journal of Mining and Mineral Engineering},
journal = {Int. J. Mining Miner. Eng. (Switzerland)},
author = {Ben-Awuah, E. and Kalantari, S. and Pourrahimian, Y. and Askari-Nasab, H.},
volume = { 2},
number = { 2},
year = {2010//},
pages = {137 - 58},
issn = {1754-890X},
address = {Switzerland},
abstract = {Mine planning involves different levels of decision-making depending on the time horizon under consideration. The main goal of this study is to develop a discrete-event simulation model to link long-term predictive mine plans with short-term production schedules in the presence of uncertainty. We have developed, verified and validated a discrete-event simulation model for open pit production scheduling using the SLAM simulation language. The simulation model takes into consideration constraints and uncertainties associated with mining and processing capacities, crusher availability, stockpiling strategy and blending requirements. Application of the simulation model is presented by an iron ore open pit mine case study.},
keywords = {blending;crushers;decision making;discrete event simulation;mining;production control;production planning;scheduling;SLAM (robots);},
note = {hierarchical mine production scheduling;mine planning;decision-making;discrete-event simulation model;open pit production scheduling;SLAM simulation language;stockpiling strategy;blending;},
URL = {http://dx.doi.org/10.1504/IJMME.2010.035314},
} 


@article{15848694 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Relaxing the planar assumption: 3D state estimation for an autonomous surface vessel},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Hitz, G. and Pomerleau, F. and Colas, F. and Siegwart, R.},
volume = { 34},
number = { 13},
year = {2015/11/},
pages = {1604 - 21},
issn = {0278-3649},
address = {UK},
abstract = {Autonomous Surface Vessels (ASVs) are increasingly being proposed as tools to automate environmental data collection, bathymetric mapping and shoreline monitoring. For many applications it can be assumed that the boat operates on a 2D plane. However, with the involvement of exteroceptive sensors like cameras or laser rangefinders, knowing the 3D pose of the boat becomes critical. In this paper, we formulate three different algorithms based on 3D extended Kalman filter state estimation for ASV localization. We compare them using field testing results with ground truth measurements, and demonstrate that the best performance is achieved with a model-based solution in combination with a complementary filter for attitude estimation. Furthermore, we present a parameter identification methodology and show that it also yields accurate results when used with inexpensive sensors. Finally, we present a long-term series (i.e. over a full year) of shoreline monitoring data sets and discuss the need for map maintenance routines based on a variant of the Iterative Closest Point algorithm.},
keywords = {attitude control;autonomous underwater vehicles;boats;estimation theory;iterative methods;Kalman filters;marine control;mobile robots;nonlinear filters;robot vision;SLAM (robots);state estimation;},
note = {3D extended Kalman filter state estimation;autonomous surface vessel;ASV localization;boat operation;attitude estimation;parameter identification;shoreline monitoring;map maintenance routine;iterative closest point algorithm;},
URL = {http://dx.doi.org/10.1177/0278364915583680},
} 


@article{12614300 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {PIRF-Nav 2.0: Fast and online incremental appearance-based loop-closure detection in an indoor environment},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Kawewong, A. and Tongprasit, N. and Hasegawa, O.},
volume = { 59},
number = { 10},
year = {2011/10/},
pages = {727 - 39},
issn = {0921-8890},
address = {Netherlands},
abstract = {This paper presents a fast and online incremental solution for an appearance-based loop-closure detection problem in a dynamic indoor environment. Closing the loop in a dynamic environment has been an important topic in robotics for decades. Recently, PIRF-Nav has been reported as being successful in achieving high recall rate at precision 1. However, PIRF-Nav has three main disadvantages: (i) the computational expense of PIRF-Nav is beyond real-time, (ii) it utilizes a large amount of memory in the redundant process of keeping signatures of places, and (iii) it is ill-suited to an indoor environment. These factors hinder the use of PIRF-Nav in a general environment for long-term, high-speed mobile robotic applications. Therefore, this paper proposes two techniques: (i) new modified PIRF extraction that makes the system more suitable for an indoor environment and (ii) new dictionary management that can eliminate redundant searching and conserve memory consumption. The results show that our proposed method can complete tasks up to 12 times faster than PIRF-Nav with only a slight percentage decline in recall. In addition, we collected additional data from a university canteen crowded during lunch time. Even in this crowded indoor environment, our proposed method has better real-time processing performance compared with other methods. [All rights reserved Elsevier].},
keywords = {feature extraction;indoor environment;mobile robots;path planning;robot vision;SLAM (robots);},
note = {PIRF-Nav 2.0;online incremental appearance-based loop-closure detection;dynamic indoor environment;computational expense;long-term high-speed mobile robotic applications;modified PIRF extraction;dictionary management;redundant searching;memory consumption;university canteen;real-time processing performance;},
URL = {http://dx.doi.org/10.1016/j.robot.2011.05.007},
} 


@article{12422242 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Robust mobile robot localization in highly non-static environments},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Netherlands)},
author = {Jung-Suk Lee and Wan Kyun Chung},
volume = { 29},
number = { 1},
year = {2010/07/},
pages = {1 - 16},
issn = {0929-5593},
address = {Netherlands},
abstract = {In this paper, we propose a robust pose tracking method for mobile robot localization with an incomplete map in a highly non-static environment. This algorithm will work with a simple map that does not include complete in- formation about the non-static environment. With only an initial incomplete map, a mobile robot cannot estimate its pose because of the inconsistency between the real observations from the environment and the predicted observations on the incomplete map. The proposed localization algorithm uses the approach of sampling from a non-corrupted window, which allows the mobile robot to estimate its pose more robustly in a non-static environment even when subjected to severe corruption of observations. The algorithm sequence involves identifying the corruption by comparing the real observations with the corresponding predicted observations of all particles, sampling particles from a non- corrupted window that consists of multiple non-corrupted sets, and filtering sensor measurements to provide weights to particles in the corrupted sets. After localization, the estimated path may still contain some errors due to long-term corruption. These errors can be corrected using nonlinear constrained least-squares optimization. The incomplete map is then updated using both the corrected path and the stored sensor information. The performance of the proposed algorithm was verified via simulations and experiments in various highly non-static environments. Our localization algorithm can increase the success rate of tracking its pose to more than 95% compared to estimates made without its use. After that, the initial incomplete map is updated based on the localization result.},
keywords = {filtering theory;image sampling;least squares approximations;mobile robots;Monte Carlo methods;object tracking;pose estimation;robot vision;SLAM (robots);},
note = {robust mobile robot localization;nonstatic environments;robust pose tracking method;incomplete map;noncorrupted window;pose estimation;multiple noncorrupted sets;sensor measurement filtering;nonlinear constrained least-squares optimization;Monte Carlo localization;},
URL = {http://dx.doi.org/10.1007/s10514-010-9184-1},
} 


@inproceedings{13851266 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Ascending stairway modeling from dense depth imagery for traversability analysis},
journal = {2013 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Delmerico, J.A. and Baran, D. and David, P. and Ryde, J. and Corso, J.J.},
year = {2013//},
pages = {2283 - 90},
address = {Piscataway, NJ, USA},
abstract = {Localization and modeling of stairways by mobile robots can enable multi-floor exploration for those platforms capable of stair traversal. Existing approaches focus on either stairway detection or traversal, but do not address these problems in the context of path planning for the autonomous exploration of multi-floor buildings. We propose a system for detecting and modeling ascending stairways while performing simultaneous localization and mapping, such that the traversability of each stairway can be assessed by estimating its physical properties. The long-term objective of our approach is to enable exploration of multiple floors of a building by allowing stairways to be considered during path planning as traversable portals to new frontiers. We design a generative model of a stairway as a single object. We localize these models with respect to the map, and estimate the dimensions of the stairway as a whole, as well as its steps. With these estimates, a robot can determine if the stairway is traversable based on its climbing capabilities. Our system consists of two parts: a computationally efficient detector that leverages geometric cues from dense depth imagery to detect sets of ascending stairs, and a stairway modeler that uses multiple detections to infer the location and parameters of a stairway that is discovered during exploration. We demonstrate the performance of this system when deployed on several mobile platforms using a Microsoft Kinect sensor.},
keywords = {image sensors;mobile robots;object detection;path planning;robot vision;SLAM (robots);},
note = {ascending stairway modeling;dense depth imagery;traversability analysis;stairway localization;multifloor exploration;stairway detection;stairway traversal;simultaneous localization and mapping;path planning;stairway generative model;robot climbing capability;computationally efficient detector;geometric cues;stairway modeler;Microsoft Kinect sensor;mobile platforms;},
URL = {http://dx.doi.org/10.1109/ICRA.2013.6630886},
} 


@inproceedings{13851614 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Robust Vision-Aided Navigation Using Sliding-Window Factor Graphs},
journal = {2013 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Han-Pang Chiu and Williams, S. and Dellaert, F. and Samarasekera, S. and Kumar, R.},
year = {2013//},
pages = {46 - 53},
address = {Piscataway, NJ, USA},
abstract = {This paper proposes a navigation algorithm that provides a low-latency solution while estimating the full nonlinear navigation state. Our approach uses Sliding-Window Factor Graphs, which extend existing incremental smoothing methods to operate on the subset of measurements and states that exist inside a sliding time window. We split the estimation into a fast short-term smoother, a slower but fully global smoother, and a shared map of 3D landmarks. A novel three-stage visual feature model is presented that takes advantage of both smoothers to optimize the 3D landmark map, while minimizing the computation required for processing tracked features in the short-term smoother. This three-stage model is formulated based on the maturity of the estimation of the 3D location of the underlying landmark in the map. Long-range associations are used as global measurements from matured landmarks in the short-term smoother and loop closure constraints in the long-term smoother. Experimental results demonstrate our approach provides highly-accurate solutions on large-scale real data sets using multiple sensors in GPS-denied settings.},
keywords = {feature extraction;graph theory;nonlinear estimation;optimisation;robot vision;sensor fusion;SLAM (robots);smoothing methods;state estimation;},
note = {GPS-denied settings;large-scale real data sets;multiple sensor data sets;long-term smoother;loop closure constraints;global measurements;long-range associations;3D location estimation;tracked feature processing;3D landmark map optimization;shared 3D landmark map;three-stage visual feature model;global smoother;fast short-term smoother;sliding time window;measurement subset;incremental smoothing methods;nonlinear navigation state estimation;low-latency solution;navigation algorithm;sliding-window factor graphs;robust vision-aided navigation;},
URL = {http://dx.doi.org/10.1109/ICRA.2013.6630555},
} 


@inproceedings{13564873 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Self Body Mapping in Mobile Robots Using Vision and Forward Models},
journal = {2012 IEEE Electronics, Robotics and Automotive Mechanics Conference (CERMA 2012). Proceedings},
author = {Escobar, E. and Hermosillo, J. and Lara, B.},
year = {2012//},
pages = {72 - 7},
address = {Los Alamitos, CA, USA},
abstract = {The work presented in this paper aims at providing an agent with basic capabilities leading towards navigation through self body-mapping using a vision system. In particular, we are interested in the study of forward models which code for the sensory consequences of an agent's self-produced actions. The research is developed in the framework of cognitive robotics and embodied cognition. The agent is a robot we let interact with its environment to know the free space around it from re-enaction of sensory-motor cycles predicting collisions from visual data. From the disparity map the robot associates intensity regions with motor commands in order to predict distances to objects reported by a tactile sensor in self-motion coordinates. In order to form this multimodal association we use a forward model, coded as a system of neural networks and trained with data coming from random trajectories executed by a Pioneer 3-XD. The resulting forward model allows the agent to navigate avoiding undesired situations by performing long term predictions of the sensory consequences of its actions. The experiments validate the hypothesis that this model allows for a basic self body-mapped navigation capability.},
keywords = {collision avoidance;mobile robots;neural nets;robot vision;SLAM (robots);tactile sensors;},
note = {self body-mapped navigation capability;mobile robots;vision model;forward model;vision system;sensory consequences;agent self-produced actions;cognitive robotics;embodied cognition;sensory-motor cycle reenaction;disparity map;motor commands;tactile sensor;self-motion coordinates;multimodal association;neural networks;Pioneer 3-XD;},
URL = {http://dx.doi.org/10.1109/CERMA.2012.20},
} 


@inproceedings{16677844 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Quantitative Performance Optimisation for Corner and Edge Based Robotic Vision Systems: A Monte-Carlo Simulation},
journal = {Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10073},
author = {Jingduo Tian and Thacker, N. and Stancu, A.},
volume = {pt.II},
year = {2016//},
pages = {544 - 54},
address = {Cham, Switzerland},
abstract = {Corner and edge based robotic vision systems have achieved enormous success in various applications. To quantify and thereby improve the system performance, the standard method is to conduct cross comparisons using benchmark datasets. Such datasets, however, are usually generated for validating specific vision algorithms (e.g. monocular SLAM [1] and stereo odometry [2]). In addition, they are not capable of evaluating robotic systems which require visual feedback signals for motion control (e.g. visual servoing [3]). To develop a more generalised framework to evaluate ordinary corner and edge based robotic vision systems, we propose a novel Monte-Carlo simulation which contains various real-world geometric uncertainty sources. An edge-based global localisation algorithm is evaluated and optimised using the proposed simulation via a large scale Monte-Carlo analysis. During a long-term optimisation, the system performance is improved by around 230 times, while preserving high robustness towards all the simulated uncertainty sources.},
keywords = {edge detection;Monte Carlo methods;motion control;optimisation;robot vision;},
note = {quantitative performance optimisation;corner-based robotic vision systems;edge-based robotic vision systems;Monte Carlo simulation;system performance improvement;benchmark datasets;visual feedback signals;motion control;geometric uncertainty sources;edge-based global localisation algorithm;},
URL = {http://dx.doi.org/10.1007/978-3-319-50832-0_53},
} 


@article{18467666 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Explorations on visual localization from active to passive},
journal = {Multimedia Tools and Applications},
journal = {Multimed. Tools Appl. (Germany)},
author = {Yongquan Yang and Yang Wu and Ning Chen},
volume = { 78},
number = { 2},
year = {2019/01/},
pages = {2269 - 309},
issn = {1380-7501},
address = {Germany},
abstract = {In this paper, we novelly consider visual localization in active and passive two ways, with simple definition that active localization assists device to estimate location of its interest while passive localization aids device to estimate its own location in environment. Expecting to indicate some insights into visual localization, we specifically performed two explorations on active localization and more importantly explored to upgrade them from active to passive localization with extra geometry information available. In order to produce unconstrained and accurate 2D location estimation of interested object, we constructed an active localization system by fusing detection, tracking and recognition. Based on recognition, we proposed a collaborative strategy making mutual enhancement between detection and tracking possible to obtain better performance on 2D location estimation. Meanwhile, to actively estimate semantic location of interested visual region, we employed latest state-of-the-art light weight CNN models specifically designed for efficiency and trained two of them with large place dataset in perspective of scene recognition. What's more, using depth information available from RGB-D camera, we improved the active system for 2D location of interested object to a passive system for relative 3D location of device to the interested object. Firstly estimated was the 3D location of the interested object in the coordinate system of device, then relative location of device to the interested object in world coordinate system was deduced with appropriate assumption. Evaluations both subjectively on a RGB-D sequence obtained in a lab environment and practically on a robotic platform in an office environment indicated that the improved system was suitable for autonomous following robot. As well, the active system for rough semantic location estimation of interested visual region was promoted to a passive system for fine location estimation of device, with available 3D map describing the visited environment. In perspective of place recognition, we first adopted one of the efficient CNN models previously trained for semantic location estimation as a base to generate CNN features for both retrieval of candidate loops in the map and geometrical consistency checking of retrieved loops, then true loops were used to deduce fine location of device itself in environment. Comparison with state-of-the-art results reflected that the promoted system was adequate for long-term robotic autonomy. Achieving favorable performances, the presented four explorations have implied adequacy for elaborating on some insights into visual localization.},
keywords = {convolutional neural nets;image colour analysis;mobile robots;object detection;object tracking;robot vision;},
note = {visual region;light weight CNN models;place recognition;rough semantic location estimation;relative location;coordinate system;passive system;active system;active localization system;passive localization aids device;active localization assists device;passive two ways;active ways;visual localization;explorations;},
URL = {http://dx.doi.org/10.1007/s11042-018-6347-0},
} 


@inproceedings{12095240 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Visual topometric localization},
journal = {2011 IEEE Intelligent Vehicles Symposium (IV)},
author = {Badino, H. and Huber, D. and Kanade, T.},
year = {2011//},
pages = {794 - 9},
address = {Piscataway, NJ, USA},
abstract = {One of the fundamental requirements of an autonomous vehicle is the ability to determine its location on a map. Frequently, solutions to this localization problem rely on GPS information or use expensive three dimensional (3D) sensors. In this paper, we describe a method for long-term vehicle localization based on visual features alone. Our approach utilizes a combination of topological and metric mapping, which we call topometric localization, to encode the coarse topology of the route as well as detailed metric information required for accurate localization. A topometric map is created by driving the route once and recording a database of visual features. The vehicle then localizes by matching features to this database at runtime. Since individual feature matches are unreliable, we employ a discrete Bayes filter to estimate the most likely vehicle position using evidence from a sequence of images along the route. We illustrate the approach using an 8.8 km route through an urban and suburban environment. The method achieves an average localization error of 2.7 m over this route, with isolated worst case errors on the order of 10 m.},
keywords = {automated highways;Bayes methods;feature extraction;filtering theory;geographic information systems;image matching;image motion analysis;image sequences;remotely operated vehicles;road vehicles;},
note = {visual topometric localization;autonomous vehicle;map location;localization problem;long-term vehicle localization;visual feature;topological mapping;metric mapping;route driving;feature matching;discrete Bayes filter;vehicle position estimation;image sequence;suburban environment;localization error;},
URL = {http://dx.doi.org/10.1109/IVS.2011.5940504},
} 


@article{14069148 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Selective Combination of Visual and Thermal Imaging for Resilient Localization in Adverse Conditions: Day and Night, Smoke and Fire},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Brunner, C. and Peynot, T. and Vidal-Calleja, T. and Underwood, J.},
volume = { 30},
number = { 4},
year = {2013/07/},
pages = {641 - 6},
issn = {1556-4959},
address = {USA},
abstract = {Long-term autonomy in robotics requires perception systems that are resilient to unusual but realistic conditions that <i>will</i> eventually occur during extended missions. For example, unmanned ground vehicles (UGVs) need to be capable of operating safely in adverse and low-visibility conditions, such as at night or in the presence of smoke. The key to a resilient UGV perception system lies in the use of multiple sensor modalities, e.g., operating at different frequencies of the electromagnetic spectrum, to compensate for the limitations of a single sensor type. In this paper, visual and infrared imaging are combined in a Visual-SLAM algorithm to achieve localization. We propose to evaluate the quality of data provided by each sensor modality prior to data combination. This evaluation is used to discard low-quality data, i.e., data most likely to induce large localization errors. In this way, perceptual failures are anticipated and mitigated. An extensive experimental evaluation is conducted on data sets collected with a UGV in a range of environments and adverse conditions, including the presence of smoke (obstructing the visual camera), fire, extreme heat (saturating the infrared camera), low-light conditions (dusk), and at night with sudden variations of artificial light. A total of 240 trajectory estimates are obtained using five different variations of data sources and data combination strategies in the localization method. In particular, the proposed approach for selective data combination is compared to methods using a single sensor type or combining both modalities without preselection. We show that the proposed framework allows for camera-based localization resilient to a large range of low-visibility conditions.},
keywords = {infrared imaging;remotely operated vehicles;robot vision;visual perception;},
note = {camera based localization;selective data combination;artificial light;low light conditions;extreme heat;sensor modality;visual-SLAM algorithm;infrared imaging;resilient UGV perception system;unmanned ground vehicles;robotics;resilient localization;thermal imaging;visual imaging;},
URL = {http://dx.doi.org/10.1002/rob.21464},
} 


@inproceedings{13851562 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Fast place recognition with plane-based maps},
journal = {2013 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Fernandez-Moral, E. and Mayol-Cuevas, W. and Arevalo, V. and Gonzalez-Jimenez, J.},
year = {2013//},
pages = {2719 - 24},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a new method for recognizing places in indoor environments based on the extraction of planar regions from range data provided by a hand-held RGB-D sensor. We propose to build a plane-based map (PbMap) consisting of a set of 3D planar patches described by simple geometric features (normal vector, centroid, area, etc.). This world representation is organized as a graph where the nodes represent the planar patches and the edges connect planes that are close by. This map structure permits to efficiently select subgraphs representing the local neighborhood of observed planes, that will be compared with other subgraphs corresponding to local neighborhoods of planes acquired previously. To find a candidate match between two subgraphs we employ an interpretation tree that permits working with partially observed and missing planes. The candidates from the interpretation tree are further checked out by a rigid registration test, which also gives us the relative pose between the matched places. The experimental results indicate that the proposed approach is an efficient way to solve this problem, working satisfactorily even when there are substantial changes in the scene (lifelong maps).},
keywords = {graph theory;mobile robots;path planning;SLAM (robots);trees (mathematics);},
note = {fast place recognition;plane-based maps;indoor environments;planar region extraction;hand-held RGB-D sensor;PbMap;3D planar patches;subgraphs;interpretation tree;mobile robots;},
URL = {http://dx.doi.org/10.1109/ICRA.2013.6630951},
} 


@article{15918125 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Automated Crack Detection on Concrete Bridges},
journal = {IEEE Transactions on Automation Science and Engineering},
journal = {IEEE Trans. Autom. Sci. Eng. (USA)},
author = {Prasanna, P. and Dana, K.J. and Gucunski, N. and Basily, B.B. and La, H.M. and 4 and 4},
volume = { 13},
number = { 2},
year = {2016/04/},
pages = {591 - 9},
issn = {1545-5955},
address = {USA},
abstract = {Detection of cracks on bridge decks is a vital task for maintaining the structural health and reliability of concrete bridges. Robotic imaging can be used to obtain bridge surface image sets for automated on-site analysis. We present a novel automated crack detection algorithm, the STRUM (spatially tuned robust multifeature) classifier, and demonstrate results on real bridge data using a state-of-the-art robotic bridge scanning system. By using machine learning classification, we eliminate the need for manually tuning threshold parameters. The algorithm uses robust curve fitting to spatially localize potential crack regions even in the presence of noise. Multiple visual features that are spatially tuned to these regions are computed. Feature computation includes examining the scale-space of the local feature in order to represent the information and the unknown salient scale of the crack. The classification results are obtained with real bridge data from hundreds of crack regions over two bridges. This comprehensive analysis shows a peak STRUM classifier performance of 95% compared with 69% accuracy from a more typical image-based approach. In order to create a composite global view of a large bridge span, an image sequence from the robot is aligned computationally to create a continuous mosaic. A crack density map for the bridge mosaic provides a computational description as well as a global view of the spatial patterns of bridge deck cracking. The bridges surveyed for data collection and testing include Long-Term Bridge Performance program's (LTBP) pilot project bridges at Haymarket, VA, USA, and Sacramento, CA, USA.},
keywords = {bridges (structures);condition monitoring;crack detection;curve fitting;image classification;image segmentation;learning (artificial intelligence);structural engineering computing;},
note = {automated crack detection;concrete bridges;structural health;reliability;Robotic imaging;bridge surface image;spatially tuned robust multifeature classifier;STRUM classifier;machine learning classification;image thresholding;bridge mosaic;long-term bridge performance program;Haymarket;USA;Sacramento;curve fitting;},
URL = {http://dx.doi.org/10.1109/TASE.2014.2354314},
} 


@article{10278266 ,
language = {English},
copyright = {Copyright 2008, The Institution of Engineering and Technology},
title = {An efficient approach to odometric non-systematic error modeling for mobile robots},
journal = {Chinese Journal of Electronics},
journal = {Chin. J. Electron. (China)},
author = {Yang Jingdong and Hong Bingrong and Piao Songhao},
volume = { 17},
number = { 1},
year = {2008/01/},
pages = {95 - 9},
issn = {1022-4653},
address = {China},
abstract = {Odometric non-systematic error modeling for mobile robot is the basis of localization. Most of the approaches to odometric non-systematic error modeling are designed for some special driving-type robots nowadays. And the long-term odometric errors without bound, which degrade the localization precision after long-distance movement, are not often capable of being compensated in real-time. Therefore, a general approach to odometric non- systematic error modeling for mobile robot is proposed in regard to both synchronous drive roller robots and differential drive roller robots. The approach assumes that the robot path is approximated to circular arcs. The function relationships, between the odometric process input and non-systematic errors, are derived on the basis of the odometric error transformation rules, further the accumulative errors of odometry in the localization process are compensated in real-time. The experiments show that the compensation of non-systematic error can reduce the odometric long-term errors efficiently, and improve the localization precision remarkably.},
keywords = {mobile robots;SLAM (robots);},
note = {odometric nonsystematic error modeling;mobile robots;synchronous drive roller robots;differential drive roller robots;odometric error transformation rules;simultaneous localization and mapping;},
} 


@inproceedings{9561798 ,
language = {English},
copyright = {Copyright 2007, The Institution of Engineering and Technology},
title = {A visual bag of words method for interactive qualitative localization and mapping},
journal = {2007 IEEE International Conference on Robotics and Automation (IEEE Cat No. 07CH37836D)},
author = {Filliat, D.},
year = {2007//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Localization for low cost humanoid or animal-like personal robots has to rely on cheap sensors and has to be robust to user manipulations of the robot. We present a visual localization and map-learning system that relies on vision only and that is able to incrementally learn to recognize the different rooms of an apartment from any robot position. This system is inspired by visual categorization algorithms called bag of words methods that we modified to make fully incremental and to allow a user-interactive training. Our system is able to reliably recognize the room in which the robot is after a short training time and is stable for long term use. Empirical validation on a real robot and on an image database acquired in real environments are presented.},
keywords = {humanoid robots;image recognition;learning (artificial intelligence);robot vision;SLAM (robots);},
note = {visual bag of words method;room recognition;user-interactive training;visual categorization;incremental learning;robot vision;map-learning system;animal-like personal robots;humanoid robots;interactive mapping;interactive qualitative localization;},
} 


@inproceedings{14921027 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Towards background flow based AUV localization},
journal = {2014 IEEE 53rd Annual Conference on Decision and Control (CDC)},
author = {Zhuoyuan Song and Mohseni, K.},
year = {2014//},
pages = {6945 - 50},
address = {Piscataway, NJ, USA},
abstract = {Underwater localization faces many constrains and long-term persistent global localization for autonomous underwater vehicles (AUVs) is very difficult. In this paper, we propose a novel AUV localization method taking advantage of the recent progress in ocean general circulation models (OGCMs). During navigation, the AUV performs intermittent local background flow velocity measurements or estimates using on-board sensors. A series of preloaded flow velocity forecast maps generated by OGCMs are referred by a particle filter in updating particle weights based on resemblance between forecasts and local estimation. A rigorous derivation of the problem in probability theory is presented to reveal the recursive structure of the target distribution function. Simulations in a simple double-gyre velocity field exhibit satisfactory converging localization error. Further simulations in a flow field with local flow fluctuations that are not resolved by OGCMs show similar convergent localization error with a slower converging rate. As a first step towards a new set of underwater localization methods, this work presents promising results and reveals the possibility of realizing converging global underwater localization through partial utilization of the background flow information that is easily accessible.},
keywords = {autonomous underwater vehicles;mobile robots;particle filtering (numerical methods);telerobotics;velocity measurement;},
note = {autonomous underwater vehicles;AUV localization method;ocean general circulation models;intermittent local background flow velocity measurements;on-board sensors;preloaded flow velocity forecast maps;particle filter;local estimation;probability theory;target distribution function recursive structure;double-gyre velocity field;converging localization error;underwater localization methods;},
URL = {http://dx.doi.org/10.1109/CDC.2014.7040480},
} 


@article{14728296 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {3D normal distributions transform occupancy maps: An efficient representation for mapping in dynamic environments},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Saarinen, J.P. and Andreasson, H. and Stoyanov, T. and Lilienthal, A.J.},
volume = { 32},
number = { 14},
year = {2013/12/},
pages = {1627 - 44},
issn = {0278-3649},
address = {USA},
abstract = {In order to enable long-term operation of autonomous vehicles in industrial environments numerous challenges need to be addressed. A basic requirement for many applications is the creation and maintenance of consistent 3D world models. This article proposes a novel 3D spatial representation for online real-world mapping, building upon two known representations: normal distributions transform (NDT) maps and occupancy grid maps. The proposed normal distributions transform occupancy map (NDT-OM) combines the advantages of both representations; compactness of NDT maps and robustness of occupancy maps. One key contribution in this article is that we formulate an exact recursive updates for NDT-OMs. We show that the recursive update equations provide natural support for multi-resolution maps. Next, we describe a modification of the recursive update equations that allows adaptation in dynamic environments. As a second key contribution we introduce NDT-OMs and formulate the occupancy update equations that allow to build consistent maps in dynamic environments. The update of the occupancy values are based on an efficient probabilistic sensor model that is specially formulated for NDT-OMs. In several experiments with a total of 17 hours of data from a milkfactory we demonstrate that NDT-OMs enable real-time performance in large-scale, long-term industrial setups.},
keywords = {control engineering computing;dairy products;industrial robots;mobile robots;normal distribution;production engineering computing;production facilities;robot dynamics;robot vision;SLAM (robots);solid modelling;transforms;},
note = {3D normal distributions transform occupancy maps;dynamic environments;autonomous vehicles;industrial environments;3D world models;3D spatial representation;online real-world mapping;normal distributions transform maps;occupancy grid maps;NDT-OM;NDT maps;exact recursive update equations;multiresolution maps;occupancy update equations;probabilistic sensor model;milkfactory;preprogrammed manipulators;},
URL = {http://dx.doi.org/10.1177/0278364913499415},
} 


@inproceedings{11431257 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Visual robot localization using compact binary landmarks},
journal = {2010 IEEE International Conference on Robotics and Automation (ICRA 2010)},
author = {Ikeda, K. and Tanaka, K.},
year = {2010//},
pages = {4397 - 403},
address = {Piscataway, NJ, USA},
abstract = {This paper is concerned with the problem of mobile robot localization using a novel compact representation of visual landmarks. With recent progress in lifelong map-learning as well as in information sharing networks, compact representation of a large-size landmark database has become crucial. In this paper, we propose a compact binary code (e.g. 32bit code) landmark representation by employing the semantic hashing technique from web-scale image retrieval. We show how well such a binary representation achieves compactness of a landmark database while maintaining efficiency of the localization system. In our contribution, we investigate the cost-performance, the semantic gap, the saliency evaluation using the presented techniques as well as challenge to further reduce the resources (#bits) per landmark. Experiments using a high-speed car-like robot show promising results.},
keywords = {control engineering computing;image retrieval;mobile robots;path planning;robot vision;},
note = {visual robot localization;compact binary landmarks;mobile robot localization;information sharing networks;landmark database;visual landmarks;compact binary code;semantic hashing technique;web-scale image retrieval;binary representation;semantic gap;saliency evaluation;high-speed car-like robot;},
URL = {http://dx.doi.org/10.1109/ROBOT.2010.5509579},
} 


@inproceedings{19891622 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {PHALANX: Expendable projectile sensor networks for planetary exploration},
journal = {2020 IEEE Aerospace Conference},
author = {Dille, M. and Nuch, D. and Gupta, S. and McCabe, S. and Verzic, N. and Fong, T. and Wong, U.},
year = {2020//},
pages = {12 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Technologies enabling long-term, wide-ranging measurement in hard-to-reach areas are a critical need for planetary science inquiry. Phenomena of interest include flows or variations in volatiles, gas composition or concentration, particulate density, or even simply temperature. Improved measurement of these processes enables understanding of exotic geologies and distributions or correlating indicators of trapped water or biological activity. However, such data is often needed in unsafe areas such as caves, lava tubes, or steep ravines not easily reached by current spacecraft and planetary robots. To address this capability gap, we have developed miniaturized, expendable sensors which can be ballistically lobbed from a robotic rover or static lander - or even dropped during a flyover. These projectiles can perform sensing during flight and after anchoring to terrain features. By augmenting exploration systems with these sensors, we can extend situational awareness, perform long-duration monitoring, and reduce utilization of primary mobility resources, all of which are crucial in surface missions. We call the integrated payload that includes a cold gas launcher, smart projectiles, planning software, network discovery, and science sensing: PHALANX. In this paper, we introduce the mission architecture for PHALANX and describe an exploration concept that pairs projectile sensors with a rover ldquomothership.rdquo Science use cases explored include reconnaissance using ballistic cameras, volatiles detection, and building timelapse maps of temperature and illumination conditions. Strategies to autonomously coordinate constellations of deployed sensors to self-discover and localize with peer ranging (i.e. a ldquolocal GPSrdquo) are summarized, thus providing communications infrastructure beyond-line-of-sight (BLOS) of the rover. Capabilities were demonstrated through both simulation and physical testing with a terrestrial prototype. The approach to developing a terrestrial prototype is discussed, including design of the launching mechanism, projectile optimization, micro-electronics fabrication, and sensor selection. Results from early testing and characterization of commercial-off-the-shelf (COTS) components are reported. Nodes were subjected to successful burn-in tests over 48 hours at full logging duty cycle. Integrated field tests were conducted in the Roverscape, a half-acre planetary analog environment at NASA Ames, where we tested up to 10 sensor nodes simultaneously coordinating with an exploration rover. Ranging accuracy has been demonstrated to be within +/-10cm over 20m using commodity radios when compared to high-resolution laser scanner ground truthing. Evolution of the design, including progressive miniaturization of the electronics and iterated modifications of the enclosure housing for streamlining and optimized radio performance are described. Finally, lessons learned to date, gaps toward eventual flight mission implementation, and continuing future development plans are discussed.},
keywords = {aerospace robotics;Global Positioning System;mobile robots;planetary rovers;space research;space vehicles;wireless sensor networks;},
note = {expendable sensors;robotic rover;static lander;commercial-off-the-shelf components;burn-in tests;integrated field tests;half-acre planetary analog environment;sensor nodes;exploration rover;ranging accuracy;streamlining radio performance;optimized radio performance;eventual flight mission implementation;future development plans;Expendable projectile sensor networks;planetary exploration;hard-to-reach areas;planetary science inquiry;particulate density;improved measurement;exotic geologies;correlating indicators;trapped water;unsafe areas;lava tubes;steep ravines;current spacecraft;planetary robots;capability gap;sensor selection;projectile optimization;terrestrial prototype;physical testing;communications infrastructure beyond-line-of-sight;peer ranging;deployed sensors;volatiles detection;ballistic cameras;rover mothership;exploration concept;mission architecture;PHALANX;science sensing;network discovery;planning software;smart projectiles;cold gas launcher;integrated payload;surface missions;primary mobility resources;long-duration monitoring;situational awareness;augmenting exploration systems;time 48.0 hour;},
URL = {http://dx.doi.org/10.1109/AERO47225.2020.9172595},
} 


@article{14533643 ,
language = {Japanese},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Development of Localization Method Using Magnetic Sensor and LIDAR},
journal = {Transactions of the Society of Instrument and Control Engineers},
journal = {Trans. Soc. Instrum. Control Eng. (Japan)},
author = {Shinohara, M. and Rahok, S.A. and Inoue, K. and Ozaki, K.},
volume = { 49},
number = { 8},
year = {2013/08/},
pages = {795 - 801},
issn = {0453-4654},
address = {Japan},
abstract = {In mobile robot's localization, it is well known that odometry can provide a reliable accuracy in short term navigation and a very high sampling rate. However, odometry produces cumulative error because of uneven terrains or wheel slippage and this error increases proportionally with the distance traveled by the mobile robot. Therefore, it is necessary to augment odometry with other sensors to improve its accuracy. This paper proposes an estimation method of mobile robot orientation using an environmental magnetic field (magnetic field that occurs in the environment). A three-axis magnetic sensor is utilized to scan the environmental magnetic field to build a magnetic database on a grid map called "a magnetic map" with the mobile robot operated with a joystick on a desired route. The mobile robot then estimates its orientation by comparing the magnetic sensor readings with the magnetic data stored in the magnetic map. However, even if the proposed method can improve the accuracy of the odometry, positioning error still remains as a major problem in long term navigation. In this work, a localization method using Monte Carlo Localization (MCL) based on a Light Detection and Ranging (LIDAR) is utilized to fix the positioning error at the areas where landmark can be observed. The experimental results showed that the mobile robot could localize robustly in any environments with the proposed method.},
keywords = {distance measurement;magnetic sensors;mobile robots;Monte Carlo methods;optical radar;},
note = {positioning error;Monte Carlo localization;magnetic map;grid map;magnetic database;three-axis magnetic sensor;environmental magnetic field;mobile robot orientation estimation method;short term navigation;odometry;light detection and ranging;LIDAR;mobile robot localization method;},
} 


@inproceedings{8724429 ,
language = {English},
copyright = {Copyright 2006, IEE},
title = {Automating answers to "where am i?"},
journal = {IEE Forum on: Autonomous Systems},
author = {Newman, P.},
year = {2005//},
pages = {7 pp. - },
address = {Stevenage, UK},
abstract = {In many situations, large-scale, long term deployment of an autonomous vehicle requires an ability to navigate in arbitrary workspaces and must be able to establish "where am I what surrounds me?". This paper describe simultaneous localisation and mapping (SLAM)techniques and implementations in which an autonomous vehicle explores its workspace using onboard sensors and inextricably binds together the tasks of mapping and localisation.},
keywords = {path planning;remotely operated vehicles;},
note = {autonomous vehicle;simultaneous localisation and mapping techniques;long term deployment;arbitrary workspaces;},
} 


@inproceedings{9552909 ,
language = {English},
copyright = {Copyright 2007, The Institution of Engineering and Technology},
title = {BiCamSLAM: two times mono is more than stereo},
journal = {2007 IEEE International Conference on Robotics and Automation (IEEE Cat No. 07CH37836D)},
author = {Sola, J. and Monin, A. and Devy, M.},
year = {2007//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {This paper is an invitation to use mono-vision techniques on stereo-vision equipped robots. By using monocular algorithms on both cameras, the advantages of mono-vision (bearing-only, with infinity range but no 3D instant information) and stereo-vision (3D information only up to a limited range) naturally add up to provide interesting possibilities, that are here developed and demonstrated using an EKF-based monocular SLAM algorithm. Mainly we obtain: a) fast 3D mapping with long term, absolute angular references; b) great landmark updating flexibility; and c) the possibility of stereo rig extrinsic self-calibration, providing a much more robust and accurate sensor. Experimental results show the pertinence of the proposed ideas, which should be easily exportable (and we encourage to do so) to other, more performing, vision-based SLAM algorithms.},
keywords = {Kalman filters;robot vision;SLAM (robots);stereo image processing;},
note = {BiCamSLAM;monovision technique;stereo-vision equipped robot;extended Kalman filter;3D mapping;absolute angular reference;landmark update;},
} 


@inproceedings{14823255 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {A geometrical approach for aerial cooperative obstacle mapping using RSSI observations},
journal = {2014 Second RSI/ISM International Conference on Robotics and Mechatronics (ICRoM). Proceedings},
author = {Mehdi Dehghan, S.M. and Moradi, H.},
year = {2014//},
pages = {197 - 202},
address = {Piscataway, NJ, USA},
abstract = {The effect of an obstacle on signal strength attenuation is the most important reason that affects the performance of distance estimation based on Received Signal Strength Indication (RSSI) observations. Consequently, in this paper, a novel approach for estimation of the location and height of an obstacle between two UAVs using RSSI observations is proposed which would help to better estimate the location of a radio frequency (RF) source. The long-term goal of developing this approach is to improve the localization of an RF source by removing the effect of obstacle(s) on the signal attenuation. This approach is based on path planning of the UAVs to estimate the tip of the obstacle between them. The change in the diffraction loss observations are used to find the Line Of Sight (LOS) positions of the UAVs. These LOS lines, constructed by connecting the UAVs in LOS situation, are the locus of the tip of the obstacle and used in an iterative geometrical approach for estimation of the location of the tip of the obstacle. Due to the uncertainty in determining the LOS position which is created because of the radius of the Fresnel zone, the motion steps of UAVs, and the non-modeled dynamics in signal attenuation, an EKF filter is used to estimate the tip of the obstacle. The approach has been simulated and the results show that the approach provides better accuracy in RF source localization compared to the basic approach which does not consider the obstacles in the localization process.},
keywords = {autonomous aerial vehicles;geometry;iterative methods;Kalman filters;nonlinear filters;path planning;radionavigation;RSSI;},
note = {aerial cooperative obstacle mapping;RSSI observations;signal strength attenuation;received signal strength indication observations;radio frequency source;path planning;UAV;line of sight positions;LOS;iterative geometrical approach;Fresnel zone;nonmodeled dynamics;EKF filter;RF source localization;},
URL = {http://dx.doi.org/10.1109/ICRoM.2014.6990900},
} 


@article{11159164 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Robust outdoor visual localization using a three-dimensional-edge map},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Nuske, S. and Roberts, J. and Wyeth, G.},
volume = { 26},
number = { 9},
year = {Sept. 2009},
pages = {728 - 56},
issn = {1556-4959},
address = {USA},
abstract = {Visual localization systems that are practical for autonomous vehicles in outdoor industrial applications must perform reliably in a wide range of conditions. Changing outdoor conditions cause difficulty by drastically altering the information available in the camera images. To confront the problem, we have developed a visual localization system that uses a surveyed three-dimensional (3D)-edge map of permanent structures in the environment. The map has the invariant properties necessary to achieve long-term robust operation. Previous 3D-edge map localization systems usually maintain a single pose hypothesis, making it difficult to initialize without an accurate prior pose estimate and also making them susceptible to misalignment with unmapped edges detected in the camera image. A multihypothesis particle filter is employed here to perform the initialization procedure with significant uncertainty in the vehicle's initial pose. A novel observation function for the particle filter is developed and evaluated against two existing functions. The new function is shown to further improve the abilities of the particle filter to converge given a very coarse estimate of the vehicle's initial pose. An intelligent exposure control algorithm is also developed that improves the quality of the pertinent information in the image. Results gathered over an entire sunny day and also during rainy weather illustrate that the localization system can operate in a wide range of outdoor conditions. The conclusion is that an invariant map, a robust multihypothesis localization algorithm, and an intelligent exposure control algorithm all combine to enable reliable visual localization through challenging outdoor conditions. &copy; 2009 Wiley Periodicals, Inc.},
keywords = {artificial intelligence;cameras;mobile robots;particle filtering (numerical methods);robust control;},
note = {robust outdoor visual localization;autonomous vehicles;industrial applications;camera images;robust operation;3D edge map localization systems;single pose hypothesis;multihypothesis particle filter;intelligent exposure control algorithm;},
URL = {http://dx.doi.org/10.1002/rob.20306},
} 


@inproceedings{16932695 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Martian Fetch: Finding and retrieving sample-tubes on the surface of Mars},
journal = {2017 IEEE Aerospace Conference},
author = {Papon, J. and Detry, R. and Vieira, P. and Brooks, S. and Srinivasan, T. and Peterson, A. and Kulczycki, E.},
year = {2017//},
pages = {9 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Mars Sample Return (MSR) was identified by the 2011 planetary science decadal survey as a high priority long-term goal for NASA. A three-mission campaign concept is currently being investigated. The Mars 2020 rover mission is intended to core and collect samples. These samples will be sealed in tubes and left on the surface for potential return to Earth. In the current MSR campaign concept, a Sample Retrieval and Launch (SRL) mission would collect the sample tubes left by the Mars 2020 rover and load them into a Mars Ascent Vehicle (MAV) to be launched into orbit. The third mission concept involves a spacecraft capturing the samples in Martian orbit and returning them to Earth. This paper focuses on the SRL mission concept to collect the sample tubes, addressing the problem of autonomously detecting, localizing, and grasping sample tubes deposited on the Martian surface. We employ two approaches: The first one is context-based. It would use a high precision map computed from images captured during tube release, to locate the tubes without directly observing them. The second approach directly detects the sample tubes visually and estimates their 6-DoF pose onboard from dense stereo data.},
keywords = {Mars;planetary rovers;planetary surfaces;space vehicles;},
note = {Mars Sample Return;planetary science decadal survey;NASA;three-mission campaign concept;Mars 2020 rover mission;Earth;MSR campaign concept;Sample Retrieval and Launch mission;sample tubes;Mars Ascent Vehicle;spacecraft;Martian orbit;SRL mission concept;sample tubes;Martian surface;high precision map;stereo data;martian fetch;},
URL = {http://dx.doi.org/10.1109/AERO.2017.7943649},
} 


@inproceedings{14966567 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Preliminary field trials of autonomous path following},
journal = {2014 IEEE/OES Autonomous Underwater Vehicles (AUV)},
author = {King, P. and Anstey, B. and Vardy, A.},
year = {2014//},
pages = {7 pp. - },
address = {Piscataway, NJ, USA},
abstract = {As part of the ongoing Responsive AUV Localization and Mapping project (REALM) Memorial University has been developing an autonomous path localization and following system. This Qualitative Navigation System (QNS) localizes an AUV to a predefined path and generates control inputs to maintain the AUV along that path without the need for an absolute position estimate. QNS processes sonar data into the two-dimensional image space and performs feature extraction and matching. The results of this matching are input into a filter which allows localization of the AUV on the path and determination of either a waypoint or heading to maintain the AUVs traversal along the path. This ability to autonomously follow a path will be of great use for long term environmental monitoring. In May of 2013 QNS was deployed on Memorial's Explorer AUV and field tested in Holyrood, Newfoundland. International Submarine Engineering, manufacturer of the Explorer AUV, provided an interface which allowed the QNS software to request control of the AUV, provide command inputs, and relinquish control. A test path consisting of two connected 755m and 470m line sections was defined and used for preliminary tests. Although testing time was constrained, a series of successful tests were completed in which the AUV autonomously detected the path, localized itself and traversed the path to completion. The results of these tests validate the concept of QNS and the AUV control interface and will drive ongoing development and testing.},
keywords = {autonomous underwater vehicles;control engineering computing;feature extraction;image matching;path planning;robot vision;},
note = {autonomous path following;responsive AUV localization and mapping project;REALM project;autonomous underwater vehicles;path localization system;path following system;qualitative navigation system;QNS;control inputs;position estimation;two-dimensional image space;feature extraction;feature matching;AUV path traversal;Memorial Explorer AUV;QNS software;},
URL = {http://dx.doi.org/10.1109/AUV.2014.7054412},
} 


@inproceedings{12688545 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {A study on wearable robotics - comfort is in the context},
journal = {2011 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
author = {Herath, D.C. and Chapman, T. and Tomkins, A. and Elliott, L. and David, M. and Cooper, A. and Burnham, D. and Kodagoda, S.},
year = {2011//},
pages = {2969 - 74},
address = {Piscataway, NJ, USA},
abstract = {WITU (Wearable Indoor Tracking Unit) is a wearable robotic device that aids indoor navigation by building maps and localizing the user within them. Applications of such a device include search and rescue, travel aid in large and complex buildings, museum guides among others where external localization information such as from a GPS is not available. However, WITU relies on human intelligence both to maintain long term consistency of its location estimates and to efficiently manage its limited memory and processing capacity. This alludes to a symbiotic relationship between the user and the device and here we look at this symbiotic relationship from an end user perspective. Thus, in order to have a successful interaction, we argue that the user needs to feel comfortable wearing the device while carrying out the intended tasks. We hypothesize that this perceived comfort is dependent on the context in which the device is used. We test our hypothesis on three different scenarios; search and rescue worker, dementia patient in a long care facility and a person at a party which acts as the baseline. Results indicate an important consequence for the development of such wearable robotic systems.},
keywords = {human-robot interaction;service robots;},
note = {wearable indoor tracking unit;wearable robotic device;travel aid;complex buildings;museum guides;external localization information;GPS;search-and-rescue;symbiotic relationship;perceived comfort;},
URL = {http://dx.doi.org/10.1109/ROBIO.2011.6181757},
} 


@inproceedings{13195216 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Realizing, reversing, recovering: Incremental robust loop closing over time using the iRRR algorithm},
journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2012)},
author = {Latif, Y. and Cadena, C. and Neira, J.},
year = {2012//},
pages = {4211 - 17},
address = {Piscataway, NJ, USA},
abstract = {The ability to reconsider information over time allows to detect failures and is crucial for long term robust autonomous robot applications. This applies to loop closure decisions in localization and mapping systems. This paper describes a method to analyze all available information up to date in order to robustly remove past incorrect loop closures from the optimization process. The main novelties of our algorithm are: 1. incrementally reconsidering loop closures and 2. handling multi-session, spatially related or unrelated experiments. We validate our proposal in real multi-session experiments showing better results than those obtained by state of the art methods.},
keywords = {mobile robots;robust control;},
note = {incremental robust loop;iRRR algorithm;robust autonomous robot applications;mapping systems;localization systems;mobile robot;realizing;reversing;recovering;},
URL = {http://dx.doi.org/10.1109/IROS.2012.6385879},
} 


@inproceedings{8065726 ,
language = {English},
copyright = {Copyright 2004, IEE},
title = {Simultaneous localisation and mapping on the Great Barrier Reef},
journal = {2004 IEEE International Conference on Robotics and Automation (IEEE Cat. No.04CH37508)},
author = {Williams, S. and Mahon, I.},
volume = {Vol.2},
year = {2004//},
pages = {1771 - 6},
address = {Piscataway, NJ, USA},
abstract = {This paper presents results of the application of the simultaneous localisation and mapping algorithm to data collected by an unmanned underwater vehicle operating on the Great Barrier Reef in Australia. By fusing information from the vehicle's on-board sonar and vision systems, it is possible to use the highly textured reef to provide estimates of the vehicle motion as well as to generate models of the gross structure of the underlying reefs. Terrain-aided navigation promises to revolutionise the ability of marine systems to track underwater bodies in many applications. This work represents a crucial step in the development of underwater technologies capable of long-term, reliable deployment. Results of the application of this technique to the tracking of the vehicle position are shown.},
keywords = {mobile robots;motion estimation;navigation;remotely operated vehicles;robot vision;sonar tracking;terrain mapping;underwater vehicles;},
note = {simultaneous localisation and mapping algorithm;unmanned underwater vehicle;onboard sonar systems;vision systems;vehicle motion estimation;terrain aided navigation;marine systems;vehicle position tracking;great barrier reef;underwater bodies;underwater technologies;reliable deployment;},
} 


@inproceedings{15458068 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Enabling persistent autonomy for underwater gliders through terrain based navigation},
journal = {OCEANS 2015},
author = {Stuntz, A. and Liebel, D. and Smith, R.N.},
year = {2015//},
pages = {10 pp. - },
address = {Piscataway, NJ, USA},
abstract = {To effectively examine ocean processes we must often sample over the duration of long (weeks to months) oscillation patterns. Such sampling requires persistent autonomous underwater vehicles, that have a similarly long deployment duration. Actively actuated (propeller-driven) underwater vehicles have proven effective in multiple sampling scenarios, however they have limited deployment endurance. The emergence of less actuated vehicles, i.e., underwater gliders, has enabled greater energy savings and thus increased endurance. Due to reduced actuation, these vehicles are more susceptible to external forces, e.g., ocean currents, causing them to have poor navigational and localization accuracy underwater. This is exacerbated in coastal regions, where current velocities are the same order of magnitude as the vehicle velocity. In this paper, we examine a method of reducing navigation and localization error, not only for navigation, but more so for more accurately reconstructing the path that the glider traversed to contextualize the gathered data, with respect to the science question at hand. We present a set of algorithms for offline processing that accurately localizes the traversed path of an underwater glider over long-term, ocean deployments. The proposed method utilizes terrain-based navigation with only depth, altimeter and compass data compared to local bathymetry maps to provide accurate reconstructions of traversed paths in the ocean.},
keywords = {autonomous underwater vehicles;mobile robots;path planning;telerobotics;},
note = {underwater gliders autonomy;terrain-based navigation;},
URL = {http://dx.doi.org/10.1109/OCEANS-Genova.2015.7271751},
} 


@inproceedings{8412039 ,
language = {English},
copyright = {Copyright 2005, IEE},
title = {Towards exteroceptive based localisation},
journal = {2004 IEEE Conference on Robotics, Automation and Mechatronics (IEEE Cat. No.04EX913)},
author = {Spero, D.J. and Jarvis, R.A.},
volume = {vol.2},
year = {2005//},
pages = {822 - 7},
address = {Piscataway, NJ, USA},
abstract = {The intelligent application of a mobile robot, outside the experimental laboratory, requires a robust locomotive strategy that is rarely conducive to stringent kinematic modeling. Localisation methods that rely upon such modeling often fail, as model boundaries succumb to unpredictable events. This paper presents the development of a self-contained localisation system that purposely obviates the need for odometric information, and an associated kinematic model, to provide robot anonymity. Without odometry, the system is oblivious to the non-systematic vagaries of the robotic platform interacting with a natural domain. The proposed system hypothesises about the robot's absolute pose by algorithmically solving the kidnapped robot problem using exteroceptive based perception. Since no a priori information is assumed, long-term pose fixes are derived within a simultaneous localisation and mapping (SLAM) framework. Preliminary results were gathered using a skid steering mobile robot, equipped with a scanning laser rangefinder, in an outdoor environment. This novel localisation approach was found to be efficient and robust, while exhibiting the capacity for widespread applicability.},
keywords = {intelligent robots;laser ranging;mobile robots;path planning;position control;robot kinematics;},
note = {exteroceptive based localisation;intelligent mobile robot;robust locomotive strategy;self-contained localisation system;kinematic model;robot anonymity;kidnapped robot problem;exteroceptive based perception;simultaneous localisation and mapping;skid steering mobile robot;scanning laser rangefinder;outdoor environment;},
} 


@inproceedings{10117381 ,
language = {English},
copyright = {Copyright 2008, The Institution of Engineering and Technology},
title = {Active vision for door localization and door opening using Playbot: a computer controlled wheelchair for people with mobility impairments},
journal = {Proceedings of the Fifth Canadian Conference on Computer and Robot Vision},
author = {Andreopoulos, A. and Tsotsos, J.K.},
year = {2008//},
pages = {3 - 10},
address = {Piscataway, NJ, USA},
abstract = {Playbot is a long-term, large-scale research project, whose goal is to provide a vision-based computer controlled wheelchair that enables children and adults with mobility impairments to become more independent. Within this context, we show how Playbot can actively search an indoor environment to localize a door, approach the door, use a mounted robotic arm to open the door, and go through the door, using exclusively vision-based sensors and without using a map of the environment. We demonstrate the effectiveness of active vision for localizing objects that are too large to fall within a single camera's field of view and show that well-calibrated vision-based sensors are sufficient to safely pass through a door frame that is narrow enough to tolerate a wheelchair localization error of at most a few centimetres. We provide experimental results demonstrating near perfect performance in an indoor environment.},
keywords = {control engineering computing;handicapped aids;medical robotics;mobile robots;robot vision;},
note = {active vision;door localization;door opening;Playbot;mobility impairments;vision-based computer controlled wheelchair;mounted robotic arm;vision-based sensors;wheelchair localization error;},
URL = {http://dx.doi.org/10.1109/CRV.2008.23},
} 


@inproceedings{15278375 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Environment selection and hierarchical place recognition},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Mohan, M. and Galvez-Lopez, D. and Monteleoni, C. and Sibley, G.},
year = {2015//},
pages = {5487 - 94},
address = {Piscataway, NJ, USA},
abstract = {As robots continue to create long-term maps, the amount of information that they need to handle increases over time. In terms of place recognition, this implies that the number of images being considered may increase until exceeding the computational resources of the robot. In this paper we consider a scenario where, given multiple independent large maps, possibly from different cities or locations, a robot must effectively and in real time decide whether it can localize itself in one of those known maps. Since the number of images to be handled by such a system is likely to be extremely large, we find that it is beneficial to decompose the set of images into independent groups or environments. This raises a new question: Given a query image, how do we select the best environment? This paper proposes a similarity criterion that can be used to solve this problem. It is based on the observation that, if each environment is described in terms of its co-occurrent features, similarity between environments can be established by comparing their co-occurrence matrices. We show that this leads to a novel place recognition algorithm that divides the collection of images into environments and arranges them in a hierarchy of inverted indices. By selecting first the relevant environment for the operating robot, we can reduce the number of images to perform the actual loop detection, reducing the execution time while preserving the accuracy. The practicality of this approach is shown through experimental results on several large datasets covering a combined distance of more than 750Km.},
keywords = {cartography;image recognition;matrix algebra;robot vision;},
note = {environment selection;hierarchical place recognition algorithm;multiple independent large maps;image decomposition;query image;similarity criterion;co-occurrent features;co-occurrence matrices;image collection;inverted indices hierarchy;loop detection;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7139966},
} 


@inproceedings{9517353 ,
language = {English},
copyright = {Copyright 2007, The Institution of Engineering and Technology},
title = {Robotic discovery of the auditory scene},
journal = {2007 IEEE International Conference on Robotics and Automation (IEEE Cat No. 07CH37836D)},
author = {Martinson, E. and Schultz, A.},
year = {2007//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {In this work, we describe an autonomous mobile robotic system for finding and investigating ambient noise sources in the environment. Motivated by the large negative effect of ambient noise sources on robot audition, the long-term goal is to provide awareness of the auditory scene to a robot, so that it may more effectively act to filter out the interference or re-position itself to increase the signal-to-noise ratio. Here, we concentrate on the discovery of new sources of sound through the use of mobility and directed investigation. This is performed in a two-step process. In the first step, a mobile robot first explores the surrounding acoustical environment, creating evidence grid representations to localize the most influential sound sources in the auditory scene. Then in the second step, the robot investigates each potential sound source location in the environment so as to improve the localization result, and identify volume and directionality characteristics of the sound source. Once every source has been investigated, a noise map of the entire auditory scene is created for use by the robot in avoiding areas of loud ambient noise when performing an auditory task.},
keywords = {audio signal processing;hearing;mobile robots;},
note = {autonomous mobile robotic system;ambient noise sources;robot audition;signal-to-noise ratio;sound source localization;},
} 


@inproceedings{9120404 ,
language = {English},
copyright = {Copyright 2006, The Institution of Engineering and Technology},
copyright = {CD-ROM},
title = {Consistency of the FastSLAM algorithm},
journal = {Proceedings. 2006 Conference on International Robotics and Automation (IEEE Cat. No. 06CH37729D)},
author = {Bailey, T. and Nieto, J. and Nebot, E.},
year = {2006//},
pages = {424 - 9},
address = {Piscataway, NJ, USA},
abstract = {This paper presents an analysis of FastSLAM - a Rao-Blackwellised particle filter formulation of simultaneous localisation and mapping. It shows that the algorithm degenerates with time, regardless of the number of particles used or the density of landmarks within the environment, and would always produce optimistic estimates of uncertainty in the long-term. In essence, FastSLAM behaves like a non-optimal local search algorithm; in the short-term it may produce consistent uncertainty estimates but, in the long-term, it is unable to adequately explore the state-space to be a reasonable Bayesian estimator. However, the number of particles and landmarks does affect the accuracy of the estimated mean and, given sufficient particles, FastSLAM can produce good non-stochastic estimates in practice. FastSLAM also has several practical advantages, particularly with regard to data association, and would probably work well in combination with other versions of stochastic SLAM, such as EKF-based SLAM.},
keywords = {Bayes methods;particle filtering (numerical methods);path planning;robots;search problems;},
note = {FastSLAM algorithm;particle filter formulation;simultaneous localisation and mapping;nonoptimal local search algorithm;Bayesian estimator;data association;},
} 


@inproceedings{10571037 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {ShopBot: progress in developing an interactive mobile shopping assistant for everyday use},
journal = {2008 IEEE International Conference on Systems, Man and Cybernetics (SMC 2008)},
author = {Gross, H.-M. and Boehme, H.-J. and Schroeter, C. and Mueller, S. and Koenig, A. and Martin, C. and Merten, M. and Bley, A.},
year = {2008//},
pages = {3471 - 8},
address = {Piscataway, NJ, USA},
abstract = {The paper describes progress achieved in our long-term research project ShopBot, which aims at the development of an intelligent and interactive mobile shopping assistant for everyday use in shopping centers or home improvement stores. It is focusing on recent progress concerning two important methodological aspects: (i) the on-line building of maps of the operation area by means of advanced Rao-Blackwellized SLAM approaches using both sonar-based gridmaps as well as vision-based graph maps as representations, and (ii) a probabilistic approach to multi-modal user detection and tracking during the guidance tour. Experimental results of both the map building characteristics and the person tracking behavior achieved in an ordinary home improvement store demonstrate the reliability of both approaches. Moreover, we present first very encouraging results of long-term field trials which have been executed with three robotic shopping assistants in another home improvement store in Bavaria since March 2008. In this field test, the robots could demonstrate their suitability for this challenging real-world application, as well as the necessary user acceptance.},
keywords = {graph theory;intelligent robots;mobile robots;probability;robot vision;service robots;sonar detection;},
note = {interactive mobile shopping assistant;home improvement store;sonar-based gridmap;vision-based graph map;probabilistic approach;multimodal user detection;robotic shopping assistant;},
URL = {http://dx.doi.org/10.1109/ICSMC.2008.4811835},
} 


@inproceedings{3288082 ,
language = {English},
copyright = {Copyright 1989, IEE},
title = {Unusual applications of SLAM in management decision making},
journal = {Simulation in the Factory of the Future and Simulation in Traffic Control. Proceedings of the European Simulation Multiconference},
author = {Prekel, H.L.},
year = {1988//},
pages = {105 - 10},
address = {Ghent, Belgium},
abstract = {SLAM, the Simulation Language for Alternate Modelling, is one of a growing number of popular simulation languages. The increasing power and sophistication of the microcomputer is making these languages accessible to a growing number of users, and has led to the development of affordable software packages. Most of the applications, however, are used to model physical processing type situations, such as production lines, inventory systems, vehicle scheduling, and mining operations. Two entirely different types of problems are analysed with the help of SLAM in this paper. In the first, Topchem, it is shown how SLAM can be used to compare two investment opportunities by generating distributions of possible internal rates of return for each of them, and analysing the results. The second, Manpower, shows how SLAM can be used to analyse the effects of various manpower planning policies on the long term staffing position of a company.},
keywords = {complete computer programs;decision support systems;financial data processing;investment;microcomputer applications;personnel;simulation languages;},
note = {management decision making;SLAM;Simulation Language;microcomputer;software packages;Topchem;investment opportunities;internal rates of return;Manpower;manpower planning policies;long term staffing position;},
} 


@inproceedings{7162264 ,
language = {English},
copyright = {Copyright 2002, IEE},
title = {Contribution to vision-based localization, tracking and navigation methods for an interactive mobile service-robot},
journal = {2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)},
author = {Gross, H.-M. and Boehme, H.-J. and Wilhelm, T.},
volume = {vol.2},
year = {2001//},
pages = {672 - 7},
address = {Piscataway, NJ, USA},
abstract = {Presents vision-based robot navigation and user localization techniques of our long-term research project PERSES (personal service system), which aims to develop an interactive mobile shopping assistant that allows a continuous and intuitively understandable interaction with customers in a home store. Against this background, the paper describes a number of new or improved approaches, addressing challenges arising from the characteristics of the operation area, and from the need to continuously interact with users in a complex environment. With our approaches to vision-based or visually-controlled map building, self-localization and navigation as well as user localization and tracking, we want to make a contribution to the real-world suitability of interactive mobile service-robots in non-trivial application areas and demanding human-robot interaction scenarios.},
keywords = {mobile robots;object detection;path planning;robot vision;},
note = {vision-based localization;vision-based tracking;vision-based navigation;interactive mobile service-robot;PERSES;personal service system;interactive mobile shopping assistant;home store;person detection;},
URL = {http://dx.doi.org/10.1109/ICSMC.2001.972991},
} 


@inproceedings{8313734 ,
language = {English},
copyright = {Copyright 2005, IEE},
title = {Obstacle avoidance and path planning for humanoid robots using stereo vision},
journal = {2004 IEEE International Conference on Robotics and Automation (IEEE Cat. No.04CH37508)},
author = {Sabe, K. and Fukuchi, M. and Gutmann, J.-S. and Ohashi, T. and Kawamoto, K. and Yoshigahara, T.},
volume = {Vol.1},
year = {2004//},
pages = {592 - 7},
address = {Piscataway, NJ, USA},
abstract = {This paper presents methods for path planning and obstacle avoidance for the humanoid robot QRIO, allowing the robot to autonomously walk around in a home environment. For an autonomous robot, obstacle detection and localization as well as representing them in a map are crucial tasks for the success of the robot. Our approach is based on plane extraction from data captured by a stereo-vision system that has been developed specifically for QRIO. We briefly overview the general software architecture composed of perception, short and long term memory, behavior control, and motion control, and emphasize on our methods for obstacle detection by plane extraction, occupancy grid mapping, and path planning. Experimental results complete the description of our system.},
keywords = {collision avoidance;humanoid robots;mobile robots;motion control;robot vision;},
note = {obstacle avoidance;path planning;QRIO humanoid robot;stereo vision;autonomous robot;plane extraction;behavior control;motion control;occupancy grid mapping;},
} 


@article{7488693 ,
language = {English},
copyright = {Copyright 2003, IEE},
title = {Principles and networks for self-organization in space-time},
journal = {Neural Networks},
journal = {Neural Netw. (UK)},
author = {Principe, J. and Euliano, N. and Garani, S.},
volume = { 15},
number = { 8-9},
year = {2002/10/},
pages = {1069 - 83},
issn = {0893-6080},
address = {UK},
abstract = {We develop a spatio-temporal memory that blends properties from long and short-term memory and is motivated by reaction diffusion mechanisms. The winning processing element of a self-organizing network creates traveling waves on the output space that gradually attenuate over time and space to diffuse temporal information and create localized spatio-temporal neighborhoods for clustering. The novelty of the model is in the creation of time varying Voronoi tessellations anticipating the learned input signal dynamics even when the cluster centers are fixed. We test the method in a robot navigation task and in vector quantization of speech. This method performs better than conventional static vector quantizers based on the same data set and similar training conditions.},
keywords = {learning (artificial intelligence);mobile robots;self-organising feature maps;vector quantisation;},
note = {self-organization;spatio-temporal memory;long term memory;short-term memory;reaction diffusion mechanisms;self-organizing network;localized spatio-temporal neighborhoods;clustering;Neural Gas;time varying Voronoi tessellations;robot navigation task;vector quantization;speech;},
URL = {http://dx.doi.org/10.1016/S0893-6080(02)00080-1},
} 


@inproceedings{6182595 ,
language = {English},
copyright = {Copyright 1999, IEE},
title = {Incorporating environmental measurements in navigation},
journal = {Proceedings of the 1998 Workshop on Autonomous Underwater Vehicles (Cat. No.98CH36290)},
author = {Feder, H.J.S. and Leonard, J.J. and Smith, C.M.},
year = {1998//},
pages = {115 - 22},
address = {Piscataway, NJ, USA},
abstract = {Extended missions in unknown regions present a significant navigational challenge for autonomous underwater vehicles (AUV). This paper investigates the long-term performance of a concurrent mapping and localization (CML) algorithm for the scenario of an AUV making observations of point features in the environment with a forward look sonar. Simulation results demonstrate that position estimates with long-term bounded errors of a few meters can be achieved under realistic assumptions about the vehicle, its sensors, and the environment. Potential failure modes of the algorithm, such as divergence and map slip, are discussed. CML technology can provide a significant improvement in the navigational capabilities of AUVs and can enable new missions in unmapped regions without reliance on acoustic beacons or surfacing for GPS resets.},
keywords = {computerised navigation;mobile robots;sonar;underwater vehicles;},
note = {environmental measurements;navigation;autonomous underwater vehicles;AUV;mapping;localization;CML;forward look sonar;long-term bounded errors;failure modes;divergence;map slip;acoustic beacons;forward-looking sonar;},
URL = {http://dx.doi.org/10.1109/AUV.1998.744447},
} 


@inproceedings{7509116 ,
language = {English},
copyright = {Copyright 2003, IEE},
copyright = {Also available on CD-ROM in PDF format},
title = {Robust asynchronous temporal event mapping},
journal = {Proceedings IEEE/RSJ International Conference on Intelligent Robots and Systems (Cat. No.02CH37332C)},
author = {Schill, F. and Zimmer, U.R.},
volume = {vol.1},
year = {2002//},
pages = {190 - 5},
address = {Piscataway, NJ, USA},
abstract = {Localisation and mapping relies on the representation and recognition of features or patterns detected in sensor data. An important aspect is the temporal relationship of observations in sensor data streams. This article proposes a new approach for simultaneous localisation and mapping based on temporal relations in the flow of characteristic events in the sensor data channels. A dynamical system is employed to acquire these correlations between simultaneous and sequential events from different sources, to map causal sequences, while considering time spans, and to recognise previously observed patterns (localisation). While this system is applicable to sensor modalities with different characteristics and timing behaviours, it is especially suitable for distributed computing. Mapping and localisation take place simultaneously in an life-long unsupervised distributed online learning process. The dynamical system was implemented as a distributed real-time system with symmetric processes. A real-time clustering network reduces the dimension of raw sensor data. Cluster transitions are used as input for the dynamical mapping system. Results from physical experiments with one sensor modality are presented.},
keywords = {distributed processing;learning (artificial intelligence);mobile robots;path planning;pattern matching;position control;real-time systems;sensor fusion;},
note = {sensor data streams;localisation;pattern recognition;dynamical system;sensor distributed real time system;sensor fusion;learning process;clustering network;mobile robots;asynchronous temporal event mapping system;},
URL = {http://dx.doi.org/10.1109/IRDS.2002.1041387},
} 


@inproceedings{6771194 ,
language = {English},
copyright = {Copyright 2000, IEE},
title = {PERSES-a vision-based interactive mobile shopping assistant},
journal = {SMC 2000 Conference Proceedings. 2000 IEEE International Conference on Systems, Man and Cybernetics. `Cybernetics Evolving to Systems, Humans, Organizations, and their Complex Interactions' (Cat. No.00CH37166)},
author = {Gross, H.-M. and Boehme, H.-J.},
volume = {vol.1},
year = {2000//},
pages = {80 - 5},
address = {Piscataway, NJ, USA},
abstract = {The paper describes the general idea, the application scenario, and selected methodological approaches of our long term research project PERSES (PERsonal SErvice System). The aim of the project consists of the development of an interactive mobile shopping assistant that allows a continuous and intuitively understandable interaction with a customer in a home improvement store. Typical tasks we have to tackle are to detect and contact potential users in the operation area, to guide them to desired areas or articles within the store or to follow them as a mobile information kiosk while continuously observing their behavior. Due to the specificity of the interaction-oriented scenario and the characteristics of the operation area, we have focused on vision based methods for both human-robot interaction and robot navigation. Besides some methodological approaches, we present preliminary results of experiments achieved with our mobile robot PERSES in the store with an emphasis on vision based methods for user localization, map building and self-localization.},
keywords = {computerised navigation;interactive systems;mobile robots;retail data processing;robot vision;user interfaces;},
note = {PERSES;vision based interactive mobile shopping assistant;application scenario;long term research project;PERsonal SErvice System;intuitively understandable interaction;home improvement store;potential users;operation area;mobile information kiosk;interaction-oriented scenario;vision based methods;human-robot interaction;robot navigation;methodological approaches;mobile robot;user localization;map building;self-localization;},
URL = {http://dx.doi.org/10.1109/ICSMC.2000.884968},
} 


@inproceedings{4214157 ,
language = {English},
copyright = {Copyright 1992, IEE},
title = {Simultaneous map building and localization for an autonomous mobile robot},
journal = {Proceedings IROS '91. IEEE/RSJ International Workshop on Intelligent Robots and Systems '91. Intelligence for Mechanical Systems (Cat. No.91TH0375-6)},
author = {Leonard, J.J. and Durrant-Whyte, H.F.},
year = {1991//},
pages = {1442 - 7},
address = {New York, NY, USA},
abstract = {Discusses a significant open problem in mobile robotics: simultaneous map building and localization, which the authors define as long-term globally referenced position estimation without a priori information. This problem is difficult because of the following paradox: to move precisely, a mobile robot must have an accurate environment map; however, to build an accurate map, the mobile robot's sensing locations must be known precisely. In this way, simultaneous map building and localization can be seen to present a question of `which came first, the chicken or the egg?' (The map or the motion?) When using ultrasonic sensing, to overcome this issue the authors equip the vehicle with multiple servo-mounted sonar sensors, to provide a means in which a subset of environment features can be precisely learned from the robot's initial location and subsequently tracked to provide precise positioning.},
keywords = {computerised navigation;mobile robots;planning (artificial intelligence);},
note = {path planning;computerised navigation;map building;localization;autonomous mobile robot;long-term globally referenced position estimation;ultrasonic sensing;multiple servo-mounted sonar sensors;precise positioning;},
URL = {http://dx.doi.org/10.1109/IROS.1991.174711},
} 


@inproceedings{7832648 ,
language = {English},
copyright = {Copyright 2004, IEE},
title = {Active vision for wearables},
journal = {IEE Eurowearable '03},
author = {Mayol, W.W. and Tordoff, B.J. and de Campos, T.E. and Davison, A.J. and Murray, D.W.},
year = {2004//},
pages = {99 - 104},
address = {London, UK},
abstract = {In this paper we report on our ongoing research on wearable active vision, where we have iteratively prototyped a wearable visual robot - a body mounted robot for which the main sensor is a camera. Two main areas have been studied: robot design and visual algorithms. In the design stage, we have analysed sensor placement through the computation of the field of view and body motion using a 3D model of the human form. A design methodology for the robot morphology was developed with the help of an optimisation algorithm based on the Pareto front. The wearability of the device has progressed over several iterations as have the sensor and control architectures. In terms of visual algorithms, we have studied methods of visual tracking fused with inertial sensors, real-time template tracking, human head pose recovery and more recently real-time simultaneous ego-localisation and autonomous 3D map building. Our main long-term application areas are enhanced remote collaboration and autonomous wearable assistants that use vision.},
keywords = {active vision;mobile computing;Pareto optimisation;real-time systems;robot vision;sensor fusion;software prototyping;tracking;visual programming;wearable computers;},
note = {wearable active vision;iterative prototyping;wearable visual robot;body mounted robot;camera sensor;robot design;visual algorithms;sensor placement;field of view;body motion;3D model;robot morphology;Pareto optimisation;control architectures;visual tracking;inertial sensor fusion;real-time template tracking;human head pose recovery;real-time simultaneous ego-localisation;autonomous 3D map building;enhanced remote collaboration;autonomous wearable assistants;},
} 


@inproceedings{8348591 ,
language = {English},
copyright = {Copyright 2005, IEE},
title = {Rotation invariant features from omnidirectional camera images using a polar higher-order local autocorrelation feature extractor},
journal = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)},
author = {Linaker, F. and Ishikawa, M.},
volume = {vol.4},
year = {2004//},
pages = {4026 - 31},
address = {Piscataway, NJ, USA},
abstract = {Proposed in this paper is a component for extracting low-dimensional rotation invariant feature vectors directly from omnidirectional camera images. The component is based on higher-order local autocorrelation (HLAC) functions, but with a modification that makes the extraction result in rotation invariant representations. As the component provides a static mapping to feature vectors, it requires no setup or learning phase and is well-suited for lifelong learning scenarios where input distributions can be nonstationary. Experiments with an actual robot system are presented and results show that the extracted feature vectors manage to capture structures in the environment. When used as the perceptual component of a sequential Monte Carlo localizer, the location of the robot can be tracked without access to long-range distance sensors. Important limitations and suitable uses for the extracted representations are also discussed.},
keywords = {feature extraction;image representation;mobile robots;Monte Carlo methods;robot vision;},
note = {rotation invariant feature;omnidirectional camera image;autocorrelation feature extractor;higher-order local autocorrelation function;robot system;sequential Monte Carlo localizer;},
} 


@inproceedings{5680121 ,
language = {English},
copyright = {Copyright 1997, IEE},
title = {Continuous localization in changing environments},
journal = {Proceedings. 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. `Towards New Computational Principles for Robotics and Automation' (Cat. No.97TB100176)},
author = {Graves, K. and Adams, W. and Schultz, A.},
year = {1997//},
pages = {28 - 33},
address = {Los Alamitos, CA, USA},
abstract = {Continuous localization is a technique that allows a robot to maintain an accurate estimate of its location by performing regular small corrections to its odometry. Continuous localization uses an evidence grid representation, a common representation scheme that is used by other map-dependent processes, such as path planning. Although techniques exist for building evidence grid maps, most are not adaptive to changes in the environment. In this research, we extend the continuous localization technique by adding a learning component. This allows continuous localization to update the long-term map (evidence grid) with current sensor readings. Results show that the addition of the learning behavior to continuous localization allows the system to adapt to changes in its environment without a loss in its ability to remain localized. This system was tested on a Nomad 200 mobile robot.},
keywords = {adaptive control;learning (artificial intelligence);mobile robots;path planning;position measurement;},
note = {continuous localization;robot;odometry corrections;evidence grid representation;map-dependent processes;Nomad 200 mobile robot;},
URL = {http://dx.doi.org/10.1109/CIRA.1997.613834},
} 


@inproceedings{19750395 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {The Implementation of IMU/Stereo Vision Slam System for Mobile Robot},
journal = {2020 27th Saint Petersburg International Conference on Integrated Navigation Systems (ICINS)},
author = {Hou Juan-Rou and Wang Zhan-Qing},
year = {2020//},
pages = {4 pp. - },
address = {Piscataway, NJ, USA},
abstract = {In the research of unmanned vehicle technology, the integrated navigation system based on inertial measurement unit and stereo camera has gradually become a research hotspot. The inertial navigation system has the characteristics of higher short-time precision, and does not radiate information to the outside world. The stereo vision navigation system collects image information of the environment, and performs feature extraction and tracking on the feature points in the acquired images to recover the motion of the carrier. In this paper, the stereo vision navigation system is used to correct the long-term error accumulation of the inertial navigation system. On the other hand, the short-time precision of the inertial navigation system can also compensate the vision navigation system caused by the blurred image information caused by the carrier moving too fast. The integrated navigation system of IMU together with stereo vision camera can gain better comprehensive performance. In this paper, Multi-State Fusion Kalman Filter (MSF) and Multi-State Constraint Kalman Filter (MSCKF) are used to fuse the inertial navigation system and vision navigation system. When constructing the MSCKF framework, we use sparse optical flow method to achieve feature tracking, and using triangulation in computer vision to calculate the positions of feature points, and use the data set to verify the accuracy of the two algorithms. Finally, constructing an environmental point cloud map using the estimated state of feature points. Under the environment of i7-8750H cpu, the experimental results show that tight coupling is more accurate than loose coupling.},
keywords = {cameras;computer vision;feature extraction;image filtering;image sequences;inertial navigation;Kalman filters;mobile robots;robot vision;SLAM (robots);stereo image processing;},
note = {integrated navigation system;inertial navigation system;short-time precision;stereo vision navigation system;feature points;stereo vision camera;},
} 


@article{17727203 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Multiple Object Detection, Tracking and Long-Term Dynamics Learning in Large 3D Maps [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Bore, N. and Jensfelt, P. and Folkesson, J.},
year = {2018/01/28},
pages = {13 pp. - },
address = {USA},
abstract = {In this work, we present a method for tracking and learning the dynamics of all objects in a large scale robot environment. A mobile robot patrols the environment and visits the different locations one by one. Movable objects are discovered by change detection, and tracked throughout the robot deployment. For tracking, we extend the Rao-Blackwellized particle filter of previous work with birth and death processes, enabling the method to handle an arbitrary number of objects. Target births and associations are sampled using Gibbs sampling. The parameters of the system are then learnt using the Expectation Maximization algorithm in an unsupervised fashion. The system therefore enables learning of the dynamics of one particular environment, and of its objects. The algorithm is evaluated on data collected autonomously by a mobile robot in an office environment during a real-world deployment. We show that the algorithm automatically identifies and tracks the moving objects within 3D maps and infers plausible dynamics models, significantly decreasing the modeling bias of our previous work. The proposed method represents an improvement over previous methods for environment dynamics learning as it allows for learning of fine grained processes.},
keywords = {expectation-maximisation algorithm;learning (artificial intelligence);mobile robots;multi-robot systems;object detection;particle filtering (numerical methods);SLAM (robots);target tracking;},
note = {environment dynamics learning;multiple object detection;long-term dynamics learning;3D maps;movable objects;change detection;robot deployment;Rao-Blackwellized particle filter;death processes;arbitrary number;Gibbs sampling;office environment;real-world deployment;moving objects;mobile robot;expectation maximization algorithm;large-scale robot environment;birth processes;},
} 


@inproceedings{16503721 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Long-term place recognition using multi-level words of spatial densities},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Maffei, R. and Jorge, V.A.M. and Rey, V.F. and Kolberg, M. and Prestes, E.},
year = {2016//},
pages = {3269 - 74},
address = {Piscataway, NJ, USA},
abstract = {Proper place recognition on an environment that can change over time is fundamental for long-term SLAM. In such scenarios the observations obtained in the same region can drastically differ due to changes caused by semi-static objects, such as doors, furniture, etc. In this work, we extend a strategy that represents environment regions using words, based on spatial density information extracted from laser readings. This time, in order to deal with changes in the environment, our method not only builds words representing the real observations made by the robot, but also alternative multi-level words to account for possible changes in a place's observations generated by non-static objects. Place recognition is made by searching matches of sequences of N consecutive words (both real or alternatives). Experiments performed in real and simulated scenarios are shown, and demonstrate the advantages associated to the use of multi-level words.},
keywords = {feature extraction;image recognition;mobile robots;SLAM (robots);},
note = {place recognition;multilevel word;spatial density;simultaneous localization and mapping;SLAM;information extraction;mobile robot;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759504},
} 


@inproceedings{18392894 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Egger, P. and Borges, P.V.K. and Catt, G. and Pfrunder, A. and Siegwart, R. and Dube, R.},
year = {2018//},
pages = {3430 - 7},
address = {Piscataway, NJ, USA},
abstract = {Reliable long-term localization is key for robotic systems in dynamic environments. In this paper, we propose a novel approach for long-term localization using 3D LiDARs, coined PoseMap. In essence, we extract distinctive features from range measurements and bundle these into local views along with observation poses. The sensor's trajectory is then estimated in a sliding window fashion by matching current and old features and minimizing the distances in-between. The map representation facilitates finding a suitable set of old features, by selecting the closest local map(s) for matching. Similarly to a visibility analysis, this procedure provides a suitable set of features for localization but at a fraction of the computational cost. PoseMap also allows for updates and extensions of the map at any time by replacing and adding local maps when necessary. We evaluate our approach using two platforms both equipped with a 3D LiDAR and an IMU, demonstrating localization at 8 Hz and robustness to changes in the environment such as moving vehicles and changing vegetation. PoseMap was implemented on an autonomous vehicle allowing it to drive autonomously over a period of 18 months through a mix of industrial and unstructured off-road environments, covering more than 100 kms without a single localization failure.},
keywords = {feature extraction;mobile robots;optical radar;path planning;},
note = {local views;sliding window fashion;matching current;old features;map representation;local maps;off-road environments;single localization failure;distinctive features;coined PoseMap;dynamic environments;robotic systems;long-term localization;multienvironment 3D LiDAR localization;frequency 8.0 Hz;time 18.0 month;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8593854},
} 


@article{17135401 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Air-SSLAM: a visual stereo indoor SLAM for aerial quadrotors},
journal = {IEEE Geoscience and Remote Sensing Letters},
journal = {IEEE Geosci. Remote Sens. Lett. (USA)},
author = {Araujo, P. and Miranda, R. and Carmo, D. and Alves, R. and Oliveira, L.},
volume = { 14},
number = { 9},
year = {Sept. 2017},
pages = {1643 - 7},
issn = {1545-598X},
address = {USA},
abstract = {In this letter, we introduce a novel method for visual simultaneous localization and mapping (SLAM)-so-called Air-SSLAM-which exploits a stereo camera configuration. In contrast to monocular SLAM, scale definition and 3-D information are issues that can be more easily dealt with in stereo cameras. Air-SSLAM starts from computing keypoints and the correspondent descriptors over the pair of images, using good features-to-track and rotated-binary robust-independent elementary features, respectively. Then a map is created by matching each pair of right and left frames. The long-term map maintenance is continuously performed by analyzing the quality of each matching, as well as by inserting new keypoints into uncharted areas of the environment. Three main contributions can be highlighted in our method: (1) a novel method to match keypoints efficiently; (2) three quality indicators with the aim of speeding up the mapping process; and (3) map maintenance with uniform distribution performed by image zones. By using a drone equipped with a stereo camera, flying indoor, the translational average error with respect to a marked ground truth was computed, demonstrating promising results.},
keywords = {autonomous aerial vehicles;cameras;feature extraction;geophysical image processing;helicopters;image matching;robot vision;SLAM (robots);stereo image processing;},
note = {Air-SSLAM;visual stereo indoor SLAM;aerial quadrotors;visual simultaneous localization and mapping;stereo camera configuration;monocular SLAM;scale definition;3D information;stereo cameras;keypoints;correspondent descriptors;features-to-track features;rotated-binary robust-independent elementary features;left frame pair matching;right frame pair matching;long-term map maintenance;quality indicators;uniform distribution;image zones;drones;translational average error;marked ground truth;},
URL = {http://dx.doi.org/10.1109/LGRS.2017.2730883},
} 


@inproceedings{17300101 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {A Robust RGB-D Image-Based SLAM System},
journal = {Computer Vision Systems. 11th International Conference, ICVS 2017. Revised Selected Papers: LNCS 10528},
author = {Liangliang Pan and Jun Cheng and Wei Feng and Xiaopeng Ji},
year = {2017//},
pages = {120 - 30},
address = {Cham, Switzerland},
abstract = {Visual SLAM is widely used in robotics and computer vision. Although there have been many excellent achievements over the past few decades, there are still some challenges. 2D feature-based SLAM algorithm has been suffering from the inaccurate or insufficient correspondences while dealing with the case of textureless or frequently repeating regions. Furthermore, most of the SLAM systems cannot be used for long-term localization in a wide range of environment because of the heavy burden of calculating and memory. In this paper, we propose a robust RGB-D keyframe-based SLAM algorithm. The novelty of proposed approach lies in using both 2D and 3D features for tracking, pose estimation and bundle adjustment. By using 2D and 3D features, the SLAM system can achieve high accuracy and robustness in some challenging environments. The experimental results on TUM RGB-D dataset [1] and ICL-NUIM dataset [2] verify the effectiveness of our algorithm.},
keywords = {pose estimation;robot vision;SLAM (robots);},
note = {SLAM system;computer vision;RGB-D image;visual feature;},
URL = {http://dx.doi.org/10.1007/978-3-319-68345-4_11},
} 


@article{18501967 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Real-time dense map fusion for stereo SLAM},
journal = {Robotica},
journal = {Robotica (UK)},
author = {Pire, T. and Baravalle, R. and D'Alessandro, A. and Civera, J.},
volume = { 36},
number = { 10},
year = {2018/10/},
pages = {1510 - 26},
issn = {0263-5747},
address = {UK},
abstract = {A robot should be able to estimate an accurate and dense 3D model of its environment (a map), along with its pose relative to it, all of it in real time, in order to be able to navigate autonomously without collisions. As the robot moves from its starting position and the estimated map grows, the computational and memory footprint of a dense 3D map increases and might exceed the robot capabilities in a short time. However, a global map is still needed to maintain its consistency and plan for distant goals, possibly out of the robot field of view. In this work, we address such problem by proposing a real-time stereo mapping pipeline, feasible for standard CPUs, which is locally dense and globally sparse and accurate. Our algorithm is based on a graph relating poses and salient visual points, in order to maintain a long-term accuracy with a small cost. Within such framework, we propose an efficient dense fusion of several stereo depths in the locality of the current robot pose. We evaluate the performance and the accuracy of our algorithm in the public datasets of Tsukuba and KITTI, and demonstrate that it outperforms single-view stereo depth. We release the code as open-source, in order to facilitate the system use and comparisons.},
keywords = {image reconstruction;mobile robots;pose estimation;robot vision;SLAM (robots);stereo image processing;},
note = {environment;dense 3D model;accurate D model;stereo SLAM;time dense map fusion;single-view stereo depth;current robot;stereo depths;efficient dense fusion;graph relating poses;real-time stereo mapping pipeline;global map;short time;robot capabilities;dense 3D map increases;computational memory footprint;estimated map;starting position;a map;},
URL = {http://dx.doi.org/10.1017/S0263574718000528},
} 


@article{17718826 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Robust Place Recognition and Loop Closing in Laser-Based SLAM for UGVs in Urban Environments},
journal = {IEEE Sensors Journal},
journal = {IEEE Sens. J. (USA)},
author = {Fengkui Cao and Yan Zhuang and Hong Zhang and Wei Wang},
volume = { 18},
number = { 10},
year = {2018/05/15},
pages = {4242 - 52},
issn = {1530-437X},
address = {USA},
abstract = {Robust place recognition plays a key role for the long-term autonomy of unmanned ground vehicles (UGVs) working in indoor or outdoor environments. Although most of the state-of-the-art that approaches for place recognition are vision-based, visual sensors lack adaptability in environments with poor or dynamically changing illumination. In this paper, a 3-D-laser-based place recognition algorithm is proposed to accomplish loop closure detection for simultaneous localization and mapping. An image model named bearing angle (BA) is adopted to convert 3-D laser points to 2-D images, and then ORB features extracted from BA images are utilized to perform scene matching. Since the computational cost for matching a query BA image with all the BA images in a database is too high to meet the requirement of performing real-time place recognition, a visual bag of words approach is used to improve search efficiency. Furthermore, a speed normalization algorithm and a 3-D geometry-based verification algorithm are proposed to complete the proposed place recognition algorithm. Experiments were conducted on two self-developed UGV platforms to verify the performance of the proposed method.},
keywords = {computational geometry;feature extraction;image matching;image sensors;mobile robots;object detection;object recognition;remotely operated vehicles;robot vision;SLAM (robots);},
note = {long-term autonomy;unmanned ground vehicles;indoor environments;outdoor environments;visual sensors lack adaptability;poor changing illumination;dynamically changing illumination;place recognition algorithm;loop closure detection;image model;2-D images;real-time place recognition;robust place recognition;loop closing;urban environments;laser-based SLAM;3-D-laser-based place recognition algorithm;simultaneous localization-and-mapping;3D laser points;query BA image matching;ORB features extraction;visual bag-of-words approach;speed normalization algorithm;3D geometry-based verification algorithm;self-developed UGV platforms;},
URL = {http://dx.doi.org/10.1109/JSEN.2018.2815956},
} 


@inproceedings{17737446 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Efficient Map Compression for Collaborative Visual SLAM},
journal = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings},
author = {Van Opdenbosch, D. and Aykut, T. and Alt, N. and Steinbach, E.},
year = {2018//},
pages = {992 - 1000},
address = {Los Alamitos, CA, USA},
abstract = {Swarm robotics is receiving increasing interest, because the collaborative completion of tasks, such as the exploration of unknown environments, leads to improved performance and reduced effort. The ability to exchange map information is an essential requirement for collaborative exploration. When moving to large-scale environments, where the communication data rate between the swarm participants is typically limited, efficient compression algorithms and an approach for discarding less informative parts of the map are key for a successful long-term operation. In this paper, we present a novel compression approach for environment maps obtained from a visual SLAM system. We apply feature coding to the visual information to compress the map efficiently. We make use of a minimum spanning tree to connect all features that serve as observations of a single map point. Thereby, we can exploit inter-feature dependencies and obtain an optimal coding order. Additionally, we add a map sparsification step to keep only useful map points by solving a linear integer programming problem, which preserves the map points that exhibit both good compression properties and high observability. We evaluate the proposed method on a standard dataset and show that our approach outperforms state-of-the-art techniques.},
keywords = {data compression;image coding;integer programming;linear programming;mobile robots;multi-robot systems;robot vision;SLAM (robots);trees (mathematics);},
note = {swarm robotics;map information;collaborative exploration;communication data rate;swarm participants;environment maps;visual SLAM system;visual information;minimum spanning tree;single map point;inter-feature dependencies;optimal coding order;map sparsification step;map compression;collaborative visual SLAM;linear integer programming problem;},
URL = {http://dx.doi.org/10.1109/WACV.2018.00114},
} 


@inproceedings{17338966 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Performance evaluation of graph-reduction in SLAM through pose rejection},
journal = {2017 56th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE). Proceedings},
author = {Cervantes, P. and Inoue, M.},
year = {2017//},
pages = {644 - 7},
address = {Piscataway, NJ, USA},
abstract = {During long-term operation graph-based SLAM methods suffer from unbounded growth of the pose graph and computational cost. To limit the graph size, various methods to reduce the graph have been studied in the past. In this paper we propose a criterion to reject poses, based on the difference between expected and real scan of the candidate. The performance of this reduction method is then evaluated in terms of error and computation time.},
keywords = {graph theory;mobile robots;SLAM (robots);},
note = {performance evaluation;graph-reduction;SLAM;pose rejection;pose graph;computational cost;},
URL = {http://dx.doi.org/10.23919/SICE.2017.8105681},
} 


@article{17465036 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {AEKF-SLAM: a new algorithm for robotic underwater navigation},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Xin Yuan and Martinez-Ortega, J.-F. and Sanchez Fernandez, J.A. and Eckert, M.},
volume = { 17},
number = { 5},
year = {2017/05/},
pages = {1174 (30 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {In this work, we focus on key topics related to underwater Simultaneous Localization and Mapping (SLAM) applications. Moreover, a detailed review of major studies in the literature and our proposed solutions for addressing the problem are presented. The main goal of this paper is the enhancement of the accuracy and robustness of the SLAM-based navigation problem for underwater robotics with low computational costs. Therefore, we present a new method called AEKF-SLAM that employs an Augmented Extended Kalman Filter (AEKF)-based SLAM algorithm. The AEKF-based SLAM approach stores the robot poses and map landmarks in a single state vector, while estimating the state parameters via a recursive and iterative estimation-update process. Hereby, the prediction and update state (which exist as well in the conventional EKF) are complemented by a newly proposed augmentation stage. Applied to underwater robot navigation, the AEKF-SLAM has been compared with the classic and popular FastSLAM 2.0 algorithm. Concerning the dense loop mapping and line mapping experiments, it shows much better performances in map management with respect to landmark addition and removal, which avoid the long-term accumulation of errors and clutters in the created map. Additionally, the underwater robot achieves more precise and efficient self-localization and a mapping of the surrounding landmarks with much lower processing times. Altogether, the presented AEKF-SLAM method achieves reliably map revisiting, and consistent map upgrading on loop closure.},
keywords = {computational complexity;iterative methods;Kalman filters;mobile robots;nonlinear filters;parameter estimation;path planning;recursive estimation;SLAM (robots);state estimation;},
note = {recursive estimation-update process;iterative estimation-update process;underwater robot navigation;dense loop mapping;robotic underwater navigation;single state vector;AEKF-SLAM method;underwater simultaneous localization and mapping;computational costs;augmented extended Kalman filter;state parameter estimation;FastSLAM 2.0 algorithm;line mapping;},
URL = {http://dx.doi.org/10.3390/s17051174},
} 


@article{15980555 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Ceiling vision-based active SLAM framework for dynamic and wide-open environments},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Su-Yong An and Lae-Kyoung Lee and Se-Young Oh},
volume = { 40},
number = { 2},
year = {2016/02/},
pages = {291 - 324},
issn = {0929-5593},
address = {Germany},
abstract = {A typical indoor environment can be divided into three categories; office (or room), hallway, and wide-open space such as lobby and hall. There have been numerous approaches for solving simultaneous localization and mapping (SLAM) problem in office (or room) and hallway. However, direct application of the existing approaches to wide-open space may be failed, because it has some distinguished features compared to other indoor places. To solve this problem, this paper proposes a new ceiling vision-based active SLAM framework, with an emphasis on practical deployment of service robot for commercial use in dynamically changing and wide-open environments by adopting the ceiling vision. First, for defining ceiling feature which can be extracted regardless of complexity of ceiling pattern we introduce a model-free landmark, i.e., visual node descriptor, which consists of edge points and their orientations in image space. Second, a recursive `explore and exploit' is proposed for autonomous mapping. It is recursively performed by spreading out mapped area gradually while the robot is actively localized in the map. It can improve map accuracy due to frequent small loop closing. Third, a dynamic edge link (DEL) is proposed to cope with environmental changes in the map. Owing to DEL, we do not need to filter out corrupted sensor data and to distinguish moving object from static one. Also, a self-repairing map mechanism is introduced to deal with unexpected installation or removal of inner structures. We therefore achieve long-term navigation. Several simulations and real experiments in various places show that the proposed active SLAM framework could build a topologically consistent map, and demonstrated that it can be applied well to real environments such as wide-open space in a city hall and railway station.},
keywords = {mobile robots;navigation;robot vision;service robots;SLAM (robots);},
note = {long-term navigation;DEL;dynamic edge link;image space;visual node descriptor;model-free landmark;service robot;simultaneous localization and mapping;indoor environment;wide-open environments;dynamic environments;ceiling vision-based active SLAM framework;},
URL = {http://dx.doi.org/10.1007/s10514-015-9453-0},
} 


@article{15658832 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Long-Term Simultaneous Localization and Mapping in Dynamic Environments},
author = {Carlevaris-Bianco, N.D.},
year = {2015//},
pages = {148 - },
address = {Ann Arbor, MI, USA},
abstract = {One of the core competencies required for autonomous mobile robotics is the ability to use sensors to perceive the environment. From this noisy sensor data, the robot must build a representation of the environment and localize itself within this representation. This process, known as simultaneous localization and mapping (SLAM), is a prerequisite for almost all higher-level autonomous behavior in mobile robotics. By associating the robot's sensory observations as it moves through the environment, and by observing the robot's ego-motion through proprioceptive sensors, constraints are placed on the trajectory of the robot and the configuration of the environment. This results in a probabilistic optimization problem to find the most likely robot trajectory and environment configuration given all of the robot's previous sensory experience. SLAM has been well studied under the assumptions that the robot operates for a relatively short time period and that the environment is essentially static during operation. However, performing SLAM over long time periods while modeling the dynamic changes in the environment remains a challenge. The goal of this thesis is to extend the capabilities of SLAM to enable long-term autonomous operation in dynamic environments. The contribution of this thesis has three main components: First, we propose a framework for controlling the computational complexity of the SLAM optimization problem so that it does not grow unbounded with exploration time. Second, we present a method to learn visual feature descriptors that are more robust to changes in lighting, allowing for improved data association in dynamic environments. Finally, we use the proposed tools in SLAM systems that explicitly models the dynamics of the environment in the map by representing each location as a set of example views that capture how the location changes with time. We experimentally demonstrate that the proposed methods enable long-term SLAM in dynamic environments using a large, realworld vision and LIDAR dataset collected over the course of more than a year. This dataset captures a wide variety of dynamics: from short-term scene changes including moving people, cars, changing lighting, and weather conditions; to long-term dynamics including seasonal conditions and structural changes caused by construction.},
keywords = {mobile robots;optical radar;sensor fusion;},
note = {autonomous mobile robotics;LIDAR dataset;real-world vision;data association;visual feature descriptors;computational complexity;long-term autonomous operation;dynamic environments;long-term simultaneous localization and mapping;},
} 


@inproceedings{15667335 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Long-term human affordance maps},
journal = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Limosani, R. and Morales, L.Y. and Even, J. and Ferreri, F. and Watanabe, A. and Cavallo, F. and Dario, P. and Hagita, N.},
year = {2015//},
pages = {5748 - 54},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a work on mapping the use of space by humans in long periods of time. Daily geometric maps with the same coordinate frame were generated with SLAM, and in a similar manner, daily affordance density maps (places people use) were generated with the output of a human tracker running on the robot. The contribution of the paper is two-fold: an approach to detect geometric changes to cluster them in similar geometric configurations and the building of geometric and affordance composite maps on each cluster. This approach avoids the loss of long term retrieved information. Geometric similarity was computed using a normal distance approach on the maps. The analysis was performed on data collected by a mobile robot for a period of 4 months accumulating data equivalent to 70 days. Experimental results show that the system is capable of detecting geometric changes in the environment and clustering similar geometric configurations.},
keywords = {mobile robots;object tracking;pattern clustering;SLAM (robots);},
note = {long-term human affordance maps;space use mapping;SLAM;daily affordance density map;human tracker;geometric change detection;geometric composite map;affordance composite map;geometric similarity;normal distance approach;mobile robot;similar geometric configuration clustering;},
URL = {http://dx.doi.org/10.1109/IROS.2015.7354193},
} 


@inproceedings{17522448 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Long-Term 3D Localization and Pose from Semantic Labellings},
journal = {2017 IEEE International Conference on Computer Vision Workshops (ICCVW)},
author = {Toft, C. and Olsson, C. and Kahl, F.},
year = {2017//},
pages = {650 - 9},
address = {Los Alamitos, CA, USA},
abstract = {One of the major challenges in camera pose estimation and 3D localization is identifying features that are approximately invariant across seasons and in different weather and lighting conditions. In this paper, we present a method for performing accurate and robust six degrees-of-freedom camera pose estimation based only on the pixelwise semantic labelling of a single query image. Localization is performed using a sparse 3D model consisting of semantically labelled points and curves, and an error function based on how well these project onto corresponding curves in the query image is developed. The method is evaluated on the recently released Oxford Robotcar dataset, showing that by minimizing this error function, the pose can be recovered with decimeter accuracy in many cases.},
keywords = {cameras;feature extraction;image classification;image reconstruction;image retrieval;mobile robots;object recognition;pose estimation;robot vision;SLAM (robots);visual databases;},
note = {semantic labellings;lighting conditions;pixelwise semantic labelling;sparse 3D model;semantically labelled points;error function;weather conditions;camera pose estimation;curves;Long-Term 3D Localization;query image;Oxford Robotcar dataset;},
URL = {http://dx.doi.org/10.1109/ICCVW.2017.83},
} 


@inproceedings{14617130 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Long-term 3D map maintenance in dynamic environments},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Pomerleau, F. and Krusi, P. and Colas, F. and Furgale, P. and Siegwart, R.},
year = {2014//},
pages = {3712 - 19},
address = {Piscataway, NJ, USA},
abstract = {New applications of mobile robotics in dynamic urban areas require more than the single-session geometric maps that have dominated simultaneous localization and mapping (SLAM) research to date; maps must be updated as the environment changes and include a semantic layer (such as road network information) to aid motion planning in dynamic environments. We present an algorithm for long-term localization and mapping in real time using a three-dimensional (3D) laser scanner. The system infers the static or dynamic state of each 3D point in the environment based on repeated observations. The velocity of each dynamic point is estimated without requiring object models or explicit clustering of the points. At any time, the system is able to produce a most-likely representation of underlying static scene geometry. By storing the time history of velocities, we can infer the dominant motion patterns within the map. The result is an online mapping and localization system specifically designed to enable long-term autonomy within highly dynamic environments. We validate the approach using data collected around the campus of ETH Zurich over seven months and several kilometers of navigation. To the best of our knowledge, this is the first work to unify long-term map update with tracking of dynamic objects.},
keywords = {image representation;mobile robots;path planning;robot vision;SLAM (robots);},
note = {long-term 3D map maintenance;mobile robotics;single-session geometric maps;simultaneous localization and mapping;SLAM;semantic layer;road network information;motion planning;long-term localization and mapping;3D laser scanner;dynamic point velocity estimation;static scene geometry representation;online mapping and localization system;ETH Zurich campus;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907397},
} 


@article{19415952 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {UcoSLAM: Simultaneous localization and mapping by fusion of keypoints and squared planar markers},
journal = {Pattern Recognition},
journal = {Pattern Recognit. (Netherlands)},
author = {Munoz-Salinas, R. and Medina-Carnicer, R.},
volume = { 101},
year = {2020/05/},
pages = {122 - 32},
issn = {0031-3203},
address = {Netherlands},
abstract = {Simultaneous Localization and Mapping is the process of simultaneously creating a map of the environment while navigating in it. Most of the SLAM approaches use natural features (e.g. keypoints) that are unstable over time, repetitive in many cases or their number insufficient for a robust tracking (e.g. in indoor buildings). Other researchers, on the other hand, have proposed the use of artificial landmarks, such as squared fiducial markers, placed in the environment to help tracking and relocalization. This paper proposes a novel SLAM approach by fusing natural and artificial landmarks in order to achieve long-term robust tracking in many scenarios.Our method has been compared to the start-of-the-art methods ORB-SLAM2 [1], LDSO [2] and SPM-SLAM [3] in the public datasets Kitti [4], Euroc-MAV [5], TUM [6] and SPM [3], obtaining better precision, robustness and speed. Our tests also show that the combination of markers and keypoints achieves better accuracy than each one of them independently. [All rights reserved Elsevier].},
keywords = {cartography;feature extraction;object tracking;robot vision;SLAM (robots);},
note = {UcoSLAM;squared planar markers;SLAM approaches;indoor buildings;artificial landmarks;squared fiducial markers;long-term robust tracking;ORB-SLAM;SPM-SLAM;robustness;simultaneous localization and mapping;keypoint fusion;LDSO;TUM;Euroc-MAV;Kitti;SPM;environment mapping;},
URL = {http://dx.doi.org/10.1016/j.patcog.2019.107193},
} 


@inproceedings{18740239 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Sparse Gaussian Process SLAM, Storage and Filtering for AUV Multibeam Bathymetry},
journal = {2018 IEEE/OES Autonomous Underwater Vehicle Workshop (AUV). Proceedings},
author = {Bore, N. and Torroba, I. and Folkesson, J.},
year = {2018//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {With dead-reckoning from velocity sensors, AUVs may construct short-term, local bathymetry maps of the sea floor using multibeam sensors. However, the position estimate from dead-reckoning will include some drift that grows with time. In this work, we focus on long-term onboard storage of these local bathymetry maps, and the alignment of maps with respect to each other. We propose using Sparse Gaussian Processes for this purpose, and show that the representation has several advantages, including an intuitive alignment optimization, data compression, and sensor noise filtering. We demonstrate these three key capabilities on two real-world datasets.},
keywords = {autonomous underwater vehicles;bathymetry;data compression;Gaussian processes;mobile robots;oceanographic techniques;path planning;SLAM (robots);},
note = {multibeam sensors;position estimate;dead-reckoning;local bathymetry maps;sensor noise filtering;AUV multibeam bathymetry;velocity sensors;short-term;sea floor;sparse Gaussian process SLAM;data compression;},
URL = {http://dx.doi.org/10.1109/AUV.2018.8729748},
} 


@inproceedings{18168052 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Visual Place Recognition in Long-term and Large-scale Environment based on CNN Feature},
journal = {2018 IEEE Intelligent Vehicles Symposium (IV)},
author = {Jianliang Zhu and Yunfeng Ai and Bin Tian and Dongpu Cao and Scherer, S.},
year = {2018//},
pages = {1679 - 85},
address = {Piscataway, NJ, USA},
abstract = {With the universal application of camera in intelligent vehicles, visual place recognition has become a major problem in intelligent vehicle localization. The traditional solution is to make visual description of place images using hand-crafted feature for matching places, but this description method is not very good for extreme variability, especially for seasonal transformation. In this paper, we propose a new method based on convolutional neural network (CNN), by putting images into the pre-trained network model to get automatically learned image descriptors, and through some operations of pooling, fusion and binarization to optimize them, then the similarity result of place recognition is presented with the Hamming distance of the place sequence. In the experimental part, we compare our method with some state-of-the-art algorithms, FABMAP, ABLE-M and SeqSLAM, to illustrate its advantages. The experimental results show that our method based on CNN achieves better performance than other methods on the representative public datasets.},
keywords = {convolution;feedforward neural nets;image matching;image recognition;intelligent transportation systems;learning (artificial intelligence);SLAM (robots);},
note = {visual place recognition;large-scale environment;CNN feature;intelligent vehicles;intelligent vehicle localization;visual description;place images;hand-crafted feature;description method;convolutional neural network;pre-trained network model;place sequence;image descriptors;Hamming distance;place matching;},
URL = {http://dx.doi.org/10.1109/IVS.2018.8500686},
} 


@article{15929729 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {An Open-source Bio-inspired Solution to Underwater SLAM},
journal = {IFAC - Papers Online},
journal = {IFAC, Pap. Online (Netherlands)},
author = {Silveira, L. and Guth, F. and Drews-Jr, P. and Ballester, P. and Machado, M. and Codevilla, F. and Duarte-Filho, N. and Botelho, S.},
volume = { 48},
number = { 2},
year = {2015//},
pages = {212 - 17},
issn = {2405-8963},
address = {Netherlands},
abstract = {We present a bio-inspired approach to deal with the localization and spatial mapping problem, extending the successful previous RatSLAM approach from 2D ground vehicles to the 3D underwater environments. Our approach, called DolphinSLAM, is a SLAM system based on mammals navigation. Experiments in simulation and real environments were conducted involving long-term navigation tasks with different robots and sensors. Our proposal is open- source, being integrated with the Robot Operating System (ROS). [All rights reserved Elsevier].},
keywords = {autonomous underwater vehicles;marine navigation;public domain software;sensors;SLAM (robots);},
note = {RatSLAM approach;2D ground vehicles;mammal navigation;robot operating system;ROS;open-source bio-inspired solution;underwater SLAM;3D underwater environments;DolphinSLAM;long-term navigation;sensors;simultaneous localization and mapping;},
URL = {http://dx.doi.org/10.1016/j.ifacol.2015.06.035},
} 


@inproceedings{17058444 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Drift-correcting self-calibration for visual-inertial SLAM},
journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Nobre, F. and Kasper, M. and Heckman, C.},
year = {2017//},
pages = {6525 - 32},
address = {Piscataway, NJ, USA},
abstract = {We present a solution for online simultaneous localization and mapping (SLAM) self-calibration in the presence of drift in calibration parameters in order to support accurate long-term operation. Calibration parameters such as the camera focal length or camera-to-IMU extrinsics are frequently subject to drift over long periods of operation, inducing cumulative error in the reconstruction. The key contributions are modeling calibration parameters as a spatiotemporal quantity: sensor-to-sensor spatial calibration and sensor intrinsic parameters are continuously time-varying, with statistical tests for change detection and regression. An analysis of the long term effects of inappropriately modeling time-varying sensor calibration is also provided. Constant-time operation is achieved by selecting only a fixed number of informative segments of the trajectory for calibration parameter estimation, giving the added benefit of avoiding early linearization errors by not rolling past measurements into a prior distribution. Our approach is validated with simulated and real-world data.},
keywords = {calibration;cameras;mobile robots;parameter estimation;robot vision;SLAM (robots);statistical testing;},
note = {drift-correcting self-calibration;visual-inertial SLAM;online simultaneous localization and mapping self-calibration;camera focal length;camera-to-IMU extrinsics;calibration parameter modeling;spatiotemporal quantity;sensor-to-sensor spatial calibration;sensor intrinsic parameters;statistical tests;change detection;regression analysis;time-varying sensor calibration modeling;constant-time operation;},
URL = {http://dx.doi.org/10.1109/ICRA.2017.7989771},
} 


@inproceedings{14718001 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Long-term topological localisation for service robots in dynamic environments using spectral maps},
journal = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014)},
author = {Krajnik, T. and Fentanes, J.P. and Mozos, O.M. and Duckett, T. and Ekekrantz, J. and Hanheide, M.},
year = {2014//},
pages = {4537 - 42},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a new approach for topological localisation of service robots in dynamic indoor environments. In contrast to typical localisation approaches that rely mainly on static parts of the environment, our approach makes explicit use of information about changes by learning and modelling the spatio-temporal dynamics of the environment where the robot is acting. The proposed spatio-temporal world model is able to predict environmental changes in time, allowing the robot to improve its localisation capabilities during long-term operations in populated environments. To investigate the proposed approach, we have enabled a mobile robot to autonomously patrol a populated environment over a period of one week while building the proposed model representation. We demonstrate that the experience learned during one week is applicable for topological localization even after a hiatus of three months by showing that the localization error rate is significantly lower compared to static environment representations.},
keywords = {indoor environment;learning (artificial intelligence);mobile robots;service robots;SLAM (robots);spatiotemporal phenomena;},
note = {long-term topological localisation;service robots;spectral maps;dynamic indoor environments;spatiotemporal dynamics learning;spatiotemporal dynamics modelling;environmental change prediction;mobile robot;model representation;localization error rate;},
URL = {http://dx.doi.org/10.1109/IROS.2014.6943205},
} 


@article{16299206 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Summary Maps for Lifelong Visual Localization},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Muhlfellner, P. and Burki, M. and Bosse, M. and Derendarz, W. and Philippsen, R. and Furgale, P.},
volume = { 33},
number = { 5},
year = {2016/08/},
pages = {561 - 90},
issn = {1556-4959},
address = {USA},
abstract = {Robots that use vision for localization need to handle environments that are subject to seasonal and structural change, and operate under changing lighting and weather conditions. We present a framework for lifelong localization and mapping designed to provide robust and metrically accurate online localization in these kinds of changing environments. Our system iterates between offline map building, map summary, and online localization. The offline mapping fuses data from multiple visually varied datasets, thus dealing with changing environments by incorporating new information. Before passing these data to the online localization system, the map is summarized, selecting only the landmarks that are deemed useful for localization. This <i>Summary Map</i> enables online localization that is accurate and robust to the variation of visual information in natural environments while still being computationally efficient. We present a number of summary policies for selecting useful features for localization from the multisession map, and we explore the tradeoff between localization performance and computational complexity. The system is evaluated on 77 recordings, with a total length of 30 kilometers, collected outdoors over 16 months. These datasets cover all seasons, various times of day, and changing weather such as sunshine, rain, fog, and snow. We show that it is possible to build consistent maps that span data collected over an entire year, and cover day-to-night transitions. Simple statistics computed on landmark observations are enough to produce a Summary Map that enables robust and accurate localization over a wide range of seasonal, lighting, and weather conditions.},
keywords = {robot vision;robust control;},
note = {summary maps;lifelong visual localization;robot vision;handle environments;weather conditions;lighting conditions;changing environments;online localization;offline map building;map summary;multiple visually varied datasets;natural environments;visual information;},
URL = {http://dx.doi.org/10.1002/rob.21595},
} 


@inproceedings{16503797 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Bridging the Appearance Gap: Multi-Experience Localization for Long-Term Visual Teach and Repeat},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Paton, M. and MacTavish, K. and Warren, M. and Barfoot, T.D.},
year = {2016//},
pages = {1918 - 25},
address = {Piscataway, NJ, USA},
abstract = {Vision-based, route-following algorithms enable autonomous robots to repeat manually taught paths over long distances using inexpensive vision sensors. However, these methods struggle with long-term, outdoor operation due to the challenges of environmental appearance change caused by lighting, weather, and seasons. While techniques exist to address appearance change by using multiple experiences over different environmental conditions, they either provide topological-only localization, require several manually taught experiences in different conditions, or require extensive offline mapping to produce metric localization. For real-world use, we would like to localize metrically to a single manually taught route and gather additional visual experiences during autonomous operations. Accordingly, we propose a novel multi-experience localization (MEL) algorithm developed specifically for route-following applications; it provides continuous, six-degree-of-freedom (6DoF) localization with relative uncertainty to a privileged (manually taught) path using several experiences simultaneously. We validate our algorithm through two experiments: i) an offline performance analysis on a 9km subset of a challenging 27km route-traversal dataset and ii) an online field trial where we demonstrate autonomy on a small 250m loop over the course of a sunny day. Both exhibit significant appearance change due to lighting variation. Through these experiments we show that safe localization can be achieved by bridging the appearance gap.},
keywords = {image sensors;mobile robots;path planning;robot vision;SLAM (robots);},
note = {environmental appearance;multiexperience localization;MEL algorithm;visual teach and repeat;VT&amp;R;vision-based algorithm;route-following algorithm;autonomous robot;vision sensor;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759303},
} 


@inproceedings{15040650 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Review of underwater SLAM techniques},
journal = {2015 6th International Conference on Automation, Robotics and Applications (ICARA). Proceedings},
author = {Hidalgo, F. and Braunl, T.},
year = {2015//},
pages = {306 - 11},
address = {Piscataway, NJ, USA},
abstract = {SLAM (Simultaneous Localization and Mapping) for underwater vehicles is a challenging research topic due to the limitations of underwater localization sensors and error accumulation over long-term operations. Furthermore, acoustic sensors for mapping often provide noisy and distorted images or low-resolution ranging, while video images provide highly detailed images but are often limited due to turbidity and lighting. This paper presents a review of the approaches used in state-of-the-art SLAM techniques: Extended Kalman Filter SLAM (EKF-SLAM), FastSLAM, GraphSLAM and its application in underwater environments.},
keywords = {Kalman filters;SLAM (robots);underwater vehicles;video signal processing;},
note = {underwater SLAM techniques;simultaneous localization and mapping;underwater vehicles;underwater localization sensors;acoustic sensors;low-resolution ranging;video images;extended Kalman filter SLAM;EKF-SLAM;FastSLAM;GraphSLAM;underwater environments;},
URL = {http://dx.doi.org/10.1109/ICARA.2015.7081165},
} 


@inproceedings{16595791 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Texture-Aware SLAM Using Stereo Imagery and Inertial Information},
journal = {2016 13th Conference on Computer and Robot Vision (CRV)},
author = {Manderson, T. and Shkurti, F. and Dudek, G.},
year = {2016//},
pages = {456 - 63},
address = {Los Alamitos, CA, USA},
abstract = {We present a gaze control method that augments an existing stereo and inertial Simultaneous Localization And Mapping (SLAM) system by directing the stereo camera towards feature-rich regions of the scene. Our integrated active SLAM system is based on careful triangulation of visual features, existing successful nonlinear optimization, and visual loop closing frameworks. It relies on the tight coupling of IMU measurements with constraints imposed by visual correspondences from both stereo and motion. Alongside the SLAM system, the gaze control module also runs in real-time and includes an efficient online classifier that segments the scene into texture classes and assigns a quality score to each class that correlates with the availability of reliable features for tracking. Based on this quality score, the gaze selection module controls a pan-tilt unit that directs the camera to focus on high-reward texture classes. We validate our system in both indoor and outdoor spaces, and we show that active gaze control crucially improves the robustness and long-term operation of the localization system.},
keywords = {gaze tracking;optimisation;SLAM (robots);stereo image processing;},
note = {stereo imagery;simultaneous localization and mapping system;active SLAM system;nonlinear optimization;visual loop closing frameworks;gaze control module;gaze selection module;},
URL = {http://dx.doi.org/10.1109/CRV.2016.69},
} 


@inproceedings{14447140 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Integration of Monte Carlo Localization and Place Recognition for Reliable Long-Term Robot Localization},
journal = {2014 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
author = {Perez, J. and Caballero, F. and Merino, L.},
year = {2014//},
pages = {85 - 91},
address = {Piscataway, NJ, USA},
abstract = {This paper proposes extending Monte Carlo Localization methods with visual information in order to build a long term robot localization system. This system is aimed to work in crowded and non-planar scenarios, where 2D laser rangefinders may not always be enough to match the robot position with the map. Thus, visual place recognition will be used in order to obtain robot position clues that can be used to detect when the robot is lost and also to reset its positions to the right one. The paper presents experimental results based on datasets gathered with a real robot in challenging scenarios.},
keywords = {mobile robots;Monte Carlo methods;navigation;path planning;pose estimation;robot vision;SLAM (robots);},
note = {robot position;nonplanar scenarios;crowded scenarios;long-term robot localization system;visual place recognition;Monte Carlo localization methods;},
URL = {http://dx.doi.org/10.1109/ICARSC.2014.6849767},
} 


@inproceedings{16503734 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Curating long-term vector maps},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Nashed, S. and Biswas, J.},
year = {2016//},
pages = {4643 - 8},
address = {Piscataway, NJ, USA},
abstract = {Autonomous service mobile robots need to consistently, accurately, and robustly localize in human environments despite changes to such environments over time. Episodic non-Markov Localization addresses the challenge of localization in such changing environments by classifying observations as arising from Long-Term, Short-Term, or Dynamic Features. However, in order to do so, EnML relies on an estimate of the Long-Term Vector Map (LTVM) that does not change over time. In this paper, we introduce a recursive algorithm to build and update the LTVM over time by reasoning about visibility constraints of objects observed over multiple robot deployments. We use a signed distance function (SDF) to filter out observations of short-term and dynamic features from multiple deployments of the robot. The remaining long-term observations are used to build a vector map by robust local linear regression. The uncertainty in the resulting LTVM is computed via Monte Carlo resampling the observations arising from long-term features. By combining occupancy-grid based SDF filtering of observations with continuous space regression of the filtered observations, our proposed approach builds, updates, and amends LTVMs over time, reasoning about all observations from all robot deployments in an environment. We present experimental results demonstrating the accuracy, robustness, and compact nature of the extracted LTVMs from several long-term robot datasets.},
keywords = {mobile robots;Monte Carlo methods;multi-robot systems;regression analysis;service robots;vectors;},
note = {long-term vector maps curating;LTVM;recursive algorithm;visibility constraints;multiple robot deployments;signed distance function;SDF;short-term features;dynamic features;robust local linear regression;Monte Carlo resampling;occupancy-grid based SDF filtering;continuous space regression;autonomous service mobile robots;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759683},
} 


@inproceedings{16503846 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Appearance-based landmark selection for efficient long-term visual localization},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Burki, M. and Gilitschenski, I. and Stumm, E. and Siegwart, R. and Nieto, J.},
year = {2016//},
pages = {4137 - 43},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we present an online landmark selection method for distributed long-term visual localization systems in bandwidth-constrained environments. Sharing a common map for online localization provides a fleet of autonomous vehicles with the possibility to maintain and access a consistent map source, and therefore reduce redundancy while increasing efficiency. However, connectivity over a mobile network imposes strict bandwidth constraints and thus the need to minimize the amount of exchanged data. The wide range of varying appearance conditions encountered during long-term visual localization offers the potential to reduce data usage by extracting only those visual cues which are relevant at the given time. Motivated by this, we propose an unsupervised method of adaptively selecting landmarks according to how likely these landmarks are to be observable under the prevailing appearance condition. The ranking function this selection is based upon exploits landmark co-observability statistics collected in past traversals through the mapped area. Evaluation is performed over different outdoor environments, large time-scales and varying appearance conditions, including the extreme transition from day-time to night-time, demonstrating that with our appearance-dependent selection method, we can significantly reduce the amount of landmarks used for localization while maintaining or even improving the localization performance.},
keywords = {minimisation;navigation;road vehicles;statistics;traffic control;},
note = {appearance-dependent selection method;landmark coobservability statistics;ranking function;adaptive landmark selection;unsupervised method;visual cue extraction;data usage reduction;exchanged data minimization;mobile network connectivity;redundancy reduction;autonomous vehicles;online localization map;bandwidth-constrained environments;distributed long-term visual localization systems;appearance-based landmark selection;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759609},
} 


@article{18626885 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Robust Photogeometric Localization Over Time for Map-centric Loop Closure},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Chanoh Park and Soohwan Kim and Moghadam, P. and Jiadong Guo and Sridharan, S. and Fookes, C.},
volume = { 4},
number = { 2},
year = {2019/04/},
pages = {1768 - 75},
issn = {2377-3774},
address = {USA},
abstract = {Map-centric Simultaneous Localization And Mapping (SLAM) is emerging as an alternative of conventional graph-based SLAM for its accuracy and efficiency in long-term mapping problems. However, in map-centric SLAM, the process of loop closure differs from that of conventional SLAM and the result of incorrect loop closure is more destructive and is not reversible. In this letter, we present a tightly coupled photogeometric metric localization for the loop closure problem in map-centric SLAM. In particular, our method combines complementary constraints from LiDAR and camera sensors, and validates loop closure candidates with sequential observations. The proposed method provides a visual evidence-based outlier rejection where failures caused by either place recognition or localization outliers can be effectively removed. We demonstrate that the proposed method is not only more accurate than the conventional global ICP methods but is also robust to incorrect initial pose guesses.},
keywords = {image recognition;SLAM (robots);},
note = {visual evidence-based outlier rejection;place recognition;map-centric loop closure;conventional graph-based SLAM;long-term mapping problems;map-centric SLAM;tightly coupled photogeometric metric localization;loop closure problem;robust photogeometric localization;map-centric simultaneous localization and mapping;},
URL = {http://dx.doi.org/10.1109/LRA.2019.2895262},
} 


@article{15484808 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {ORB-SLAM: A Versatile and Accurate Monocular SLAM System},
journal = {IEEE Transactions on Robotics},
journal = {IEEE Trans. Robot. (USA)},
author = {Mur-Artal, R. and Montiel, J.M.M. and Tardos, J.D.},
volume = { 31},
number = { 5},
year = {2015/10/},
pages = {1147 - 63},
issn = {1552-3098},
address = {USA},
abstract = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
keywords = {SLAM (robots);},
note = {ORB-SLAM system;feature-based monocular simultaneous localization and mapping system;survival of the fittest strategy;},
URL = {http://dx.doi.org/10.1109/TRO.2015.2463671},
} 


@article{10725547 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {Experimental analysis of sample-based maps for long-term SLAM},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Biber, P. and Duckett, T.},
volume = { 28},
number = { 1},
year = {2009/01/},
pages = {20 - 33},
issn = {0278-3649},
address = {USA},
abstract = {This paper presents a system for long-term SLAM (simultaneous localization and mapping) by mobile service robots and its experimental evaluation in a real dynamic environment. To deal with the stability-plasticity dilemma (the trade-off between adaptation to new patterns and preservation of old patterns), the environment is represented by multiple timescales simultaneously (five in our experiments). A sample-based representation is proposed, where older memories fade at different rates depending on the timescale and robust statistics are used to interpret the samples. The dynamics of this representation are analyzed in a five-week experiment, measuring the relative influence of short- and long-term memories over time and further demonstrating the robustness of the approach.},
keywords = {mobile robots;robot dynamics;service robots;SLAM (robots);stability;},
note = {sample-based maps;long-term SLAM;experimental analysis;mobile service robots;stability-plasticity dilemma;sample-based representation;robust statistics;},
URL = {http://dx.doi.org/10.1177/0278364908096286},
} 


@article{14490182 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Long-term mapping and localization using feature stability histograms},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Bacca, B. and Salvi, J. and Cufi, X.},
volume = { 61},
number = { 12},
year = {2013/12/},
pages = {1539 - 58},
issn = {0921-8890},
address = {Netherlands},
abstract = {This work proposes a system for long-term mapping and localization based on the Feature Stability Histogram (FSH) model which is an innovative feature management approach able to cope with changing environments. FSH is built using a voting schema, where re-observed features are promoted; otherwise the feature progressively decreases its corresponding FSH value. FSH is inspired by the human memory model. This model introduces concepts of Short-Term Memory (STM), which retains information long enough to use it, and Long-Term Memory (LTM), which retains information for longer periods of time. If the entries in STM are continuously rehearsed, they become part of LTM. However, this work proposes a change in the pipeline of this model, allowing any feature to be part of STM or LTM depending on the feature strength. FSH stores the stability values of local features, stable features are only used for localization and mapping. Experimental validation of the FSH model was conducted using the FastSLAM framework and a long-term dataset collected during a period of one year at different environmental conditions. The experiments carried out include qualitative and quantitative results such as: filtering out dynamic objects, increasing map accuracy, scalability, and reducing the data association effort in long-term runs. [All rights reserved Elsevier].},
keywords = {mobile robots;SLAM (robots);},
note = {long-term mapping and localization;feature stability histograms;FSH model;feature management approach;voting schema;reobserved features;human memory model;short-term memory;STM;long-term memory;LTM;local feature stability values;FastSLAM framework;dynamic object filtering;map accuracy;data association effort reduction;mobile robotics;},
URL = {http://dx.doi.org/10.1016/j.robot.2013.07.003},
} 


@inproceedings{16559180 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Information-based Active SLAM via topological feature graphs},
journal = {2016 IEEE 55th Conference on Decision and Control (CDC)},
author = {Beipeng Mu and Giamou, M. and Paull, L. and Agha-mohammadi, A.-A. and Leonard, J. and How, J.},
year = {2016//},
pages = {5583 - 90},
address = {Piscataway, NJ, USA},
abstract = {Exploring an unknown space and building maps is a fundamental capability for mobile robots. For fully autonomous systems, the robot would further need to actively plan its paths during exploration. The problem of designing robot trajectories to actively explore an unknown environment and minimize the map error is referred to as active simultaneous localization and mapping (active SLAM). Existing work has focused on planning paths with occupancy grid maps, which do not scale well and suffer from long term drift. This work proposes a Topological Feature Graph (TFG) representation that scales well and develops an active SLAM algorithm with it. The TFG uses graphical models, which utilize independences between variables, and enables a unified quantification of exploration and exploitation gains with a single entropy metric. Hence, it facilitates a natural and principled balance between map exploration and refinement. A probabilistic roadmap path-planner is used to generate robot paths in real time. Experimental results demonstrate that the proposed approach achieves better accuracy than a standard grid-map based approach while requiring orders of magnitude less computation and memory resources.},
keywords = {entropy;graph theory;mobile robots;path planning;probability;SLAM (robots);trajectory control;},
note = {information-based active SLAM;mobile robots;fully autonomous systems;path planning;robot trajectories;occupancy grid maps;topological feature graph representation;TFG representation;entropy metric;probabilistic roadmap path-planner;robot path generation;standard grid-map based approach;magnitude less computation;memory resources;active simultaneous localization and mapping;},
URL = {http://dx.doi.org/10.1109/CDC.2016.7799127},
} 


@inproceedings{15278280 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Towards life-long visual localization using an efficient matching of binary sequences from images},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Arroyo, R. and Alcantarilla, P.F. and Bergasa, L.M. and Romera, E.},
year = {2015//},
pages = {6328 - 35},
address = {Piscataway, NJ, USA},
abstract = {Life-long visual localization is one of the most challenging topics in robotics over the last few years. The difficulty of this task is in the strong appearance changes that a place suffers due to dynamic elements, illumination, weather or seasons. In this paper, we propose a novel method (ABLE-M) to cope with the main problems of carrying out a robust visual topological localization along time. The novelty of our approach resides in the description of sequences of monocular images as binary codes, which are extracted from a global LDB descriptor and efficiently matched using FLANN for fast nearest neighbor search. Besides, an illumination invariant technique is applied. The usage of the proposed binary description and matching method provides a reduction of memory and computational costs, which is necessary for long-term performance. Our proposal is evaluated in different life-long navigation scenarios, where ABLE-M outperforms some of the main state-of-the-art algorithms, such as WI-SURF, BRIEF-Gist, FAB-MAP or SeqSLAM. Tests are presented for four public datasets where a same route is traversed at different times of day or night, along the months or across all four seasons.},
keywords = {binary codes;image matching;pattern classification;robot vision;},
note = {life-long visual localization;binary sequences;image matching;robotics;ABLE-M;robust visual topological localization;binary codes;FLANN;fast nearest neighbor search;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7140088},
} 


@inproceedings{16503823 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Erasing bad memories: Agent-side summarization for long-term mapping},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Dymczyk, M. and Schneider, T. and Gilitschenski, I. and Siegwart, R. and Stumm, E.},
year = {2016//},
pages = {4572 - 9},
address = {Piscataway, NJ, USA},
abstract = {Precisely estimating the pose of an agent in a global reference frame is a crucial goal that unlocks a multitude of robotic applications, including autonomous navigation and collaboration. In order to achieve this, current state-of-the-art localization approaches collect data provided by one or more agents and create a single, consistent localization map, maintained over time. However, with the introduction of lengthier sorties and the growing size of the environments, data transfers between the backend server where the global map is stored and the agents are becoming prohibitively large. While some existing methods partially address this issue by building compact summary maps, the data transfer from the agents to the backend can still easily become unmanageable. In this paper, we propose a method that is designed to reduce the amount of data that needs to be transferred from the agent to the backend, functioning in large-scale, multi-session mapping scenarios. Our approach is based upon a landmark selection method that exploits information coming from multiple, possibly weak and correlated, landmark utility predictors; fused using learned feature coefficients. Such a selection yields a drastic reduction in data transfer while maintaining localization performance and the ability to efficiently summarize environments over time. We evaluate our approach on a data set that was autonomously collected in a dynamic indoor environment over a period of several months.},
keywords = {data flow computing;multi-agent systems;},
note = {agent-side summarization;long-term mapping;pose estimation;robotic applications;autonomous navigation;collaboration;multiagent data collection;localization map;data transfers;multisession mapping;landmark selection method;learned feature coefficients;data flow reduction;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759673},
} 


@article{15339140 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Superpixel-based appearance change prediction for long-term navigation across seasons},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Neubert, P. and Sunderhauf, N. and Protzel, P.},
volume = { 69},
year = {2015/07/},
pages = {15 - 27},
issn = {0921-8890},
address = {Netherlands},
abstract = {Changing environments pose a serious problem to current robotic systems aiming at long term operation under varying seasons or local weather conditions. This paper is built on our previous work where we propose to learn to <i>predict</i> the changes in an environment. Our key insight is that the occurring scene changes are in part systematic, repeatable and therefore predictable. The goal of our work is to support existing approaches to place recognition by learning how the visual appearance of an environment changes over time and by using this learned knowledge to predict its appearance under different environmental conditions. We describe the general idea of appearance change prediction (ACP) and investigate properties of our novel implementation based on vocabularies of superpixels (SP-ACP). Our previous work showed that the proposed approach significantly improves the performance of SeqSLAM and BRIEF-Gist for place recognition on a subset of the Nordland dataset under extremely different environmental conditions in summer and winter. This paper deepens the understanding of the proposed SP-ACP system and evaluates the influence of its parameters. We present the results of a large-scale experiment on the complete 10 h Nordland dataset and appearance change predictions between different combinations of seasons. [All rights reserved Elsevier].},
keywords = {image recognition;knowledge based systems;mobile robots;navigation;robot vision;SLAM (robots);},
note = {superpixel-based appearance change prediction;long-term navigation;robotic systems;weather conditions;place recognition;visual appearance;learned knowledge;SP-ACP;SeqSLAM;BRIEF-Gist;Nordland dataset;},
URL = {http://dx.doi.org/10.1016/j.robot.2014.08.005},
} 


@inproceedings{15667018 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Multi-robot 6D graph SLAM connecting decoupled local reference filters},
journal = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Schuster, M.J. and Brand, C. and Hirschmuller, H. and Suppa, M. and Beetz, M.},
year = {2015//},
pages = {5093 - 100},
address = {Piscataway, NJ, USA},
abstract = {Teams of mobile robots can be deployed in search and rescue missions to explore previously unknown environments. Methods for joint localization and mapping constitute the basis for (semi-)autonomous cooperative action, in particular when navigating in GPS-denied areas. As communication losses may occur, a decentralized solution is required. With these challenges in mind, we designed a submap-based SLAM system that relies on inertial measurements and stereo-vision to create multi-robot dense 3D maps. For online pose and map estimation, we integrate the results of keyframe-based local reference filters through incremental graph SLAM. To the best of our knowledge, we are the first to combine these two methods to benefit from their particular advantages for 6D multi-robot localization and mapping: Local reference filters on each robot provide real-time, long-term stable state estimates that are required for stabilization, control and fast obstacle avoidance, whereas online graph optimization provides global multi-robot pose and map estimates needed for cooperative planning. We propose a novel graph topology for a decoupled integration of local filter estimates from multiple robots into a SLAM graph according to the filters' uncertainty estimates and independence assumptions and evaluated its benefits on two different robots in indoor, outdoor and mixed scenarios. Further, we performed two extended experiments in a multi-robot setup to evaluate the full SLAM system, including visual robot detections and submap matches as inter-robot loop closure constraints.},
keywords = {collision avoidance;graph theory;image filtering;multi-robot systems;optimisation;pose estimation;rescue robots;robot vision;SLAM (robots);stereo image processing;},
note = {SLAM graph;visual robot detections;interrobot loop closure constraints;local filter;decoupled integration;graph topology;cooperative planning;global multirobot pose estimation;online graph optimization;obstacle avoidance;6D multirobot localization and mapping;incremental graph SLAM;keyframe-based local reference filters;map estimation;online pose estimation;multirobot dense 3D maps;stereo-vision;inertial measurements;submap-based SLAM system;decentralized solution;semiautonomous cooperative action;joint mapping;joint localization;search and rescue missions;mobile robot teams;decoupled local reference filters;multirobot 6D graph SLAM;},
URL = {http://dx.doi.org/10.1109/IROS.2015.7354094},
} 


@inproceedings{14002387 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Long-term simultaneous localization and mapping with generic linear constraint node removal},
journal = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2013)},
author = {Carlevaris-Bianco, N. and Eustice, R.M.},
year = {2013//},
pages = {1034 - 41},
address = {Piscataway, NJ, USA},
abstract = {This paper reports on the use of generic linear constraint (GLC) node removal as a method to control the computational complexity of long-term simultaneous localization and mapping. We experimentally demonstrate that GLC provides a principled and flexible tool enabling a wide variety of complexity management schemes. Specifically, we consider two main classes: batch multi-session node removal, in which nodes are removed in a batch operation between mapping sessions, and online node removal, in which nodes are removed as the robot operates. Results are shown for 34.9 h of real-world indoor-outdoor data covering 147.4 km collected over 27 mapping sessions spanning a period of 15 months.},
keywords = {graph theory;SLAM (robots);},
note = {long-term simultaneous localization and mapping;generic linear constraint node removal;GLC node removal;complexity management scheme;batch multi-session node removal;},
URL = {http://dx.doi.org/10.1109/IROS.2013.6696478},
} 


@inproceedings{15798792 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Bathymetric Factor Graph SLAM with Sparse Point Cloud Alignment},
journal = {OCEANS 2015 - MTS/IEEE Washington},
author = {Bichucher, V. and Walls, J.M. and Ozog, P. and Skinner, K.A. and Eustice, R.M.},
year = {2015//},
pages = {732 - 8},
address = {Piscataway, NJ, USA},
abstract = {This paper reports on a factor graph simultaneous localization and mapping framework for autonomous underwater vehicle localization based on terrain-aided navigation. The method requires no prior bathymetric map and only assumes that the autonomous underwater vehicle has the ability to sparsely sense the local water column depth, such as with a bottom-looking Doppler velocity log. Since dead-reckoned navigation is accurate in short time windows, the vehicle accumulates several water column depth point clouds- or submaps-during the course of its survey. We propose an xy-alignment procedure between these submaps in order to enforce consistent bathymetric structure over time, and therefore attempt to bound long-term navigation drift. We evaluate the submap alignment method in simulation and present performance results from multiple autonomous underwater vehicle field trials.},
keywords = {autonomous underwater vehicles;bathymetry;marine navigation;SLAM (robots);terrain mapping;},
note = {bathymetric factor graph SLAM;sparse point cloud alignment;simultaneous localization and mapping framework;terrain-aided navigation;autonomous underwater vehicle localization;bathymetric map;local water column depth sparse sensing;dead-reckoned navigation;short time windows;xy-alignment procedure;long-term navigation drift;consistent bathymetric structure;submap alignment method;multiple autonomous underwater vehicle field trial;},
} 


@article{16594137 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Opportunistic sampling-based active visual SLAM for underwater inspection},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Chaves, S.M. and Kim, A. and Galceran, E. and Eustice, R.M.},
volume = { 40},
number = { 7},
year = {2016/10/},
pages = {1245 - 65},
issn = {0929-5593},
address = {Germany},
abstract = {This paper reports on an active SLAM framework for performing large-scale inspections with an underwater robot. We propose a path planning algorithm integrated with visual SLAM that plans loop-closure paths in order to decrease navigation uncertainty. While loop-closing revisit actions bound the robot's uncertainty, they also lead to redundant area coverage and increased path length. Our proposed opportunistic framework leverages sampling-based techniques and information filtering to plan revisit paths that are coverage efficient. We employ Gaussian process regression for modeling the prediction of camera registrations and use a two-step optimization procedure for selecting revisit actions. We show that the proposed method offers many benefits over existing solutions and good performance for bounding navigation uncertainty in long-term autonomous operations with hybrid simulation experiments and real-world field trials performed by an underwater inspection robot.},
keywords = {Gaussian processes;inspection;mobile robots;path planning;regression analysis;robot vision;sampling methods;SLAM (robots);underwater vehicles;},
note = {hybrid simulation experiments;autonomous operations;bounding navigation uncertainty;optimization procedure;camera registrations;Gaussian process regression;information filtering;sampling-based techniques;loop-closure paths;path planning algorithm;large-scale inspections;underwater inspection robot;active visual SLAM;opportunistic sampling;},
URL = {http://dx.doi.org/10.1007/s10514-016-9597-6},
} 


@inproceedings{13488746 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {A framework for RF-visual SLAM},
journal = {2013 10th International Bhurban Conference on Applied Sciences and Technology (IBCAST 2013). Proceedings},
author = {Anwar, S. and Qingjie Zhao and Qadeer, N. and Khan, S.I.},
year = {2013//},
pages = {103 - 8},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous Localization and Mapping, SLAM, is an important topic in the field of robotics and autonomous navigation. The metric SLAM suffers from sensor inaccuracies and thus cannot be used for long-term navigation. In such case, Visual SLAM or a Hybrid SLAM based on both metric and visual approach is a good alternative. In this paper, in order to speed up a Visual SLAM, we propose a novel concept of dynamic dictionary generated on the results of triangulation done on RF, radio frequency, signals from nearest cell towers of a cellular network. This dynamic dictionary efficiently manages the scalability of a Visual SLAM and make it possible to work in a large-scale environment. A framework is proposed along with triangulation data of a city and with simulations to support the concept.},
keywords = {cellular radio;mobile robots;navigation;path planning;SLAM (robots);},
note = {RF-visual SLAM;simultaneous localization and mapping;robotics;autonomous navigation;metric SLAM;long-term navigation;hybrid SLAM;dynamic dictionary;radio frequency;cellular network;large-scale environment;triangulation data;},
URL = {http://dx.doi.org/10.1109/IBCAST.2013.6512139},
} 


@inproceedings{10801924 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {Towards a robust visual SLAM approach: addressing the challenge of life-long operation},
journal = {2009 14th International Conference on Advanced Robotics (ICAR 2009)},
author = {Hochdorfer, S. and Schlegel, C.},
year = {2009//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Localization and mapping are fundamental problems in service robotics. Knowledge about the own pose and representations of the environment are needed for a series of high level applications. Service robots should be designed for life-long and robust operation in dynamic environments. The contribution of this paper is twofold. First, an approach to address the ever growing number of landmarks in life-long operation is presented. Typically, SLAM approaches just accumulate features over time and do not discard them anymore. Therefore, the required resources in terms of memory and processing power are growing over time. In our approach, the absolute number of landmarks can be restricted by an upper bound since we introduce a method to specifically select and replace landmarks once the upper bound has been reached. The second contribution is related to improving the robustness of the landmark assignment problem in case of image based features as needed with natural landmarks. The approach has been successfully evaluated in a real world experiment on a Pioneer-3DX platform within a complex unmodified indoor environment.},
keywords = {mobile robots;robot vision;robust control;service robots;SLAM (robots);},
note = {robust visual SLAM approach;life-long operation;service robotics;dynamic environment;landmark assignment problem;robustness;image-based feature;Pioneer-3DX platform;complex unmodified indoor environment;},
} 


@inproceedings{15285972 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {The gist of maps - summarizing experience for lifelong localization},
journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
author = {Dymczyk, M. and Lynen, S. and Cieslewski, T. and Bosse, M. and Siegwart, R. and Furgale, P.},
year = {2015//},
pages = {2767 - 73},
address = {Piscataway, NJ, USA},
abstract = {Robust, scalable place recognition is a core competency for many robotic applications. However, when revisiting places over and over, many state-of-the-art approaches exhibit reduced performance in terms of computation and memory complexity and in terms of accuracy. For successful deployment of robots over long time scales, we must develop algorithms that get better with repeated visits to the same environment, while still working within a fixed computational budget. This paper presents and evaluates an algorithm that alternates between online place recognition and offline map maintenance with the goal of producing the best performance with a fixed map size. At the core of the algorithm is the concept of a Summary Map, a reduced map representation that includes only the landmarks that are deemed most useful for place recognition. To assign landmarks to the map, we use a scoring function that ranks the utility of each landmark and a sampling policy that selects the landmarks for each place. The Summary Map can then be used by any descriptor-based inference method for constant-complexity online place recognition. We evaluate a number of scoring functions and sampling policies and show that it is possible to build and maintain maps of a constant size and that place-recognition performance improves over multiple visits.},
keywords = {cartography;image recognition;image sampling;},
note = {online place recognition;offline map maintenance;summary map;reduced map representation;scoring function;sampling policy;vision-based localization;visual navigation;lifelong localization;},
URL = {http://dx.doi.org/10.1109/ICRA.2015.7139575},
} 


@article{16518911 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age},
journal = {IEEE Transactions on Robotics},
journal = {IEEE Trans. Robot. (USA)},
author = {Cadena, C. and Carlone, L. and Carrillo, H. and Latif, Y. and Scaramuzza, D. and Neira, J. and Reid, I. and Leonard, J.J.},
volume = { 32},
number = { 6},
year = {2016/12/},
pages = {1309 - 32},
issn = {1552-3098},
address = {USA},
abstract = {Simultaneous localization and mapping (SLAM) consists in the concurrent construction of a model of the environment (the map), and the estimation of the state of the robot moving within it. The SLAM community has made astonishing progress over the last 30 years, enabling large-scale real-world applications and witnessing a steady transition of this technology to industry. We survey the current state of SLAM and consider future directions. We start by presenting what is now the de-facto standard formulation for SLAM. We then review related work, covering a broad set of topics including robustness and scalability in long-term mapping, metric and semantic representations for mapping, theoretical performance guarantees, active SLAM and exploration, and other new frontiers. This paper simultaneously serves as a position paper and tutorial to those who are users of SLAM. By looking at the published research with a critical eye, we delineate open challenges and new research issues, that still deserve careful scientific investigation. The paper also contains the authors' take on two questions that often animate discussions during robotics conferences: Do robots need SLAM? and Is SLAM solved?},
keywords = {SLAM (robots);},
note = {simultaneous-localization-and-mapping;SLAM community;long-term mapping;semantic representations;SLAM users;critical eye;},
URL = {http://dx.doi.org/10.1109/TRO.2016.2624754},
} 


@inproceedings{18976340 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Robust Loop-Closure Detection with a Learned Illumination Invariant Representation for Robot vSLAM},
journal = {2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM)},
author = {Shilang Chen and Junjun Wu and Yanran Wang and Lin Zhou and Qinghua Lu and Yunzhi Zhang},
year = {2019//},
pages = {342 - 7},
address = {Piscataway, NJ, USA},
abstract = {Robust loop-closure detection plays a key role for the long-term robot visual Simultaneous Localization and Mapping (SLAM) in indoor or outdoor environment, due to illumination changes can greatly affect the accuracy of online image matching, and keypoints may fail to match between images taken at the same location but different seasons. In this paper, we propose a robust loop-closure detection method for robot visual SLAM, which adopts invariant representation as image descriptors composed of learned features and adapts to changes in illumination and seasons. We evaluate our method on real datasets and demonstrate its excellent ability to handle illumination changes.},
keywords = {feature extraction;image matching;image representation;robot vision;robust control;SLAM (robots);},
note = {learned illumination invariant representation;illumination changes;online image matching;robust loop-closure detection method;robot visual SLAM;robot vSLAM;long-term robot visual simultaneous localization;},
URL = {http://dx.doi.org/10.1109/ICARM.2019.8833730},
} 


@article{15644674 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Learning place-dependant features for long-term vision-based localisation},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {McManus, C. and Upcroft, B. and Newman, P.},
volume = { 39},
number = { 3},
year = {2015/10/},
pages = {363 - 87},
issn = {0929-5593},
address = {Germany},
abstract = {This paper presents an alternative approach to the problem of outdoor, persistent visual localisation against a known map. Instead of blindly applying a feature detector/descriptor combination over all images of all places, we leverage prior experiences of a place to learn place-dependent feature detectors (i.e., features that are unique to each place in our map and used for localisation). Furthermore, as these features do not represent low-level structure, like edges or corners, but are in fact mid-level patches representing distinctive visual elements (e.g., windows, buildings, or silhouettes), we are able to localise across extreme appearance changes. Note that there is no requirement that the features posses semantic meaning, only that they are optimal for the task of localisation. This work is an extension on previous work (McManus et al. in Proceedings of robotics science and systems, 2014b) in the following ways: (i) we have included a landmark refinement and outlier rejection step during the learning phase, (ii) we have implemented an asynchronous pipeline design, (iii) we have tested on data collected in an urban environment, and (iv) we have implemented a purely monocular system. Using over 100 km worth of data for training, we present localisation results from Begbroke Science Park and central Oxford.},
keywords = {computer vision;feature extraction;geographic information systems;learning (artificial intelligence);},
note = {Oxford;Begbroke Science Park;urban environment;asynchronous pipeline design;outlier rejection;landmark refinement;place-dependant feature learning;outdoor localisation;monocular system;semantic meaning;feature detection;long-term vision-based localisation;},
URL = {http://dx.doi.org/10.1007/s10514-015-9463-y},
} 


@inproceedings{14617151 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Episodic non-Markov localization: Reasoning about short-term and long-term features},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Biswas, J. and Veloso, M.},
year = {2014//},
pages = {3969 - 74},
address = {Piscataway, NJ, USA},
abstract = {Markov localization and its variants are widely used for localization of mobile robots. These methods assume Markov independence of observations, implying that observations made by a robot correspond to a static map. However, in real human environments, observations include occlusions due to unmapped objects like chairs and tables, and dynamic objects like humans. We introduce an episodic non-Markov localization algorithm that maintains estimates of the belief over the trajectory of the robot while explicitly reasoning about observations and their correlations arising from unmapped static objects, moving objects, as well as objects from the static map. Observations are classified as arising from long-term features, short-term features, or dynamic features, which correspond to mapped objects, unmapped static objects, and unmapped dynamic objects respectively. By detecting time steps along the robot's trajectory where unmapped observations prior to such time steps are unrelated to those afterwards, non-Markov localization limits the history of observations and pose estimates to &ldquo;episodes&rdquo; over which the belief is computed. We demonstrate non-Markov localization in challenging real world indoor and outdoor environments over multiple datasets, comparing it with alternative state-of-the-art approaches, showing it to be robust as well as accurate.},
keywords = {inference mechanisms;Markov processes;mobile robots;trajectory control;},
note = {episodic nonMarkov localization;short-term features;long-term features;mobile robots;Markov observation independence;static map;robot trajectory;unmapped static objects;static map;mapped objects;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907435},
} 


@article{14807139 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Generic Node Removal for Factor-Graph SLAM},
journal = {IEEE Transactions on Robotics},
journal = {IEEE Trans. Robot. (USA)},
author = {Carlevaris-Bianco, N. and Kaess, M. and Eustice, R.M.},
volume = { 30},
number = { 6},
year = {2014/12/},
pages = {1371 - 85},
issn = {1552-3098},
address = {USA},
abstract = {This paper reports on a generic factor-based method for node removal in factor-graph simultaneous localization and mapping (SLAM), which we call generic linear constraints (GLCs). The need for a generic node removal tool is motivated by long-term SLAM applications, whereby nodes are removed in order to control the computational cost of graph optimization. GLC is able to produce a new set of linearized factors over the elimination clique that can represent either the true marginalization (i.e., dense GLC) or a sparse approximation of the true marginalization using a ChowLiu tree (i.e., sparse GLC). The proposed algorithm improves upon commonly used methods in two key ways: First, it is not limited to graphs with strictly full-state relative-pose factors and works equally well with other low-rank factors, such as those produced by monocular vision. Second, the new factors are produced in such a way that accounts for measurement correlation, which is a problem encountered in other methods that rely strictly upon pairwise measurement composition. We evaluate the proposed method over multiple real-world SLAM graphs and show that it outperforms other recently proposed methods in terms of Kullback-Leibler divergence. Additionally, we experimentally demonstrate that the proposed GLC method provides a principled and flexible tool to control the computational complexity of long-term graph SLAM, with results shown for 34.9 h of real-world indoor-outdoor data covering 147.4 km collected over 27 mapping sessions spanning a period of 15 months.},
keywords = {computational complexity;graph theory;mobile robots;SLAM (robots);},
note = {generic node removal;factor-graph SLAM;generic factor-based method;simultaneous localization and mapping;generic linear constraints;monocular vision;Kullback-Leibler divergence;GLC method;computational complexity;},
URL = {http://dx.doi.org/10.1109/TRO.2014.2347571},
} 


@article{21054716 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Loop-Closure Detection With a Multiresolution Point Cloud Histogram Mode in Lidar Odometry and Mapping for Intelligent Vehicles},
journal = {IEEE/ASME Transactions on Mechatronics},
journal = {IEEE/ASME Trans. Mechatron. (USA)},
author = {Qingyu Meng and Hongyan Guo and Xiaoming Zhao and Dongpu Cao and Hong Chen},
volume = { 26},
number = { 3},
year = {2021//},
pages = {1307 - 17},
issn = {1083-4435},
address = {USA},
abstract = {Precise positioning is the basic condition for intelligent vehicles to complete perception, decision making and control tasks. In response to this challenge, in this article, lidar simultaneous localization and mapping (SLAM) is taken as the research object, and a SLAM system is designed that integrates motion compensation and ground information removal functions, and can construct a real-time environment map and determine its own position on the map while the vehicle is driving. A loop-closure detection method with a multiresolution point cloud histogram mode is proposed, which can effectively detect whether the vehicle passes through the same position and perform optimization to obtain globally consistent pose and map information in the urban conditions with more driving loops. We conduct experiments on the well-known KITTI dataset and compare the results with those of state-of-the-art systems. The experiments confirm that the lidar SLAM system designed in this article can provide accurate and effective positioning information for intelligent vehicles. The proposed loop-closure detection algorithm has an excellent real-time performance and accuracy, which can guarantee the long-term driving operation of these vehicles.},
keywords = {distance measurement;mobile robots;motion compensation;object detection;optical radar;position control;robot vision;SLAM (robots);},
note = {control tasks;lidar simultaneous localization;integrates motion compensation;ground information removal functions;real-time environment map;loop-closure detection method;multiresolution point cloud histogram mode;map information;urban conditions;driving loops;lidar SLAM system;accurate positioning information;effective positioning information;intelligent vehicles;loop-closure detection algorithm;lidar odometry;precise positioning;basic condition;decision making;},
URL = {http://dx.doi.org/10.1109/TMECH.2021.3062647},
} 


@article{20520244 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {AUV Bathymetric Simultaneous Localisation and Mapping Using Graph Method},
journal = {Journal of Navigation},
journal = {J. Navig. (UK)},
author = {Teng Ma and Ye Li and Yusen Gong and Rupeng Wang and Mingwei Sheng and Qiang Zhang},
volume = { 72},
number = { 6},
year = {2019/11/},
pages = {1602 - 22},
issn = {0373-4633},
address = {UK},
abstract = {Although topographic mapping missions and geological surveys carried out by Autonomous Underwater Vehicles (AUVs) are becoming increasingly prevalent, the lack of precise navigation in these scenarios still limits their application. This paper deals with the problems of long-term underwater navigation for AUVs and provides new mapping techniques by developing a Bathymetric Simultaneous Localisation And Mapping (BSLAM) method based on graph SLAM technology. To considerably reduce the calculation cost, the trajectory of the AUV is divided into various submaps based on Differences of Normals (DoN). Loop closures between submaps are obtained by terrain matching; meanwhile, maximum likelihood terrain estimation is also introduced to build weak data association within the submap. Assisted by one weight voting method for loop closures, the global and local trajectory corrections work together to provide an accurate navigation solution for AUVs with weak data association and inaccurate loop closures. The viability, accuracy and real-time performance of the proposed algorithm are verified with data collected onboard, including an 8 km planned track recorded at a speed of 4 knots in Qingdao, China.},
keywords = {autonomous underwater vehicles;Global Positioning System;mobile robots;navigation;robot vision;SLAM (robots);terrain mapping;},
note = {global trajectory corrections;weight voting method;weak data association;maximum likelihood terrain estimation;submap;graph SLAM technology;mapping techniques;long-term underwater navigation;autonomous underwater vehicles;geological surveys;topographic mapping missions;graph method;terrain matching;DoN;BSLAM;differences of normals;AUV bathymetric simultaneous localisation and mapping;navigation solution;local trajectory corrections;},
URL = {http://dx.doi.org/10.1017/S0373463319000286},
} 


@article{21360732 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Autonomous Vehicle Localization with Prior Visual Point Cloud Map Constraints in GNSS-Challenged Environments},
journal = {Remote Sensing},
journal = {Remote Sens. (Switzerland)},
author = {Xiaohu Lin and Fuhong Wang and Bisheng Yang and Wanwei Zhang},
volume = { 13},
number = { 3},
year = {2021//},
pages = {506 (18 pp.) - },
issn = {2072-4292},
address = {Switzerland},
abstract = {Accurate vehicle ego-localization is key for autonomous vehicles to complete high-level navigation tasks. The state-of-the-art localization methods adopt visual and light detection and ranging (LiDAR) simultaneous localization and mapping (SLAM) to estimate the position of the vehicle. However, both of them may suffer from error accumulation due to long-term running without loop optimization or prior constraints. Actually, the vehicle cannot always return to the revisited location, which will cause errors to accumulate in Global Navigation Satellite System (GNSS)-challenged environments. To solve this problem, we proposed a novel localization method with prior dense visual point cloud map constraints generated by a stereo camera. Firstly, the semi-global-block-matching (SGBM) algorithm is adopted to estimate the visual point cloud of each frame and stereo visual odometry is used to provide the initial position for the current visual point cloud. Secondly, multiple filtering and adaptive prior map segmentation are performed on the prior dense visual point cloud map for fast matching and localization. Then, the current visual point cloud is matched with the candidate sub-map by normal distribution transformation (NDT). Finally, the matching result is used to update pose prediction based on the last frame for accurate localization. Comprehensive experiments were undertaken to validate the proposed method, showing that the root mean square errors (RMSEs) of translation and rotation are less than 5.59 m and 0.08&deg;, respectively.},
keywords = {cameras;distance measurement;image matching;image reconstruction;mobile robots;object detection;optical radar;robot vision;satellite navigation;SLAM (robots);stereo image processing;traffic engineering computing;},
note = {autonomous vehicle localization;prior visual point cloud map constraints;GNSS-challenged environments;accurate vehicle ego-localization;autonomous vehicles;high-level navigation tasks;state-of-the-art localization methods;error accumulation;long-term running;prior constraints;Global Navigation Satellite System-challenged environments;novel localization method;prior dense visual point cloud map constraints;semiglobal-block-matching algorithm;stereo visual odometry;current visual point cloud;adaptive prior map segmentation;fast matching;candidate sub-map;size 5.59 m;},
URL = {http://dx.doi.org/10.3390/rs13030506},
} 


@article{20885804 ,
language = {Chinese},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Simultaneous localization and tracking algorithm utilizing FastSLAM framework for autonomous underwater vehicles},
journal = {Control Theory &amp; Applications},
journal = {Control Theory Appl. (China)},
author = {Lu Jian and Chen Xu and Liu Tong and Ma Cheng-xian and He Jin-xin},
volume = { 37},
number = { 1},
year = {2020/01/},
pages = {89 - 97},
issn = {1000-8152},
address = {China},
abstract = {The cooperative localization is an important research question in the field of Tr-Co Robots study. The scheme of the cooperative localization algorithm depends on the ability of information interaction between the robots. To solve the problem that the cooperative localization accuracy is obviously reduced when the communication is interrupted for a long time between the autonomous underwater vehicles (AUV), the simultaneous localization and tracking (SLAT) algorithms based on the FastSLAM framework are developed in this research, borrowing the principle of the simultaneous localization and mapping (SLAM) algorithms. The master AUV is regarded as a non-cooperative target and a motion estimator used to track the master AUV is built in the slaver AUV. When the motion state of the master AUV is estimated, the improvement of the self localization accuracy of the slaver AUV is achieved, using the relative measurement information obtained from the sonar sensor on the slaver AUV in real time. The simulation experimental results show that the proposed SLATFI.0 and 2.0 algorithms can effectively reduce the localization errors compared to the conventional dead reckoning method under the condition of long-term communication interruption, and the 2.0 algorithm has better adaptability to the influence of the detection accuracy variety.},
keywords = {autonomous underwater vehicles;mobile robots;motion estimation;path planning;remotely operated vehicles;SLAM (robots);underwater vehicles;},
note = {tracking algorithm;FastSLAM framework;autonomous underwater vehicles;important research question;localization algorithm;mapping algorithms;master AUV;slaver AUV;self localization accuracy;localization errors;long-term communication interruption;},
URL = {http://dx.doi.org/10.7641/CTA.2019.80747},
} 


@inproceedings{14718027 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Online global loop closure detection for large-scale multi-session graph-based SLAM},
journal = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014)},
author = {Labbe, M. and Michaud, F.},
year = {2014//},
pages = {2661 - 6},
address = {Piscataway, NJ, USA},
abstract = {For large-scale and long-term simultaneous localization and mapping (SLAM), a robot has to deal with unknown initial positioning caused by either the kidnapped robot problem or multi-session mapping. This paper addresses these problems by tying the SLAM system with a global loop closure detection approach, which intrinsically handles these situations. However, online processing for global loop closure detection approaches is generally influenced by the size of the environment. The proposed graph-based SLAM system uses a memory management approach that only consider portions of the map to satisfy online processing requirements. The approach is tested and demonstrated using five indoor mapping sessions of a building using a robot equipped with a laser rangefinder and a Kinect.},
keywords = {graph theory;laser ranging;mobile robots;position control;robot vision;SLAM (robots);},
note = {Kinect;laser rangefinder;indoor mapping sessions;online processing requirements;memory management approach;multisession mapping;kidnapped robot problem;initial positioning;simultaneous localization and mapping;large-scale multisession graph-based SLAM;online global loop closure detection;},
URL = {http://dx.doi.org/10.1109/IROS.2014.6942926},
} 


@inproceedings{20076245 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Self Localization Based On Neighborhood Probability Mapping for Humanoid Robot},
journal = {2020 4th International Conference on Vocational Education and Training (ICOVET)},
author = {Jiono, M. and Mahandi, Y.D. and Norma Mustika, S. and Sendari, S. and Dzikri, A.M.},
year = {2020//},
pages = {355 - 9},
address = {Piscataway, NJ, USA},
abstract = {The humanoid robot competition is an autonomous robot with a human-like body platform with a single camera as a vision sensor and balancing sensor to support them to play soccer in the specific field. The technical challenges in this competition such following the ball, running during search the ball, dynamic walking, kicking while maintaining the balance body condition, decision making with other robot, localization and mapping as research issues investigated in the Humanoid competition. Localization and mapping still big challenges in humanoid competition, it was only single camera is used in competition rule and no others sensor to support the position and orientation during playing the game. The proposed system was developed is neighborhood probability mapping. The long-term goal of this research is to realize an ideal system to accelerate the redesign field condition and implementation process in a humanoid robot that can be monitored in real-time. The aim of this research is to take the opportunities: (a) increasing the robot's performance of vision and intelligence on the humanoid robot; (b) with this SLAM method the robot can distinguish between the balls that are in the field and outside the field; (c) able to distinguish the enemy goal from the goal itself based on goal detection and line detection; (d) the goal keeper robot capable of acting as an attacker and scanning the kick towards the enemy goal. The testing condition was implemented between simulation testing and real testing in same times. Based on the data experimental result, the robot can estimate their position and orientation during searching the ball position, goal position and obstacle coordinate with high real time accuracy. The result shows that the proposed system can be applied to the humanoid soccer robot in the real time directly and it worked with less error.},
keywords = {decision making;humanoid robots;legged locomotion;path planning;robot vision;SLAM (robots);},
note = {neighborhood probability mapping;humanoid robot competition;autonomous robot;single camera;vision sensor;balancing sensor;balance body condition;humanoid competition;goal detection;line detection;robot capable;ball position;goal position;humanoid soccer robot;SLAM method;},
URL = {http://dx.doi.org/10.1109/ICOVET50258.2020.9230237},
} 


@inproceedings{14161665 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Part-based SLAM for partially changing environments},
journal = {2013 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
author = {Yuuto, C. and Kanji, T. and Masatoshi, A.},
year = {2013//},
pages = {1629 - 34},
address = {Piscataway, NJ, USA},
abstract = {We consider the task of long-term visual SLAM, i.e., simultaneous localization and mapping, in a partially changing environment (SLAM-PCE). The main problem we face is how to obtain discriminative and compact visual landmarks, which are necessary to cope with changes in appearance in an environment and with a large amount of visual information. We address this issue by proposing the use of common object patterns, which are inherent in typical environments (e.g., indoor, street, forests, suburban, etc.), as visual landmarks for a SLAM-PCE task. In our contributions, we describe our approach, &ldquo;part-based SLAM&rdquo;, and validate its effectiveness within a standard problem of view image retrieval. The main novelty of this approach lies in that the common landmark objects are extracted in an unsupervised manner via common pattern discovery, and can be used for compact characterization and efficient retrieval of view images. Our method is also innovative in its use of traditional bounding box-based part annotation: an image is represented in a compact form, &ldquo;bag-of-bounding-boxes (BoBB)&rdquo; and then, the scene matching can be solved efficiently as a low dimensional problem of matching bounding boxes. The results of challenging experiments show that it is possible to have high retrieval performance with compact image representation with only 16 words per image.},
keywords = {feature extraction;image matching;image representation;image retrieval;robot vision;SLAM (robots);},
note = {part-based SLAM;partially changing environments;long-term visual SLAM;simultaneous localization and mapping;SLAM-PCE;discriminative visual landmark;compact visual landmark;common object patterns;visual landmarks;view image retrieval;common landmark object extraction;compact characterization;bounding box-based part annotation;bag-of-bounding-boxes;BoBB;scene matching;bounding box matching;compact image representation;},
URL = {http://dx.doi.org/10.1109/ROBIO.2013.6739700},
} 


@inproceedings{13266145 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Scale-preserving long-term visual odometry for indoor navigation},
journal = {2012 International Conference on Indoor Positioning and Indoor Navigation (IPIN 2012)},
author = {Hilsenbeck, S. and Moller, A. and Huitl, R. and Schroth, G. and Kranz, M. and Steinbach, E.},
year = {2012//},
pages = {10 pp. - },
address = {Piscataway, NJ, USA},
abstract = {We present a visual odometry system for indoor navigation with a focus on long-term robustness and consistency. As our work is targeting mobile phones, we employ monocular SLAM to jointly estimate a local map and the device's trajectory. We specifically address the problem of estimating the scale factor of both, the map and the trajectory. State-of-the-art solutions approach this problem with an Extended Kalman Filter (EKF), which estimates the scale by fusing inertial and visual data, but strongly relies on good initialization and takes time to converge. Each visual tracking failure introduces a new arbitrary scale factor, forcing the filter to re-converge. We propose a fast and robust method for scale initialization that exploits basic geometric properties of the learned local map. Using random projections, we efficiently compute geometric properties from the feature point cloud produced by the visual SLAM system. From these properties (e.g., corridor width or height) we estimate scale changes caused by tracking failures and update the EKF accordingly. As a result, previously achieved convergence is preserved despite re-initializations of the map. To minimize the time required to continue tracking after failure, we perform recovery and re-initialization in parallel. This increases the time available for recovery and hence the likelihood for success, thus allowing almost seamless tracking. Moreover, fewer re-initializations are necessary. We evaluate our approach using extensive and diverse indoor datasets. Results demonstrate that errors and convergence times for scale estimation are considerably reduced, thus ensuring consistent and accurate scale estimation. This enables long-term odometry despite of tracking failures which are inevitable in realistic scenarios.},
keywords = {distance measurement;indoor radio;Kalman filters;mobile radio;navigation;nonlinear filters;SLAM (robots);},
note = {scale-preserving long-term visual odometry;indoor navigation;visual odometry system;mobile phones;monocular SLAM;local map estimation;extended Kalman filter;EKF;visual data;visual tracking failure;random projections;feature point cloud;visual SLAM system;scale estimation;},
URL = {http://dx.doi.org/10.1109/IPIN.2012.6418934},
} 


@inproceedings{14023826 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Appearance change prediction for long-term navigation across seasons},
journal = {2013 European Conference on Mobile Robots. Proceedings},
author = {Neubert, P. and Sunderhauf, N. and Protzel, P.},
year = {2013//},
pages = {198 - 203},
address = {Piscataway, NJ, USA},
abstract = {Changing environments pose a serious problem to current robotic systems aiming at long term operation. While place recognition systems perform reasonably well in static or low-dynamic environments, severe appearance changes that occur between day and night, between different seasons or different local weather conditions remain a challenge. In this paper we propose to learn to predict the changes in an environment. Our key insight is that the occurring appearance changes are in part systematic, repeatable and therefore predictable. The goal of our work is to support existing approaches to place recognition by learning how the visual appearance of an environment changes over time and by using this learned knowledge to predict its appearance under different environmental conditions. We describe the general idea of appearance change prediction (ACP) and a novel implementation based on vocabularies of superpixels (SP-ACP). Despite its simplicity, we can further show that the proposed approach can improve the performance of SeqSLAM and BRIEF-Gist for place recognition on a large-scale dataset that traverses an environment under extremely different conditions in winter and summer.},
keywords = {mobile robots;object recognition;path planning;robot vision;SLAM (robots);},
note = {appearance change prediction;long-term navigation;seasons;changing environments;robotic systems;place recognition;SP-ACP;superpixel vocabulary;SeqSLAM;BRIEF-Gist;},
URL = {http://dx.doi.org/10.1109/ECMR.2013.6698842},
} 


@article{18732636 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Distributed stereo vision-based 6D localization and mapping for multi-robot teams},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Schuster, M.J. and Schmid, K. and Brand, C. and Beetz, M.},
volume = { 36},
number = { 2},
year = {2019/03/},
pages = {305 - 32},
issn = {1556-4959},
address = {USA},
abstract = {Joint simultaneous localization and mapping (SLAM) constitutes the basis for cooperative action in multi-robot teams. We designed a stereo vision-based 6D SLAM system combining local and global methods to benefit from their particular advantages: (1) Decoupled local reference filters on each robot for real-time, long-term stable state estimation required for stabilization, control and fast obstacle avoidance; (2) Online graph optimization with a novel graph topology and intra- as well as inter-robot loop closures through an improved submap matching method to provide global multi-robot pose and map estimates; (3) Distribution of the processing of high-frequency and high-bandwidth measurements enabling the exchange of aggregated and thus compacted map data. As a result, we gain robustness with respect to communication losses between robots. We evaluated our improved map matcher on simulated and real-world datasets and present our full system in five real-world multi-robot experiments in areas of up 3,000 m<sup>2</sup> (bounding box), including visual robot detections and submap matches as loop-closure constraints. Further, we demonstrate its application to autonomous multi-robot exploration in a challenging rough-terrain environment at a Moon-analogue site located on a volcano.},
keywords = {collision avoidance;graph theory;mobile robots;multi-robot systems;path planning;robot vision;SLAM (robots);state estimation;stereo image processing;},
note = {autonomous multirobot exploration;visual robot detections;real-world multirobot experiments;improved map matcher;robots;thus compacted map data;aggregated compacted map data;high-bandwidth measurements;(3) Distribution;map estimates;inter-robot loop closures;intra;novel graph topology;graph optimization;fast obstacle avoidance;control;long-term stable state estimation;(1) Decoupled local reference filters;global methods;local methods;stereo vision-based 6D;SLAM;multirobot teams;mapping;},
URL = {http://dx.doi.org/10.1002/rob.21812},
} 


@inproceedings{21430784 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Development of an Autonomous Robotic System Using the Graph-based SPLAM Algorithm},
journal = {2021 International Conference on Information Technology and Nanotechnology (ITNT)},
author = {Kozlov, D. and Myasnikov, V.},
year = {2021//},
pages = {5 pp. - },
address = {Piscataway, NJ, USA},
abstract = {For long-term planning, localization and mapping, the robot must constantly update the map by the changing environment and new areas that the robot is exploring. At the same time, this map should not take up too much of the robot's memory, since the robot's performance is limited due to the small size of the robot and increased performance requirements. The robot must interact with the map on time, updating its location to build a further route to explore areas that have not been visited. In addition to compiling a map, when solving the problem of exploration rooms, the following steps are also important: forming a plan for bypassing an unknown room, calculating the trajectory, resolving collisions with obstacles, and following the trajectory. In the course of this work, an autonomous robotic system was developed, the task of which is to map previously unknown premises. For this, SPLAM algorithms, algorithms for building map and working with graphs, algorithms for following a trajectory were used.},
keywords = {graph theory;mobile robots;path planning;SLAM (robots);},
note = {long-term planning;localization;changing environment;increased performance requirements;exploration rooms;autonomous robotic system;SPLAM algorithms;building map;graph-based SPLAM algorithm;},
URL = {http://dx.doi.org/10.1109/ITNT52450.2021.9649028},
} 


@inproceedings{13851181 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Switchable constraints vs. max-mixture models vs. RRR - A comparison of three approaches to robust pose graph SLAM},
journal = {2013 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Sunderhauf, N. and Protzel, P.},
year = {2013//},
pages = {5198 - 203},
address = {Piscataway, NJ, USA},
abstract = {SLAM algorithms that can infer a correct map despite the presence of outliers have recently attracted increasing attention. In the context of SLAM, outlier constraints are typically caused by a failed place recognition due to perceptional aliasing. If not handled correctly, they can have catastrophic effects on the inferred map. Since robust robotic mapping and SLAM are among the key requirements for autonomous long-term operation, inference methods that can cope with such data association failures are a hot topic in current research. Our paper compares three very recently published approaches to robust pose graph SLAM, namely switchable constraints, max-mixture models and the RRR algorithm. All three methods were developed as extensions to existing factor graph-based SLAM back-ends and aim at improving the overall system's robustness to false positive loop closure constraints. Due to the novelty of the three proposed algorithms, no direct comparison has been conducted so far.},
keywords = {graph theory;inference mechanisms;SLAM (robots);},
note = {switchable constraints;max-mixture models;RRR;realizing-reversing-recovering algorithm;robust pose graph SLAM algorithm;simultaneous localization and mapping;outlier constraints;place recognition;perceptional aliasing;robust robotic mapping;inference methods;data association failures;factor graph-based SLAM back-ends;false positive loop closure constraints;},
URL = {http://dx.doi.org/10.1109/ICRA.2013.6631320},
} 


@inproceedings{13850971 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Temporally scalable visual SLAM using a reduced pose graph},
journal = {2013 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Johannsson, H. and Kaess, M. and Fallon, M. and Leonard, J.J.},
year = {2013//},
pages = {54 - 61},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we demonstrate a system for temporally scalable visual SLAM using a reduced pose graph representation. Unlike previous visual SLAM approaches that maintain static keyframes, our approach uses new measurements to continually improve the map, yet achieves efficiency by avoiding adding redundant frames and not using marginalization to reduce the graph. To evaluate our approach, we present results using an online binocular visual SLAM system that uses place recognition for both robustness and multi-session operation. Additionally, to enable large-scale indoor mapping, our system automatically detects elevator rides based on accelerometer data. We demonstrate long-term mapping in a large multi-floor building, using approximately nine hours of data collected over the course of six months. Our results illustrate the capability of our visual SLAM system to map a large are over extended period of time.},
keywords = {graph theory;pose estimation;SLAM (robots);},
note = {temporally scalable visual SLAM;reduced pose graph;pose graph representation;online binocular visual SLAM system;indoor mapping;accelerometer data;},
URL = {http://dx.doi.org/10.1109/ICRA.2013.6630556},
} 


@inproceedings{17413794 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {A continuously learning feature-based map using a bernoulli filtering approach},
journal = {2017 Sensor Data Fusion: Trends, Solutions, Applications (SDF)},
author = {Stubler, M. and Reuter, S. and Dietmayer, K.},
year = {2017//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {One of the huge challenges of map-based localization is a rapidly changing environment. The present contribution addresses this problem by first constructing a new framework for feature-based long-term mapping using a Bernoulli filter. This framework is then applied to construct a continuously learning map. It is based on Simultaneous Localization and Mapping (SLAM) to create a short-term map which provides a momentary image of the environment during one mapping run. The proposed fusion algorithm then estimates the landmark map on a long-term basis by incorporating those short-term maps. Landmarks in the long-term map that reach a negligible spatial uncertainty can then be used again as a prior for the short-term mapping process. Since a Bernoulli filter can only handle a single object, independent groups of landmarks are constructed where only those with exactly one landmark are updated. As a result, landmarks that are part of the long-term map are quite distinct. By incorporating additional but probably outdated a priori information, the proposed method is able to restrict the inevitable error propagation of SLAM algorithms. The long-term mapping process is further distributable to several agents: every agent simultaneously localizes itself while it generates a new snapshot that is fused into the long-term map afterwards. An evaluation using real-world data completes this contribution.},
keywords = {filtering theory;learning (artificial intelligence);mobile robots;sensor fusion;SLAM (robots);},
note = {long-term mapping process;Bernoulli filter;continuously learning map;mapping run;landmark map;long-term basis;short-term mapping process;SLAM algorithms;},
URL = {http://dx.doi.org/10.1109/SDF.2017.8126353},
} 


@article{14728293 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Robust loop closing over time for pose graph SLAM},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Latif, Y. and Cadena, C. and Neira, J.},
volume = { 32},
number = { 14},
year = {2013/12/},
pages = {1611 - 26},
issn = {0278-3649},
address = {USA},
abstract = {Long-term autonomous mobile robot operation requires considering place recognition decisions with great caution. A single incorrect decision that is not detected and reconsidered can corrupt the environment model that the robot is trying to build and maintain. This work describes a consensus-based approach to robust place recognition over time, that takes into account all the available information to detect and remove past incorrect loop closures. The main novelties of our work are: (1) the ability of realizing that, in light of new evidence, an incorrect past loop closing decision has been made; the incorrect information can be removed thus recovering the correct estimation with a novel algorithm; (2) extending our proposal to incremental operation; and (3) handling multi-session, spatially related or unrelated scenarios in a unified manner. We demonstrate our proposal, the RRR algorithm, on different odometry systems, e.g. visual or laser, using different front-end loop-closing techniques. For our experiments we use the efficient graph optimization framework g2o as back-end. We back our claims up with several experiments carried out on real data, in single and multi-session experiments showing better results than those obtained by state-of-the-art methods, comparisons against whom are also presented.},
keywords = {graph theory;mobile robots;optimisation;SLAM (robots);},
note = {pose graph SLAM;long term autonomous mobile robot operation;place recognition decisions;single incorrect decision;correct estimation;RRR algorithm;odometry systems;different front end loop closing techniques;graph optimization framework;},
URL = {http://dx.doi.org/10.1177/0278364913498910},
} 


@inproceedings{20500853 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Localizing backscatters by a single robot with zero start-up cost},
journal = {2019 IEEE Global Communications Conference (GLOBECOM)},
author = {Shengkai Zhang and Wei Wang and Sheyang Tang and Shi Jin and Tao Jiang},
year = {2019//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Recent years have witnessed the rapid proliferation of low- power backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such low-power backscatter tags is crucial for IoT-based smart services. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, increasing the deployment cost. To empower universal localization service, this paper presents Rover, an indoor localization system that simultaneously localizes multiple backscatter tags with zero start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing WiFi-based positioning measurements with inertial measurements to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues such as the interference among multiple tags and the real- time processing for solving the SLAM problem. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords = {backscatter;indoor radio;Internet of Things;radionavigation;SLAM (robots);wireless LAN;},
note = {low- power backscatter technologies;long-term connectivity;smart cities;smart homes;low-power backscatter tags;IoT-based smart services;current backscatter localization systems;deployment cost;universal localization service;indoor localization system;multiple backscatter tags;zero start-up cost;inertial sensors;joint optimization framework;WiFi-based positioning measurements;inertial measurements;connected tags;prototype Rover;localization accuracies;SLAM problem;WiFi chips;},
URL = {http://dx.doi.org/10.1109/GLOBECOM38437.2019.9013768},
} 


@article{19387209 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Relocalization With Submaps: Multi-Session Mapping for Planetary Rovers Equipped With Stereo Cameras},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Giubilato, R. and Vayugundla, M. and Schuster, M.J. and Stuumlrzl, W. and Wedler, A. and Triebel, R. and Debei, S.},
volume = { 5},
number = { 2},
year = {2020/04/},
pages = {580 - 7},
issn = {2377-3774},
address = {USA},
abstract = {To enable long term exploration of extreme environments such as planetary surfaces, heterogeneous robotic teams need the ability to localize themselves on previously built maps. While the Localization and Mapping problem for single sessions can be efficiently solved with many state of the art solutions, place recognition in natural environments still poses great challenges for the perception system of a robotic agent. In this paper we propose a relocalization pipeline which exploits both 3D and visual information from stereo cameras to detect matches across local point clouds of multiple SLAM sessions. Our solution is based on a Bag of Binary Words scheme where binarized SHOT descriptors are enriched with visual cues to recall in a fast and efficient way previously visited places. The proposed relocalization scheme is validated on challenging datasets captured using a planetary rover prototype on Mount Etna, designated as a Moon analogue environment.},
keywords = {graph theory;mobile robots;multi-robot systems;path planning;planetary rovers;robot vision;SLAM (robots);},
note = {Binary Words scheme;binarized SHOT descriptors;visual cues;relocalization scheme;planetary rover prototype;Moon analogue environment;submaps;multisession Mapping;planetary rovers equipped;stereo cameras;long term exploration;extreme environments;planetary surfaces;heterogeneous robotic teams;built maps;single sessions;art solutions;place recognition;natural environments;perception system;robotic agent;relocalization pipeline;visual information;local point clouds;multiple SLAM sessions;},
URL = {http://dx.doi.org/10.1109/LRA.2020.2964157},
} 


@article{19847162 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Lidar-based system for high-precision localization and real-time 3D map construction},
journal = {Journal of Applied Remote Sensing},
journal = {J. Appl. Remote Sens. (USA)},
author = {Guofeng Tong and Yong Li and Yuanyuan Li and Fan Gao and Lihao Cao},
volume = { 14},
number = { 2},
year = {2020/04/},
pages = {020501 (8 pp.) - },
issn = {1931-3195},
address = {USA},
abstract = {Simultaneous localization and mapping (SLAM) is considered as the core of building high-precision three-dimensional environmental maps. For this, we proposed a high-precision laser-based SLAM system. Aiming at the problem that the point cloud data obtained from laser is not representative and the iterative closest point method for calculating pose transformation is time-consuming, we proposed an efficient and stable matching algorithm. It uses fused feature points to align with the occupancy grid submaps with less registration error and is less time-consuming. Then, to address the problem in which the registration result falls into the local optimum early, we proposed a quadratic registration algorithm. This method effectively improves the initial value of the registration process. Finally, a time consistency and global consistency loop detection algorithm are used to reduce the cumulative error. The system we proposed has been tested on the University of Michigan North Campus Long-Term dataset and Cartographer dataset. Experiments show that our system has good accuracy under low-speed motion conditions (speeds ranging from 1 to 2 m / s).},
keywords = {cartography;image matching;image registration;iterative methods;optical radar;quadratic programming;SLAM (robots);},
note = {Michigan North Campus University;fused feature point;registration error;occupancy grid submaps;stable matching algorithm;calculating pose transformation;iterative closest point method;point cloud data;high-precision laser-based SLAM system;high-precision three-dimensional environmental maps;high-precision localization;lidar-based system;loop detection algorithm;registration process;quadratic registration algorithm;},
URL = {http://dx.doi.org/10.1117/1.JRS.14.020501},
} 


@article{14744624 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Lifelong localization in changing environments},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Tipaldi, G.D. and Meyer-Delius, D. and Burgard, W.},
volume = { 32},
number = { 14},
year = {2013/12/},
pages = {1662 - 78},
issn = {0278-3649},
address = {UK},
abstract = {Robot localization systems typically assume that the environment is static, ignoring the dynamics inherent in most realworld settings. Corresponding scenarios include households, offices, warehouses and parking lots, where the location of certain objects such as goods, furniture or cars can change over time. These changes typically lead to inconsistent observations with respect to previously learned maps and thus decrease the localization accuracy or even prevent the robot from globally localizing itself. In this paper we present a soundprobabilistic approach to lifelong localization in changing environments using a combination ofa Rao-Blackwellizedparticlefilter with a hidden Markov model. By exploiting several properties of this model, we obtain a highly efficient map management approach for dynamic environments, which makes it feasible to run our algorithm online. Extensive experiments with a real robot in a dynamically changing environment demonstrate that our algorithm reliably adapts to changes in the environment and also outperforms the popular MonteCarlo localization approach.},
keywords = {mobile robots;Monte Carlo methods;particle filtering (numerical methods);},
note = {lifelong localization;changing environments;robot localization systems;realworld settings;learned maps;sound probabilistic approach;Rao-Blackwellized particle filter;hidden Markov model;dynamic environments;MonteCarlo localization approach;mobile robots;},
URL = {http://dx.doi.org/10.1177/0278364913502830},
} 


@article{14744625 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Localization and navigation of the CoBots over long-term deployments},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Biswas, J. and Veloso, M.M.},
volume = { 32},
number = { 14},
year = {2013/12/},
pages = {1679 - 94},
issn = {0278-3649},
address = {UK},
abstract = {For the last three years, we have developed and researched multiple collaborative robots, CoBots, which have been autonomously traversing our multi-floor buildings. We pursue the goal of long-term autonomy for indoor service mobile robots as the ability for them to be deployed indefinitely while they perform tasks in an evolving environment. The CoBots include several levels of autonomy, and in this paper we focus on their localization and navigation algorithms. We present the Corrective Gradient Refinement (CGR) algorithm, which refines the proposal distribution of the particle filter used for localization with sensor observations using analytically computed state space derivatives on a vector map. We also present the Fast Sampling Plane Filtering algorithm that extracts planar regions from depth images in real time. These planar regions are then projected onto the 2D vector map of the building, and along with the laser rangefinder observations, used with CGR for localization. For navigation, we present a hierarchical planner, which computes a topological policy using a graph representation of the environment, computes motion commands based on the topological policy, and then modifies the motion commands to side-step perceived obstacles. We started logging the deployments of the CoBots one and a halfyears ago, and have since collected logs of the CoBots traversing more than 130 km over 1082 deployments and a total run time of 182 h, which we publish as a dataset consisting of more than 10 million laser scans. The logs show that although there have been continuous changes in the environment, the robots are robust to most of them, and there exist only afew locations where changes in the environment cause increased uncertainty in localization.},
keywords = {mobile robots;multi-robot systems;navigation;particle filtering (numerical methods);path planning;service robots;topology;},
note = {motion commands;graph representation;topological policy;hierarchical planner;laser rangefinder observations;2D-vector map;fast sampling plane filtering algorithm;analytically computed state space derivatives;sensor observations;particle filter;CGR algorithm;corrective gradient refinement algorithm;navigation algorithms;localization algorithms;indoor service mobile robots;multifloor buildings;multiple collaborative robots;long-term deployments;},
URL = {http://dx.doi.org/10.1177/0278364913503892},
} 


@article{13538816 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Appearance-Based Loop Closure Detection for Online Large-Scale and Long-Term Operation},
journal = {IEEE Transactions on Robotics},
journal = {IEEE Trans. Robot. (USA)},
author = {Labbe, M. and Michaud, F.},
volume = { 29},
number = { 3},
year = {2013/06/},
pages = {734 - 45},
issn = {1552-3098},
address = {USA},
abstract = {In appearance-based localization and mapping, loop-closure detection is the process used to determinate if the current observation comes from a previously visited location or a new one. As the size of the internal map increases, so does the time required to compare new observations with all stored locations, eventually limiting online processing. This paper presents an online loop-closure detection approach for large-scale and long-term operation. The approach is based on a memory management method, which limits the number of locations used for loop-closure detection so that the computation time remains under real-time constraints. The idea consists of keeping the most recent and frequently observed locations in a working memory (WM) that is used for loop-closure detection, and transferring the others into a long-term memory (LTM). When a match is found between the current location and one stored in WM, associated locations that are stored in LTM can be updated and remembered for additional loop-closure detections. Results demonstrate the approach's adaptability and scalability using ten standard datasets from other appearance-based loop-closure approaches, one custom dataset using real images taken over a 2-km loop of our university campus, and one custom dataset (7 h) using virtual images from the racing video game &ldquo;Need for Speed: Most Wanted&rdquo;.},
keywords = {computer games;control engineering computing;Internet;mobile robots;robot vision;storage management;},
note = {Need for Speed: Most Wanted;racing video game;virtual images;LTM;long-term memory;WM;working memory;real-time constraints;memory management method;appearance-based localization;online long-term operation;online large-scale operation;appearance-based loop closure detection;},
URL = {http://dx.doi.org/10.1109/TRO.2013.2242375},
} 


@article{20399324 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Deep Samplable Observation Model for Global Localization and Kidnapping},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Runjian Chen and Huan Yin and Yanmei Jiao and Dissanayake, G. and Yue Wang and Rong Xiong},
volume = { 6},
number = { 2},
year = {2021/04/},
pages = {2296 - 303},
issn = {2377-3766},
address = {USA},
abstract = {Global localization and kidnapping are two challenging problems in robot localization. The popular method, Monte Carlo Localization (MCL) addresses the problem by iteratively updating a set of particles with a &ldquo;sampling-weighting&rdquo; loop. Sampling is decisive to the performance of MCL [1]. However, traditional MCL can only sample from a uniform distribution over the state space. Although variants of MCL propose different sampling models, they fail to provide an accurate distribution or generalize across scenes. To better deal with these problems, we present a distribution proposal model named Deep Samplable Observation Model (DSOM). DSOM takes a map and a 2D laser scan as inputs and outputs a conditional multimodal probability distribution of the pose, making the samples more focusing on the regions with higher likelihood. With such samples, the convergence is expected to be more effective and efficient. Considering that the learning-based sampling model may fail to capture the accurate pose sometimes, we furthermore propose the Adaptive Mixture MCL (AdaM MCL), which deploys a trusty mechanism to adaptively select updating mode for each particle to tolerate this situation. Equipped with DSOM, AdaM MCL can achieve more accurate estimation, faster convergence and better scalability than previous methods in both synthetic and real scenes. Even in real environments with long-term changes, AdaM MCL is able to localize the robot using DSOM trained only by simulation observations from a SLAM map or a blueprint map. Source code for this paper is available here: https://github.com/Runjian-Chen/AdaM_MCL.},
keywords = {learning (artificial intelligence);learning systems;mobile robots;Monte Carlo methods;probability;SLAM (robots);},
note = {robot localization;sampling-weighting loop;uniform distribution;accurate distribution;distribution proposal model;DSOM;2D laser scan;conditional multimodal probability distribution;learning-based sampling;AdaM MCL;deep samplable observation model;global localization;adaptive mixture MCL;Monte Carlo localization;kidnapping;SLAM map;blueprint map;source code;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3061339},
} 


@article{15062845 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {A SLAM based on auxiliary marginalised particle filter and differential evolution},
journal = {International Journal of Systems Science},
journal = {Int. J. Syst. Sci. (UK)},
author = {Havangi, R. and Nekoui, M.A. and Teshnehlab, M. and Taghirad, H.D.},
volume = { 45},
number = { 9},
year = {2014//},
pages = {1913 - 26},
issn = {0020-7721},
address = {UK},
abstract = {FastSLAM is a framework for simultaneous localisation and mapping (SLAM) using a Rao-Blackwellised particle filter. In FastSLAM, particle filter is used for the robot pose (position and orientation) estimation, and parametric filter (i.e. EKF and UKF) is used for the feature location's estimation. However, in the long term, FastSLAM is an inconsistent algorithm. In this paper, a new approach to SLAM based on hybrid auxiliary marginalised particle filter and differential evolution (DE) is proposed. In the proposed algorithm, the robot pose is estimated based on auxiliary marginal particle filter that operates directly on the marginal distribution, and hence avoids performing importance sampling on a space of growing dimension. In addition, static map is considered as a set of parameters that are learned using DE. Compared to other algorithms, the proposed algorithm can improve consistency for longer time periods and also, improve the estimation accuracy. Simulations and experimental results indicate that the proposed algorithm is effective.},
keywords = {evolutionary computation;Kalman filters;mobile robots;nonlinear filters;particle filtering (numerical methods);pose estimation;robot vision;SLAM (robots);},
note = {simultaneous localisation-and-mapping;hybrid auxiliary marginalised particle filter;differential evolution;Rao-Blackwellised particle filter;robot pose estimation;position estimation;orientation estimation;parametric filter;EKF;UKF;feature location estimation;marginal distribution;static map;FastSLAM;},
URL = {http://dx.doi.org/10.1080/00207721.2012.759299},
} 


@inproceedings{17210977 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Dynamic map update of non-static facility logistics environment with a multi-robot system},
journal = {KI 2017: Advances in Artificial Intelligence. 40th Annual German Conference on AI. Proceedings: LNAI 10505},
author = {Shaik, N. and Liebig, T. and Kirsch, C. and Muller, H.},
year = {2017//},
pages = {249 - 61},
address = {Cham, Switzerland},
abstract = {Autonomous robots need to perceive and represent their environments and act accordingly. Using simultaneous localization and mapping (SLAM) methods, robots can build maps of the environment which are efficient for localization and path planning as long as the environment remains unchanged. However, facility logistics environments are not static because pallets and other obstacles are stored temporarily. This paper proposes a novel solution for updating maps of changing environments (i.e. environments with low-dynamic or semi-static objects) in real-time with multiple robots. Each robot is equipped with a laser range sensor and runs localization to estimate its position. Each robot senses the change in the environment with respect to a current map, initially built with a SLAM method, and constructs a temporary map which will be merged into the current map using localization information and line features of the map. This procedure enables the creation of long-term mapping robot systems for facility logistics.},
keywords = {cartography;estimation theory;laser ranging;logistics;multi-robot systems;path planning;sensors;SLAM (robots);},
note = {dynamic map update;nonstatic facility logistics environment;mult-robot system;autonomous robots;simultaneous localization and mapping;SLAM methods;map building;path planning;facility logistics environments;multiple robots;laser range sensor;position estimation;long-term mapping robot systems;},
URL = {http://dx.doi.org/10.1007/978-3-319-67190-1_19},
} 


@inproceedings{11010225 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Landmark rating and selection according to localization coverage: addressing the challenge of lifelong operation of SLAM in service robots},
journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2009)},
author = {Hochdorfer, S. and Schlegel, C.},
year = {2009//},
pages = {382 - 7},
address = {Piscataway, NJ, USA},
abstract = {Acting in everyday-life environments is still a great challenge in service robotics. Although algorithms and solutions already exist for many relevant subproblems, in particular the aspect of robustness and suitability for everyday use has been neglected so far very often. Robustness and suitability for everyday use are features affecting not only the overall system design but have impact on each single algorithm of each component.},
keywords = {service robots;},
note = {landmark rating;landmark selection;localization coverage;simultaneous localization and mapping;service robotics;P3DX-platform;},
URL = {http://dx.doi.org/10.1109/IROS.2009.5354433},
} 


@inproceedings{12318120 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {CD SLAM - Continuous localization and mapping in a dynamic world},
journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
author = {Pirker, K. and Ruther, M. and Bischof, H.},
year = {2011//},
pages = {3990 - 7},
address = {Piscataway, NJ, USA},
abstract = {When performing large-scale perpetual localization and mapping one faces problems like memory consumption or repetitive and dynamic scene elements requiring robust data association. We propose a visual SLAM method which handles short- and long-term scene dynamics in large environments using a single camera only. Through visibility-dependent map filtering and efficient keyframe organization we reach a considerable performance gain only through incorporation of a slightly more complex map representation. Experiments on a large, mixed indoor/outdoor dataset over a time period of two weeks demonstrate the scalability and robustness of our approach.},
keywords = {filtering theory;mobile robots;robot dynamics;robot vision;sensor fusion;SLAM (robots);},
note = {CD SLAM;continuous localization and mapping;perpetual localization and mapping;memory consumption;repetitive scene elements;dynamic scene elements;robust data association;visual SLAM method;visibility-dependent map filtering;keyframe organization;complex map representation;mobile robot;},
URL = {http://dx.doi.org/10.1109/IROS.2011.6048253},
} 


@article{21542474 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Planning on topological map using omnidirectional images and spherical CNNs},
journal = {Advanced Robotics},
journal = {Adv. Robot. (UK)},
author = {Taniguchi, A. and Sasaki, F. and Muroi, M. and Yamashina, R.},
volume = { 36},
number = { 3},
year = {2022//},
pages = {153 - 66},
issn = {1568-5535},
address = {UK},
abstract = {Map building and path planning are essential for long-term visual navigation. Methods, such as semi-parametric topological memory (SPTM), use deep learning to build a topological map consisting of nodes sampled from an agent's past observations and edges generated by a neural network (NN) and perform path planning on the map. Such methods require neither accurate sensor nor advanced expertise for map building needed in classical metric-map-based techniques such as visual simultaneous localization and mapping. However, they often plan improper paths including teleportations and detours, even if we have collected enough training data. In this paper, we propose a topological-map-based planning method that includes two modifications to SPTM to address this problem. The first modification is that we observe omnidirectional images and construct an NN on the basis of spherical convolutional NNs, which guarantee rotation invariance on omnidirectional images. The second modification is that we train the NN on a metric learning framework. We conducted experiments to show the effectiveness and applicability to real world of our method for path planning in visual navigation. The results indicate that the first modification prevents detouring and the second one prevents teleportations in path planning.},
keywords = {learning (artificial intelligence);mobile robots;neural nets;path planning;robot vision;SLAM (robots);topology;},
note = {map building;path planning;long-term visual navigation;semiparametric topological memory;SPTM;topological map;NN;accurate sensor nor advanced expertise;classical metric-map-based techniques;visual simultaneous localization;improper paths;topological-map-based planning;omnidirectional images;},
URL = {http://dx.doi.org/10.1080/01691864.2021.1997641},
} 


@article{13945211 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {OpenRatSLAM: an open source brain-based SLAM system},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Netherlands)},
author = {Ball, D. and Heath, S. and Wiles, J. and Wyeth, G. and Corke, P. and Milford, M.},
volume = { 34},
number = { 3},
year = {2013/04/},
pages = {149 - 76},
issn = {0929-5593},
address = {Netherlands},
abstract = {RatSLAM is a navigation system based on the neural processes underlying navigation in the rodent brain, capable of operating with low resolution monocular image data. Seminal experiments using RatSLAM include mapping an entire suburb with a web camera and a long term robot delivery trial. This paper describes OpenRatSLAM, an open-source version of RatSLAM with bindings to the Robot Operating System framework to leverage advantages such as robot and sensor abstraction, networking, data playback, and visualization. OpenRatSLAM comprises connected ROS nodes to represent RatSLAM's pose cells, experience map, and local view cells, as well as a fourth node that provides visual odometry estimates. The nodes are described with reference to the RatSLAM model and salient details of the ROS implementation such as topics, messages, parameters, class diagrams, sequence diagrams, and parameter tuning strategies. The performance of the system is demonstrated on three publicly available open-source datasets.},
keywords = {control engineering computing;data visualisation;distance measurement;image sensors;mobile robots;operating systems (computers);path planning;public domain software;robot vision;SLAM (robots);},
note = {OpenRatSLAM;open source brain-based SLAM system;navigation system;neural processes;rodent brain;low resolution monocular image data;web camera;long term robot delivery trial;robot operating system framework;robot abstraction;sensor abstraction;networking;data playback;data visualization;experience map;local view cells;pose cells;visual odometry estimation;messages;parameters;class diagrams;sequence diagrams;parameter tuning strategies;open-source datasets;appearance-based simultaneous localization-and-mapping systems;},
URL = {http://dx.doi.org/10.1007/s10514-012-9317-9},
} 


@inproceedings{14616713 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Long-term exploration  tours for energy constrained robots with online proprioceptive traversability estimation},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Martin, S. and Corke, P.},
year = {2014//},
pages = {5778 - 85},
address = {Piscataway, NJ, USA},
abstract = {This paper is concerned with how a localised and energy-constrained robot can maximise its time in the field by taking paths and tours that minimise its energy expenditure. A significant component of a robot's energy is expended on mobility and is a function of terrain traversability. We estimate traversability online from data sensed by the robot as it moves, and use this to generate maps, explore and ultimately converge on minimum energy tours of the environment. We provide results of detailed simulations and parameter studies that show the efficacy of this approach for a robot moving over terrain with unknown traversability as well as a number of a priori unknown hard obstacles. We also present preliminary experimental results to show the feasibility of this approach in natural terrain.},
keywords = {energy conservation;mobile robots;path planning;},
note = {energy constrained robots;online proprioceptive traversability estimation;localised robot;energy expenditure;robot energy;terrain traversability function;traversability estimation;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907708},
} 


@inproceedings{21257451 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation},
journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Thomas, H. and Agro, B. and Gridseth, M. and Zhang, J. and Barfoot, T.D.},
year = {2021//},
pages = {14047 - 14053},
address = {Piscataway, NJ, USA},
abstract = {We present a self-supervised learning approach for the semantic segmentation of lidar frames. Our method is used to train a deep point cloud segmentation architecture without any human annotation. The annotation process is automated with the combination of simultaneous localization and mapping (SLAM) and ray-tracing algorithms. By performing multiple navigation sessions in the same environment, we are able to identify permanent structures, such as walls, and disentangle short-term and long-term movable objects, such as people and tables, respectively. New sessions can then be performed using a network trained to predict these semantic labels. We demonstrate the ability of our approach to improve itself over time, from one session to the next. With semantically filtered point clouds, our robot can navigate through more complex scenarios, which, when added to the training pool, help to improve our network predictions. We provide insights into our network predictions and show that our approach can also improve the performances of common localization techniques.},
keywords = {image segmentation;indoor communication;learning (artificial intelligence);mobile robots;optical radar;path planning;robot vision;SLAM (robots);},
note = {training pool;network predictions;common localization techniques;lidar segmentation;autonomous indoor navigation;self-supervised learning approach;semantic segmentation;lidar frames;deep point cloud segmentation architecture;human annotation;annotation process;SLAM;ray-tracing algorithms;multiple navigation sessions;permanent structures;disentangle short-term;long-term movable objects;new sessions;semantic labels;session;semantically filtered point clouds;},
URL = {http://dx.doi.org/10.1109/ICRA48506.2021.9561701},
} 


@article{18517446 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {AUV robust bathymetric simultaneous localization and mapping},
journal = {Ocean Engineering},
journal = {Ocean Eng. (Netherlands)},
author = {Teng Ma and Ye Li and Rupeng Wang and Zheng Cong and Yusen Gong},
volume = { 166},
year = {2018/10/15},
pages = {336 - 49},
issn = {0029-8018},
address = {Netherlands},
abstract = {Bathymetric simultaneous localization and mapping (BSLAM) technique could provide long-term underwater navigation results for autonomous underwater vehicles (AUVs) and produce a self-consistent bathymetric map simultaneously. However, the inter-frame motion inside BSLAM is still difficult to estimate, and BSLAM might fail catastrophically with invalid loop closures caused by the measurement errors of vehicle states and bathymetric data. To deal with these problems, an AUV robust BSLAM algorithm is proposed based on graph SLAM. In this algorithm, weak data association is constructed via sparse pseudo-input Gaussian process (SPGP) regression to predict inter-frame motion, and a multi-window consistency method (MCM) is introduced to identify invalid loop closures. Various simulation experiments are conducted under different environments. Comparisons are made between more standard approaches, and our proposed algorithm is shown to be viable, accurate, and could robustly handle invalid loop closures. [All rights reserved Elsevier].},
keywords = {autonomous underwater vehicles;filtering theory;Gaussian processes;navigation;SLAM (robots);terrain mapping;},
note = {multiwindow consistency method;sparse pseudoinput Gaussian process regression;weak data association;AUV robust BSLAM algorithm;bathymetric data;vehicle states;measurement errors;inter-frame motion;self-consistent bathymetric map;autonomous underwater vehicles;long-term underwater navigation results;AUV robust bathymetric simultaneous localization;invalid loop closures;},
URL = {http://dx.doi.org/10.1016/j.oceaneng.2018.08.029},
} 


@inproceedings{19299260 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Deep Supervised Hashing with Similar Hierarchy for Place Recognition},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Lang Wu and Yihong Wu},
year = {2019//},
pages = {3781 - 6},
address = {Piscataway, NJ, USA},
abstract = {Place recognition as one of the most significant requirements for long-term simultaneous localization and mapping (SLAM) has been developed rapidly in recent years. Also, deep learning is proved to be more capable than traditional methods to extract features under some complex environments. However, in real-world environments, there are many challenging problems such as viewpoint changes and illumination changes. The existing deep learning-based place recognition in extracting feature phases and matching process is both time-consuming. Moreover, features extracted from convolution neural network (CNN) are floating-point type with high dimension. In this paper, we propose deep supervised hashing for place recognition, where we design a similar hierarchy loss function to learn a model. The model can distinguish the similar images more accurately which is well suitable to place recognition. Besides the model can learn high quality hash codes by maximizing the likelihood of triplet labels. Experiments on several benchmark datasets for place recognition show that our approach is robust to viewpoints, illuminations and season changes with high accuracy. Furthermore, the trained model can extract features and match in real time on CPU with less memory consumption.},
keywords = {convolutional neural nets;feature extraction;image matching;object recognition;robot vision;SLAM (robots);supervised learning;},
note = {deep supervised hashing;long-term simultaneous localization;deep learning;similar hierarchy loss function;high quality hash codes;place recognition;simultaneous localization and mapping;SLAM;convolution neural network;CNN;CPU;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8968599},
} 


@inproceedings{13336904 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Minimalistic vision-based cognitive slam},
journal = {Proceedings of the 4th International Conference on Agents and Artificial Intelligence (ICAART 2012)},
author = {Saleiro, M. and Rodrigues, J.M.F. and du Buf, J.M.H.},
volume = {vol.1},
year = {2012//},
pages = {614 - 23},
address = {Setubal, Portugal},
abstract = {The interest in cognitive robotics is still increasing, a major goal being to create a system which can adapt to dynamic environments and which can learn from its own experiences. We present a new cognitive SLAM architecture, but one which is minimalistic in terms of sensors and memory. It employs only one camera with pan and tilt control and three memories, without additional sensors nor any odometry. Short-teen memory is an egocentric map which holds information at close range at the actual robot position. Long-term memory is used for mapping the environment and registration of encountered objects. Object memory holds features of learned objects which are used as navigation landmarks and task targets. Saliency maps are used to sequentially focus important areas for object and obstacle detection, but also for selecting directions of movements. Reinforcement learning is used to consolidate or enfeeble environmental information in long-term memory. The system is able to achieve complex tasks by executing sequences of visuomotor actions, decisions being taken by goal-detection and goal-completion tasks. Experimental results show that the system is capable of executing tasks like localizing specific objects while building a map, after which it manages to return to the start position even when new obstacles have appeared.},
keywords = {collision avoidance;learning (artificial intelligence);mobile robots;robot vision;SLAM (robots);},
note = {minimalistic vision based cognitive slam;cognitive robotics;dynamic environments;autonomous mobile robots;visuomotor actions;reinforcement learning;object detection;obstacle detection;saliency maps;navigation landmarks;object memory;robot position;egocentric map;short teen memory;odometry;cognitive SLAM architecture;},
} 


@inproceedings{11430980 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {FAB-MAP + RatSLAM: Appearance-based SLAM for multiple times of day},
journal = {2010 IEEE International Conference on Robotics and Automation (ICRA 2010)},
author = {Glover, A.J. and Maddern, W.P. and Milford, M.J. and Wyeth, G.F.},
year = {2010//},
pages = {3507 - 12},
address = {Piscataway, NJ, USA},
abstract = {Appearance-based mapping and localisation is especially challenging when separate processes of mapping and localisation occur at different times of day. The problem is exacerbated in the outdoors where continuous change in sun angle can drastically affect the appearance of a scene. We confront this challenge by fusing the probabilistic local feature based data association method of FAB-MAP with the pose cell filtering and experience mapping of RatSLAM. We evaluate the effectiveness of our amalgamation of methods using five datasets captured throughout the day from a single camera driven through a network of suburban streets. We show further results when the streets are re-visited three weeks later, and draw conclusions on the value of the system for lifelong mapping.},
keywords = {probability;sensor fusion;SLAM (robots);},
note = {FAB-MAP + RatSLAM;appearance-based SLAM;probabilistic local feature;data association method;pose cell filtering;lifelong mapping;},
URL = {http://dx.doi.org/10.1109/ROBOT.2010.5509547},
} 


@article{17320689 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Simultaneous Localization and Mapping: A Survey of Current Trends in Autonomous Driving},
journal = {IEEE Transactions on Intelligent Vehicles},
journal = {IEEE Trans. Intell. Veh. (USA)},
author = {Bresson, G. and Alsayed, Z. and Li Yu and Glaser, S.},
volume = { 2},
number = { 3},
year = {Sept. 2017},
pages = {194 - 220},
issn = {2379-8858},
address = {USA},
abstract = {In this paper, we propose a survey of the Simultaneous Localization And Mapping (SLAM) field when considering the recent evolution of autonomous driving. The growing interest regarding self-driving cars has given new directions to localization and mapping techniques. In this survey, we give an overview of the different branches of SLAM before going into the details of specific trends that are of interest when considered with autonomous applications in mind. We first present the limits of classical approaches for autonomous driving and discuss the criteria that are essential for this kind of application. We then review the methods where the identified challenges are tackled. We mostly focus on approaches building and reusing long-term maps in various conditions (weather, season, etc.). We also go through the emerging domain of multivehicle SLAM and its link with self-driving cars. We survey the different paradigms of that field (centralized and distributed) and the existing solutions. Finally, we conclude by giving an overview of the various large-scale experiments that have been carried out until now and discuss the remaining challenges and future orientations.},
keywords = {automobiles;mobile robots;SLAM (robots);},
note = {autonomous driving;Mapping field;self-driving cars;autonomous applications;multivehicle SLAM;Simultaneous Localization and Mapping;large-scale experiments;},
URL = {http://dx.doi.org/10.1109/TIV.2017.2749181},
} 


@article{18185001 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Distributed and collaborative monocular simultaneous localization and mapping for multi-robot systems in large-scale environments},
journal = {International Journal of Advanced Robotic Systems},
journal = {Int. J. Adv. Robot. Syst. (UK)},
author = {Hui Zhang and Xieyuanli Chen and Huimin Lu and Junhao Xiao},
volume = { 15},
number = { 3},
year = {2018/05/},
pages = {173 - 92},
issn = {1729-8814},
address = {UK},
abstract = {In this article, we propose a distributed and collaborative monocular simultaneous localization and mapping system for the multi-robot system in large-scale environments, where monocular vision is the only exteroceptive sensor. Each robot estimates its pose and reconstructs the environment simultaneously using the same monocular simultaneous localization and mapping algorithm. Meanwhile, they share the results of their incremental maps by streaming keyframes through the robot operating system messages and the wireless network. Subsequently, each robot in the group can obtain the global map with high efficiency. To build the collaborative simultaneous localization and mapping architecture, two novel approaches are proposed. One is a robust relocalization method based on active loop closure, and the other is a vision-based multi-robot relative pose estimating and map merging method. The former is used to solve the problem of tracking failures when robots carry out long-term monocular simultaneous localization and mapping in large-scale environments, while the latter uses the appearance-based place recognition method to determine multi-robot relative poses and build the large-scale global map by merging each robot's local map. Both KITTI data set and our own data set acquired by a handheld camera are used to evaluate the proposed system. Experimental results show that the proposed distributed multi-robot collaborative monocular simultaneous localization and mapping system can be used in both indoor small-scale and outdoor large-scale environments.},
keywords = {cameras;mobile robots;multi-robot systems;pose estimation;robot vision;SLAM (robots);},
note = {local map;large-scale global map;multirobot relative poses;long-term monocular simultaneous localization;robots;map merging method;vision-based multirobot;mapping architecture;collaborative simultaneous localization;robot operating system messages;incremental maps;mapping algorithm;monocular vision;multirobot system;mapping system;collaborative monocular simultaneous localization;large-scale environments;},
URL = {http://dx.doi.org/10.1177/1729881418780178},
} 


@inproceedings{11046421 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Lifelong localization of a mobile service-robot in everyday indoor environments using omnidirectional vision},
journal = {2009 IEEE International Conference on Technologies for Practical Robot Applications. TePRA 2009},
author = {Hochdorfer, S. and Lutz, M. and Schlegel, C.},
year = {2009//},
pages = {161 - 6},
address = {Piscataway, NJ, USA},
abstract = {SLAM (Simultaneous Localization and Mapping) mechanisms are a key component towards advanced service robotics applications. Currently, a major hurdle on the way to lifelong localization is the handling of the ever growing amount of landmarks over time. Therefore, the required resources in terms of memory and processing power are also growing over time.},
keywords = {mobile robots;pattern clustering;service robots;SLAM (robots);},
note = {lifelong localization;mobile service-robot;indoor environments;omnidirectional vision;simultaneous localization and mapping;processing power;memory resources;},
URL = {http://dx.doi.org/10.1109/TEPRA.2009.5339626},
} 


@inproceedings{21407176 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Multi-layer VI-GNSS Global Positioning Framework with Numerical Solution aided MAP Initialization},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Bing Han and Zhongyang Xiao and Shuai Huang and Tao Zhang},
year = {2021//},
pages = {5448 - 55},
address = {Piscataway, NJ, USA},
abstract = {Motivated by the goal of achieving long-term drift-free camera pose estimation in complex scenarios, we propose a global positioning framework fusing visual, inertial and Global Navigation Satellite System (GNSS) measurements in multiple layers. Different from previous loosely- and tightly-coupled methods, the proposed multi-layer fusion allows us to delicately correct the drift of visual odometry and keep reliable positioning while GNSS degrades. In particular, local motion estimation is conducted in the inner-layer, solving the problem of scale drift and inaccurate bias estimation in visual odometry by fusing the velocity of GNSS, pre-integration of Inertial Measurement Unit (IMU) and camera measurement in a tightly-coupled way. The global localization is achieved in the outer-layer, where the local motion is further fused with GNSS position and course in a long-term period in a loosely-coupled way. Furthermore, a dedicated initialization method is proposed to guarantee fast and accurate estimation for all state variables and parameters. We give exhaustive tests of the proposed framework on indoor and outdoor public datasets. The mean localization error is reduced up to 63%, with a promotion of 69% in initialization accuracy compared with state-of-the-art works. We have applied the algorithm to Augmented Reality (AR) navigation, crowd sourcing high-precision map update and other large-scale applications.},
keywords = {augmented reality;cameras;distance measurement;Global Positioning System;image fusion;inertial navigation;Kalman filters;mobile robots;motion estimation;pose estimation;robot vision;satellite navigation;sensor fusion;SLAM (robots);},
note = {long-term drift-free camera;multiple layers;tightly-coupled methods;multilayer fusion;visual odometry;reliable positioning;GNSS degrades;particular motion estimation;local motion estimation;inner-layer;scale drift;inaccurate bias estimation;camera measurement;global localization;outer-layer;long-term period;dedicated initialization method;multilayer VI-GNSS Global positioning framework;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9636871},
} 


@inproceedings{20334114 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Radar-on-Lidar: metric radar localization on prior lidar maps},
journal = {2020 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
author = {Huan Yin and Yue Wang and Li Tang and Rong Xiong},
year = {2020//},
pages = {1 - 7},
address = {Piscataway, NJ, USA},
abstract = {Radar and lidar, provided by two different range sensors, each has pros and cons of various perception tasks on mobile robots or autonomous driving. In this paper, a Monte Carlo system is used to localize the robot with a rotating radar sensor on 2D lidar maps. We first train a conditional generative adversarial network to transfer raw radar data to lidar data, and achieve reliable radar points from generator. Then an efficient radar odometry is included in the Monte Carlo system. Combining the initial guess from odometry, a measurement model is proposed to match the radar data and prior lidar maps for final 2D positioning. We demonstrate the effectiveness of the proposed localization framework on the public multisession dataset. The experimental results show that our system can achieve high accuracy for long-term localization in outdoor scenes.},
keywords = {distance measurement;geophysical image processing;mobile robots;Monte Carlo methods;optical radar;radar detection;radar imaging;radar tracking;remote sensing by laser beam;remote sensing by radar;robot vision;SLAM (robots);},
note = {long-term localization;localization framework;efficient radar odometry;reliable radar points;raw radar data;conditional generative adversarial network;2D lidar maps;rotating radar sensor;Monte Carlo system;autonomous driving;mobile robots;perception tasks;different range sensors;prior lidar maps;metric radar localization;radar-on-Lidar;},
URL = {http://dx.doi.org/10.1109/RCAR49640.2020.9303291},
} 


@inproceedings{21134296 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Using UHF-RFID Signals for Robot Localization Inside Pipelines},
journal = {2021 IEEE 16th Conference on Industrial Electronics and Applications (ICIEA)},
author = {Gunatilake, A. and Galea, M. and Thiyagarajan, K. and Kodagoda, S. and Piyathilaka, L. and Darji, P.},
year = {2021//},
pages = {1109 - 14},
address = {Piscataway, NJ, USA},
abstract = {Underground water pipes are important to any country's infrastructure. Overtime, the metallic pipes are prone to corrosion, which can lead to water leakage and pipe bursts. In order to prolong the service life of those assets, water utilities in Australia apply protective pipe linings. Long-term monitoring and timely intervention are crucial for maintaining those lining assets. However, the water utilities do not possess the comprehensive technology to achieve it. The main reasons for lacking such technology are the unavailability of sensors and accurate robot localization technologies. Feature based localization methods such as SLAM has limited use as the application of liners alters the features and the environment. Encoder based localization is not accurate enough to observe the evolution of defects over a long period of time requiring unique defect correspondence. This motivates us to explore accurate contact-less and wireless based localization methods. We propose a cost-effective localization method using UHF-RFID signals for robot localization inside pipelines based on Gaussian process combined particle filter. Experiments carried out in field extracted pipe samples from the Sydney water pipe network show that using the RSSI and Phase data together in the measurement model with particle filter algorithm improves the localization accuracy up to 15 centimeters precision.},
keywords = {condition monitoring;flaw detection;Gaussian processes;particle filtering (numerical methods);pipelines;radiofrequency identification;SLAM (robots);},
note = {Sydney water pipe network;localization accuracy;pipe samples;Gaussian process combined particle filter;cost-effective localization method;wireless based localization methods;unique defect correspondence;encoder based localization;accurate robot localization technologies;lining assets;long-term monitoring;protective pipe linings;water utilities;service life;pipe bursts;water leakage;metallic pipes;underground water pipes;pipelines;UHF-RFID signals;},
URL = {http://dx.doi.org/10.1109/ICIEA51954.2021.9516284},
} 


@inproceedings{12406627 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {High level assisted control mode based on SLAM for a remotely controlled robot},
journal = {2011 15th International Conference on Advanced Robotics},
author = {Devaux, J.-C. and Nadrag, P. and Colle, E. and Hoppenot, P.},
year = {2011//},
pages = {186 - 91},
address = {Piscataway, NJ, USA},
abstract = {One aim of ambient assistive technologies is to reduce long term hospitalization for elderly people, especially with pathologies such as Mild Cognitive Impairment (MCI). The smart environment assists these people and their families with safety and cognitive stimulation, so they stay as long as possible at home. The originality comes from using the robot in the elderly person's home. This robot is remote controlled by a distant user, a therapist or a relative, for determining alarming situations or for participating in stimulation exercises. Several modes are available for controlling the robot. This paper deals with an assisted control mode in which the remote user gives to the robot one goal and the robot reaches the goal by itself. During the robot movement, the user can dynamically change the current goal. An important hypothesis is that the robot has no a priori knowledge of its environment at the beginning. The knowledge will increase with time and the planned trajectory will be refreshed at two levels: a local one - faster but not always sufficient - and a global one - slower but which always finds a path if one exists. The idea is to work only with local information, using the robot sensors, the operator keeping the high level control. To assure that control, the remote operator uses video feedback and information from a laser range scanner.},
keywords = {geriatrics;handicapped aids;human-robot interaction;mobile robots;motion control;robot dynamics;SLAM (robots);telecontrol;},
note = {high level assisted control mode;SLAM;remotely controlled robot;ambient assistive technologies;long term hospitalization reduction;elderly people;mild cognitive impairment;safety stimulation;cognitive stimulation;alarming situation determination;robot sensors;video feedback;laser range scanner;},
URL = {http://dx.doi.org/10.1109/ICAR.2011.6088615},
} 


@inproceedings{20755586 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Real-Time Robot Localization Based on 2D Lidar Scan-to-Submap Matching},
journal = {China Satellite Navigation Conference (CSNC 2021). Proceedings. Lecture Notes in Electrical Engineering (LNEE 773)},
author = {Qipeng Li and Jianzhu Huai and Dong Chen and Yuan Zhuang},
volume = {vol.II},
year = {2021//},
pages = {414 - 23},
address = {Singapore, Singapore},
abstract = {In this paper, we propose a real-time and low-drift localization method for lidar-equipped robot in indoor environments. State-of-the-art lidar localization research mostly uses a scan-to-scan method, which produces high drifts during the localization of the robot. It is not suitable for robots to operate indoors (such as factory environment) for a long term. Besides, the mapping and localization of this method are susceptible to the dynamic objects (such as pedestrians). To solve above problems, we propose the scan-to-submap matching method for real-time localization. Currently, this method has been used for building maps, and there are few studies to use it for localization, especially for real-time localization. In our research, we build the hardware and software platform for the scan-to-submap matching method. We extensively evaluate our approach with simulations and real-world tests. Compared with the scan-to-scan method, the results demonstrate that our approach can cope with the mapping and localization problem with high localization accuracy and low drift.},
keywords = {mobile robots;optical radar;path planning;robot vision;SLAM (robots);},
note = {time robot localization;lidar scan-to-submap matching;low-drift localization method;lidar-equipped robot;indoor environments;state-of-the-art lidar localization research;scan-to-scan method;high drifts;factory environment;scan-to-submap matching method;real-time localization;high localization accuracy;low drift;},
URL = {http://dx.doi.org/10.1007/978-981-16-3142-9_39},
} 


@inproceedings{20424730 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Pauls, J.-H. and Petek, K. and Poggenhans, F. and Stiller, C.},
year = {2020//},
pages = {4595 - 601},
address = {Piscataway, NJ, USA},
abstract = {Easy, yet robust long-term localization is still an open topic in research. Existing approaches require either dense maps, expensive sensors, specialized map features or proprietary detectors.We propose using semantic segmentation on a monocular camera to localize directly in a HD map as used for automated driving. This combines lightweight, yet powerful HD maps with the simplicity of monocular vision and the flexibility of neural networks.The major challenges arising from this combination are data association and robustness against misdetections. Association is solved efficiently by applying distance transform on binary per-class images. This provides not only a fast lookup table for a smooth gradient as needed for pose-graph optimization, but also dynamic association by default.A sliding-window pose graph optimization combines single image detections with vehicle odometry, smoothing results and helping overcome even misclassifications in consecutive frames.Evaluation against a highly accurate 6D visual localization shows that our approach can achieve accuracy levels as required for automated driving, being one of the most lightweight and flexible methods to do so.},
keywords = {cameras;computer vision;distance measurement;feature extraction;graph theory;image segmentation;image sensors;mobile robots;object detection;optimisation;pose estimation;robot vision;SLAM (robots);table lookup;},
note = {monocular localization;HD map;semantic segmentation;distance transform;long-term localization;open topic;dense maps;expensive sensors;specialized map features;proprietary detectors;monocular camera;automated driving;powerful HD maps;monocular vision;data association;binary per-class images;pose-graph optimization;dynamic association;highly accurate 6D visual localization;lightweight methods;flexible methods;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9341003},
} 


@inproceedings{21545854 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Sequence-based mapping for probabilistic visual loop-closure detection},
journal = {2021 IEEE International Conference on Imaging Systems and Techniques (IST)},
author = {Tsintotas, K.A. and Bampis, L. and Shan An and Fragulis, G.F. and Mouroutsos, S.G. and Gasteratos, A.},
year = {2021//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {During simultaneous localization and mapping, the robot should build a map of its surroundings and simultaneously estimate its pose in the generated map. However, a fundamental task is to detect loops, i.e., previously visited areas, allowing consistent map generation. Moreover, within long-term mapping, every autonomous system needs to address its scalability in terms of storage requirements and database search. In this paper, we present a low-complexity sequence-based visual loop-closure detection pipeline. Our system dynamically segments the traversed route through a feature matching technique in order to define sub-maps. In addition, visual words are generated incrementally for the corresponding sub-maps representation. Comparisons among these sequences-of-images are performed thanks to probabilistic scores originated from a voting scheme. When a candidate sub-map is indicated, global descriptors are utilized for image-to-image pairing. Our evaluation took place on several publicly-available datasets exhibiting the system's low complexity and high recall compared to other state-of-the-art approaches.},
keywords = {feature extraction;image matching;image segmentation;mobile robots;path planning;robot vision;SLAM (robots);},
note = {sequence-based mapping;probabilistic visual loop-closure detection;generated map;visited areas;consistent map generation;long-term mapping;autonomous system;low-complexity sequence-based visual loop-closure detection pipeline;feature matching technique;visual words;sequences-of-images;probabilistic scores;candidate sub-map;image-to-image pairing;sub-map representation;voting scheme;},
URL = {http://dx.doi.org/10.1109/IST50367.2021.9651458},
} 


@article{17199501 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Survey on advances on terrain based navigation for autonomous underwater vehicles},
journal = {Ocean Engineering},
journal = {Ocean Eng. (Netherlands)},
author = {Melo, J. and Matos, A.},
volume = { 139},
year = {2017/07/15},
pages = {250 - 64},
issn = {0029-8018},
address = {Netherlands},
abstract = {The autonomy of robotic underwater vehicles is dependent on the ability to perform long-term and long-range missions without need of human intervention. While current state-of-the-art underwater navigation techniques are able to provide sufficient levels of precision in positioning, they require the use of support vessels or acoustic beacons. This can pose limitations on the size of the survey area, but also on the whole cost of the operations. Terrain Based Navigation is a sensor-based navigation technique that bounds the error growth of dead-reckoning using a map with terrain information, provided that there is enough terrain variability. An obvious advantage of Terrain Based Navigation is the fact that no external aiding signals or devices are required. Because of this unique feature, terrain navigation has the potential to dramatically improve the autonomy of Autonomous Underwater Vehicles (AUVs). This paper consists on a comprehensive survey on the recent developments for Terrain Based Navigation methods proposed for AUVs. The survey includes a brief introduction to the original Terrain Based Navigation formulations, as well as a description of the algorithms, and a list of the different implementation alternatives found in the literature. Additionally, and due to the relevance, Bathymetric SLAM techniques will also be discussed. [All rights reserved Elsevier].},
keywords = {autonomous underwater vehicles;bathymetry;marine navigation;mobile robots;path planning;sensors;SLAM (robots);},
note = {autonomous underwater vehicles;robotic underwater vehicles;long-range missions;underwater navigation techniques;terrain information;terrain variability;Terrain Based Navigation methods;original Terrain;Navigation formulations;long-term missions;positioning;sensor-based navigation;AUV;Bathymetric SLAM;},
URL = {http://dx.doi.org/10.1016/j.oceaneng.2017.04.047},
} 


@inproceedings{18955253 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {SDF-Loc: signed Distance Field based 2D Relocalization and Map Update in Dynamic Environments},
journal = {2019 American Control Conference (ACC)},
author = {Mingming Zhang and Yiming Chen and Mingyang Li},
year = {2019//},
pages = {1997 - 2004},
address = {Piscataway, NJ, USA},
abstract = {To empower an autonomous robot to perform long-term navigation in a given area, a concurrent localization and map update algorithm is required. In this paper, we tackle this problem by providing both theoretical analysis and algorithm design for robotic systems equipped with 2D laser range finders. The first key contribution of this paper is that we propose a hybrid signed distance field (SDF) framework for laser based localization. The proposed hybrid SDF integrates two methods with complementary characteristics, namely Euclidean SDF (ESDF) and Truncated SDF (TSDF). With our framework, accurate pose estimation and fast map update can be performed simultaneously. Moreover, we introduce a novel sliding window estimator which attains better accuracy by consistently utilizing sensor and map information with both scan-to-scan and scan-to-map data association. Real-world experimental results demonstrate that the proposed algorithm can be used for commercial robots in various environments with long-term usage. Experiments also show that our approach outperforms competing approaches by a wide margin.},
keywords = {laser ranging;mobile robots;path planning;pose estimation;robot vision;sensor fusion;SLAM (robots);},
note = {robotic systems;2D laser range finders;hybrid signed distance field framework;laser based localization;hybrid SDF;complementary characteristics;Euclidean SDF;fast map update;sensor;map information;scan-to-map data association;long-term usage;SDF-loc;2D relocalization;dynamic environments;autonomous robot;long-term navigation;concurrent localization;theoretical analysis;scan-to-scan data association;pose estimation;signed distance field;truncated SDF;sliding window estimator;},
} 


@inproceedings{21622554 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {An EKF-Based Fusion of Visual-Inertial Odometry and GPS for Global Robot Pose Estimation},
journal = {2021 6th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE)},
author = {Nguyen Hoang Khoi Tran and Vinh-Hao Nguyen},
volume = {vol.6},
year = {2021//},
pages = {5 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Globally accurate and drift-free pose estimation is essential for long-term navigation of autonomous robots. In this paper, we present an efficient method to fuse visual-inertial odometry and GPS measurements using the Extended Kalman Filter (EKF). The filter state is propagated by relative visual-inertial estimates and updated by absolute GPS readings to eliminate the accumulated drift on position and heading. We built the sensor hardware and implemented the algorithm on an embedded computer for real-time computation. The experimental datasets were collected in an outdoor environment with 6-degree-of-freedom ground truth. Evaluation results showed that the proposed algorithm achieved consistent and accurate pose estimation with different motion trajectories.},
keywords = {distance measurement;Global Positioning System;Kalman filters;mobile robots;nonlinear filters;pose estimation;robot vision;SLAM (robots);},
note = {EKF-based fusion;visual-inertial odometry;global robot pose estimation;drift-free pose estimation;long-term navigation;autonomous robots;GPS measurements;Extended Kalman Filter;filter state;visual-inertial estimates;absolute GPS readings;accumulated drift;consistent pose estimation;accurate pose estimation;},
URL = {http://dx.doi.org/10.1109/ICRAIE52900.2021.9703965},
} 


@article{18877447 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Evolving Indoor Navigational Strategies Using Gated Recurrent Units In NEAT [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Butterworth, J. and Savani, R. and Tuyls, K.},
year = {2019/04/12},
pages = {15 pp. - },
address = {USA},
abstract = {Simultaneous Localisation and Mapping (SLAM) algorithms are expensive to run on smaller robotic platforms such as Micro-Aerial Vehicles. Bug algorithms are an alternative that use relatively little processing power, and avoid high memory consumption by not building an explicit map of the environment. Bug Algorithms achieve relatively good performance in simulated and robotic maze solving domains. However, because they are hand-designed, a natural question is whether they are globally optimal control policies. In this work we explore the performance of Neuroevolution - specifically NEAT - at evolving control policies for simulated differential drive robots carrying out generalised maze navigation. We extend NEAT to include Gated Recurrent Units (GRUs) to help deal with long term dependencies. We show that both NEAT and our NEAT-GRU can repeatably generate controllers that outperform I-Bug (an algorithm particularly well-suited for use in real robots) on a test set of 209 indoor maze like environments. We show that NEAT-GRU is superior to NEAT in this task but also that out of the 2 systems, only NEAT-GRU can continuously evolve successful controllers for a much harder task in which no bearing information about the target is provided to the agent.},
keywords = {mobile robots;optimal control;path planning;recurrent neural nets;robot vision;SLAM (robots);},
note = {evolving indoor navigational strategies;SLAM;smaller robotic platforms;relatively little processing power;high memory consumption;explicit map;relatively good performance;simulated maze solving domains;robotic maze solving domains;hand-designed;natural question;globally optimal control policies;evolving control policies;simulated differential drive robots;generalised maze navigation;long term dependencies;NEAT-GRU;I-Bug;successful controllers;simultaneous localisation and mapping algorithms;indoor maze;bug algorithms;microaerial vehicles;NEAT;gated recurrent units;},
} 


@article{19204254 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Localizing Backscatters by a Single Robot With Zero Start-up Cost [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Shengkai Zhang and Wei Wang and Sheyang Tang and Shi Jin and Tao Jiang},
year = {2019/08/08},
pages = {6 pp. - },
address = {USA},
abstract = {Recent years have witnessed the rapid proliferation of low-power backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such low-power backscatter tags is crucial for IoT-based smart services. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, increasing the deployment cost. To empower universal localization service, this paper presents Rover, an indoor localization system that simultaneously localizes multiple backscatter tags with zero start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing WiFi-based positioning measurements with inertial measurements to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues such as the interference among multiple tags and the real-time processing for solving the SLAM problem. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords = {indoor radio;Internet of Things;mobile robots;position measurement;radiofrequency identification;SLAM (robots);wireless LAN;wireless sensor networks;},
note = {backscatters;zero start-up cost;universal localization service;indoor localization system;multiple backscatter tags;inertial sensors;joint optimization framework;WiFi-based positioning measurements;inertial measurements;connected tags;design addresses practical issues;multiple tags;prototype Rover;localization accuracies;deployment cost;known positions;map;current backscatter localization systems;IoT-based smart services;low-power backscatter tags;smart homes;smart cities;long-term connectivity;ubiquitous term connectivity;low-power backscatter technologies;rapid proliferation;size 74.6 cm;size 39.3 cm;},
} 


@article{19989810 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Beyond Controlled Environments: 3D Camera Re-Localization in Changing Indoor Scenes [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Wald, J. and Sattler, T. and Golodetz, S. and Cavallari, T. and Tombari, F.},
year = {2020/08/05},
pages = {37 pp. - },
address = {USA},
abstract = {Long-term camera re-localization is an important task with numerous computer vision and robotics applications. Whilst various outdoor benchmarks exist that target lighting, weather and seasonal changes, far less attention has been paid to appearance changes that occur indoors. This has led to a mismatch between popular indoor benchmarks, which focus on static scenes, and indoor environments that are of interest for many real-world applications. In this paper, we adapt 3RScan - a recently introduced indoor RGB-D dataset designed for object instance re-localization - to create RIO10, a new long-term camera re-localization benchmark focused on indoor scenes. We propose new metrics for evaluating camera re-localization and explore how state-of-the-art camera re-localizers perform according to these metrics. We also examine in detail how different types of scene change affect the performance of different methods, based on novel ways of detecting such changes in a given RGB-D frame. Our results clearly show that long-term indoor re-localization is an unsolved problem. Our benchmark and tools are publicly available at waldjohannau.github.io/RIO10.},
keywords = {image colour analysis;image reconstruction;indoor environment;object detection;robot vision;SLAM (robots);stereo image processing;},
note = {computer vision;robotics applications;static scenes;indoor environments;indoor RGB-D dataset;indoor scenes;3D camera relocalization;long-term indoor relocalization;camera relocalizers;long-term camera relocalization;object instance relocalization;},
} 


@article{12156923 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Online and Incremental Appearance-based SLAM in Highly Dynamic Environments},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (USA)},
author = {Kawewong, A. and Tongprasit, N. and Tangruamsub, S. and Hasegawa, O.},
volume = { 30},
number = { 1},
year = {2011/01/},
pages = {33 - 55},
issn = {0278-3649},
address = {USA},
abstract = {In this paper we present a novel method for online and incremental appearance-based localization and mapping in a highly dynamic environment. Using position-invariant robust features (PIRFs), the method can achieve a high rate of recall with 100% precision. It can handle both strong perceptual aliasing and dynamic changes of places efficiently. Its performance also extends beyond conventional images; it is applicable to omnidirectional images for which the major portions of scenes are similar for most places. The proposed PIRF-based Navigation method named PIRF-Nav is evaluated by testing it on two standard datasets in a similar manner as in FAB-MAP and on an additional omnidirectional image dataset that we collected. This extra dataset was collected on 2 days with different specific events, i.e. an open-campus event, to present challenges related to illumination variance and strong dynamic changes, and to test assessment of dynamic scene changes. Results show that PIRF-Nav outperforms FAB-MAP; PIRF-Nav at precision-1 yields a recall rate about twice as high (approximately 80% increase) than that of FAB-MAP. Its computation time is sufficiently short for real-time applications. The method is fully incremental, and requires no offline process for dictionary creation. Additional testing using combined datasets proves that PIRF-Nav can function over the long term and can solve the kidnapped robot problem.},
keywords = {feature extraction;mobile robots;path planning;SLAM (robots);},
note = {online appearance-based SLAM;incremental appearance-based SLAM;highly dynamic environments;position-invariant robust features;PIRF-based navigation method;FAB-MAP;omnidirectional image dataset;kidnapped robot problem;perceptual aliasing;},
URL = {http://dx.doi.org/10.1177/0278364910371855},
} 


@inproceedings{20424922 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {SolarSLAM: battery-free loop closure for indoor localisation},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Bo Wei and Weitao Xu and Chengwen Luo and Zoppi, G. and Dong Ma and Sen Wang},
year = {2020//},
pages = {4485 - 90},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we propose SolarSLAM, a batteryfree loop closure method for indoor localisation. Inertial Measurement Unit (IMU) based indoor localisation method has been widely used due to its ubiquity in mobile devices, such as mobile phones, smartwatches and wearable bands. However, it suffers from the unavoidable long term drift. To mitigate the localisation error, many loop closure solutions have been proposed using sophisticated sensors, such as cameras, laser, etc. Despite achieving high-precision localisation performance, these sensors consume a huge amount of energy. Different from those solutions, the proposed SolarSLAM takes advantage of an energy harvesting solar cell as a sensor and achieves effective battery-free loop closure method. The proposed method suggests the key-point dynamic time warping for detecting loops and uses robust simultaneous localisation and mapping (SLAM) as the optimiser to remove falsely recognised loop closures. Extensive evaluations in the real environments have been conducted to demonstrate the advantageous photocurrent characteristics for indoor localisation and good localisation accuracy of the proposed method.},
keywords = {energy harvesting;indoor radio;mobile robots;photoconductivity;robot vision;sensors;SLAM (robots);solar cells;wireless sensor networks;},
note = {SolarSLAM;batteryfree loop closure method;Inertial Measurement Unit;indoor localisation method;mobile devices;mobile phones;unavoidable long term drift;localisation error;loop closure solutions;sophisticated sensors;high-precision localisation performance;energy harvesting solar cell;effective battery-free loop closure method;robust simultaneous localisation;falsely recognised loop closures;good localisation accuracy;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9340962},
} 


@article{18926823 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Learning by Inertia: self-supervised monocular visual odometry for road vehicles [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Chengze Wang and Yuan Yuan and Qi Wang},
year = {2019/05/05},
pages = {5 pp. - },
address = {USA},
abstract = {In this paper, we present iDVO (inertia-embedded deep visual odometry), a self-supervised learning based monocular visual odometry (VO) for road vehicles. When modelling the geometric consistency within adjacent frames, most deep VO methods ignore the temporal continuity of the camera pose, which results in a very severe jagged fluctuation in the velocity curves. With the observation that road vehicles tend to perform smooth dynamic characteristics in most of the time, we design the inertia loss function to describe the abnormal motion variation, which assists the model to learn the consecutiveness from long-term camera ego-motion. Based on the recurrent convolutional neural network (RCNN) architecture, our method implicitly models the dynamics of road vehicles and the temporal consecutiveness by the extended Long Short-Term Memory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to handle the non-consistency in fast camera motion by blocking the boundary part and which generates more efficiency in the whole non-consistency mask. The proposed method is evaluated on the KITTI dataset, and the results demonstrate state-of-the-art performance with respect to other monocular deep VO and SLAM approaches.},
keywords = {cameras;convolutional neural nets;distance measurement;image motion analysis;image sequences;learning (artificial intelligence);mobile robots;motion estimation;pose estimation;recurrent neural nets;road vehicles;robot vision;SLAM (robots);traffic engineering computing;},
note = {extended long short-term memory block;RCNN;LSTM;KITTI dataset;SLAM approaches;monocular deep VO;fast camera motion;recurrent convolutional neural network architecture;long-term camera ego-motion;abnormal motion variation;inertia loss function;smooth dynamic characteristics;deep VO methods;self-supervised learning;inertia-embedded deep visual odometry;road vehicles;self-supervised monocular visual odometry;},
} 


@inproceedings{12317927 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Efficient information-theoretic graph pruning for graph-based SLAM with laser range finders},
journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
author = {Kretzschmar, H. and Stachniss, C. and Grisetti, G.},
year = {2011//},
pages = {865 - 71},
address = {Piscataway, NJ, USA},
abstract = {In graph-based SLAM, the pose graph encodes the poses of the robot during data acquisition as well as spatial constraints between them. The size of the pose graph has a substantial influence on the runtime and the memory requirements of a SLAM system, which hinders long-term mapping. In this paper, we address the problem of efficient information-theoretic compression of pose graphs. Our approach estimates the expected information gain of laser measurements with respect to the resulting occupancy grid map. It allows for restricting the size of the pose graph depending on the information that the robot acquires about the environment or based on a given memory limit, which results in an any-space SLAM system. When discarding laser scans, our approach marginalizes out the corresponding pose nodes from the graph. To avoid a densely connected pose graph, which would result from exact marginalization, we propose an approximation to marginalization that is based on local Chow-Liu trees and maintains a sparse graph. Real world experiments suggest that our approach effectively reduces the growth of the pose graph while minimizing the loss of information in the resulting grid map.},
keywords = {data acquisition;graph theory;laser ranging;SLAM (robots);trees (mathematics);},
note = {information-theoretic graph pruning;graph-based SLAM;laser range finders;pose graph;data acquisition;Chow-Liu trees;sparse graph;},
URL = {http://dx.doi.org/10.1109/IROS.2011.6048060},
} 


@article{20770100 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Improving Image Description with Auxiliary Modality for Visual Localization in Challenging Conditions},
journal = {International Journal of Computer Vision},
journal = {Int. J. Comput. Vis. (Germany)},
author = {Piasco, N. and Sidibe, D. and Gouet-Brunet, V. and Demonceaux, C.},
volume = { 129},
number = { 1},
year = {2021/01/},
pages = {185 - 202},
issn = {0920-5691},
address = {Germany},
abstract = {Image indexing for lifelong localization is a key component for a large panel of applications, including robot navigation, autonomous driving or cultural heritage valorization. The principal difficulty in long-term localization arises from the dynamic changes that affect outdoor environments. In this work, we propose a new approach for outdoor large scale image-based localization that can deal with challenging scenarios like cross-season, cross-weather and day/night localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We show through extensive evaluation that our method can improve localization performances, especially in challenging scenarios when the visual appearance of the scene has changed. Our method is able to leverage both visual and geometric clues from monocular images to create discriminative descriptors for cross-season localization and effective matching of images acquired at different time periods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images. Finally we extended our method to reflectance modality and we compare multi-modal descriptors respectively based on geometry, material reflectance and a combination of both.},
keywords = {computer vision;feature extraction;image classification;image colour analysis;image matching;image recognition;image representation;image retrieval;learning (artificial intelligence);mobile robots;object detection;object recognition;path planning;robot vision;},
note = {principal difficulty;long-term localization;dynamic changes;outdoor environments;outdoor large scale image-based localization;cross-weather;learned global image descriptor;scene geometry information;query image;localization accuracy;localization performances;visual appearance;visual clues;geometric clues;monocular images;cross-season localization;night images;daytime images;multimodal descriptors;image description;auxiliary modality;visual localization;image indexing;lifelong localization;robot navigation;autonomous driving heritage valorization;cultural heritage valorization;},
URL = {http://dx.doi.org/10.1007/s11263-020-01363-6},
} 


@inproceedings{11009737 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Towards lifelong visual maps},
journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2009)},
author = {Konolige, K. and Bowman, J.},
year = {2009//},
pages = {1156 - 63},
address = {Piscataway, NJ, USA},
abstract = {The typical SLAM mapping system assumes a static environment and constructs a map that is then used without regard for ongoing changes. Most SLAM systems, such as FastSLAM, also require a single connected run to create a map. In this paper we present a system of visual mapping, using only input from a stereo camera, that continually updates an optimized metric map in large indoor spaces with movable objects: people, furniture, partitions, etc. The system can be stopped and restarted at arbitrary disconnected points, is robust to occlusion and localization failures, and efficiently maintains alternative views of a dynamic environment. It operates completely online at a 30 Hz frame rate.},
keywords = {image sensors;mobile robots;robot vision;SLAM (robots);},
note = {visual maps;SLAM mapping system;FastSLAM;occlusion;localization failures;mobile robots;},
URL = {http://dx.doi.org/10.1109/IROS.2009.5354121},
} 


@inproceedings{12346760 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {Long-term experiment using an adaptive appearance-based map for visual navigation by mobile robots},
journal = {Towards Autonomous Robotic Systems. Proceedings 12th Annual Conference (TAROS 2011)},
author = {Dayoub, F. and Cielniak, G. and Duckett, T.},
year = {2011//},
pages = {400 - 1},
address = {Berlin, Germany},
abstract = {Building functional and useful mobile service robots means that these robots have to be able to share physical spaces with humans, and to update their internal representation of the world in response to changes in the arrangement of objects and appearance of the environment - changes that may be spontaneous and unpredictable - as a result of human activities. However, almost all past research on robot mapping addresses only the initial learning of an environment, a phase which will only be a short moment in the lifetime of a service robot that may be expected to operate for many years.},
keywords = {intelligent robots;learning (artificial intelligence);mobile robots;navigation;robot vision;service robots;SLAM (robots);},
note = {mobile service robots;human activities;robot mapping;initial learning;adaptive appearance based map;visual navigation;},
URL = {http://dx.doi.org/10.1007/978-3-642-23232-9_47},
} 


@inproceedings{11578263 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Fast Odometry Integration in Local Bundle Adjustment-based Visual SLAM},
journal = {Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010)},
author = {Eudes, A. and Lhuillier, M. and Naudet-Collette, S. and Dhome, M.},
year = {2010//},
pages = {290 - 3},
address = {Los Alamitos, CA, USA},
abstract = {The Simultaneous Localisation And Mapping (SLAM) for a camera moving in a scene is a long term research problem. Here we improve a recent visual SLAM which applies Local Bundle Adjustments (LBA) on selected key-frames of a video: we show how to correct the scale drift observed in long monocular video sequence using an additional odometry sensor. Our method and results are interesting for several reasons: (1) the pose accuracy is improved on real examples (2) we do not sacrifice the consistency between the reconstructed 3D points and image features to fit odometry data (3) the modification of the original visual SLAM method is not difficult.},
keywords = {robot vision;SLAM (robots);video signal processing;},
note = {odometry integration;local bundle adjustment;visual SLAM;simultaneous localisation and mapping;LBA;monocular video sequence;odometry sensor;},
URL = {http://dx.doi.org/10.1109/ICPR.2010.80},
} 


@inproceedings{17058365 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {SemanticFusion: Dense 3D semantic mapping with convolutional neural networks},
journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
author = {McCormac, J. and Handa, A. and Davison, A. and Leutenegger, S.},
year = {2017//},
pages = {4628 - 35},
address = {Piscataway, NJ, USA},
abstract = {Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need to extend beyond geometry and appearance - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state-of-the-art dense Simultaneous Localization and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondences between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of &ap;25Hz.},
keywords = {feedforward neural nets;image fusion;image segmentation;indoor navigation;mobile robots;robot vision;SLAM (robots);video signal processing;},
note = {SemanticFusion;dense 3D semantic mapping;convolutional neural networks;visual sensing;mobile robots;robot intelligence;intuitive user interaction;CNN;simultaneous localisation and mapping system;SLAM system;ElasticFusion;long-term dense correspondences;indoor RGB-D video frames;loopy scanning trajectories;semantic predictions;NYUv2 dataset;2D semantic labelling;},
URL = {http://dx.doi.org/10.1109/ICRA.2017.7989538},
} 


@inproceedings{20425133 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {A Fast and Robust Place Recognition Approach for Stereo Visual Odometry Using LiDAR Descriptors},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Jiawei Mo and Sattar, J.},
year = {2020//},
pages = {5893 - 900},
address = {Piscataway, NJ, USA},
abstract = {Place recognition is a core component of Simultaneous Localization and Mapping (SLAM) algorithms. Particularly in visual SLAM systems, previously-visited places are recognized by measuring the appearance similarity between images representing these locations. However, such approaches are sensitive to visual appearance change and also can be computationally expensive. In this paper, we propose an alternative approach adapting LiDAR descriptors for 3D points obtained from stereo-visual odometry for place recognition. 3D points are potentially more reliable than 2D visual cues (e.g., 2D features) against environmental changes (e.g., variable illumination) and this may benefit visual SLAM systems in long-term deployment scenarios. Stereo-visual odometry generates 3D points with an absolute scale, which enables us to use LiDAR descriptors for place recognition with high computational efficiency. Through extensive evaluations on standard benchmark datasets, we demonstrate the accuracy, efficiency, and robustness of using 3D points for place recognition over 2D methods.},
keywords = {distance measurement;feature extraction;image recognition;mobile robots;optical radar;robot vision;SLAM (robots);stereo image processing;},
note = {2D visual cues;visual SLAM systems;stereo-visual odometry;robust place recognition approach;stereo visual odometry;Mapping algorithms;previously-visited places;visual appearance change;alternative approach adapting LiDAR descriptors;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9341733},
} 


@article{21210403 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {High-definition map update framework for intelligent autonomous transfer vehicles},
journal = {Journal of Experimental &amp; Theoretical Artificial Intelligence},
journal = {J. Exp. Theor. Artif. Intell. (UK)},
author = {Tas, M.O. and Yavuz, H.S. and Yazici, A.},
volume = { 33},
number = { 5},
year = {2021//},
pages = {847 - 65},
issn = {0952-813X},
address = {UK},
abstract = {Autonomous transfer vehicles (ATVs) can be considered as one of the critical components of context-aware structured smart factories in Industry 4.0 era. Conventional mapping methods such as grid maps can provide information for navigation, but they are not enough for complex environments that require interactions. On the other hand, high-definition (HD) mapping, which is mainly used in traffic networks, includes more information about an environment to perform excellent autonomous behaviour. In order to increase the efficiency of ATVs in flexible factories, an up-to-date environmental map information is required to perform successful long-term autonomous navigation. Therefore, when there exists a change in the environment, a simultaneous update of HD-map is as important as the creation of it. In this study, we propose an HD-map update methodology for ATVs that operates in smart factories. To the best of our knowledge, HD mapping has not been applied in smart factories. The proposed method includes the object detection and localisation tool to detect objects visually and determines their positions in connection with the conventional maps of the environment. Experimental results of a simulated factory environment demonstrate that the ATV can properly update the HD-map when a predefined sign is removed from or a new sign is added to the environment.},
keywords = {factory automation;industrial robots;mobile robots;object detection;path planning;production engineering computing;robot vision;SLAM (robots);},
note = {object detection;autonomous navigation;up-to-date environmental map information;Industry 4.0;context-aware structured smart factories;intelligent autonomous transfer vehicles;high-definition map update framework;HD mapping;ATV;},
URL = {http://dx.doi.org/10.1080/0952813X.2020.1789754},
} 


@inproceedings{20385429 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {A Robust Localization Method in Indoor Dynamic Environment},
journal = {IOP Conference Series: Materials Science and Engineering},
journal = {IOP Conf. Ser., Mater. Sci. Eng. (UK)},
author = {Shan Huang and Hong-Zhong Huang and Qi Zeng},
volume = { 1043},
year = {2021//},
pages = {052025 (17 pp.) - },
issn = {1757-8981},
address = {UK},
abstract = {Localization is one of the core technologies for mobile robots to achieve full autonomous movement, and is a prerequisite for other autonomous tasks. The robot working environment is dynamic in most cases, so the localization algorithm must overcome the effects of dynamic changes in the environment. The paper proposed a localization algorithm that allows the robot to perform robust and life-long localization in dynamic environment. The algorithm filter out high-dynamic objects and update semi-static object on the map at the same time, it can also use the information provided in semi-static objects to improve localization performance. In this paper, the processing of dynamic objects is divided into two parts: filtering of high-dynamic objects and updating of semi-static objects. For high dynamic object filtering, a dynamic object detection method combining a delay comparison method and a tracking method is proposed by observed the characteristics of localization system; For the update of semi-static objects, this paper uses the pose graph optimization and occupancy map to implement the dynamic update of the map. The combination of the two methods allows the robot to achieve long-term stable localization in a dynamic environment. The experimental results demonstrate that the proposed method allows the robot achieve long-term localization, overcome the effects of high-dynamic objects and keeping the map always consistent with the environment.},
keywords = {delays;distance measurement;graph theory;indoor navigation;mobile robots;object detection;optical radar;sensor fusion;SLAM (robots);},
note = {mobile robots;robot working environment;localization stability;robust localization;indoor dynamic environment;dynamic object detection;dynamic object filtering;autonomous movement;delay comparison;tracking method;semi static objects;LIDAR sensors;wheel odometer sensors;pose graph optimization;occupancy map;},
URL = {http://dx.doi.org/10.1088/1757-899X/1043/5/052025},
} 


@article{20655845 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {OpenCSI: An Open-Source Dataset for Indoor Localization Using CSI-Based Fingerprinting [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Gassner, A. and Musat, C. and Rusu, A. and Burg, A.},
year = {2021/04/16},
pages = {9 pp. - },
address = {USA},
abstract = {Many applications require accurate indoor localization. Fingerprint-based localization methods propose a solution to this problem, but rely on a radio map that is effort-intensive to acquire. We automate the radio map acquisition phase using a software-defined radio (SDR) and a wheeled robot. Furthermore, we open-source a radio map acquired with our automated tool for a 3GPP Long-Term Evolution (LTE) wireless link. To the best of our knowledge, this is the first publicly available radio map containing channel state information (CSI). Finally, we describe first localization experiments on this radio map using a convolutional neural network to regress for location coordinates.},
keywords = {fingerprint identification;indoor communication;indoor radio;Long Term Evolution;mobile robots;neural nets;software radio;wireless LAN;},
note = {accurate indoor localization;radio map acquisition phase;automated tool;publicly available radio map;localization experiments;},
} 


@article{20585555 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Localization in Unstructured Environments: Towards Autonomous Robots in Forests with Delaunay Triangulation},
journal = {Remote Sensing},
journal = {Remote Sens. (Switzerland)},
author = {Qingqing Li and Nevalainen, P. and Queralta, J.P. and Heikkonen, J. and Westerlund, T.},
volume = { 12},
number = { 11},
year = {2020/06/},
pages = {1870 (22 pp.) - },
issn = {2072-4292},
address = {Switzerland},
abstract = {Autonomous harvesting and transportation is a long-term goal of the forest industry. One of the main challenges is the accurate localization of both vehicles and trees in a forest. Forests are unstructured environments where it is difficult to find a group of significant landmarks for current fast feature-based place recognition algorithms. This paper proposes a novel approach where local point clouds are matched to a global tree map using the Delaunay triangularization as the representation format. Instead of point cloud based matching methods, we utilize a topology-based method. First, tree trunk positions are registered at a prior run done by a forest harvester. Second, the resulting map is Delaunay triangularized. Third, a local submap of the autonomous robot is registered, triangularized and matched using triangular similarity maximization to estimate the position of the robot. We test our method on a dataset accumulated from a forestry site at Lieksa, Finland. A total length of 200m of harvester path was recorded by an industrial harvester with a 3D laser scanner and a geolocation unit fixed to the frame. Our experiments show a 12 cm s.t.d. in the location accuracy and with real-time data processing for speeds not exceeding 0.5 m/s. The accuracy and speed limit are realistic during forest operations.},
keywords = {feature extraction;forestry;image matching;mesh generation;mobile robots;optical scanners;robot vision;SLAM (robots);},
note = {point cloud;matching methods;topology-based method;tree trunk positions;forest harvester;resulting map;local submap;autonomous robot;triangular similarity maximization;harvester path;industrial harvester;forest operations;unstructured environments;towards autonomous robots;Delaunay triangulation;transportation;forest industry;vehicles;trees;significant landmarks;current fast feature-based place recognition algorithms;local point clouds;global tree map;Delaunay triangularization;wavelength 200.0 m;size 12.0 cm;velocity 0.5 m/s;},
URL = {http://dx.doi.org/10.3390/rs12111870},
} 


@inproceedings{18778770 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Learning by Inertia: Self-supervised Monocular Visual Odometry for Road Vehicles},
journal = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings},
author = {Chengze Wang and Yuan Yuan and Qi Wang},
year = {2019//},
pages = {2252 - 6},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we present iDVO (inertia-embedded deep visual odometry), a self-supervised learning based monocular visual odometry (VO) for road vehicles. When modelling the geometric consistency within adjacent frames, most deep VO methods ignore the temporal continuity of the camera pose, which results in a very severe jagged fluctuation in the velocity curves. With the observation that road vehicles tend to perform smooth dynamic characteristics in most of the time, we design the inertia loss function to describe the abnormal motion variation, which assists the model to learn the consecutiveness from long-term camera ego-motion. Based on the recurrent convolutional neural network (RCNN) architecture, our method implicitly models the dynamics of road vehicles and the temporal consecutiveness by the extended Long Short-Term Memory (LSTM) block. Furthermore, we develop the dynamic hard-edge mask to handle the non-consistency in fast camera motion by blocking the boundary part and which generates more efficiency in the whole non-consistency mask. The proposed method is evaluated on the KITTI dataset, and the results demonstrate state-of-the-art performance with respect to other monocular deep VO and SLAM approaches.},
keywords = {cameras;convolutional neural nets;distance measurement;image motion analysis;pose estimation;recurrent neural nets;road vehicles;robot vision;SLAM (robots);supervised learning;},
note = {smooth dynamic characteristics;long-term camera ego-motion;recurrent convolutional neural network architecture;road vehicles;extended Long Short-Term Memory block;self-supervised monocular visual odometry;inertia-embedded deep visual odometry;self-supervised learning;camera motion;},
URL = {http://dx.doi.org/10.1109/ICASSP.2019.8683446},
} 


@inproceedings{11689801 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {6 DoF SLAM using a ToF camera: the challenge of a continuously growing number of landmarks},
journal = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2010)},
author = {Hochdorfer, S. and Schlegel, C.},
year = {2010//},
pages = {3981 - 6},
address = {Piscataway, NJ, USA},
abstract = {Localization and mapping are fundamental problems in service robotics since representations of the environment and knowledge about the own pose significantly simplify the implementation of a series of high-level applications. ToF (time-of-flight) cameras are a relatively new kind of sensors in robotics. They enable the real-time capture of the distance and the grayscale information of a scene. Due to the increase of the image resolution of ToF cameras, now highlevel computer vision algorithms for visual feature extraction (e.g. SIFT or SURF) can be applied to the captured images. These visual features combined with the corresponding distance information give a full measurement of 3D landmarks. An obvious problem to be solved is the continuously growing number of landmarks. So far, all ever seen landmarks are just accumulated irrespective of their utility and the then required resources. Rather, one should keep only really useful landmarks, e.g. such that localization quality in the whole operational area is kept above a given threshold. In fact a lifelong running SLAM approach is dependent on means to select and discard landmarks. That is even more acute in case of feature-rich sensor data as provided with high update rates by sensors like a ToF camera. We run our SLAM approach in a real-world experiment within an indoor environment. The experiment was performed on a P3DX-platform equipped with a PMD CamCube 2.0 and a Xsens IMU.},
keywords = {cameras;feature extraction;image resolution;robot vision;service robots;SLAM (robots);},
note = {DoF SLAM;ToF camera;localization and mapping;service robotics;time-of-flight;sensors;image resolution;computer vision;feature extraction;3D landmark;},
URL = {http://dx.doi.org/10.1109/IROS.2010.5651229},
} 


@inproceedings{11747598 ,
language = {English},
copyright = {Copyright 2011, The Institution of Engineering and Technology},
title = {SIFT based monocular SLAM with multi-clouds features for indoor navigation},
journal = {2010 IEEE Region 10 Conference (TENCON 2010)},
author = {Ali, A.M. and Nordin, M.J.},
year = {2010//},
pages = {2326 - 31},
address = {Piscataway, NJ, USA},
abstract = {This work introduces a monocular SLAM method, which uses the Scale Invariant Features Transform (SIFT) representation for the scene. The scene represented as clouds of SIFT features within the map. This hierarchical representation of space, serving to estimate the current direction in the environment within the current session. The system exploits the tracking of the same features of successive frames to calculate scalar weights for these features, to build a map of the environment indicating the camera movement, helping the blind persons to navigate more confidently through auditory pathway of their surroundings. EKF is used to estimate the features tracked within the successive frames. The system is tested for using the proposed method with a hand-held camera walking in indoor environment. The results show a good estimation on the spatial locations of the camera within a few milliseconds. The paper shows an electronic cane for navigating in indoor environment using these clouds of features for long-term appearance-based localization of a cane with web camera vision as the external sensor.},
keywords = {SLAM (robots);},
note = {SIFT based monocular SLAM;multiclouds features;indoor navigation;scale invariant features transform;hierarchical representation;camera movement;web camera vision;external sensor;},
URL = {http://dx.doi.org/10.1109/TENCON.2010.5685972},
} 


@inproceedings{21256800 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Tightly-Coupled Multi-Sensor Fusion for Localization with LiDAR Feature Maps},
journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Liangliang Pan and Kaijin Ji and Ji Zhao},
year = {2021//},
pages = {5215 - 21},
address = {Piscataway, NJ, USA},
abstract = {Robust and accurate pose estimation in long-term localization is crucial to autonomous driving. In this paper, we dealt with absolute localization with a LiDAR feature map and multi-sensor measurements. We proposed a tightly-coupled fusion method with fixed-lag smoothing. A sliding window of recently maintained states is estimated by minimizing a joint cost function. This cost function includes residuals of global LiDAR registration and relative kinematic constraints from an IMU and wheel encoders. In addition, we enhance the robustness of our method by improving LiDAR registration. To achieve this goal, LiDAR feature maps with a hybrid of geometric and normal distribution features are constructed and exploited. The effectiveness of the proposed method is verified in several challenging test sequences over 200km. The experimental results demonstrate that the proposed method achieves accurate localization and high robustness in challenging scenarios even when the LiDAR observation is degraded.},
keywords = {distance measurement;feature extraction;image registration;mobile robots;optical radar;pose estimation;remote sensing by laser beam;sensor fusion;},
note = {geometric distribution features;normal distribution features;LiDAR observation;multisensor fusion;LiDAR feature map;accurate pose estimation;long-term localization;absolute localization;multisensor measurements;fusion method;joint cost function;global LiDAR registration;size 200.0 km;},
URL = {http://dx.doi.org/10.1109/ICRA48506.2021.9561547},
} 


@inproceedings{21256806 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {A Flexible and Efficient Loop Closure Detection Based on Motion Knowledge},
journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Bingxi Liu and Fulin Tang and Yujie Fu and Yanqun Yang and Yihong Wu},
year = {2021//},
pages = {11241 - 7},
address = {Piscataway, NJ, USA},
abstract = {Loop closure detection (LCD) is an essential module for simultaneous localization and mapping (SLAM), which can correct accumulated errors after long-term explorations. The widely used bag-of-words (BoW) model can not satisfy well the requirements of both low time consumption and high accuracy for a mobile platform. In this paper, we propose a novel LCD algorithm based on motion knowledge. We give a flexible and efficient detection strategy and also give flexible and efficient combinations of a global binary feature extracted by convolutional neural network (CNN) and a hand-crafted local binary feature. We take a continuous motion model, grid-based motion statistics (GMS) and motion states as motion knowledge. Furthermore, we fuse the proposed LCD with a visual-inertial odometry (VIO) system to correct localization errors by a pose graph optimization. Comparative experiments with state-of-the-art LCD algorithms on typical datasets have been carried out, and the results demonstrate that our proposed method achieves quite high recall rates and quite high speed at 100% precision. Moreover, experimental results from VIO further validate the effectiveness of the proposed method.},
keywords = {distance measurement;feature extraction;graph theory;mobile robots;neural nets;object detection;pose estimation;robot vision;SLAM (robots);},
note = {novel LCD algorithm;motion knowledge;flexible detection strategy;efficient detection strategy;flexible combinations;efficient combinations;global binary feature;hand-crafted local binary feature;continuous motion model;motion states;visual-inertial odometry system;localization errors;state-of-the-art LCD algorithms;loop closure detection;essential module;long-term explorations;bag-of-words model;low time consumption;},
URL = {http://dx.doi.org/10.1109/ICRA48506.2021.9561126},
} 


@article{19350016 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Place Recognition for Stereo VisualOdometry using LiDAR Descriptors [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Jiawei Mo and Sattar, J.},
year = {16 Sept. 2019},
pages = {17 pp. - },
address = {USA},
abstract = {Place recognition is a core component in SLAM, and in most visual SLAM systems, it is based on the similarity between 2D images. However, the 3D points generated by visual odometry, and the structure information embedded within, are not exploited. In this paper, we adapt place recognition methods for 3D point clouds into stereo visual odometry. Stereo visual odometry generates 3D point clouds with a consistent scale. Thus, we are able to use global LiDAR descriptors for 3D point clouds to determine the similarity between places. 3D point clouds are more reliable than 2D visual cues (e.g., 2D features) against environmental changes such as varying illumination and can benefit visual SLAM systems in long-term deployment scenarios. Extensive evaluation on a public dataset (Oxford RobotCar) demonstrates the accuracy and efficiency of using 3D point clouds for place recognition over 2D methods.},
keywords = {distance measurement;feature extraction;object recognition;optical radar;SLAM (robots);stereo image processing;},
note = {global LiDAR descriptors;structure information;place recognition methods;visual SLAM systems;2D visual cues;3D point clouds;stereo visual odometry;},
} 


@article{20886414 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Thomas, H. and Agro, B. and Gridseth, M. and Jian Zhang and Barfoot, T.D.},
year = {2020/12/10},
pages = {7 pp. - },
address = {USA},
abstract = {We present a self-supervised learning approach for the semantic segmentation of lidar frames. Our method is used to train a deep point cloud segmentation architecture without any human annotation. The annotation process is automated with the combination of simultaneous localization and mapping (SLAM) and ray-tracing algorithms. By performing multiple navigation sessions in the same environment, we are able to identify permanent structures, such as walls, and disentangle short-term and long-term movable objects, such as people and tables, respectively. New sessions can then be performed using a network trained to predict these semantic labels. We demonstrate the ability of our approach to improve itself over time, from one session to the next. With semantically filtered point clouds, our robot can navigate through more complex scenarios, which, when added to the training pool, help to improve our network predictions. We provide insights into our network predictions and show that our approach can also improve the performances of common localization techniques.},
keywords = {image segmentation;mobile robots;optical radar;path planning;robot vision;SLAM (robots);supervised learning;},
note = {permanent structures;new sessions;semantic labels;semantically filtered point clouds;training pool;network predictions;common localization techniques;lidar segmentation;semantic segmentation;lidar frames;deep point cloud segmentation architecture;human annotation;annotation process;SLAM;multiple navigation sessions;},
} 


@inproceedings{10363992 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {An adaptive appearance-based map for long-term topological localization of mobile robots},
journal = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
author = {Dayoub, F. and Duckett, T.},
year = {2008//},
pages = {3364 - 9},
address = {Piscataway, NJ, USA},
abstract = {This work considers a mobile service robot which uses an appearance-based representation of its workplace as a map, where the current view and the map are used to estimate the current position in the environment. Due to the nature of real-world environments such as houses and offices, where the appearance keeps changing, the internal representation may become out of date after some time. To solve this problem the robot needs to be able to adapt its internal representation continually to the changes in the environment. This paper presents a method for creating an adaptive map for long-term appearance-based localization of a mobile robot using long-term and short-term memory concepts, with omni-directional vision as the external sensor.},
keywords = {mobile robots;path planning;robot vision;service robots;},
note = {adaptive appearance-based map;long-term topological localization;mobile service robots;current position estimation;omni-directional vision;},
URL = {http://dx.doi.org/10.1109/IROS.2008.4650701},
} 


@article{17961800 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {DynSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Bescos, B. and Facil, J.M. and Civera, J. and Neira, J.},
year = {2018/06/14},
pages = {8 pp. - },
address = {USA},
abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this paper we present DynSLAM, a visual SLAM system that, building over ORB-SLAM2 [1], adds the capabilities of dynamic object detection and background inpainting. DynSLAM is robust in dynamic scenarios for monocular, stereo and RGB-D configurations. We are capable of detecting the moving objects either by multi-view geometry, deep learning or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments.},
keywords = {image motion analysis;image restoration;object detection;object tracking;robot vision;service robots;SLAM (robots);stereo image processing;},
note = {DynSLAM;dynamic scenes;scene rigidity;SLAM algorithms;visual SLAM system;real-world environments;service robotics;autonomous vehicles;ORB-SLAM2;dynamic object detection;background inpainting;multiview geometry;static map;frame background;dynamic objects;standard visual SLAM baselines;moving object detection;RGB-D configurations;deep learning;stereo data set;public monocular dataset;},
} 


@article{20261642 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Map recovery and fusion for collaborative augment reality of multiple mobile devices},
journal = {IEEE Transactions on Industrial Informatics},
journal = {IEEE Trans. Ind. Inform. (USA)},
author = {Jianhua Zhang and Jialing Liu and Kaiqi Chen and Zhiying Pan and Ruyu Liu and Yanyan Wang and Yang, T. and Shengyong Chen},
volume = { 17},
number = { 3},
year = {2021/03/},
pages = {2081 - 9},
issn = {1551-3203},
address = {USA},
abstract = {The map recovery and fusion is a key issue in the application of large scale and long-term augmented reality (AR) scenarios. However, they are still not addressed well in an efficient and precise way, especially for complex industrial environments. In this article, we propose a map recovery and fusion strategy based on vision-inertial simultaneous localization and mapping. We first develop a heuristic strategy that can fast search and match map points among multiple maps, and can be used for efficient map fusion. For map recovery, we leverage the inertial sensors for short time motion estimation, and transform the previous lost map to the current map. Based on this strategy, a novel framework for collaborative AR is implemented and can parallelly run in multiple mobile devices in real time. Extensive experiments have been carried out on a public data set, and the results show that the proposed method can recovery and fuse multiple maps with high completeness and precision.},
keywords = {augmented reality;mobile computing;motion estimation;SLAM (robots);},
note = {map recovery;collaborative augment reality;multiple mobile devices;vision-inertial simultaneous localization;map points;multiple maps;efficient map fusion;current map;},
URL = {http://dx.doi.org/10.1109/TII.2020.2999924},
} 


@article{19483531 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Map as the hidden sensor: fast odometry-based global localization [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Cheng Peng and Weikersdorfer, D.},
year = {20 Sept. 2019},
pages = {7 pp. - },
address = {USA},
abstract = {Accurate and robust global localization is essential to robotics applications. We propose a novel global localization method that employs the map traversability as a hidden observation. The resulting map-corrected odometry localization is able to provide an accurate belief tensor of the robot state. Our method can be used for blind robots in dark or highly reflective areas. In contrast to odometry drift in long-term, our method using only odometry and the map converges in longterm. Our method can also be integrated with other sensors to boost the localization performance. The algorithm does not have any initial state assumption and tracks all possible robot states at all times. Therefore, our method is global and is robust in the event of ambiguous observations. We parallel each step of our algorithm such that it can be performed in real-time (up to ~ 300 Hz) using GPU. We validate our algorithm in different publicly available floor-plans and show that it is able to converge to the ground truth fast while being robust to ambiguities.},
keywords = {distance measurement;mobile robots;path planning;robot vision;SLAM (robots);},
note = {fast odometry-based global localization;belief tensor;map-corrected odometry localization;map traversability;robotics applications;robust global localization;odometry drift;blind robots;robot state;hidden observation;},
} 


@article{18001975 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Bescos, B. and Facil, J.M. and Civera, J. and Neira, J.},
volume = { 3},
number = { 4},
year = {2018/10/},
pages = {4076 - 83},
issn = {2377-3774},
address = {USA},
abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this letter we present DynaSLAM, a visual SLAM system that, building on ORB-SLAM2, adds the capabilities of dynamic object detection and background inpainting. DynaSLAM is robust in dynamic scenarios for monocular, stereo, and RGB-D configurations. We are capable of detecting the moving objects either by multiview geometry, deep learning, or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo, and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynaSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments.},
keywords = {image colour analysis;image restoration;learning (artificial intelligence);mobile robots;object detection;service robots;SLAM (robots);stereo image processing;},
note = {deep learning;multiview geometry;monocular datasets;stereo datasets;RGB-D datasets;frame background inpainting;scene rigidity;static map;dynamic object detection;ORB-SLAM2;DynaSLAM;autonomous vehicles;service robotics;visual SLAM system;SLAM algorithms;},
URL = {http://dx.doi.org/10.1109/LRA.2018.2860039},
} 


@inproceedings{19854437 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {ViPR: Visual-Odometry-aided Pose Regression for 6DoF Camera Localization},
journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings},
author = {Ott, F. and Feigl, T. and Loffler, C. and Mutschler, C.},
year = {2020//},
pages = {187 - 98},
address = {Piscataway, NJ, USA},
abstract = {Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with optical flow (OF), research that combines absolute pose regression with OF is limited.We propose ViPR, a novel modular architecture for longterm 6DoF VO that leverages temporal information and synergies between absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) by combining both through recurrent layers. Experiments on known datasets and on our own Industry dataset show that our modular design outperforms state ofthe art in long-term navigation tasks.},
keywords = {cameras;convolutional neural nets;distance measurement;image sequences;mobile robots;path planning;pose estimation;robot vision;},
note = {ViPR;visual-Odometry-aided pose regression;6DoF camera localization;visual odometry;positional drift;long-term robot navigation tasks;convolutional neural networks;moving obstacles;poor textures;visual information;depth maps;optical flow;temporal information;long-term navigation tasks;},
URL = {http://dx.doi.org/10.1109/CVPRW50498.2020.00029},
} 


@article{20229951 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {ClusterMap Building and Relocalization in Urban Environments for Unmanned Vehicles},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Zhichen Pan and Haoyao Chen and Silin Li and Yunhui Liu},
volume = { 19},
number = { 19},
year = {2019/10/},
pages = {4252 (22 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {Map building and map-based relocalization techniques are important for unmanned vehicles operating in urban environments. The existing approaches require expensive high-density laser range finders and suffer from relocalization problems in long-term applications. This study proposes a novel map format called the ClusterMap, on the basis of which an approach to achieving relocalization is developed. The ClusterMap is generated by segmenting the perceived point clouds into different point clusters and filtering out clusters belonging to dynamic objects. A location descriptor associated with each cluster is designed for differentiation. The relocalization in the global map is achieved by matching cluster descriptors between local and global maps. The solution does not require high-density point clouds and high-precision segmentation algorithms. In addition, it prevents the effects of environmental changes on illumination intensity, object appearance, and observation direction. A consistent ClusterMap without any scale problem is built by utilizing a 3D visual-LIDAR simultaneous localization and mapping solution by fusing LIDAR and visual information. Experiments on the KITTI dataset and our mobile vehicle illustrates the effectiveness of the proposed approach.},
keywords = {image segmentation;laser ranging;mobile robots;optical radar;path planning;pattern clustering;remotely operated vehicles;robot vision;SLAM (robots);},
note = {ClusterMap building;urban environments;unmanned vehicles;map building;map-based relocalization techniques;high-density laser range finders;relocalization problems;long-term applications;map format;perceived point clouds;point clusters;dynamic objects;location descriptor;cluster descriptors;local maps;global maps;high-density point clouds;high-precision segmentation algorithms;consistent ClusterMap;scale problem;mapping solution;mobile vehicle;3D visual-LIDAR simultaneous localization and mapping solution;visual information;KITTI dataset;},
URL = {http://dx.doi.org/10.3390/s19194252},
} 


@article{10358724 ,
language = {English},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {Subjective local maps for hybrid metric-topological SLAM},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Blanco, J.L. and Gonzalez, J. and Fernandez-Madrigal, J.-A.},
volume = { 57},
number = { 1},
year = {2009/01/31},
pages = {64 - 74},
issn = {0921-8890},
address = {Netherlands},
abstract = {Hybrid maps where local metric submaps are kept in the nodes of a graph-based topological structure are gaining relevance as the focus of robot Simultaneous Localization and Mapping (SLAM) shifts towards spatial scalability and long-term operation. In this paper we examine the applicability of spectral graph partitioning techniques to the automatic generation of metric submaps by establishing groups in the sequence of observations gathered by the robot. One of the main aims of this work is to provide a probabilistically grounded interpretation of such a partitioning technique in the context of generating local maps. We also discuss how to apply it to different kinds of sensory data (landmarks extracted from stereo images and laser range scans) and how to consider them simultaneously. An important feature of our approach is that the partitioning takes into account the intrinsic characteristics of the sensors, such as the sensor field of view, instead of applying heuristics supplied by a human as in other works. Thus the robot builds &ldquo;subjective&rdquo; local maps whose size will be determined by the nature of the sensors. The ideas presented here are supported by experimental results from a real mobile robot as well as simulations for statistical analysis. We discuss the effects of considering different combinations of sensors in the resulting clustering of the environment.[All rights reserved Elsevier].},
keywords = {mobile robots;path planning;SLAM (robots);statistical analysis;},
note = {subjective local maps;hybrid metric-topological SLAM;local metric submaps;graph-based topological structure;robot simultaneous localization and mapping;spatial scalability;mobile robot;statistical analysis;},
URL = {http://dx.doi.org/10.1016/j.robot.2008.02.002},
} 


@inproceedings{18942604 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A Novel Global Relocalization Method Based on Hierarchical Registration of 3D Point Cloud Map for Mobile Robot},
journal = {2019 5th International Conference on Control, Automation and Robotics (ICCAR)},
author = {Qi Tian and YunFeng Gao and GuoLin Li and JiaXin Song},
year = {2019//},
pages = {68 - 73},
address = {Piscataway, NJ, USA},
abstract = {Indoor service mobile robots need to relocate when they are kidnapped, powered off, or lost in long-term work, thus unable to perform daily tasks. Solving this problem is challenging, especially for 3D maps due to the computational complexity. In order to solve this issue, a novel relocalization algorithm based on hierarchical registration is proposed for a known 3D map in this paper. For 3D point cloud maps, the algorithm obtains multi-layer information in the vertical direction through hierarchical registration at the robot's current position. To obtain the best 3D pose for relocalization, we fuse the poses calculated by the multi-layered point cloud into one and use it as the initial pose of the iterative closest point algorithm. The hierarchical registration based algorithm solves the problem of unknown initial value for registration between two large point clouds, improves the recall rate, and ensures the accuracy of algorithm at the same time. The related relocalization experiments are carried out in the indoor environment and the results verify the effectiveness and robustness of the algorithm.},
keywords = {image registration;iterative methods;mobile robots;path planning;position control;robot vision;service robots;SLAM (robots);},
note = {mobile robot;indoor service mobile robots;long-term work;multilayer information;iterative closest point algorithm;hierarchical registration based algorithm;relocalization algorithm;relocalization experiments;global relocalization method;3D point cloud map;},
URL = {http://dx.doi.org/10.1109/ICCAR.2019.8813720},
} 


@article{20002222 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Improved and scalable online learning of spatial concepts and language models with mapping},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Taniguchi, A. and Hagiwara, Y. and Taniguchi, T. and Inamura, T.},
volume = { 44},
number = { 6},
year = {2020/07/},
pages = {927 - 46},
issn = {0929-5593},
address = {Germany},
abstract = {We propose a novel online learning algorithm, called SpCoSLAM 2.0, for spatial concepts and lexical acquisition with high accuracy and scalability. Previously, we proposed SpCoSLAM as an online learning algorithm based on unsupervised Bayesian probabilistic model that integrates multimodal place categorization, lexical acquisition, and SLAM. However, our original algorithm had limited estimation accuracy owing to the influence of the early stages of learning, and increased computational complexity with added training data. Therefore, we introduce techniques such as fixed-lag rejuvenation to reduce the calculation time while maintaining an accuracy higher than that of the original algorithm. The results show that, in terms of estimation accuracy, the proposed algorithm exceeds the original algorithm and is comparable to batch learning. In addition, the calculation time of the proposed algorithm does not depend on the amount of training data and becomes constant for each step of the scalable algorithm. Our approach will contribute to the realization of long-term spatial language interactions between humans and robots.},
keywords = {Bayes methods;estimation theory;human-robot interaction;knowledge acquisition;robot programming;SLAM (robots);spatial reasoning;unsupervised learning;},
note = {online learning;spatial concepts;language models;mapping;SpCoSLAM 2.0;lexical acquisition;fixed-lag rejuvenation;estimation accuracy;batch learning;calculation time;training data;long-term spatial language interactions;unsupervised Bayesian probabilistic model;multimodal place categorization integration;human-robot interaction;},
URL = {http://dx.doi.org/10.1007/s10514-020-09905-0},
} 


@article{19629255 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {ViPR: visual-odometry-aided pose regression for 6DoF camera localization [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Ott, F. and Feigl, T. and Loumlffler, C. and Mutschler, C.},
year = {2019/12/17},
pages = {11 pp. - },
address = {USA},
abstract = {Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with the optical flow (OF), research that combines absolute pose regression with OF is limited. We propose ViPR, a novel architecture for long-term 6DoF VO that leverages synergies between absolute pose estimates (from PoseNet-like architectures) and relative pose estimates (from FlowNet-based architectures) by combining both through recurrent layers. Experiments with known publicly available datasets and with our own Industry dataset show that our novel design outperforms existing techniques in long-term navigation tasks.},
keywords = {cameras;convolutional neural nets;distance measurement;feature extraction;image sequences;mobile robots;navigation;path planning;pose estimation;robot vision;},
note = {PoseNet-like architecture;long-term 6DoF VO;optical flow;depth maps;visual information;poor textures;moving obstacles;convolutional neural networks;long-term robot navigation tasks;positional drift;Visual Odometry;6DoF camera localization;visual-odometry-aided;ViPR;long-term navigation tasks;FlowNet-based architectures;},
} 


@article{18999326 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A realtime autonomous robot navigation framework for human like high-level interaction and task planning in global dynamic environment [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Sung-Hyeon Joo and Manzoor, S. and Rocha, Y.G. and Hyun-Uk Lee and Tae-Yong Kuc},
year = {2019/05/30},
pages = {4 pp. - },
address = {USA},
abstract = {In this paper, we present a framework for real-time autonomous robot navigation based on cloud and on-demand databases to address two major issues of human-like robot interaction and task planning in global dynamic environment, which is not known a priori. Our framework contributes to make human-like brain GPS mapping system for robot using spatial information and performs 3D visual semantic SLAM for independent robot navigation. We accomplish the feat by separating robot's memory system into Long-Term Memory (LTM) and Short-Term Memory (STM). We also form robot's behavior and knowledge system by linking these memories to Autonomous Navigation Module (ANM), Learning Module (LM), and Behavior Planner Module (BPM). The proposed framework is assessed through simulation using ROS-based Gazebo-simulated mobile robot, RGB-D camera (3D sensor) and a laser range finder (2D sensor) in 3D model of realistic indoor environment. Simulation corroborates the substantial practical merit of our proposed framework.},
keywords = {control engineering computing;laser ranging;mobile robots;navigation;path planning;robot vision;robots;SLAM (robots);},
note = {real-time autonomous robot navigation;on-demand databases;robot interaction;task planning;global dynamic environment;brain GPS mapping system;performs 3D visual semantic SLAM;independent robot navigation;memory system;knowledge system;Autonomous Navigation Module;Behavior Planner Module;ROS-based Gazebo-simulated mobile robot;3D sensor;2D sensor;realistic indoor environment;realtime autonomous robot navigation framework;high-level interaction;},
} 


@inproceedings{17058118 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {SLAMinDB: Centralized graph databases for mobile robotics},
journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Fourie, D. and Claassens, S. and Pillai, S. and Mata, R. and Leonard, J.},
year = {2017//},
pages = {6331 - 7},
address = {Piscataway, NJ, USA},
abstract = {Robotic systems typically require memory recall mechanisms for a variety of tasks including localization, mapping, planning, visualization etc. We argue for a novel memory recall framework that enables more complex inference schemas by separating the computation from its associated data. In this work we propose a shared, centralized data persistence layer that maintains an ensemble of online, situationally-aware robot states. This is realized through a queryable graph-database with an accompanying key-value store for larger data. In turn, this approach is scalable and enables a multitude of capabilities such as experience-based learning and long-term autonomy. Using multi-modal simultaneous localization and mapping and a few example use-cases, we demonstrate the versatility and extensible nature that centralized persistence and SLAMinDB can provide. In order to support the notion of life-long autonomy, we envision robots to be endowed with such a persistence model, enabling them to revisit previous experiences and improve upon their existing task-specific capabilities.},
keywords = {control engineering computing;database management systems;graph theory;mobile robots;SLAM (robots);},
note = {SLAMinDB;centralized graph databases;mobile robotics;memory recall mechanisms;complex inference schemas;shared centralized data persistence layer;online situationally-aware robot states;queryable graph-database;key-value store;experience-based learning;long-term autonomy;multimodal simultaneous localization and mapping;persistence model;},
URL = {http://dx.doi.org/10.1109/ICRA.2017.7989749},
} 


@article{18567598 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Navigation for Indoor Robot: Straight Line Movement via Navigator},
journal = {Mathematical Problems in Engineering},
journal = {Math. Probl. Eng. (UK)},
author = {Chaozheng Zhu and Ming He and Pan Chen and Kang Sun and Jinglei Wang and Qian Huang},
volume = { 2018},
year = {2018//},
pages = {8419384 (10 pp.) - },
issn = {1024-123X},
address = {UK},
abstract = {Due to the need of zigzag overlay strategy, long-term linear motion is essential for sweep robot. However, the existing indoor sweep robot navigation algorithm has many problems; for instance, algorithm with high complexity demands high hardware performance and is incapable of working at night. To overcome those problems, in this paper, a new method for indoor robot Straight Line Movement via Navigator (SLMN) is proposed to ensure long linear motion of robot with an acceptable error threshold and realize multiroom navigation. Firstly, in a short time, robot runs a suitable distance when it is covered by navigator's ultrasonic sensor. We can obtain a triangle with twice the distance between navigator and robot and the distance of robot motion. The forward angle of the robot can be conveniently obtained by the trigonometric functions. Comparing the robot's current angle with expected angle, the robot could correct itself and realize the indoor linear navigation. Secondly, discovering dozens of the magnitude gaps between the distance of robot run and the distance between navigator and robot, we propose an optimized method using approximate scaling which increases efficiency by nearly 70.8%. Finally, to realize multiroom navigation, we introduce the conception of the depth-first search stack and a unique encode rule on rooms and navigators. It is proved by extensive quantitative evaluations that the proposed method realizes indoor full coverage at a lower cost than other state-of-the-art indoor vision navigation schemes, such as ORB-SLAM.},
keywords = {mobile robots;navigation;path planning;robot vision;SLAM (robots);},
note = {long linear motion;indoor robot Straight Line Movement;high complexity demands high hardware performance;existing indoor sweep robot navigation algorithm;long-term linear motion;state-of-the-art indoor vision navigation schemes;navigators;rooms;multiroom navigation;navigator;robot run;indoor linear navigation;robot motion;},
URL = {http://dx.doi.org/10.1155/2018/8419384},
} 


@article{11573958 ,
language = {English},
copyright = {Copyright 2010, The Institution of Engineering and Technology},
title = {Position-invariant robust features for long-term recognition of dynamic outdoor scenes},
journal = {IEICE Transactions on Information and Systems},
journal = {IEICE Trans. Inf. Syst. (Japan)},
author = {Kawewong, A. and Tangruamsub, S. and Hasegawa, O.},
volume = { E93-D},
number = { 9},
year = {2010//},
pages = {2587 - 601},
issn = {0916-8532},
address = {Japan},
abstract = {A novel Position-Invariant Robust Feature, designated as PIRF, is presented to address the problem of highly dynamic scene recognition. The PIRF is obtained by identifying existing local features (i.e. SIFT) that have a wide baseline visibility within a place (one place contains more than one sequential images). These wide-baseline visible features are then represented as a single PIRF, which is computed as an average of all descriptors associated with the PIRF. Particularly, PIRFs are robust against highly dynamical changes in scene: a single PIRF can be matched correctly against many features from many dynamical images. This paper also describes an approach to using these features for scene recognition. Recognition proceeds by matching an individual PIRF to a set of features from test images, with subsequent majority voting to identify a place with the highest matched PIRF. The PIRF system is trained and tested on 2000+ outdoor omnidirectional images and on COLD datasets. Despite its simplicity, PIRF offers a markedly better rate of recognition for dynamic outdoor scenes (ca. 90%) than the use of other features. Additionally, a robot navigation system based on PIRF (PIRF-Nav) can outperform other incremental topological mapping methods in terms of time (70% less) and memory. The number of PIRFs can be reduced further to reduce the time while retaining high accuracy, which makes it suitable for long-term recognition and localization.},
keywords = {image recognition;},
note = {position-invariant robust features;long-term recognition;dynamic outdoor scenes;scale invariant feature transformation;topological mapping;},
URL = {http://dx.doi.org/10.1587/transinf.E93.D.2587},
} 


@article{10766102 ,
language = {Chinese},
copyright = {Copyright 2009, The Institution of Engineering and Technology},
title = {Consistency of simultaneous localization and map building (SLAM) with Rao-Blackwellised particle filter},
journal = {Journal of System Simulation},
journal = {J. Syst. Simul. (China)},
author = {Guo Jian-hui and Zhao Chun-xia and Lu Jian-feng and Kang Liang},
volume = { 20},
number = { 23},
year = {2008/12/},
pages = {6401 - 5},
issn = {1004-731X},
address = {China},
abstract = {Rao-Blackwellised particle filtering (RBPF) SLAM is a linear time algorithm proportional to number of landmarks, and the algorithm has obvious computational superiority for dense map or large-scale SLAM, but is inconsistent for long-term. The inconsistency problem of the algorithm was analyzed using the normalized estimation error square (NEES). The result shows that it is a sample impoverishment of particle filter which causes the inconsistency. So it is necessary to reduce the impact of resampling. Auxiliary particle filtering and regularized particle filtering were used to improve the RBPF SLAM in order to obtain consistent RBPF SLAM. Finally, plentiful Monte-Carlo simulations were carried out to evaluate the algorithm's performance and the simulation results indicate that the methods are valid.},
keywords = {Monte Carlo methods;particle filtering (numerical methods);SLAM (robots);},
note = {simultaneous localization and map building;Rao-Blackwellised particle filter;large-scale SLAM;linear time algorithm;normalized estimation error square;Monte-Carlo simulations;},
} 


@article{16270088 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {True scaled 6 DoF egocentric localisation with monocular wearable systems},
journal = {Image and Vision Computing},
journal = {Image Vis. Comput. (Netherlands)},
author = {Gutierrez-Gomez, D. and Guerrero, J.J.},
volume = { 52},
year = {2016/08/},
pages = {178 - 94},
issn = {0262-8856},
address = {Netherlands},
abstract = {In this work we present a novel approach to obtain scaled odometry and map estimates when performing monocular SLAM with wearable cameras. After proving first that the oscillation of the body during walking can be observed in the odometric estimate from a monocular SLAM algorithm, we develop a method to estimate the walking speed from the frequency of this oscillation. Having the real walking speed, a scale factor can be dynamically computed to obtain a true scaled estimate of the map and visual odometry, avoiding scale drift on long term trajectories. Although the algorithm requires the person to be walking in order to estimate the scale, the experiments, carried out in outdoor and indoor environments and with different types of cameras, show that our method is reliable and robust to challenging situations like stops, changes in pace or stairs, and provides a significant improvement with respect to the initial unscaled estimate. It also outperforms state-of-the-art solutions to correct the scale drift in monocular SLAM, giving in addition the absolute scale of the trajectory and the 3D observed scene. [All rights reserved Elsevier].},
keywords = {cameras;estimation theory;oscillations;robot vision;SLAM (robots);},
note = {6DoF egocentric localization;monocular wearable camera;simultaneous localization and mapping;monocular SLAM algorithm;walking speed estimation;oscillation frequency;visual odometry;},
URL = {http://dx.doi.org/10.1016/j.imavis.2016.05.015},
} 


@inproceedings{19987497 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Day and Night Collaborative Dynamic Mapping in Unstructured Environment Based on Multimodal Sensors},
journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Yufeng Yue and Chule Yang and Jun Zhang and Mingxing Wen and Zhenyu Wu and Haoyuan Zhang and Danwei Wang},
year = {2020//},
pages = {2981 - 7},
address = {Piscataway, NJ, USA},
abstract = {Enabling long-term operation during day and night for collaborative robots requires a comprehensive understanding of the unstructured environment. Besides, in the dynamic environment, robots must be able to recognize dynamic objects and collaboratively build a global map. This paper proposes a novel approach for dynamic collaborative mapping based on multimodal environmental perception. For each mission, robots first apply heterogeneous sensor fusion model to detect humans and separate them to acquire static observations. Then, the collaborative mapping is performed to estimate the relative position between robots and local 3D maps are integrated into a globally consistent 3D map. The experiment is conducted in the day and night rainforest with moving people. The results show the accuracy, robustness, and versatility in 3D map fusion missions.},
keywords = {groupware;image fusion;mobile robots;multi-robot systems;path planning;sensor fusion;SLAM (robots);},
note = {dynamic collaborative mapping;multimodal environmental perception;heterogeneous sensor fusion model;local 3D maps;night rainforest;3D map fusion missions;multimodal sensors;long-term operation;collaborative robots;dynamic environment;dynamic objects;},
URL = {http://dx.doi.org/10.1109/ICRA40945.2020.9197072},
} 


@article{20115767 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Persistent Stereo Visual Localization on Cross-Modal Invariant Map},
journal = {IEEE Transactions on Intelligent Transportation Systems},
journal = {IEEE Trans. Intell. Transp. Syst. (USA)},
author = {Xiaqing Ding and Yue Wang and Rong Xiong and Dongxuan Li and Li Tang and Huan Yin and Liang Zhao},
volume = { 21},
number = { 11},
year = {2020/11/},
pages = {4646 - 58},
issn = {1524-9050},
address = {USA},
abstract = {Autonomous mobile vehicles are expected to perform persistent and accurate localization with low-cost equipment. To achieve this goal, we propose a stereo camera based visual localization method using a modified laser map, which takes the advantage of both the low cost of camera, and high geometric precision of laser data to achieve long-term performance. Considering that LiDAR and camera give measurements of the same environment in different modalities, the cross-modal invariance is investigated to modify the laser map for visual localization. Specifically, a map learning algorithm is introduced to sample the robust subsets in laser maps that are useful for visual localization using multi-session visual and laser data. Further, a generative map model is derived to describe this cross-modal invariance, based on which two types of measurements are defined to model the laser map points as appropriate visual observations. Tightly coupling these measurements within the local bundle adjustment during online sliding-window based visual odometry, the vehicle can achieve robust localization even one year after the map was built. The effectiveness of the proposed method is evaluated on both the public KITTI datasets and self-collected datasets in our campus, which include seasonal, illumination and object variations. On all experimental localization sessions, our method provides satisfactory results, even when the direction is opposite to that in the mapping session, verifying the superior performance of the laser map based visual localization method.},
keywords = {cameras;distance measurement;invariance;learning (artificial intelligence);mobile robots;path planning;road traffic control;robot vision;SLAM (robots);stereo image processing;traffic engineering computing;},
note = {persistent stereo visual localization;cross-modal invariant map;autonomous mobile vehicles;low-cost equipment;stereo camera;laser data;cross-modal invariance;map learning algorithm;multisession visual;generative map model;laser map points;local bundle adjustment;online sliding-window based visual odometry;public KITTI datasets;self-collected datasets;},
URL = {http://dx.doi.org/10.1109/TITS.2019.2942760},
} 


@inproceedings{17428227 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Model-aided monocular visual-inertial state estimation and dense mapping},
journal = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Qiu, K. and Shen, S.},
year = {2017//},
pages = {1783 - 9},
address = {Piscataway, NJ, USA},
abstract = {Robust state estimation and real-time dense mapping are two core capabilities for autonomous navigation of mobile robots. Global Navigation Satellite System (GNSS) and visual odometry/SLAM are popular methods for state estimation. However, when working between tall buildings or in indoor environments, GNSS fails due to limited sky view or obstruction from buildings. Visual odometry/SLAM are prone to long-term drifting in the absence of reliable loop closure detection. A state estimation method with global-consistent guarantee is desirable for navigation applications. As for real-time mapping, SLAM methods usually get a sparse map that is not good enough for obstacle avoidance and path-planning, and high-quality dense mapping is often computationally too demanding for mobile devices. Realizing the availability of city-scale 3D models, in this work, we improve our previous work on model-based global localization, and propose a model-aided monocular visual-inertial state estimation and dense mapping solution. We first develop a global-consistent state estimator by fusing visual-inertial odometry with the model-based localization results. Utilizing depth prior from the model, we perform motion stereo with semi-global disparity smoothing. Our dense mapping pipeline is capable of online detection of obstacles that are originally not included in the offline 3D model. Our method runs onboard an embedded computer in real-time. We validate both the state estimation and mapping accuracy in real-world experiments.},
keywords = {collision avoidance;image motion analysis;mobile robots;robot vision;SLAM (robots);state estimation;stereo image processing;},
note = {visual-inertial odometry;model-based localization results;semiglobal disparity smoothing;dense mapping pipeline;visual-inertial state estimation;robust state estimation;real-time dense mapping;autonomous navigation;Global Navigation Satellite System;GNSS;visual odometry/SLAM;reliable loop closure detection;state estimation method;navigation applications;real-time mapping;SLAM methods;high-quality dense mapping;city-scale 3D models;global localization;dense mapping solution;global-consistent state estimator;},
URL = {http://dx.doi.org/10.1109/IROS.2017.8205992},
} 


@inproceedings{20195130 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Global Alignment of Deep Features for Robot Localization in Changing Environment},
journal = {2019 3rd European Conference on Electrical Engineering and Computer Science (EECS)},
author = {Oh, J.H. and Heung-Jae Lee},
year = {2019//},
pages = {72 - 5},
address = {Piscataway, NJ, USA},
abstract = {Localization is an elemental requirement for autonomous navigation, simultaneous localization and mapping for mobile robots. As robots can perform long-term and large-scale tasks, finding locations in changing environment is a crucial problem. To resolve the problem, we present a robust localization system under severe appearance changes. The system consists of two stages. First, a robust feature extraction method using a deep convolutional auto-encoder is proposed. Then, global alignment of extracted feature sequences is proposed to find the actual robot's locations. Since the proposed method not only uses the condition-robust features but also considers the actual trajectory of the robot by aligning features sequences, it can show accurate localization performances in changing environments. Experiments were conducted to prove the effective of the proposed method, and the results showed that our method outperformed than existing methods.},
keywords = {control engineering computing;convolutional neural nets;feature extraction;image sequences;mobile robots;navigation;robot vision;SLAM (robots);},
note = {simultaneous localization and mapping;localization performances;condition-robust features;feature sequence extraction;deep convolutional auto-encoder;robust feature extraction method;robust localization system;large-scale tasks;mobile robots;autonomous navigation;robot localization;deep features;global alignment;},
URL = {http://dx.doi.org/10.1109/EECS49779.2019.00026},
} 


@article{12401560 ,
language = {English},
copyright = {Copyright 2012, The Institution of Engineering and Technology},
title = {Long-term experiments with an adaptive spherical view representation for navigation in changing environments},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Dayoub, F. and Cielniak, G. and Duckett, T.},
volume = { 59},
number = { 5},
year = {2011/05/},
pages = {285 - 95},
issn = {0921-8890},
address = {Netherlands},
abstract = {Real-world environments such as houses and offices change over time, meaning that a mobile robot's map will become out of date. In this work, we introduce a method to update the reference views in a hybrid metric-topological map so that a mobile robot can continue to localize itself in a changing environment. The updating mechanism, based on the multi-store model of human memory, incorporates a spherical metric representation of the observed visual features for each node in the map, which enables the robot to estimate its heading and navigate using multi-view geometry, as well as representing the local 3D geometry of the environment. A series of experiments demonstrate the persistence performance of the proposed system in real changing environments, including analysis of the long-term stability. [All rights reserved Elsevier].},
keywords = {mobile robots;path planning;},
note = {adaptive spherical view representation;changing environment navigation;real world environments;mobile robots;hybrid metric topological map;human memory;spherical metric representation;3D geometry;},
URL = {http://dx.doi.org/10.1016/j.robot.2011.02.013},
} 


@inproceedings{16776669 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Consistent Cuboid Detection for Semantic Mapping},
journal = {2017 IEEE 11th International Conference on Semantic Computing (ICSC)},
author = {Hashemifar, Z.S. and Kyung Won Lee and Napp, N. and Dantu, K.},
year = {2017//},
pages = {526 - 31},
address = {Los Alamitos, CA, USA},
abstract = {Building and storing efficient maps is an essential feature for long-term autonomy of robots. Modern sensors (such as Kinect) tend to produce a lot of data. However, long-term autonomy requires us to store this information in a succinct manner. One way to reduce dimensionality of information is to attribute semantics. Most indoor objects are cuboidal in nature. We conjecture that cuboids are a suitable semantic feature to attribute to indoor objects for efficient mapping. We adapt a cuboid fitting algorithm previously proposedfor object recognition, for indoor mapping. Our work stems from the observation that landmark detection for mappingrequires consistent detection of those landmarks. We implement several modifications to this cuboid detection algorithm that lead to consistent detection such as emptiness, orientation, surface coverage, distance from edges, and others. We incorporate these in the identification of the cuboid candidates in a scene, as well as an optimization algorithm for finding the best set of consistent cubes to cover a given scene. Our experiments show that in comparison, the set of cuboids detected by our algorithm are at least 50% more consistent based on our metrics.SLAM.},
keywords = {object recognition;optimisation;SLAM (robots);},
note = {semantic mapping;cuboid fitting algorithm;landmark detection;cuboid detection algorithm;optimization algorithm;SLAM;},
URL = {http://dx.doi.org/10.1109/ICSC.2017.78},
} 


@article{20103294 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {A Samplable Multimodal Observation Model for Global Localization and Kidnapping [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Runjian Chen and Yue Wang and Huan Yin and Yanmei Jiao and Dissanayake, G. and Rong Xiong},
year = {2020/08/31},
pages = {15 pp. - },
address = {USA},
abstract = {Global localization and kidnapping are two challenging problems in robot localization. The popular method, Monte Carlo Localization (MCL) addresses the problem by sampling uniformly over the state space, which is unfortunately inefficient when the environment is large. To better deal with the the problems, we present a proposal model, named Deep Multimodal Observation Model (DMOM). DMOM takes a map and a 2D laser scan as inputs and outputs a conditional multimodal probability distribution of the pose, making the samples more focusing on the regions with higher likelihood. With such samples, the convergence is expected to be much efficient. Considering that learning based Samplable Observation Model may fail to capture the true pose sometimes, we furthermore propose the ADAPTIVE MIXTURE MCL, which adaptively selects updating mode for each particle to tolerate this situation. Equipped with DMOM, ADAPTIVE MIXTURE MCL can achieve more accurate estimation, faster convergence and better scalability compared with previous methods in both synthetic and real scenes. Even in real environment with long-term changing, ADAPTIVE MIXTURE MCL is able to localize the robot using DMON trained only on simulated observations from a SLAM map, or even a blueprint map.},
keywords = {learning (artificial intelligence);mobile robots;Monte Carlo methods;path planning;probability;SLAM (robots);statistical distributions;},
note = {samplable observation model;samplable multimodal observation model;Monte Carlo localization;named deep multimodal observation model;proposal model;state space;robot localization;kidnapping;global localization;simulated observations;ADAPTIVE MIXTURE MCL;conditional multimodal probability distribution;2D laser scan;DMOM;},
} 


@article{21162558 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Three-dimensional guidance and control for ground moving target tracking by a quadrotor},
journal = {Aeronautical Journal},
journal = {Aeronaut. J. (UK)},
author = {Sepehri Movafegh, M. and Dehghan, S.M.M. and Zardashti, R.},
volume = { 125},
number = { 1290},
year = {2021//},
pages = {1380 - 407},
issn = {0001-9240},
address = {UK},
abstract = {This paper develops a three-dimensional guidance and control algorithm to ensure that a manoeuverable target is preserved by a quadrotor in a long-term tracking scenario. The proposed guidance approach determines the desired altitude of the quadrotor to adjust the field of view (FOV) to the union of two desired trusted and critical regions. The dimensions of the desired trusted region depend on the controller performance that is evaluated by the distance of the target from the center of the FOV. The critical region is a predefined margin around the trusted region that is defined by the operator based on the upper bounds of the quadrotor and target localisation errors. It also depends on the duration and magnitude of the temporal increase in the target velocity compared to the quadrotor velocity. A sufficient condition is provided for the minimum desired altitude of the quadrotor to ensure that the target is maintained in the FOV. Furthermore, a model predictive control (MPC) is employed to preserve the target at the center of the aerial image and the desired altitude determined by the guidance law. Also, the integrals of the position errors are used to achieve null steady-state errors in the presence of wind disturbances. The simulation results show the effectiveness of the proposed approach in preserving the manoeuverable target in the FOV in the presence of the wind, the uncertainty of the target and quadrotor localisation, accelerations estimation errors, and terrain altitude variation.},
keywords = {aircraft control;autonomous aerial vehicles;helicopters;mobile robots;optimal control;position control;predictive control;robot vision;SLAM (robots);target tracking;velocity control;},
note = {three-dimensional guidance and control;quadrotor localisation;guidance law;model predictive control;quadrotor velocity;target velocity;target localisation errors;trusted region;FOV;long-term tracking;manoeuverable target;control algorithm;target tracking;},
URL = {http://dx.doi.org/10.1017/aer.2021.23},
} 


@inproceedings{18417691 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A real-time visual-inertial mapping and localization method by fusing unstable GPS},
journal = {2018 13th World Congress on Intelligent Control and Automation (WCICA). Proceedings},
author = {Zhongyuan Zhang and Hesheng Wang and Weidong Chen},
year = {2018//},
pages = {1397 - 402},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a novel method which fuse visual, IMU and GPS tightly to realize high-precision real-time localization and mapping simultaneously (SLAM). Our method is based on the bundle adjustment (BA). The confidence of the GPS signal is used to determine the window size in the local mapping thread and judge whether the keyframe is reliable. The long-term unreliable keyframe linking with large uncertainty of GPS which called GPS-restricted or GPS-denied situation will cause the drift when mapping. To eliminate the drift, in contrast to use the closed-loop detection and global optimization which will increase the computational burden extremely with the size of the map enlarged, a semi-global optimization method is proposed to relieve the burden, which make the localization estimated by this method possible to be used to navigate for unmanned vehicles. In our method, the confidence of the GPS signal is significantly important, however, the covariance supplied by the GPS receiver may not be trustworthy sometimes, which cause some unnecessary mistake when optimizing, thus a semi-supervised clustering method taking the information of GPS and IMU into account synthetically is introduced to get that confidence more robustly.},
keywords = {distance measurement;Global Positioning System;mobile robots;path planning;pattern clustering;SLAM (robots);},
note = {GPS signal;window size;local mapping thread;GPS-denied situation;closed-loop detection;global optimization;semiglobal optimization method;GPS receiver;semisupervised clustering method;IMU;real-time visual-inertial mapping;localization method;unstable GPS;high-precision real-time localization;bundle adjustment;GPS-restricted situation;},
URL = {http://dx.doi.org/10.1109/WCICA.2018.8630513},
} 


@inproceedings{19283967 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Mobile robot localization based on low-cost LTE and odometry in GPS-denied outdoor environment},
journal = {2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
author = {Ismail, K. and Ran Liu and Jie Zheng and Chau Yuen and Yong Liang Guan and U-Xuan Tan},
year = {2019//},
pages = {2338 - 43},
address = {Piscataway, NJ, USA},
abstract = {GPS localization has always been the go-to method for localizing mobile robots in outdoor environments. However, in GPS denied environments such as urban canyons, LTE becomes a better alternative. LTE localization exploits existing infrastructures and transmitted signals to provide an estimated position of the robot. As a low-cost solution, it benefits robots under the constraint of cost, size and weight. This paper proposes a particle filter based localization method by using only LTE and wheel odometry for GPS-denied outdoor environments. We used the fingerprinting method by obtaining LTE Cell ID, mean RSS and GPS location and associating these data to the grids in an initialized map. We resolved the position of the robot using recursive Bayesian estimation and particle filter for its implementation. In our experiment, we used a mobile robot and five smartphones to obtain the wheel odometry and LTE data respectively while travelling along a particular route in an outdoor environment. The method was able to obtain accurate localization results with RMSE of 13.07m. We further evaluated the parameters of the method effects on the localization accuracy achieved.},
keywords = {Bayes methods;distance measurement;indoor radio;Long Term Evolution;mobile robots;particle filtering (numerical methods);recursive estimation;RSSI;},
note = {wheel odometry;GPS-denied outdoor environment;fingerprinting method;LTE Cell ID;LTE data;accurate localization results;localization accuracy;mobile robot localization;low-cost LTE;GPS localization;LTE localization;low-cost solution;particle filter based localization method;},
URL = {http://dx.doi.org/10.1109/ROBIO49542.2019.8961750},
} 


@inproceedings{18994624 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Obstacle Persistent Adaptive Map Maintenance for Autonomous Mobile Robots using Spatio-temporal Reasoning},
journal = {2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)},
author = {Pitschl, M.L. and Pryor, M.W.},
year = {2019//},
pages = {1023 - 8},
address = {Piscataway, NJ, USA},
abstract = {Mobile robotic systems operate in increasingly realistic scenarios even as users have increased expectations for the duration of autonomous tasks. Mobile robots face unique challenges when operating in environments that change over time, where systems must maintain an accurate representation of the environment with respect to both spatial and temporal dimensions. This paper describes a spatio-temporal technique for extending the autonomy of a mobile robot in a changing environment. This new technique called Obstacle Persistent Adaptive Map Maintenance (OPAMM) uses navigation data collected during normal operations to perform periodic self-maintenance of its environment model. OPAMM implements a probabilistic feature persistence model to predict the survival state of obstacles and update the world model. Maintaining an accurate world model is necessary for extending the long-term autonomy of robots in realistic scenarios. Results show that robots using OPAMM had localizations scores higher than other methods, thus reducing long-term localization degradation.},
keywords = {mobile robots;temporal reasoning;},
note = {changing environment;OPAMM;periodic self-maintenance;environment model;probabilistic feature persistence model;autonomous mobile robots;spatio-temporal reasoning;mobile robotic systems;autonomous tasks;spatial dimensions;temporal dimensions;spatio-temporal technique;obstacle persistent adaptive map maintenance;long-term localization degradation;},
URL = {http://dx.doi.org/10.1109/COASE.2019.8843095},
} 


@inproceedings{13919302 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Towards Persistent Localization and Mapping with a Continuous Appearance-Based Topology},
journal = {2012 Robotics: Science and Systems},
author = {Maddern, W. and Milford, M. and Wyeth, G.},
volume = {vol.8},
year = {2013//},
pages = {302 - 9},
address = {Cambridge, MA, USA},
abstract = {Appearance-based localization can provide loop closure detection at vast scales regardless of accumulated metric error. However, the computation time and memory requirements of current appearance-based methods scale not only with the size of the environment but also with the operation time of the platform. Additionally, repeated visits to locations will develop multiple competing representations, which will reduce recall performance over time. These properties impose severe restrictions on long-term autonomy for mobile robots, as loop closure performance will inevitably degrade with increased operation time. In this paper we present a graphical extension to CAT-SLAM, a particle filter-based algorithm for appearance-based localization and mapping, to provide constant computation and memory requirements over time and minimal degradation of recall performance during repeated visits to locations. We demonstrate loop closure detection in a large urban environment with capped computation time and memory requirements and performance exceeding previous appearance-based methods by a factor of 2. We discuss the limitations of the algorithm with respect to environment size, appearance change over time and applications in topological planning and navigation for long-term robot operation.},
keywords = {mobile robots;particle filtering (numerical methods);path planning;SLAM (robots);},
note = {topological navigation;long-term robot operation;topological planning;memory requirements;appearance- based localization and mapping;particle filter-based algorithm;CAT-SLAM;loop closure performance;mobile robots;loop closure detection;continuous appearance-based topology;persistent localization and mapping;},
} 


@inproceedings{18903518 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {MRS-VPR: a multi-resolution sampling based global visual place recognition method},
journal = {2019 International Conference on Robotics and Automation (ICRA)},
author = {Peng Yin and Srivatsan, R.A. and Yin Chen and Xueqian Li and Hongda Zhang and Lingyun Xu and Lu Li and Zhenzhong Ji and Jianmin Ji and Yuqing He},
year = {2019//},
pages = {7137 - 42},
address = {Piscataway, NJ, USA},
abstract = {Place recognition and loop closure detection are challenging for long-term visual navigation tasks. SeqSLAM is considered to be one of the most successful approaches to achieve long-term localization under varying environmental conditions and changing viewpoints. SeqSLAM uses a brute-force sequential matching method, which is computationally intensive. In this work, we introduce a multi-resolution sampling-based global visual place recognition method (MRS-VPR), which can significantly improve the matching efficiency and accuracy in sequential matching. The novelty of this method lies in the coarse-to-fine searching pipeline and a particle filter-based global sampling scheme, that can balance the matching efficiency and accuracy in the long-term navigation task. Moreover, our model works much better than SeqSLAM when the testing sequence is over a much smaller time scale than the reference sequence. Our experiments demonstrate that MRSVPR is efficient in locating short temporary trajectories within long-term reference ones without compromising on the accuracy compared to SeqSLAM.},
keywords = {image filtering;image matching;image recognition;image resolution;image sampling;image sequences;mobile robots;navigation;object recognition;particle filtering (numerical methods);path planning;robot vision;SLAM (robots);},
note = {global visual place recognition method;multiresolution sampling;particle filter-based global sampling scheme;matching efficiency;brute-force sequential matching method;long-term localization;long-term visual navigation tasks;loop closure detection;MRS-VPR;SeqSLAM;},
URL = {http://dx.doi.org/10.1109/ICRA.2019.8793853},
} 


@article{21027045 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Research on Autonomous Underwater Vehicle Homing Method Based on Fuzzy-Q-FastSLAM},
journal = {Journal of Offshore Mechanics and Arctic Engineering},
journal = {J. Offshore Mech. Arct. Eng. (USA)},
author = {Lingyan Dong and Hongli Xu and Xisheng Feng and Ning Li},
volume = { 143},
number = { 5},
year = {2021//},
pages = {051401 (9 pp.) - },
issn = {0892-7219},
address = {USA},
abstract = {Autonomous docking guidance is one of the key technologies to achieve the autonomous underwater vehicle (AUV) docking with the sub-sea docking station (DS) to realize long-term resident operation. In the process of AUV docking, the combination of long-distance acoustic guidance based on acoustic sensor and terminal visual guidance based on camera is often adopted. However, affected by the accuracy of the navigation sensor and acoustic positioning sensor carried by AUV, as well as the ocean current, AUV cannot accurately know its own position and the position of the DS, resulting in a large acoustic guidance error and the inability to enter the visual guidance stage with a reasonable deviation, thus leading to the docking failure. In this article, an improved FastSLAM algorithm is proposed to estimate the position of AUV and DS simultaneously. The positioning accuracy of traditional FastSLAM algorithm is affected by such factors as the estimation accuracy of the statistical characteristics of process noise. An improved algorithm for FastSLAM based on fuzzy Q-learning is proposed. The homing path is planned based on the Dubins theory. The path is tracked by line-of-sight guidance. The results of matlab simulation and experimental data analyzing of the portable AUV are applied to verify the effectiveness of the proposed algorithm.},
keywords = {autonomous underwater vehicles;mobile robots;navigation;path planning;position control;robot vision;SLAM (robots);},
note = {vehicle homing method;fuzzy-Q-FastSLAM;autonomous docking guidance;autonomous underwater vehicle docking;sub-sea docking station;AUV docking;long-distance acoustic guidance;acoustic sensor;terminal visual guidance;navigation sensor;acoustic positioning sensor;acoustic guidance error;visual guidance stage;docking failure;positioning accuracy;estimation accuracy;homing path;line-of-sight guidance;portable AUV;},
URL = {http://dx.doi.org/10.1115/1.4049325},
} 


@article{18623538 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Marker-Based Multi-Sensor Fusion Indoor Localization System for Micro Air Vehicles},
journal = {Sensors},
journal = {Sensors (Switzerland)},
author = {Boyang Xing and Quanmin Zhu and Feng Pan and Xiaoxue Feng},
volume = { 18},
number = { 6},
year = {2018/06/},
pages = {1706 (19 pp.) - },
issn = {1424-8220},
address = {Switzerland},
abstract = {A novel multi-sensor fusion indoor localization algorithm based on ArUco marker is designed in this paper. The proposed ArUco mapping algorithm can build and correct the map of markers online with Grubbs criterion and K-mean clustering, which avoids the map distortion due to lack of correction. Based on the conception of multi-sensor information fusion, the federated Kalman filter is utilized to synthesize the multi-source information from markers, optical flow, ultrasonic and the inertial sensor, which can obtain a continuous localization result and effectively reduce the position drift due to the long-term loss of markers in pure marker localization. The proposed algorithm can be easily implemented in a hardware of one Raspberry Pi Zero and two STM32 micro controllers produced by STMicroelectronics (Geneva, Switzerland). Thus, a small-size and low-cost marker-based localization system is presented. The experimental results show that the speed estimation result of the proposed system is better than Px4flow, and it has the centimeter accuracy of mapping and positioning. The presented system not only gives satisfying localization precision, but also has the potential to expand other sensors (such as visual odometry, ultra wideband (UWB) beacon and lidar) to further improve the localization performance. The proposed system can be reliably employed in Micro Aerial Vehicle (MAV) visual localization and robotics control.},
keywords = {aerospace robotics;autonomous aerial vehicles;image fusion;image sequences;Kalman filters;microrobots;mobile robots;object tracking;pose estimation;robot vision;SLAM (robots);},
note = {STM32 microcontrollers;low-cost marker-based localization system;positioning;localization precision;localization performance;robotics control;Microair vehicles;ArUco marker;ArUco mapping algorithm;map distortion;multisensor information fusion;inertial sensor;marker localization;multisensor fusion indoor localization algorithm;Raspberry Pi Zero;STM32 micro controllers;STMicroelectronics;Grubbs criterion;K-mean clustering;federated Kalman filter;},
URL = {http://dx.doi.org/10.3390/s18061706},
} 


@article{19932273 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Robot-Assisted Backscatter Localization for IoT Applications},
journal = {IEEE Transactions on Wireless Communications},
journal = {IEEE Trans. Wirel. Commun. (USA)},
author = {Shengkai Zhang and Wei Wang and Sheyang Tang and Shi Jin and Tao Jiang},
volume = { 19},
number = { 9},
year = {Sept. 2020},
pages = {5807 - 18},
issn = {1536-1276},
address = {USA},
abstract = {Recent years have witnessed the rapid proliferation of backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such backscatter tags is crucial for IoT-based smart applications. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, which is laborious for deployment. To empower universal localization service, this paper presents Rover, an indoor localization system that localizes multiple backscatter tags without any start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing measurements from backscattered WiFi signals and inertial sensors to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues including interference among multiple tags, real-time processing, as well as the data marginalization problem in dealing with degenerated motions. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords = {indoor radio;Internet of Things;mobile robots;optimisation;radiofrequency identification;sensor placement;wireless LAN;wireless sensor networks;},
note = {robot-assisted backscatter localization;IoT applications;rapid proliferation;backscatter technologies;ubiquitous term connectivity;long-term connectivity;smart cities;smart homes;IoT-based smart applications;current backscatter localization systems;map;known positions;universal localization service;indoor localization system;multiple backscatter tags;inertial sensors;joint optimization framework;backscattered WiFi signals;connected tags;design addresses practical issues including interference;multiple tags;prototype Rover;localization accuracies;size 74.6 cm;size 39.3 cm;},
URL = {http://dx.doi.org/10.1109/TWC.2020.2997393},
} 


@article{19784652 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Robot-assisted Backscatter Localization for IoT Applications [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Shengkai Zhang and Wei Wang and Sheyang Tang and Shi Jin and Tao Jiang},
year = {2020/05/21},
pages = {13 pp. - },
address = {USA},
abstract = {Recent years have witnessed the rapid proliferation of backscatter technologies that realize the ubiquitous and long-term connectivity to empower smart cities and smart homes. Localizing such backscatter tags is crucial for IoT-based smart applications. However, current backscatter localization systems require prior knowledge of the site, either a map or landmarks with known positions, which is laborious for deployment. To empower universal localization service, this paper presents Rover, an indoor localization system that localizes multiple backscatter tags without any start-up cost using a robot equipped with inertial sensors. Rover runs in a joint optimization framework, fusing measurements from backscattered WiFi signals and inertial sensors to simultaneously estimate the locations of both the robot and the connected tags. Our design addresses practical issues including interference among multiple tags, real-time processing, as well as the data marginalization problem in dealing with degenerated motions. We prototype Rover using off-the-shelf WiFi chips and customized backscatter tags. Our experiments show that Rover achieves localization accuracies of 39.3 cm for the robot and 74.6 cm for the tags.},
keywords = {indoor radio;Internet of Things;mobile robots;optimisation;radiofrequency identification;sensor placement;wireless LAN;wireless sensor networks;},
note = {backscattered WiFi signals;joint optimization framework;inertial sensors;multiple backscatter tags;indoor localization system;universal localization service;known positions;map;current backscatter localization systems;IoT-based smart applications;smart homes;smart cities;long-term connectivity;ubiquitous term connectivity;backscatter technologies;rapid proliferation;IoT applications;robot-assisted backscatter localization;localization accuracies;prototype Rover;multiple tags;design addresses practical issues including interference;connected tags;size 74.6 cm;size 39.3 cm;},
} 


@inproceedings{20425215 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Multi-Robot Joint Visual-Inertial Localization and 3-D Moving Object Tracking},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Pengxiang Zhu and Wei Ren},
year = {2020//},
pages = {11573 - 80},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we present a novel distributed algorithm to track a moving object's state by utilizing a heterogenous mobile robot network in a three-dimensional (3-D) environment, wherein the robots' poses (positions and orientations) are unknown. Each robot is equipped with a monocular camera and an inertial measurement unit (IMU), and has the ability to communicate with its neighbors. Rather than assuming a known common global frame for all the robots (which is often the case in the literature regarding multi-robot systems), we allow each robot to perform motion estimation locally. For localization, we propose a multi-robot visual-inertial navigation systems (VINS) where one robot builds a prior map and then the map is used to bound the long-term drifts of the visual-inertial odometry (VIO) running on the other robots. Moreover, a novel distributed Kalman filter is introduced and employed to cooperatively track the six degree-of-freedom (6-DoF) motion of the object which is represented as a point cloud. Further, the object can be totally invisible to some robots during the tracking period. The proposed algorithm is extensively validated in Monte-Carlo simulations.},
keywords = {computational geometry;distance measurement;distributed algorithms;inertial navigation;Kalman filters;mobile robots;Monte Carlo methods;motion estimation;multi-robot systems;object tracking;path planning;pose estimation;position control;robot vision;SLAM (robots);},
note = {visual-inertial odometry;tracking period;multirobot joint visual-inertial localization;distributed algorithm;heterogenous mobile robot network;inertial measurement unit;multirobot visual-inertial navigation systems;3D moving object tracking;three-dimensional environment;monocular camera;IMU;motion estimation;multirobot VINS;distributed Kalman filter;six degree-of-freedom motion;6-DoF motion;point cloud;Monte-Carlo simulations;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9341393},
} 


@inproceedings{21504184 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Geometry-based Graph Pruning for Lifelong SLAM},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Kurz, G. and Holoch, M. and Biber, P.},
year = {2021//},
pages = {3313 - 20},
address = {Piscataway, NJ, USA},
abstract = {Lifelong SLAM considers long-term operation of a robot where already mapped locations are revisited many times in changing environments. As a result, traditional graph-based SLAM approaches eventually become extremely slow due to the continuous growth of the graph and the loss of sparsity. Both problems can be addressed by a graph pruning algorithm. It carefully removes vertices and edges to keep the graph size reasonable while preserving the information needed to provide good SLAM results. We propose a novel method that considers geometric criteria for choosing the vertices to be pruned. It is efficient, easy to implement, and leads to a graph with evenly spread vertices that remain part of the robot trajectory. Furthermore, we present a novel approach of marginalization that is more robust to wrong loop closures than existing methods. The proposed algorithm is evaluated on two publicly available real-world long-term datasets and compared to the unpruned case as well as ground truth. We show that even on a long dataset (25h), our approach manages to keep the graph sparse and the speed high while still providing good accuracy (40 times speed up, 6cm map error compared to unpruned case).},
keywords = {geometry;graph theory;SLAM (robots);trajectory control;},
note = {mapped locations;robot trajectory;real-world long-term datasets;geometry-based graph pruning;lifelong SLAM;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9636530},
} 


@article{21101958 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {A Life-Long SLAM Approach Using Adaptable Local Maps Based on Rasterized LIDAR Images},
journal = {IEEE Sensors Journal},
journal = {IEEE Sens. J. (USA)},
author = {Ali, W. and Peilin Liu and Rendong Ying and Zheng Gong},
volume = { 21},
number = { 19},
year = {2021//},
pages = {21740 - 9},
issn = {1530-437X},
address = {USA},
abstract = {Most real-time autonomous robot applications require a robot to traverse through a dynamic space for a long time. In some cases, a robot needs to work in the same environment. Such applications give rise to the problem of a life-long SLAM system. Life-long SLAM presents two main challenges i.e. the tracking should not fail in a dynamic environment and the need for a robust and efficient mapping strategy. The system should update maps with new information; while also keeping track of older observations. But, mapping for a long time can require higher computational requirements. In this paper, we propose a solution to the problem of life-long SLAM. We represent the global map as a set of rasterized images of local maps along with a map management system responsible for updating local maps and keeping track of older values. We also present an efficient approach of using the bag of visual words method for loop closure detection and relocalization. We evaluate the performance of our system on the KITTI dataset and an indoor dataset. Our loop closure system reported recall and precision of above 90 percent. The computational cost of our system is much lower as compared to state-of-the-art methods. Our method reports lower computational requirements even for long-term operation.},
keywords = {mobile robots;object detection;optical radar;path planning;robot vision;SLAM (robots);},
note = {life-long SLAM system;dynamic environment;robust mapping strategy;efficient mapping strategy;computational requirements;rasterized images;map management system;loop closure system;lower computational requirements;long-term operation;adaptable local maps;rasterized LIDAR;real-time autonomous robot applications;dynamic space;},
URL = {http://dx.doi.org/10.1109/JSEN.2021.3100882},
} 


@inproceedings{21259340 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Hierarchical Loop Closure Detection for Long-term Visual SLAM with Semantic-Geometric Descriptors},
journal = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
author = {Singh, G. and Meiqing Wu and Siew-Kei Lam and Do Van Minh},
year = {2021//},
pages = {2909 - 16},
address = {Piscataway, NJ, USA},
abstract = {Modern visual Simultaneous Localization and Mapping (SLAM) systems rely on loop closure detection methods for correcting drifts in maps and poses. Existing loop closure detection methods mainly employ conventional feature descriptors to create vocabulary for describing places using bag-of-words (BOW). Such methods do not perform well in long-term SLAM applications as the scene content may change over time due to the presence of dynamic objects, even though the locations are revisited with the same viewpoint. This work enhances the loop closure detection capability of long-term visual SLAM by reducing the number of false matches through the use of location semantics. We extend a semantic visual SLAM framework to build compact global semantic-geometric location descriptors and local semantic vocabulary trees, by leveraging on the already available features and semantics. The local semantic vocabulary trees support incremental vocabulary learning, which is well-suited for long-term SLAM scenarios where the scenes encountered are not known beforehand. A novel hierarchical place recognition method that leverages the global and local location semantics is proposed to enable fast and accurate loop closure detection. The proposed method outperforms recent state-of-the-art methods (i.e., FABMAP2, SeqSLAM, iBOW-LCD, and HTMap) on all datasets considered (i.e., KITTI, Synthia, and CBD), with highest loop closure detection accuracy and lowest query time.},
keywords = {feature extraction;image matching;mobile robots;robot vision;SLAM (robots);},
note = {loop closure detection capability;long-term SLAM applications;conventional feature descriptors;loop closure detection methods;Mapping systems;Modern visual Simultaneous Localization;semantic-geometric descriptors;hierarchical loop closure detection;lowest query time;highest loop closure detection accuracy;recent state-of-the-art methods;accurate loop closure detection;local location semantics;global location semantics;novel hierarchical place recognition method;long-term SLAM scenarios;available features;local semantic vocabulary trees;semantic-geometric location descriptors;semantic visual SLAM framework;long-term visual SLAM;},
URL = {http://dx.doi.org/10.1109/ITSC48978.2021.9564866},
} 


@article{21562672 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Accurate Dynamic SLAM Using CRF-Based Long-Term Consistency},
journal = {IEEE Transactions on Visualization and Computer Graphics},
journal = {IEEE Trans. Vis. Comput. Graph. (USA)},
author = {Zheng-Jun Du and Shi-Sheng Huang and Tai-Jiang Mu and Qunhe Zhao and Martin, R.R. and Kun Xu},
volume = { 28},
number = { 4},
year = {2022//},
pages = {1745 - 57},
issn = {1941-0506},
address = {USA},
abstract = {Accurate camera pose estimation is essential and challenging for real world dynamic 3D reconstruction and augmented reality applications. In this article, we present a novel RGB-D SLAM approach for accurate camera pose tracking in dynamic environments. Previous methods detect dynamic components only across a short time-span of consecutive frames. Instead, we provide a more accurate dynamic 3D landmark detection method, followed by the use of long-term consistency via conditional random fields, which leverages long-term observations from multiple frames. Specifically, we first introduce an efficient initial camera pose estimation method based on distinguishing dynamic from static points using graph-cut RANSAC. These static/dynamic labels are used as priors for the unary potential in the conditional random fields, which further improves the accuracy of dynamic 3D landmark detection. Evaluation using the TUM and Bonn RGB-D dynamic datasets shows that our approach significantly outperforms state-of-the-art methods, providing much more accurate camera trajectory estimation in a variety of highly dynamic environments. We also show that dynamic 3D reconstruction can benefit from the camera poses estimated by our RGB-D SLAM approach.},
keywords = {augmented reality;cameras;feature extraction;graph theory;image reconstruction;image sequences;motion estimation;pose estimation;SLAM (robots);},
note = {efficient initial camera;estimation method;distinguishing dynamic;conditional random fields;accurate camera trajectory estimation;highly dynamic environments;accurate dynamic SLAM;CRF-based long-term consistency;world dynamic 3D reconstruction;reality applications;novel RGB-D SLAM approach;dynamic components;short time-span;consecutive frames;accurate dynamic 3D landmark detection method;long-term observations;},
URL = {http://dx.doi.org/10.1109/TVCG.2020.3028218},
} 


@inproceedings{19986830 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Are We Ready for Service Robots? The OpenLORIS-Scene Datasets for Lifelong SLAM},
journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Xuesong Shi and Dongjiang Li and Pengpeng Zhao and Qinbin Tian and Yuxin Tian and 2 and 2 and 2 and 2 and 3 and 3 and 1 and 1 and 3 and 4 and 4 and 5 and 1},
year = {2020//},
pages = {3139 - 45},
address = {Piscataway, NJ, USA},
abstract = {Service robots should be able to operate autonomously in dynamic and daily changing environments over an extended period of time. While Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems for robotic autonomy, most existing SLAM works are evaluated with data sequences that are recorded in a short period of time. In real-world deployment, there can be out-of-sight scene changes caused by both natural factors and human activities. For example, in home scenarios, most objects may be movable, replaceable or deformable, and the visual features of the same place may be significantly different in some successive days. Such out-of-sight dynamics pose great challenges to the robustness of pose estimation, and hence a robot's long-term deployment and operation. To differentiate the forementioned problem from the conventional works which are usually evaluated in a static setting in a single run, the term lifelong SLAM is used here to address SLAM problems in an ever-changing environment over a long period of time. To accelerate lifelong SLAM research, we release the OpenLORIS-Scene datasets. The data are collected in real-world indoor scenes, for multiple times in each place to include scene changes in real life. We also design benchmarking metrics for lifelong SLAM, with which the robustness and accuracy of pose estimation are evaluated separately. The datasets and benchmark are available online at lifelong-robotic-vision.github.io/dataset/scene.},
keywords = {mobile robots;path planning;pose estimation;robot vision;service robots;SLAM (robots);},
note = {simultaneous localization and mapping;data sequences;robotic autonomy;service robots;real-world indoor scenes;OpenLORIS-Scene datasets;SLAM problems;pose estimation;},
URL = {http://dx.doi.org/10.1109/ICRA40945.2020.9196638},
} 


@inproceedings{19298535 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Long-Term Visual Inertial SLAM based on Time Series Map Prediction},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Bowen Song and Weidong Chen and Jingchuan Wang and Hesheng Wang},
year = {2019//},
pages = {5364 - 9},
address = {Piscataway, NJ, USA},
abstract = {With the advance in the field of mobile robots, autonomous robots are required for long-term deployment in dynamic and complex environments. However, the performance of Visual Inertial SLAM systems in long-term operation is not satisfactory, and most long-term SLAM systems assumes periodic changes in the environment. This paper presents a novel solution for long-term monocular VI SLAM system in dynamic environment based on autoregression(AR) modeling and map prediction. Map points are first classified into static and semi-static map points according to a memory model. Modeling and prediction of the different states of semi-static map points are performed that are derived from time series models. The predicted map is then fused with the current map to achieve a better forecast for the next frame if the prediction is not satisfactory enough. Experiments are carried out on an embedded system. The results indicate that the map prediction is reliable and the proposed approach improves the performance of long-term localization and mapping in dynamic environments.},
keywords = {mobile robots;regression analysis;robot vision;SLAM (robots);},
note = {long-term monocular VI SLAM system;dynamic environment;semistatic map points;memory model;time series models;embedded system;visual inertial SLAM;time series map prediction;mobile robots;autonomous robots;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8968017},
} 


@inproceedings{19193983 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {How to Match Tracks of Visual Features for Automotive Long-Term SLAM},
journal = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
author = {Luthardt, S. and Ziegler, C. and Willert, V. and Adamy, J.},
year = {2019//},
pages = {934 - 41},
address = {Piscataway, NJ, USA},
abstract = {Accurate localization is a vital prerequisite for future assistance or autonomous driving functions in intelligent vehicles. To achieve the required localization accuracy and availability, long-term visual SLAM algorithms like LLama-SLAM are a promising option. In such algorithms visual feature tracks, i. e. landmark observations over several consecutive image frames, have to be matched to feature tracks recorded days, weeks or months earlier. This leads to a more challenging matching problem than in short-term visual localization and known descriptor matching methods cannot be applied directly. In this paper, we devise several approaches to compare and match feature tracks and evaluate their performance on a long-term data set. With the proposed descriptor combination and masking ("CoMa") method the best track matching performance is achieved with minor computational cost. This method creates a single combined descriptor for each feature track and furthermore increases the robustness by capturing the appearance variations of this track in a descriptor mask.},
keywords = {feature extraction;image matching;mobile robots;pose estimation;robot vision;SLAM (robots);},
note = {visual features;automotive long-term SLAM;vital prerequisite;future assistance;autonomous driving functions;LLama-SLAM;consecutive image frames;feature track;challenging matching problem;short-term visual localization;matching methods;long-term data;track matching performance;visual feature tracks;},
URL = {http://dx.doi.org/10.1109/ITSC.2019.8916895},
} 


@inproceedings{18274256 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Sensors, SLAM and Long-term Autonomy: A Review},
journal = {2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)},
author = {Zaffar, M. and Ehsan, S. and Stolkin, R. and Maier, K.M.},
year = {2018//},
pages = {285 - 90},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous Localization and Mapping, commonly known as SLAM, has been an active research area in the field of Robotics over the past three decades. For solving the SLAM problem, every robot is equipped with either a single sensor or a combination of similar/different sensors. This paper attempts to review, discuss, evaluate and compare these sensors. Keeping an eye on future, this paper also assesses the characteristics of these sensors against factors critical to the long-term autonomy challenge.},
keywords = {robot vision;SLAM (robots);},
note = {SLAM problem;long-term autonomy challenge;simultaneous localization and mapping;SLAM;robotics;},
URL = {http://dx.doi.org/10.1109/AHS.2018.8541483},
} 


@inproceedings{21407018 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {A General Framework for Lifelong Localization and Mapping in Changing Environment},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Min Zhao and Xin Guo and Le Song and Baoxing Qin and Xuesong Shi and Gim Hee Lee and Guanghui Sun},
year = {2021//},
pages = {3305 - 12},
address = {Piscataway, NJ, USA},
abstract = {The environment of most real-world scenarios such as malls and supermarkets changes at all times. A pre-built map that does not account for these changes becomes out-of-date easily. Therefore, it is necessary to have an up-to-date model of the environment to facilitate long-term operation of a robot. To this end, this paper presents a general lifelong simultaneous localization and mapping (SLAM) framework. Our framework uses a multiple session map representation, and exploits an efficient map updating strategy that includes map building, pose graph refinement and sparsification. To mitigate the unbounded increase of memory usage, we propose a map-trimming method based on the Chow-Liu maximum-mutual-information spanning tree. The proposed SLAM framework has been comprehensively validated by over a month of robot deployment in real supermarket environment. Furthermore, we release the dataset collected from the indoor and outdoor changing environment with the hope to accelerate lifelong SLAM research in the community. Our dataset is available at https://github.com/sanduan168/lifelong-SLAM-dataset.},
keywords = {graph theory;mobile robots;path planning;robot vision;SLAM (robots);trees (mathematics);},
note = {graph refinement;sparsification;unbounded increase;memory usage;map-trimming method;Chow-Liu maximum-mutual-information spanning tree;SLAM framework;robot deployment;supermarket environment;indoor changing environment;outdoor changing environment;SLAM research;lifelong localization;malls;supermarkets changes;pre-built map;out-of-date;up-to-date model;long-term operation;general lifelong simultaneous localization;multiple session map representation;efficient map updating strategy;map building;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9635985},
} 


@article{18218555 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Sequence-based sparse optimization methods for long-term loop closure detection in visual SLAM},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Fei Han and Hua Wang and Guoquan Huang and Hao Zhang},
volume = { 42},
number = { 7},
year = {2018/10/},
pages = {1323 - 35},
issn = {0929-5593},
address = {Germany},
abstract = {Loop closure detection is one of the most important module in Simultaneously Localization and Mapping (SLAM) because it enables to find the global topology among different places. A loop closure is detected when the current place is recognized to match the previous visited places. When the SLAM is executed throughout a long-term period, there will be additional challenges for the loop closure detection. The illumination, weather, and vegetation conditions can often change significantly during the life-long SLAM, resulting in the critical strong perceptual aliasing and appearance variation problems in loop closure detection. In order to address this problem, we propose a new Robust Multimodal Sequence-based (ROMS) method for robust loop closure detection in long-term visual SLAM. A sequence of images is used as the representation of places in our ROMS method, where each image in the sequence is encoded by multiple feature modalites so that different places can be recognized discriminatively. We formulate the robust place recognition problem as a convex optimization problem with structured sparsity regularization due to the fact that only a small set of template places can match the query place. In addition, we also develop a new algorithm to solve the formulated optimization problem efficiently, which guarantees to converge to the global optima theoretically. Our ROMS method is evaluated through extensive experiments on three large-scale benchmark datasets, which record scenes ranging from different times of the day, months, and seasons. Experimental results demonstrate that our ROMS method outperforms the existing loop closure detection methods in long-term SLAM, and achieves the state-of-the-art performance.},
keywords = {mobile robots;optimisation;robot vision;SLAM (robots);},
note = {Robust Multimodal Sequence-based method;appearance variation problems;critical strong perceptual aliasing;life-long SLAM;long-term period;previous visited places;current place;long-term loop closure detection;Sequence-based sparse optimization methods;long-term SLAM;existing loop closure detection methods;formulated optimization problem;query place;template places;robust place recognition problem;different places;ROMS method;long-term visual SLAM;robust loop closure detection;},
URL = {http://dx.doi.org/10.1007/s10514-018-9736-3},
} 


@inproceedings{18308743 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {LLama-SLAM: Learning High-quality Visual Landmarks for Long-term Mapping and Localization},
journal = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
author = {Luthardt, S. and Willert, V. and Adamy, J.},
year = {2018//},
pages = {2645 - 52},
address = {Piscataway, NJ, USA},
abstract = {The precise localization of vehicles is an important requirement for autonomous driving or advanced driver assistance systems. Using common GNSS the ego position can be measured but not with the reliability and precision necessary. An alternative approach to achieve precise localization is the usage of visual landmarks observed by a camera mounted in the vehicle. However, this raises the necessity of reliable visual landmarks that are easily recognizable and persistent. We propose a novel SLAM algorithm that focuses on learning and mapping such visual long-term landmarks (LLamas). The algorithm therefore processes stereo image streams from several recording sessions in the same spatial area. The key part within LLama-SLAM is the assessment of the landmarks with quality values that are inferred as viewpoint dependent probabilities from observation statistics. By adding solely landmarks of high quality to the final LLama Map, it can be kept compact while still allowing reliable localization. Due to the long-term evaluation of the GNSS measurement during the sessions, the landmarks can be positioned precisely in a global referenced coordinate system. For a first assessment of the algorithm's capabilities, we present some experimental results from the mapping process combining three sessions recorded over two months on the same route.},
keywords = {control engineering computing;driver information systems;mobile robots;probability;road traffic control;robot vision;SLAM (robots);statistics;stereo image processing;},
note = {LLama-SLAM;autonomous driving;stereo image streams;GNSS measurement;statistics;high-quality visual landmark learning;long-term mapping and localization;GNSS;visual long-term landmarks;r advanced driver assistance systems;},
URL = {http://dx.doi.org/10.1109/ITSC.2018.8569323},
} 


@article{21408662 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Tightly-Coupled Magneto-Visual-Inertial Fusion for Long Term Localization in Indoor Environment},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Coulin, J. and Guillemard, R. and Gay-Bellile, V. and Joly, C. and De La Fortelle, A.},
volume = { 7},
number = { 2},
year = {2022//},
pages = {952 - 9},
issn = {2377-3766},
address = {USA},
abstract = {We propose in this letter a tightly-coupled fusion of visual, inertial and magnetic data for long-term localization in indoor environment. Unlike state-of-the-art Visual-Inertial SLAM (VISLAM) solutions that reuse visual map to prevent drift, we present in this letter an extension of the Multi-State Constraint Kalman Filter (MSCKF) that takes advantage of a magnetic map. It makes our solution more robust to variations of the environment appearance. The experimental results demonstrate that the localization accuracy of the proposed approach is almost the same over time periods longer than a year.},
keywords = {inertial navigation;Kalman filters;mobile robots;SLAM (robots);},
note = {magneto-Visual-Inertial fusion;long term localization;indoor environment;visual data;inertial data;magnetic data;long-term localization;state-of-the-art Visual-Inertial SLAM solutions;reuse visual map;MultiState Constraint Kalman Filter;magnetic map;environment appearance;localization accuracy;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3136241},
} 


@article{18046291 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Exactly sparse delayed state filter on Lie groups for long-term pose graph SLAM},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Lenac, K. and Cesic, J. and Markovic, I. and Petrovic, I.},
volume = { 37},
number = { 6},
year = {2018/05/},
pages = {585 - 610},
issn = {0278-3649},
address = {UK},
abstract = {In this paper we propose a simultaneous localization and mapping (SLAM) back-end solution called the exactly sparse delayed state filter on Lie groups (LG-ESDSF). We derive LG-ESDSF and demonstrate that it retains all the good characteristics of the classic Euclidean ESDSF, the main advantage being the exact sparsity of the information matrix. The key advantage of LG-ESDSF in comparison with the classic ESDSF lies in the ability to respect the state space geometry by negotiating uncertainties and employing filtering equations directly on Lie groups. We also exploit the special structure of the information matrix in order to allow long-term operation while the robot is moving repeatedly through the same environment. To prove the effectiveness of the proposed SLAM solution, we conducted extensive experiments on two different publicly available datasets, namely the KITTI and EuRoC datasets, using two front-ends: one based on the stereo camera and the other on the 3D LIDAR. We compare LG-ESDSF with the general graph optimization framework (g<sup>2</sup>o) when coupled with the same front-ends. Similarly to g<sup>2</sup>o the proposed LG-ESDSF is front-end agnostic and the comparison demonstrates that our solution can match the accuracy of g<sup>2</sup>o, while maintaining faster computation times. Furthermore, the proposed back-end coupled with the stereo camera front-end forms a complete visual SLAM solution dubbed LG-SLAM. Finally, we evaluated LG-SLAM using the online KITTI protocol and at the time of writing it achieved the second best result among the stereo odometry solutions and the best result among the tested SLAM algorithms.},
keywords = {cameras;geometry;graph theory;Lie groups;matrix algebra;mobile robots;optimisation;SLAM (robots);},
note = {exactly sparse delayed state filter;LG-SLAM;Lie groups;uncertainties;state space geometry;LG-ESDSF;information matrix;stereo odometry solutions;online KITTI protocol;stereo camera front-end;graph optimization framework;3D LIDAR;EuRoC datasets;simultaneous localization and mapping back-end solution;long-term pose graph SLAM;visual SLAM solution;Euclidean ESDSF;filtering equations;},
URL = {http://dx.doi.org/10.1177/0278364918767756},
} 


@inproceedings{21487524 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Robust SLAM Systems: Are We There Yet?},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Bujanca, M. and Xuesong Shi and Spear, M. and Pengpeng Zhao and Lennox, B. and Lujan, M.},
year = {2021//},
pages = {5320 - 7},
address = {Piscataway, NJ, USA},
abstract = {Progress in the last decade has brought about significant improvements in the accuracy and speed of SLAM systems, broadening their mapping capabilities. Despite these advancements, long-term operation remains a major challenge, primarily due to the wide spectrum of perturbations robotic systems may encounter.Increasing the robustness of SLAM algorithms is an ongoing effort, however it usually addresses a specific perturbation. Generalisation of robustness across a large variety of challenging scenarios is not well-studied nor understood. This paper presents a systematic evaluation of the robustness of open-source state-of-the-art SLAM algorithms with respect to challenging conditions such as fast motion, non-uniform illumination, and dynamic scenes. The experiments are performed with perturbations present both independently of each other, as well as in combination in long-term deployment settings in unconstrained environments (lifelong operation).The detailed results (approx. 20,000 experiments) along with comprehensive documentation of the benchmarking tool for integrating new datasets and evaluating SLAM algorithms not studied in this work are available at https://robustslam.github.io/evaluation.},
keywords = {mobile robots;robot vision;SLAM (robots);},
note = {long-term deployment settings;robust SLAM systems;mapping capabilities;perturbations robotic systems;open-source SLAM algorithms;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9636814},
} 


@article{18732638 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Labbe, M. and Michaud, F.},
volume = { 36},
number = { 2},
year = {2019/03/},
pages = {416 - 46},
issn = {1556-4959},
address = {USA},
abstract = {Distributed as an open-source library since 2013, real-time appearance-based mapping (RTAB-Map) started as an appearance-based loop closure detection approach with memory management to deal with large-scale and long-term online operation. It then grew to implement simultaneous localization and mapping (SLAM) on various robots and mobile platforms. As each application brings its own set of constraints on sensors, processing capabilities, and locomotion, it raises the question of which SLAM approach is the most appropriate to use in terms of cost, accuracy, computation power, and ease of integration. Since most of SLAM approaches are either visual- or lidar-based, comparison is difficult. Therefore, we decided to extend RTAB-Map to support both visual and lidar SLAM, providing in one package a tool allowing users to implement and compare a variety of 3D and 2D solutions for a wide range of applications with different robots and sensors. This paper presents this extended version of RTAB-Map and its use in comparing, both quantitatively and qualitatively, a large selection of popular real-world datasets (e.g., KITTI, EuRoC, TUM RGB-D, MIT Stata Center on PR2 robot), outlining strengths, and limitations of visual and lidar SLAM configurations from a practical perspective for autonomous navigation applications.},
keywords = {image colour analysis;mobile robots;object detection;optical radar;robot vision;SLAM (robots);},
note = {RTAB-Map;open-source lidar;visual simultaneous localization;mapping library;long-term online operation;open-source library;real-time appearance-based mapping;appearance-based loop closure detection approach;SLAM approach;SLAM approaches;visual- lidar-based;visual SLAM configurations;lidar SLAM configurations;},
URL = {http://dx.doi.org/10.1002/rob.21831},
} 


@inproceedings{20367334 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Using Image Sequences for Long-Term Visual Localization},
journal = {2020 International Conference on 3D Vision (3DV)},
author = {Stenborg, E. and Sattler, T. and Hammarstrand, L.},
year = {2020//},
pages = {938 - 48},
address = {Piscataway, NJ, USA},
abstract = {Estimating the pose of a camera in a known scene, i.e., visual localization, is a core task for applications such as self-driving cars. In many scenarios, image sequences are available and existing work on combining single-image localization with odometry offers to unlock their potential for improving localization performance. Still, the largest part of the literature focuses on single-image localization and ignores the availability of sequence data. The goal of this paper is to demonstrate the potential of image sequences in challenging scenarios, e.g., under day-night or seasonal changes. Combining ideas from the literature, we describe a sequence-based localization pipeline that combines odometry with both a coarse and a fine localization module. Experiments on long-term localization datasets show that combining single-image global localization against a prebuilt map with a visual odometry/SLAM pipeline improves performance to a level where the extended CMU Seasons dataset can be considered solved. We show that SIFT features can perform on par with modern state-of-the-art features in our framework, despite being much weaker and a magnitude faster to compute. Our code is publicly available at github.com/rulllars.},
keywords = {cameras;distance measurement;feature extraction;image sequences;mobile robots;robot vision;SLAM (robots);transforms;},
note = {localization performance improvement;sequence data;image sequences;sequence-based localization pipeline;fine localization module;long-term localization datasets;single-image global localization;long-term visual localization;camera pose estimation;visual odometry-SLAM pipeline;SIFT features;},
URL = {http://dx.doi.org/10.1109/3DV50981.2020.00104},
} 


@inproceedings{20842379 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {ExMaps: Long-Term Localization in Dynamic Scenes using Exponential Decay},
journal = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
author = {Rotsidis, A. and Lutteroth, C. and Hall, P. and Richardt, C.},
year = {2021//},
pages = {2866 - 75},
address = {Los Alamitos, CA, USA},
abstract = {Visual camera localization using offline maps is widespread in robotics and mobile applications. Most state-of-the-art localization approaches assume static scenes, so maps are often reconstructed once and then kept constant. However, many scenes are dynamic and as changes in the scene happen, future localization attempts may struggle or fail entirely. Therefore, it is important for successful long-term localization to update and maintain maps as new observations of the scene, and changes in it, arrive. We propose a novel method for automatically discovering which points in a map remain stable over time, and which are due to transient changes. To this end, we calculate a stability store for each point based on its visibility over time, weighted by an exponential decay over time. This allows us to consider the impact of time when scoring points, and to distinguish which points are useful for long-term localization. We evaluate our method on the CMU Extended Seasons dataset (outdoors) and a new indoor dataset of a retail shop, and show the benefit of maintaining a `live map' that integrates updates over time using our exponential decay based method over a static `base map'.},
keywords = {cameras;geophysical image processing;indoor radio;mobile computing;mobile robots;robot vision;SLAM (robots);terrain mapping;},
note = {static scenes;long-term localization;live map;exponential decay based method;static base map;dynamic scenes;visual camera localization;offline maps;CMU Extended Seasons dataset;},
URL = {http://dx.doi.org/10.1109/WACV48630.2021.00291},
} 


@article{21256418 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Domain-Invariant Similarity Activation Map Contrastive Learning for Retrieval-Based Long-Term Visual Localization},
journal = {IEEE/CAA Journal of Automatica Sinica},
journal = {IEEE/CAA J. Autom. Sin. (USA)},
author = {Hanjiang Hu and Hesheng Wang and Zhe Liu and Weidong Chen},
volume = { 9},
number = { 2},
year = {2022//},
pages = {313 - 28},
issn = {2329-9266},
address = {USA},
abstract = {Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.},
keywords = {feature extraction;image classification;image retrieval;learning (artificial intelligence);mobile robots;object recognition;SLAM (robots);unsupervised learning;visual databases;},
note = {general architecture;domain-invariant features;multidomain image translation;novel gradient-weighted similarity activation mapping loss;finer localization;adaptive triplet loss;coarse-to-fine image retrieval pipeline;Grad-SAM loss;CMU-Seasons dataset;strong generalization ability;RobotCar dataset using models;state-of-the-art image-based localization baselines;night-time images;domain-invariant similarity activation map contrastive learning;retrieval-based long-term;mobile robot;autonomous driving;efficient technique;image-based localization methods;drastic variability;illumination changes;retrieval-based visual localization;},
URL = {http://dx.doi.org/10.1109/JAS.2021.1003907},
} 


@inproceedings{21257530 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Lifelong Localization in Semi-Dynamic Environment},
journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Shifan Zhu and Xinyu Zhang and Shichun Guo and Jun Li and Huaping Liu},
year = {2021//},
pages = {14389 - 14395},
address = {Piscataway, NJ, USA},
abstract = {Mapping and localization in non-static environments are fundamental problems in robotics. Most of previous methods mainly focus on static and highly dynamic objects in the environment, which may suffer from localization failure in semi-dynamic scenarios without considering objects with lower dynamics, such as parked cars and stopped pedestrians. In this paper, we introduce semantic mapping and lifelong localization approaches to recognize semi-dynamic objects in non-static environments. We also propose a generic framework that can integrate mainstream object detection algorithms with mapping and localization algorithms. The mapping method combines an object detection algorithm and a SLAM algorithm to detect semi-dynamic objects and constructs a semantic map that only contains semi-dynamic objects in the environment. During navigation, the localization method can classify observation corresponding to static and non-static objects respectively and evaluate whether those semi-dynamic objects have moved, to reduce the weight of invalid observation and localization fluctuation. Real-world experiments show that the proposed method can improve the localization accuracy of mobile robots in non-static scenarios.},
keywords = {mobile robots;object detection;robot vision;SLAM (robots);},
note = {lifelong localization;semidynamic environment;nonstatic environments;static objects;highly dynamic objects;localization failure;semidynamic scenarios;lower dynamics;semantic mapping;semidynamic objects;mainstream object detection algorithms;localization algorithms;object detection algorithm;semantic map;localization method;nonstatic objects;invalid observation;localization fluctuation;nonstatic scenarios;},
URL = {http://dx.doi.org/10.1109/ICRA48506.2021.9561584},
} 


@inproceedings{20587484 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Keyframes retrieval for robust long-term visual localization in changing conditions},
journal = {2021 IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI)},
author = {Bouaziz, Y. and Royer, E. and Bresson, G. and Dhome, M.},
year = {2021//},
pages = {000093 - 100},
address = {Piscataway, NJ, USA},
abstract = {Appearance changes are a challenge for visual localization in outdoor environments. Revisiting familiar places but retrieving keyframes that were taken under different environmental condition can result in inaccurate localization. To overcome this difficulty, we propose a localization approach able to take advantage of a visual landmark map composed of N sequences gathered at different times and conditions. During this localization process, we exploit information collected in the beginning of the trajectory to compute a ranking function which will be used in the rest of the trajectory to retrieve from the map the keyframes that maximise the number of matched points. The retrieval depends on the geometric distance between the pose of the keyframe and the current pose of the vehicle, and the similarity of this keyframe with the current environmental condition. The results demonstrate that our approach has significantly improved localization performance in challenging conditions (snow, rain, change of season ...).},
keywords = {image matching;image retrieval;mobile robots;robot vision;SLAM (robots);video signal processing;},
note = {visual landmark map;environmental condition;localization performance;keyframes retrieval;outdoor environments;robust long-term visual localization approach;},
URL = {http://dx.doi.org/10.1109/SAMI50585.2021.9378614},
} 


@inproceedings{21259668 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Preventing and Correcting Mistakes in Lifelong Mapping},
journal = {2021 European Conference on Mobile Robots (ECMR)},
author = {Banerjee, N. and Lisin, D. and Albanese, V. and Zhu, Z. and Lenser, S.R. and Shriver, J. and Ramaswamy, T. and Briggs, J. and Fong, P.},
year = {2021//},
pages = {8 pp. - },
address = {Piscataway, NJ, USA},
abstract = {A Graph SLAM system is only as good as the edges in its pose graph. Critical mistakes in the generation of these edges can instantly render a map inconsistent, misleading, and ultimately unusable. For a lifelong mapping system, where the map is updated continuously, avoiding these errors altogether is infeasible. Instead, we propose a system for detection of and recovery from severe errors in edge generation. Our system remedies both edges created by view observations and edges created by an odometry motion model. For observation edges, we pair a novel method for monitoring ambiguous views with an intelligent graph-merging algorithm capable of rejecting a relocalization in progress. For motion edges, we propose a qualitative geometric approach for detecting structural aberrations characteristic of odometry failures. We conclude with an analysis of our results based on an empirical study of thousands of robot runs.},
keywords = {computational complexity;distance measurement;graph theory;merging;mobile robots;particle filtering (numerical methods);robot kinematics;robot vision;SLAM (robots);},
note = {Graph SLAM system;pose graph;critical mistakes;map inconsistent;lifelong mapping system;severe errors;edge generation;system remedies;view observations;odometry motion model;observation edges;ambiguous views;intelligent graph-merging algorithm;motion edges;},
URL = {http://dx.doi.org/10.1109/ECMR50962.2021.9568826},
} 


@article{21589128 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Topographic SLAM Using a Single Terrain Altimeter in GNSS-Restricted Environment},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Junwoo Jang and Jinwhan Kim},
volume = { 10},
year = {2022//},
pages = {10806 - 10815},
issn = {2169-3536},
address = {USA},
abstract = {In a Global Navigation Satellite System (GNSS)-restricted area, a mobile robot navigation system exploits surrounding environment information. For an aerial or underwater vehicle, undulating terrain of a land or seabed surface is a valuable information resource that leads to the development of terrain-referenced navigation (TRN) algorithms. However, due to the vast amount of a vehicle's activity area, surveying all the regions to obtain a high-resolution terrain map is impractical and requires simultaneous localization and mapping (SLAM) as a highly desirable capability. This paper presents a topographic SLAM algorithm using only a single terrain altimeter, which is low-cost, computationally efficient, and sufficiently stable for long-term operation. The proposed rectangular panel map structure and update method enable robust and efficient SLAM. As terrain elevation changes are inherently nonlinear, an extended Kalman filter (EKF)-based SLAM filter is adopted. The feasibility and validity of the proposed algorithm are demonstrated through simulations using terrain elevation data from a real-world undersea environment.},
keywords = {altimeters;Kalman filters;mobile robots;navigation;nonlinear filters;satellite navigation;SLAM (robots);telecommunication control;terrain mapping;},
note = {single terrain altimeter;GNSS-restricted environment;mobile robot navigation system;environment information;aerial vehicle;underwater vehicle;undulating terrain;valuable information resource;terrain-referenced navigation algorithms;high-resolution terrain map;topographic SLAM algorithm;rectangular panel map structure;terrain elevation changes;extended Kalman filter-based SLAM filter;terrain elevation data;real-world undersea environment;global navigation satellite system-restricted area;TRN algorithms;vehicle activity area;simultaneous localization and mapping;EKF-based SLAM filter;},
URL = {http://dx.doi.org/10.1109/ACCESS.2022.3145978},
} 


@article{15912087 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Nonlinear factor recovery for long-term SLAM},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Mazuran, M. and Burgard, W. and Tipaldi, G.D.},
volume = { 35},
number = { 1-3},
year = {2016/01/},
pages = {50 - 72},
issn = {0278-3649},
address = {UK},
abstract = {For long-term operations, graph-based simultaneous localization and mapping (SLAM) approaches require nodes to be marginalized in order to control the computational cost. In this paper, we present a method to recover a set of nonlinear factors that best represents the marginal distribution in terms of Kullback-Leibler divergence. The proposed method, which we call <i>nonlinear factor recovery</i> (NFR), estimates both the mean and the information matrix of the set of nonlinear factors, where the recovery of the latter is equivalent to solving a convex optimization problem. NFR is able to provide either the dense distribution or a sparse approximation of it. In contrast to previous algorithms, our method does not necessarily require a global linearization point and can be used with any nonlinear measurement function. Moreover, we are not restricted to only using tree-based sparse approximations and binary factors, but we can include any topology and correlations between measurements. Experiments performed on several publicly available datasets demonstrate that our method outperforms the state of the art with respect to the Kullback-Leibler divergence and the sparsity of the solution.},
keywords = {approximation theory;convex programming;costing;mobile robots;SLAM (robots);trees (mathematics);},
note = {nonlinear factor recovery;long-term SLAM;graph-based simultaneous localization and mapping approach;computational cost;marginal distribution;Kullback-Leibler divergence;NFR;convex optimization problem;nonlinear measurement function;tree-based sparse approximations;binary factors;mobile robotics;},
URL = {http://dx.doi.org/10.1177/0278364915581629},
} 


@inproceedings{20278128 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Collaborative Augmented Reality on Smartphones via Life-long City-scale Maps},
journal = {2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
author = {Platinsky, L. and Szabados, M. and Hlasek, F. and Hemsley, R. and Del Pero, L. and Pancik, A. and Baum, B. and Grimmett, H. and Ondruska, P.},
year = {2020//},
pages = {533 - 41},
address = {Los Alamitos, CA, USA},
abstract = {In this paper we present the first published end-to-end production computer-vision system for powering city-scale shared augmented reality experiences on mobile devices. In doing so we propose a new formulation for an experience-based mapping framework as an effective solution to the key issues of city-scale SLAM scalability, robustness, map updates and all-time all-weather performance required by a production system. Furthermore, we propose an effective way of synchronising SLAM systems to deliver seamless real-time localisation of multiple edge devices at the same time. All this in the presence of network latency and bandwidth limitations. The resulting system is deployed and tested at scale in San Francisco where it delivers AR experiences in a mapped area of several hundred kilometers. To foster further development of this area we offer the data set to the public, constituting the largest of this kind to date.},
keywords = {augmented reality;Global Positioning System;groupware;mobile computing;robot vision;SLAM (robots);smart phones;},
note = {multiple edge devices;AR experiences;collaborative augmented reality;life-long city-scale maps;city-scale shared augmented reality experiences;mobile devices;experience-based mapping framework;city-scale SLAM scalability;map updates;end-to-end production computer-vision system;all-time all-weather performance;smartphones;},
URL = {http://dx.doi.org/10.1109/ISMAR50242.2020.00081},
} 


@inproceedings{20088009 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Lifelong Object Localization in Robotic Applications},
journal = {Advances in Physical Agents. Proceedings of the 21st International Workshop of Physical Agents (WAF 2020). Advances in Intelligent Systems and Computing (AISC 1285)},
author = {Romero-Gonzalez, C. and Martinez-Gomez, J. and Garcia-Varea, I.},
volume = {pt.II},
year = {2021//},
pages = {18 - 29},
address = {Cham, Switzerland},
abstract = {One of the most common tasks in assistive robotics is to find some specific object in a home environment. Usually, this task is tackled by adding the objects of interest to a map of the environment as soon as the objects are detected by the vision system of the robot. However, these maps are usually static, and do not take into account the dynamic nature of a home, where anyone could move an object after the robot has seen it. In this paper, we propose a lifelong system to address this problem. The robot takes into account different possible locations for each object, and chooses the more probable one when it is required. We have designed a probability based system that stores possible locations for each object, and updates the probabilities of past locations based on newer detections.},
keywords = {learning (artificial intelligence);mobile robots;object detection;probability;robot vision;},
note = {specific object;assistive robotics;common tasks;robotic applications;lifelong object localization;newer detections;probability based system;account different possible locations;lifelong system;vision system;home environment;},
URL = {http://dx.doi.org/10.1007/978-3-030-62579-5_2},
} 


@inproceedings{20778617 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Long-Term Localization With Time Series Map Prediction for Mobile Robots in Dynamic Environments},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Lisai Wang and Weidong Chen and Jingchuan Wang},
year = {2020//},
pages = {7 pp. - },
address = {Piscataway, NJ, USA},
abstract = {In many applications of mobile robot, the environment is constantly changing. How to use historical information to analysis environmental changes and generate a map corresponding with current environment is important to achieve high-precision localization. Inspired by predictive mechanism of brain, this paper presents a long-term localization approach named ArmMPU (ARMA-based Map Prediction and Update) based on time series modeling and prediction. Autoregressive moving average model (ARMA), a kind of time series modeling method, is employed for environmental map modeling and prediction, then predicted map and filtered observation are fused to fix the prediction error. The simulation and experiment results show that the proposed method improves long-term localization performance in dynamic environments.},
keywords = {autoregressive moving average processes;mobile robots;robot dynamics;time series;},
note = {prediction error;long-term localization performance;dynamic environments;time series Map Prediction;mobile robot;historical information;high-precision localization;long-term localization approach;ARMA-based Map Prediction;average model;time series modeling method;environmental map modeling;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9468884},
} 


@inproceedings{19759361 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Edge-SLAM: Edge-Assisted Visual Simultaneous Localization and Mapping},
journal = {MobiSys '20: Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services},
author = {Ben Ali, A.J. and Hashemifar, Z.S. and Dantu, K.},
year = {2020/06/15},
pages = {325 - 37},
address = {New York, NY, USA},
abstract = {Localization in urban environments is becoming increasingly important and used in tools such as ARCore [11], ARKit [27] and others. One popular mechanism to achieve accurate indoor localization as well as a map of the space is using Visual Simultaneous Localization and Mapping (Visual-SLAM). However, Visual-SLAM is known to be resource-intensive in memory and processing time. Further, some of the operations grow in complexity over time, making it challenging to run on mobile devices continuously. Edge computing provides additional compute and memory resources to mobile devices to allow offloading of some tasks without the large latencies seen when offloading to the cloud. In this paper, we present Edge-SLAM, a system that uses edge computing resources to offload parts of Visual-SLAM. We use ORB-SLAM2 as a prototypical Visual-SLAM system and modify it to a split architecture between the edge and the mobile device. We keep the tracking computation on the mobile device and move the rest of the computation, i.e., local mapping and loop closure, to the edge. We describe the design choices in this effort and implement them in our prototype. Our results show that our split architecture can allow the functioning of the Visual-SLAM system long-term with limited resources without affecting the accuracy of operation. It also keeps the computation and memory cost on the mobile device constant which would allow for deployment of other end applications that use Visual-SLAM.},
keywords = {indoor radio;mobile computing;mobile robots;path planning;robot vision;SLAM (robots);},
note = {indoor localization;visual simultaneous localization and mapping;processing time;offloading;edge computing resources;ORB-SLAM2;local mapping;loop closure;mobile device constant;edge-SLAM;visual-SLAM system;},
URL = {http://dx.doi.org/10.1145/3386901.3389033},
} 


@inproceedings{20592293 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Towards life-long mapping of dynamic environments using temporal persistence modeling},
journal = {2020 25th International Conference on Pattern Recognition (ICPR)},
author = {Tsamis, G. and Kostavelis, I. and Giakoumis, D. and Tzovaras, D.},
year = {2020//},
pages = {10480 - 5},
address = {Piscataway, NJ, USA},
abstract = {The contemporary SLAM mapping systems assume a static environment and build a map that is then used for mobile robot navigation disregarding the dynamic changes in this environment. The paper at hand presents a novel solution for the problem of life-long mapping that continually updates a metric map represented as a 2D occupancy grid in large scale indoor environments with movable objects such as people, robots, objects etc. suitable for industrial applications. We formalize each cell's occupancy as a failure analysis problem and contribute temporal persistence modeling (TPM), an algorithm for probabilistic prediction of the time that a cell in an observed location is expected to be &ldquo;occupied&rdquo; or &ldquo;empty&rdquo; given sparse prior observations from a task specific mobile robot. Our work is evaluated in Gazebo simulation environment against the nominal occupancy of cells and the estimated obstacles persistence. We also show that robot navigation with life-long mapping demands less replans and leads to more efficient navigation in highly dynamic environments.},
keywords = {collision avoidance;control engineering computing;failure analysis;mobile robots;path planning;robot vision;SLAM (robots);},
note = {scale indoor environments;movable objects;failure analysis problem;temporal persistence modeling;task specific mobile robot;Gazebo simulation environment;estimated obstacles persistence;life-long mapping demands less replans;highly dynamic environments;contemporary SLAM mapping systems;static environment;mobile robot navigation;dynamic changes;metric map;2D occupancy grid;},
URL = {http://dx.doi.org/10.1109/ICPR48806.2021.9413161},
} 


@inproceedings{18167959 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Map Management for Efficient Long-Term Visual Localization in Outdoor Environments},
journal = {2018 IEEE Intelligent Vehicles Symposium (IV)},
author = {Burki, M. and Dymczyk, M. and Gilitschenski, I. and Cadena, C. and Siegwart, R. and Nieto, J.},
year = {2018//},
pages = {682 - 8},
address = {Piscataway, NJ, USA},
abstract = {We present a complete map management process for a visual localization system designed for multi-vehicle long-term operations in resource constrained outdoor environments. Outdoor visual localization generates large amounts of data that need to be incorporated into a lifelong visual map in order to allow localization at all times and under all appearance conditions. Processing these large quantities of data is non-trivial, as it is subject to limited computational and storage capabilities both on the vehicle and on the mapping backend. We address this problem with a two-fold map update paradigm capable of, either, adding new visual cues to the map, or updating co-observation statistics. The former, in combination with offline map summarization techniques, allows enhancing the appearance coverage of the lifelong map while keeping the map size limited. On the other hand, the latter is able to significantly boost the appearance-based landmark selection for efficient online localization without incurring any additional computational or storage burden. Our evaluation in challenging outdoor conditions shows that our proposed map management process allows building and maintaining maps for precise visual localization over long time spans in a tractable and scalable fashion.},
keywords = {automobiles;mobile robots;path planning;robot vision;SLAM (robots);},
note = {appearance-based landmark selection;visual localization system;multivehicle long-term operations;resource constrained outdoor environments;outdoor visual localization;lifelong visual map;appearance conditions;mapping backend;two-fold map update paradigm;visual cues;offline map summarization techniques;appearance coverage;long-term visual localization;map management process;autonomous cars;},
URL = {http://dx.doi.org/10.1109/IVS.2018.8500432},
} 


@inproceedings{19732479 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {SLAM using LTE Multipath Component Delays},
journal = {2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring)},
author = {Junshi Chen and Meifang Zhu and Tufvesson, F.},
year = {2020//},
pages = {5 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Cellular radio based localization can be an important complement or alternative to other localization technologies, as base stations continuously transmit signals of opportunity with beneficial positioning properties. In this paper, we use the long term evolution (LTE) cell-specific reference signal for this purpose. The multipath component delays are estimated by the ESPRIT algorithm, and the estimated multipath component delays of different snapshots are associated by global nearest neighbor with a Kalman filter. Rao-Blackwellized particle filter based simultaneous localization and mapping (SLAM) is then applied to estimate the position of user equipment and that of the base station and virtual transmitters. In a measurement campaign, data from one base station was logged, and the analysis based on the data shows that, at the end of the measurement, the SLAM performance is 11 meters better than that with only inertial measurement unit (IMU).},
keywords = {cellular radio;delay estimation;direction-of-arrival estimation;indoor radio;Kalman filters;Long Term Evolution;mobile robots;multipath channels;particle filtering (numerical methods);SLAM (robots);},
note = {SLAM performance;base station;Rao-Blackwellized particle filter;Kalman filter;global nearest neighbor;different snapshots;estimated multipath component delays;ESPRIT algorithm;long term evolution cell-specific reference signal;beneficial positioning properties;localization technologies;important complement;cellular radio based localization;LTE multipath component delays;},
URL = {http://dx.doi.org/10.1109/VTC2020-Spring48590.2020.9128437},
} 


@article{20322850 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {A Geodetic Normal Distribution Map for Long-Term LiDAR Localization on Earth},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Chansoo Kim and Sungjin Cho and Myoungho Sunwoo and Resende, P. and Bradai, B. and Kichun Jo},
volume = { 9},
year = {2021//},
pages = {470 - 84},
issn = {2169-3536},
address = {USA},
abstract = {Light detection and ranging (LiDAR) sensors enable a vehicle to estimate a pose by matching their measurements with a point cloud (PCD) map. However, the PCD map structure, widely used in robot fields, has some problems to be applied for mass production in automotive fields. First, the PCD map is too big to store all map data at in-vehicle units or download the map data from a wireless network according to the vehicle location. Second, the PCD map, represented by a single origin in the Cartesian coordinates, causes coordinate conversion errors due to an inaccurate plane-orb projection, when the vehicle estimate the geodetic pose on Earth. To solve two problems, this paper presents a geodetic normal distribution (GND) map structure. The GND map structure supports a geodetic quad-tree tiling system with multiple origins to minimize the coordinate conversion errors. The map data managed by the GND map structure are compressed by using Cartesian probabilistic distributions of points as map features. The truncation errors by heterogeneous coordinates between the geodetic tiling system and Cartesian distributions are compensated by the Cartesian voxelization rule. In order to match the LiDAR measurements with the GND map structure, the paper proposes map-matching approaches based on Monte-Carlo and optimization. The paper performed some experiments to evaluate the map size compression and the long-term localization on Earth: comparison with the PCD map structure, localization in various continents, and long-term localization.},
keywords = {cartography;Monte Carlo methods;optical radar;optimisation;pose estimation;},
note = {map size compression;PCD map structure;long-term LiDAR localization;point cloud map;map data;geodetic normal distribution map structure;GND map structure;geodetic quad-tree tiling system;map-matching approaches;light detection and ranging sensors;LiDAR sensors;Cartesian voxelization rule;},
URL = {http://dx.doi.org/10.1109/ACCESS.2020.3047421},
} 


@article{15592676 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Long-term RFID SLAM using Short-Range Sparse Tags},
journal = {International Journal of Automation and Smart Technology},
journal = {Int. J. Autom. Smart Technol. (Taiwan)},
author = {Jiun-Fu Chen and Chieh-Chih Wang},
volume = { 5},
number = { 1},
year = {2015/03/},
pages = {61 - 75},
issn = {2223-9766},
address = {Taiwan},
abstract = {While on the path forward to the long-term or lifelong robotics, one of the most important capabilities is to have a reliable localization and mapping module. Data association and loop detection play critical roles in the localization and mapping problem. By utilizing the radio frequency identification (RFID) technology, these problems can be solved using the extended Kalman filter (EKF) based simultaneous localization and mapping (SLAM) with the tag information. But one of the critical barriers to the long-term SLAM is the overconfidence issue. In this paper, we focus on solving the overconfidence issue, which is introduced by the linearization errors. An Unit Circle Representation (UCR) is proposed to diminish the error in the prediction stage and a Correlation Coefficient Preserved Inflation (CCPI) is developed to recover the overconfidence issue in the update stage. Based on only odometry and sparse short-range RFID data, the proposed method is capable to compensate the linearization errors in both simulation and real experiments.},
keywords = {Kalman filters;nonlinear filters;radiofrequency identification;sensor fusion;SLAM (robots);},
note = {long-term RFID SLAM;short-range sparse tags;data association;loop detection;localization and mapping problem;radio frequency identification;extended Kalman filter;EKF;unit circle representation;UCR;correlation coefficient preserved inflation;CCPI;},
URL = {http://dx.doi.org/10.5875/ausmt.v5i1.843},
} 


@inproceedings{19298973 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {View management for lifelong visual maps},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Banerjee, N. and Connolly, R.C. and Lisin, D. and Briggs, J. and Munich, M.E.},
year = {2019//},
pages = {7871 - 8},
address = {Piscataway, NJ, USA},
abstract = {The time complexity of making observations and loop closures in a graph-based visual SLAM system is a function of the number of views stored [1], [2]. Clever algorithms, such as approximate nearest neighbor search, can make this function sub-linear. Despite this, over time the number of views can still grow to a point at which the speed and/or accuracy of the system becomes unacceptable, especially in computation- and memory-constrained SLAM systems. However, not all views are created equal. Some views are rarely observed, because they have been created in an unusual lighting condition, or from low quality images, or in a location whose appearance has changed. These views can be removed to improve the overall performance of a SLAM system. In this paper, we propose a method for pruning views in a visual SLAM system to maintain its speed and accuracy for long term use.},
keywords = {computational complexity;graph theory;image capture;mobile robots;pose estimation;robot vision;SLAM (robots);},
note = {view management;time complexity;loop closures;graph-based visual SLAM system;memory-constrained SLAM systems;lifelong visual maps;computation-constrained SLAM systems;mobile robots;robot pose;image capture;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8968245},
} 


@inproceedings{19987356 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships},
journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Peng Gao and Hao Zhang},
year = {2020//},
pages = {1070 - 6},
address = {Piscataway, NJ, USA},
abstract = {Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.},
keywords = {graph theory;image matching;mobile robots;robot vision;SLAM (robots);},
note = {robotics applications;simultaneously localization and mapping;spatial relationship similarities;spatial cues;visual cues;old landmarks;long-term environment changes;landmark information;integrate landmark appearances;worst-case graph matching;place recognition performance;long-term place recognition;worst appearance similarity;similar appearances;worst-case scenario;graph matching problem;visual appearances;angular spatial relationships;graph representation;},
URL = {http://dx.doi.org/10.1109/ICRA40945.2020.9196906},
} 


@article{19469740 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Duckiefloat: a Collision-Tolerant Resource-Constrained Blimp for Long-Term Autonomy in Subterranean Environments [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Yi-Wei Huang and Chen-Lung Lu and Kuan-Lin Chen and Po-Sheng Ser and Jui-Te Huang and Yu-Chia Shen and Pin-Wei Chen and Po-Kai Chang and Sheng-Cheng Lee and Hsueh-Cheng Wang},
year = {2019/10/31},
pages = {7 pp. - },
address = {USA},
abstract = {There are several challenges for search and rescue robots: mobility, perception, autonomy, and communication. Inspired by the DARPA Subterranean (SubT) Challenge, we propose an autonomous blimp robot, which has the advantages of low power consumption and collision-tolerance compared to other aerial vehicles like drones. This is important for search and rescue tasks that usually last for one or more hours. However, the underground constrained passages limit the size of blimp envelope and its payload, making the proposed system resource-constrained. Therefore, a careful design consideration is needed to build a blimp system with on-board artifact search and SLAM. In order to reach long-term operation, a failure-aware algorithm with minimal communication to human supervisor to have situational awareness and send control signals to the blimp when needed.},
keywords = {aerospace robotics;rescue robots;SLAM (robots);},
note = {Duckiefloat;situational awareness;failure-aware algorithm;SLAM;blimp envelope;underground constrained passages;rescue tasks;aerial vehicles;collision-tolerance;low power consumption;autonomous blimp robot;DARPA Subterranean Challenge;rescue robots;subterranean environments;long-term autonomy;collision-tolerant resource-constrained blimp;minimal communication;long-term operation;on-board artifact search;blimp system;},
} 


@inproceedings{20256982 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Segmented Matching Method of Multi-Geophysics Field SLAM Data Based on LSTM},
journal = {2020 3rd International Conference on Unmanned Systems (ICUS)},
author = {ZiYuan Li and HuaPeng Yu and TongSheng Shen and ZhiHui Li},
year = {2020//},
pages = {147 - 51},
address = {Piscataway, NJ, USA},
abstract = {At present, simultaneous localization and mapping (SLAM) has become an important method for autonomous underwater vehicles (AUVs) to realize long-term navigation. However, using only bathymetric data in unknown environment has its own disadvantages, that are low precision and large computational load. To tackle with requirements of high-precision navigation under large-scale and long-term voyage condition, a SLAM method and corresponding matching algorithm for integrating multi-geophysical field data are proposed. By dividing the feature data and location data of geophysical field obtained into various submaps and sub-segments during AUV sailing, the dominant navigation data of each segment is identified using long short-term memory network. Validity of the proposed method is done by simulation experiments. During the simulation, the loop closure detection of each submap is used, and the matching counter is set to check the correct matching rate. Finally, the matching results with single geophysics field data under the same conditions are compared with multi-geophysics field data and analyzed. The experimental results have demonstrated the feasibility and correctness of the proposed method.},
keywords = {autonomous underwater vehicles;geophysics computing;image matching;mobile robots;path planning;recurrent neural nets;robot vision;SLAM (robots);},
note = {short-term memory network;matching counter;single geophysics field data;segmented matching method;multigeophysics field SLAM data;autonomous underwater vehicles;long-term navigation;bathymetric data;high-precision navigation;long-term voyage condition;dominant navigation data;LSTM;simultaneous localization and mapping;loop closure detection;},
URL = {http://dx.doi.org/10.1109/ICUS50048.2020.9274964},
} 


@article{21505942 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Keeping an Eye on Things: Deep Learned Features for Long-Term Visual Localization},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Gridseth, M. and Barfoot, T.D.},
volume = { 7},
number = { 2},
year = {2022//},
pages = {1016 - 23},
issn = {2377-3766},
address = {USA},
abstract = {In this letter, we learn visual features that we use to first build a map and then localize a robot driving autonomously across a full day of lighting change, including in the dark. We train a neural network to predict sparse keypoints with associated descriptors and scores that can be used together with a classical pose estimator for localization. Our training pipeline includes a differentiable pose estimator such that training can be supervised with ground truth poses from data collected earlier, in our case from 2016 and 2017 gathered with multi-experience Visual Teach and Repeat (VT&amp;R). We insert the learned features into the existing VT&amp;R pipeline to perform closed-loop path following in unstructured outdoor environments. We show successful path following across all lighting conditions despite the robot's map being constructed using daylight conditions. Moreover, we explore generalizability of the features by driving the robot across all lighting conditions in new areas not present in the feature training dataset. In all, we validated our approach with 35.5 km of autonomous path following experiments in challenging conditions.},
keywords = {deep learning (artificial intelligence);feature extraction;mobile robots;path planning;pose estimation;robot vision;SLAM (robots);},
note = {deep learning;visual localization;visual features;neural network;ground truth poses;VT&amp;R;daylight conditions;autonomous path following;sparse keypoints prediction;Visual Teach and Repeat;closed-loop path following;map building;},
URL = {http://dx.doi.org/10.1109/LRA.2021.3136867},
} 


@article{21014775 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Semi-Direct Monocular SLAM With Three Levels of Parallel Optimizations},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Shouyi Lu and Yongshuai Zhi and Sumin Zhang and Rui He and Zhipeng Bao},
volume = { 9},
year = {2021//},
pages = {86801 - 86810},
issn = {2169-3536},
address = {USA},
abstract = {In practical applications, how to use the complementary strengths of the direct and the feature-based methods for effective fusion may be the main challenge of simultaneous localization and mapping (SLAM). To solve this challenge, we propose the DO-SLAM, a novel fast and accurate semi-direct visual SLAM framework, which can maintain the direct method's fast performance and the high precision and loop closure capability of the feature-based method. The direct method is used as the first half of the DO-SLAM to track the camera pose rapidly and robustly. The feature-based method is used as the second half of the DO-SLAM to refine the keyframe poses, perform loop closures, and build a globally consistent, long-term, sparse feature map that can be reused. The proposed pipeline fuses direct odometry and feature-based SLAM to perform three levels of parallel optimizations: (1) In the direct method module, the keyframe poses are estimated by minimizing the photometric error, (2) In the feature-based module, using the poses calculated by the inter-frame matching to correct and fuse the poses calculated by the direct method module as the initial poses, and the initial poses are optimized by the motion-only bundle adjustment, and (3) A pose graph optimization is used to achieve global map consistency in the presence of loop closures. Experimental evaluation on two benchmark datasets demonstrates that the proposed approach achieves higher accuracy and robustness on motion estimation compared to the other state-of-the-art methods.},
keywords = {cameras;distance measurement;graph theory;mobile robots;motion estimation;pose estimation;robot vision;SLAM (robots);},
note = {DO-SLAM;keyframe poses;loop closures;sparse feature map;pipeline fuses direct odometry;parallel optimizations;direct method module;feature-based module;initial poses;semidirect monocular SLAM;feature-based method;semidirect visual SLAM framework;loop closure capability;},
URL = {http://dx.doi.org/10.1109/ACCESS.2021.3071921},
} 


@inproceedings{19298876 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Predictive and adaptive maps for long-term visual navigation in changing environments},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Halodova, L. and Dvorrakova, E. and Majer, F. and Vintr, T. and Martinez Mozos, O. and Dayoub, F. and Krajnik, T.},
year = {2019//},
pages = {7033 - 9},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we compare different map management techniques for long-term visual navigation in changing environments. In this scenario, the navigation system needs to continuously update and refine its feature map in order to adapt to the environment appearance change. To achieve reliable long-term navigation, the map management techniques have to (i) select features useful for the current navigation task, (ii) remove features that are obsolete, (iii) and add new features from the current camera view to the map. We propose several map management strategies and evaluate their performance with regard to the robot localisation accuracy in long-term teach-and-repeat navigation. Our experiments, performed over three months, indicate that strategies which model cyclic changes of the environment appearance and predict which features are going to be visible at a particular time and location, outperform strategies which do not explicitly model the temporal evolution of the changes.},
keywords = {cameras;feature selection;mobile robots;navigation;robot vision;SLAM (robots);},
note = {predictive maps;adaptive maps;long-term visual navigation;navigation system;feature map;environment appearance change;long-term navigation;map management strategies;teach- repeat navigation;model cyclic changes;long-term teach-and-repeat navigation;feature selection;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8967994},
} 


@article{19387316 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Learning matchable image transformations for long-term metric visual localization},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Clement, L. and Gridseth, M. and Tomasi, J. and Kelly, J.},
volume = { 5},
number = { 2},
year = {2020/04/},
pages = {1492 - 9},
issn = {2377-3774},
address = {USA},
abstract = {Long-term metric self-localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems due to appearance changes caused by lighting, weather, or seasonal variations. While experience-based mapping has proven to be an effective technique for bridging the `appearance gap,' the number of experiences required for reliable metric localization over days or months can be very large, and methods for reducing the necessary number of experiences are needed for this approach to scale. Taking inspiration from color constancy theory, we learn a nonlinear RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature matches for images captured under different lighting and weather conditions, and use it as a pre-processing step in a conventional single-experience localization pipeline to improve its robustness to appearance change. We train this mapping by approximating the target non-differentiable localization pipeline with a deep neural network, and find that incorporating a learned low-dimensional context feature can further improve cross-appearance feature matching. Using synthetic and real-world datasets, we demonstrate substantial improvements in localization performance across day-night cycles, enabling continuous metric localization over a 30-hour period using a single mapping experience, and allowing experience-based localization to scale to long deployments with dramatically reduced data requirements.},
keywords = {feature extraction;image colour analysis;image matching;learning (artificial intelligence);mobile robots;neural nets;object recognition;robot vision;SLAM (robots);},
note = {matchable image transformations;long-term metric visual localization;long-term metric self-localization;autonomous mobile robots;vision-based systems;appearance change;seasonal variations;experience-based mapping;appearance gap;reliable metric localization;color constancy theory;RGB-to-grayscale mapping;inlier feature;weather conditions;single-experience localization pipeline;target nondifferentiable localization pipeline;low-dimensional context feature;cross-appearance feature matching;localization performance;day-night cycles;continuous metric localization;single mapping experience;experience-based localization;dramatically reduced data requirements;},
URL = {http://dx.doi.org/10.1109/LRA.2020.2967659},
} 


@article{20174800 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {A Hybrid Map with Permanent 3D Wireframes and Temporal Line Segments toward Long-Term Visual Localization},
journal = {SICE Journal of Control, Measurement, and System Integration},
journal = {SICE J. Control Meas. Syst. Integr. (Japan)},
author = {Kaneko, T. and Takahashi, J. and Ito, S. and Tobe, Y.},
volume = { 12},
number = { 4},
year = {2019/07/},
pages = {149 - 55},
issn = {1882-4889},
address = {Japan},
abstract = {This paper deals with map-construction problems for visual localization. Basically, the map is an aggregation of visual landmarks, and it is desirable that such landmarks exist densely and permanently for long-term localization. However, there are no landmarks with these attributes at the same time. In order to solve this problem, we propose a hybrid map with permanent landmarks and temporal landmarks. As a permanent landmark, we employ 3D wireframe which can be easily obtained from architectural CAD. For a temporal landmark, we use line segments which are visually detected in images captured by a camera. To handle these two types of landmarks on the same map, we develop two algorithms. One is to extract temporal line segments from images containing two mixed landmarks, and the other is to reconstruct them into the 3D wireframe map. We experimentally demonstrated that the proposed hybrid map outperformed the 3D wireframe map in terms of localization accuracy.},
keywords = {architectural CAD;cameras;feature extraction;graph theory;image matching;image segmentation;mobile robots;robot vision;SLAM (robots);},
note = {visual landmarks;map-construction problems;long-term visual localization;permanent 3D wireframes;localization accuracy;3D wireframe map;mixed landmarks;temporal line segments;temporal landmark;permanent landmark;hybrid map;long-term localization;},
} 


@inproceedings{16709687 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Mining DCNN landmarks for long-term visual SLAM},
journal = {2016 IEEE International Conference on Robotics and Biomimetics (ROBIO). Proceedings},
author = {Taisho, T. and Kanji, T.},
year = {2016//},
pages = {570 - 6},
address = {Piscataway, NJ, USA},
abstract = {Long-term visual SLAM, in familiar, semi-dynamic, and partially changing environments is an important area of research in robotics. The main problem we faced is the question of how to describe a scene discriminatively and compactly-both of which are necessary in order to cope with changes in appearance and a large amount of visual information. In this study, we address the above issues by mining visual experience. Our strategy is to mine a library of raw visual images, termed visual experience, to find the relevant visual patterns to effectively explain the input scene. From a practical point of view, our work offers three main contributions over the previous work. First, it is the first application of discriminative visual features from deep convolutional neural networks (DCNN) to the task of visual landmark mining. Second, we show how to interpret a high-dimensional DCNN feature to a compact semantic representation of visual word. Third, we show that our approach can turn the scene description task with any feature (including the DCNN feature) into the task of mining visual experience. Experiments on a challenging cross-domain visual place recognition validate efficacy of the proposed approach.},
keywords = {data mining;feedforward neural nets;object recognition;SLAM (robots);},
note = {DCNN landmarks mining;visual SLAM;discriminative visual features;deep convolutional neural networks;visual landmark mining;visual word semantic representation;cross-domain visual place recognition;},
URL = {http://dx.doi.org/10.1109/ROBIO.2016.7866383},
} 


@inproceedings{20199512 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Lightweight SLAM with automatic orientation correction using 2D LiDAR scans},
journal = {2020 23rd International Symposium on Measurement and Control in Robotics (ISMCR)},
author = {Peter, G. and Kiss, B.},
year = {2020//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Simultaneous localization and mapping (SLAM) is about consistent maps in the long run. Loop closing is the most popular way for ensure long-term consistency in presence of multiple measurements by the same or multiple robots. Loop closure can be executed using raw odometrical data, but a more sophisticated, yet still light-weight method is presented in this paper: a landmark descriptor-based relative displacement calculation method for diminishing unwanted orientation errors that otherwise often lead to map inconsistency. Landmark descriptors are created using light detection and ranging (LiDAR) scans and the relation is calculated using scan-matching. The novelty of this research is a method providing long-term orientation and position correction without additional overhead between landmark detections, thus enabling simple agents to do the SLAM in a cooperative way.},
keywords = {feature extraction;mobile robots;optical radar;path planning;robot vision;SLAM (robots);},
note = {scan-matching;long-term orientation;position correction;landmark detections;lightweight SLAM;automatic orientation correction;2D LiDAR scans;consistent maps;loop closing;long-term consistency;same robots;multiple robots;loop closure;raw odometrical data;light-weight method;landmark descriptor-based relative displacement calculation method;unwanted orientation errors;map inconsistency;landmark descriptors;light detection;},
URL = {http://dx.doi.org/10.1109/ISMCR51255.2020.9263722},
} 


@inproceedings{21406958 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Evaluation of Long-term LiDAR Place Recognition},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Peltomaki, J. and Alijani, F. and Puura, J. and Huttunen, H. and Rahtu, E. and Kamarainen, J.-K.},
year = {2021//},
pages = {4487 - 92},
address = {Piscataway, NJ, USA},
abstract = {We compare a state-of-the-art deep image retrieval and a deep place recognition method for place recognition using LiDAR data. Place recognition aims to detect previously visited locations and thus provides an important tool for navigation, mapping, and localisation. Experimental comparisons are conducted using challenging outdoor and indoor datasets, Oxford Radar RobotCar and COLD, in the "long-term" setting where the test conditions differ substantially from the training and gallery data. Based on our results the image retrieval methods using LiDAR depth images can achieve accurate localization (the single best match recall 80%) within 5.00 m in urban outdoors. In office indoors the comparable accuracy is 50 cm but is more sensitive to changes in the environment.},
keywords = {feature extraction;image classification;image matching;image recognition;image retrieval;object recognition;optical radar;remote sensing by laser beam;robot vision;SLAM (robots);visual databases;},
note = {long-term LiDAR place recognition;state-of-the-art deep image retrieval;deep place recognition method;LiDAR data;place recognition aims;visited locations;experimental comparisons;challenging outdoor datasets;indoor datasets;Oxford Radar RobotCar;gallery data;image retrieval methods;LiDAR depth images;size 5.0 m;size 50.0 cm;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9636320},
} 


@inproceedings{21503847 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Robust and Long-term Monocular Teach and Repeat Navigation using a Single-experience Map},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Li Sun and Taher, M. and Wild, C. and Cheng Zhao and Yu Zhang and Majer, F. and Zhi Yan and Krajnik, T. and Prescott, T. and Duckett, T.},
year = {2021//},
pages = {2635 - 42},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a robust monocular visual teach-and-repeat (VT&amp;R) navigation system for long-term operation in outdoor environments. The approach leverages deep-learned descriptors to deal with the high illumination variance of the real world. In particular, a tailored self-supervised descriptor, DarkPoint, is proposed for autonomous navigation in outdoor environments. We seamlessly integrate the localisation with control, in which proportional-integral control is used to eliminate the visual error with the pitfall of the unknown depth. Consequently, our approach achieves day-to-night navigation using a single-experience map and is able to repeat complex and fast manoeuvres. To verify our approach, we performed a vast array of navigation experiments in various outdoor environments, where both navigation accuracy and robustness of the proposed system are investigated. The experimental results show that our approach is superior to the baseline method with regards to accuracy and robustness.},
keywords = {deep learning (artificial intelligence);distance measurement;mobile robots;radionavigation;robot vision;},
note = {proportional-integral control;visual error;day-to-night navigation;single-experience map;navigation experiments;outdoor environments;long-term operation;deep-learned descriptors;high illumination variance;self-supervised descriptor;autonomous navigation;robust monocular teach;long-term monocular teach;robust monocular visual teach-and-repeat navigation system;DarkPoint;vast array;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9635886},
} 


@inproceedings{19302076 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Resilience Perception for Active Long-term Autonomy},
journal = {2019 19th International Conference on Control, Automation and Systems (ICCAS)},
author = {Ayoung Kim},
year = {2019//},
pages = {19 - },
address = {Piscataway, NJ, USA},
abstract = {Robot navigation relies on both extrovert and introvert sensors to infer the robot status. The extrovert sensors including cameras and LiDARs have been widely utilized, dominantly leading recent autonomous navigation research. Unfortunately, these sensors are strongly depending on the environment that encompasses both structural and temporal variance (e.g., illumination and structural variance). Without resilient sensor exploitation, the robust navigation is never achievable yielding even catastrophic results. This talk will introduce robust sensor fusion and control and its application to perceptual Simultaneous Localization and Mapping (SLAM) covering from underwater to urban environment.},
keywords = {mobile robots;navigation;optical radar;robot vision;sensor fusion;SLAM (robots);},
note = {resilience perception;active long-term autonomy;robot navigation;extrovert sensors;introvert sensors;structural variance;temporal variance;resilient sensor exploitation;sensor fusion;urban environment;LiDARs;autonomous navigation research;perceptual simultaneous localization and mapping;SLAM;},
} 


@inproceedings{19321492 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Tightly Coupled Semantic RGB-D Inertial Odometry for Accurate Long-Term Localization and Mapping},
journal = {2019 19th International Conference on Advanced Robotics (ICAR). Proceedings},
author = {Patel, N. and Khorrami, F. and Krishnamurthy, P. and Tzes, A.},
year = {2019//},
pages = {523 - 8},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we utilize semantically enhanced feature matching and visual inertial bundle adjustment to improve the robustness of odometry especially in feature-sparse environments. A novel semantically enhanced feature matching algorithm is developed for robust: 1) medium and long-term tracking, and 2) loop-closing. Additionally, a semantic visual inertial bundle adjustment algorithm is introduced to robustly estimate pose in presence of ambiguous correspondences or in feature sparse environment. Our tightly coupled semantic RGB-D odometry approach is demonstrated on a real world indoor dataset collected using our unmanned ground vehicle (UGV). Our approach improves traditional visual odometry relying on low-level geometric features like corners, points, and planes for localization and mapping. Additionally, prior approaches are limited due to their sensitivity to scene geometry and changes in light intensity. The semantic inertial odometry is especially important to significantly reduce drifts in longer intervals.},
keywords = {distance measurement;feature extraction;geometry;image matching;mobile robots;pose estimation;robot vision;SLAM (robots);},
note = {feature-sparse environments;long-term tracking;semantic visual inertial bundle adjustment algorithm;feature sparse environment;tightly coupled semantic RGB-D odometry approach;traditional visual odometry;low-level geometric features;semantic inertial odometry;semantic RGB-D inertial odometry;accurate long-term localization;semantically enhanced feature matching algorithm;},
URL = {http://dx.doi.org/10.1109/ICAR46387.2019.8981658},
} 


@article{18787621 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Communication constrained cloud-based long-term visual localization in real time [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Xiaqing Ding and Yue Wang and Li Tang and Huan Yin and Rong Xiong},
year = {2019/03/10},
pages = {8 pp. - },
address = {USA},
abstract = {Visual localization is one of the primary capabilities for mobile robots. Long-term visual localization in real time is particularly challenging, in which the robot is required to efficiently localize itself using visual data where appearance may change significantly over time. In this paper, we propose a cloud-based visual localization system targeting at long-term localization in real time. On the robot, we employ two estimators to achieve accurate and real-time performance. One is a sliding-window based visual inertial odometry, which integrates constraints from consecutive observations and selfmotion measurements, as well as the constraints induced by localization on the cloud. This estimator builds a local visual submap as the virtual observation which is then sent to the cloud as new localization constraints. The other one is a delayed state Extended Kalman Filter to fuse the pose of the robot localized from the cloud, the local odometry and the high-frequency inertial measurements. On the cloud, we propose a longer sliding-window based localization method to aggregate the virtual observations for larger field of view, leading to more robust alignment between virtual observations and the map. Under this architecture, the robot can achieve drift-free and real-time localization using onboard resources even in a network with limited bandwidth, high latency and existence of package loss, which enables the autonomous navigation in real-world environment. We evaluate the effectiveness of our system on a dataset with challenging seasonal and illuminative variations. We further validate the robustness of the system under challenging network conditions.},
keywords = {distance measurement;Kalman filters;mobile robots;nonlinear filters;path planning;robot vision;SLAM (robots);},
note = {cloud-based long-term;mobile robots;long-term visual localization;visual data;cloud-based visual localization system;long-term localization;real-time performance;sliding-window based visual inertial odometry;local visual submap;virtual observation;localization constraints;local odometry;high-frequency inertial measurements;localization method;real-time localization;},
} 


@inproceedings{20414223 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Integration and evaluation of SLAM-based backpack mobile mapping system},
journal = {E3S Web of Conferences},
journal = {E3S Web Conf. (France)},
author = {Peidong Yu and Mengke Wang and Huanjian Chen},
volume = { 206},
year = {2020//},
pages = {03014 (6 pp.) - },
issn = {2267-1242},
address = {France},
abstract = {Mobile mapping is an efficient technology to acquire spatial data of the environment. As a supplement of vehicle-borne and air-borne methods, Backpack mobile mapping system (MMS) has a wide application prospect in indoor and underground space. High-precision positioning and attitude determination are the key to MMS. Usually, GNSS/INS integrated navigation system provides reliable pose information. However, in the GNSS-denied environments, there is no effective long-term positioning method. With the development of simultaneous localization and mapping (SLAM) algorithm, it provides a new solution for indoor mobile mapping. This paper develops a portable backpack mobile mapping system, which integrates multi-sensor such as LiDAR, IMU, GNSS and panoramic camera. The 3D laser SLAM algorithm is applied to the mobile mapping to realize the acquisition of geographic information data in various complex environments. The experimental results in typical indoor and outdoor scenes show that the system can achieve high-precision and efficient acquisition of 3D information, and the relative precision of point cloud is 2~4cm, which meets the requirements of scene mapping and reconstruction.},
keywords = {cameras;geographic information systems;image fusion;image reconstruction;indoor navigation;inertial navigation;mobile computing;optical radar;satellite navigation;SLAM (robots);},
note = {SLAM-based backpack mobile mapping system;spatial data;backpack MMS;indoor space;underground space;attitude determination;GNSS-denied environments;long-term positioning method;indoor mobile mapping;portable backpack mobile mapping system;3D laser SLAM algorithm;geographic information data;complex environments;typical indoor scenes;outdoor scenes;scene mapping;high-precision positioning;GNSS-INS integrated navigation system;simultaneous localization and mapping algorithm;LiDAR;IMU;panoramic camera;scene reconstruction;},
URL = {http://dx.doi.org/10.1051/e3sconf/202020603014},
} 


@inproceedings{20424976 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Tong Qin and Tongqing Chen and Yilun Chen and Qing Su},
year = {2020//},
pages = {5939 - 45},
address = {Piscataway, NJ, USA},
abstract = {Autonomous valet parking is a specific application for autonomous vehicles. In this task, vehicles need to navigate in narrow, crowded and GPS-denied parking lots. Accurate localization ability is of great importance. Traditional visual-based methods suffer from tracking lost due to texture-less regions, repeated structures, and appearance changes. In this paper, we exploit robust semantic features to build the map and localize vehicles in parking lots. Semantic features contain guide signs, parking lines, speed bumps, etc, which typically appear in parking lots. Compared with traditional features, these semantic features are long-term stable and robust to the perspective and illumination change. We adopt four surround-view cameras to increase the perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system generates a global visual semantic map. This map is further used to localize vehicles at the centimeter level. We analyze the accuracy and recall of our system and compare it against other methods in real experiments. Furthermore, we demonstrate the practicability of the proposed system by the autonomous parking application.},
keywords = {cameras;data visualisation;driver information systems;feature extraction;Global Positioning System;mobile robots;object detection;road vehicles;robot vision;SLAM (robots);traffic engineering computing;},
note = {AVP-SLAM;semantic visual mapping;autonomous vehicles;parking lot;autonomous valet parking;GPS-denied parking lots;accurate localization ability;traditional visual-based methods;appearance changes;robust semantic features;localize vehicles;traditional features;global visual semantic map;autonomous parking application;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9340939},
} 


@inproceedings{19078690 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from 3-D Lidar Scans},
journal = {2019 European Conference on Mobile Robots (ECMR). Proceedings},
author = {Schaefer, A. and Buscher, D. and Vertens, J. and Luft, L. and Burgard, W.},
year = {2019//},
pages = {7 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation [1]. In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15 months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy.},
keywords = {mobile robots;optical radar;},
note = {long-term urban vehicle localization;long-term stability;pole-like objects;urban environments;complete mapping;long-term localization system;3D lidar data;novel pole detector;mapping module;online localization module;open-source implementation;long-term reliability;urban area;initial map;3D lidar scans;pole landmarks extraction;mobile robot;time 15.0 month;},
URL = {http://dx.doi.org/10.1109/ECMR.2019.8870928},
} 


@inproceedings{20425206 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Improving Visual SLAM in Car-Navigated Urban Environments with Appearance Maps},
journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Jaenal, A. and Zuniga-Noel, D. and Gomez-Ojeda, R. and Gonzalez-Jimenez, J.},
year = {2020//},
pages = {4679 - 85},
address = {Piscataway, NJ, USA},
abstract = {This paper describes a method that corrects errors of a VSLAM-estimated trajectory for cars driving in GPS-denied environments, by applying constraints from public databases of geo-tagged images (Google Street View, Mapillary, etc). The method, dubbed Appearance-based Geo-Alignment for Simultaneous Localisation and Mapping (AGA-SLAM), encodes the available image database as an appearance map, which represents the space with a compact holistic descriptor for each image plus its associated geo-tag. The VSLAM trajectory is corrected on-line by incorporating constraints from the recognized places along the trajectory into a position-based optimization framework. The paper presents a seamless formulation to combine local and absolute metric observations with associations from Visual Place Recognition. The robustness of the holistic image descriptor to changes due to weather or illumination variations ensures a long-term consistent method to improve car localization. The proposed method has been extensively evaluated on more than 70 sequences from 4 different datasets, proving out its effectiveness and endurance to appearance challenges.},
keywords = {Global Positioning System;mobile robots;navigation;optimisation;pose estimation;robot vision;SLAM (robots);visual databases;},
note = {Visual SLAM;car-navigated urban environments;VSLAM-estimated trajectory;cars;GPS-denied environments;public databases;geo-tagged images;Google Street View;Mapillary;dubbed Appearance-based Geo-Alignment;appearance challenges;car localization;long-term consistent method;holistic image descriptor;Visual Place Recognition;absolute metric observations;local observations;seamless formulation;position-based optimization framework;recognized places;incorporating constraints;VSLAM trajectory;associated geo-tag;compact holistic descriptor;appearance map;available image database;AGA-SLAM;Mapping;Simultaneous Localisation;},
URL = {http://dx.doi.org/10.1109/IROS45743.2020.9341451},
} 


@article{19955315 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Structure-SLAM: Low-Drift Monocular SLAM in Indoor Environments},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Yanyan Li and Brasch, N. and Yida Wang and Navab, N. and Tombari, F.},
volume = { 5},
number = { 4},
year = {2020/10/},
pages = {6583 - 90},
issn = {2377-3774},
address = {USA},
abstract = {In this letter a low-drift monocular SLAM method is proposed targeting indoor scenarios, where monocular SLAM often fails due to the lack of textured surfaces. Our approach decouples rotation and translation estimation of the tracking process to reduce the long-term drift in indoor environments. In order to take full advantage of the available geometric information in the scene, surface normals are predicted by a convolutional neural network from each input RGB image in real-time. First, a drift-free rotation is estimated based on lines and surface normals using spherical mean-shift clustering, leveraging the weak Manhattan World assumption. Then translation is computed from point and line features. Finally, the estimated poses are refined with a map-to-frame optimization strategy. The proposed method outperforms the state of the art on common SLAM benchmarks such as ICL-NUIM and TUM RGB-D.},
keywords = {convolutional neural nets;image colour analysis;indoor environment;optimisation;pose estimation;robot vision;SLAM (robots);},
note = {tracking process;long-term drift;indoor environments;geometric information;surface normals;convolutional neural network;input RGB image;drift-free rotation;estimated poses;common SLAM benchmarks;structure-SLAM;low-drift monocular SLAM method;indoor scenarios;textured surfaces;Structure-SLAM;decouples rotation;rotation estimation;translation estimation;spherical mean-shift clustering;Manhattan World assumption;map-to-frame optimization;ICL-NUIM benchmarks;TUM RGB-D benchmarks;},
URL = {http://dx.doi.org/10.1109/LRA.2020.3015456},
} 


@article{17782133 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Are you ABLE to perform a life-long visual topological localization?},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Arroyo, R. and Alcantarilla, P.F. and Bergasa, L.M. and Romera, E.},
volume = { 42},
number = { 3},
year = {2018/03/},
pages = {665 - 85},
issn = {0929-5593},
address = {Germany},
abstract = {Visual topological localization is a process typically required by varied mobile autonomous robots, but it is a complex task if long operating periods are considered. This is because of the appearance variations suffered in a place: dynamic elements, illumination or weather. Due to these problems, long-term visual place recognition across seasons has become a challenge for the robotics community. For this reason, we propose an innovative method for a robust and efficient life-long localization using cameras. In this paper, we describe our approach (ABLE), which includes three different versions depending on the type of images: monocular, stereo and panoramic. This distinction makes our proposal more adaptable and effective, because it allows to exploit the extra information that can be provided by each type of camera. Besides, we contribute a novel methodology for identifying places, which is based on a fast matching of global binary descriptors extracted from sequences of images. The presented results demonstrate the benefits of using ABLE, which is compared to the most representative state-of-the-art algorithms in long-term conditions.},
keywords = {image matching;image sequences;mobile robots;robot dynamics;robot vision;SLAM (robots);},
note = {life-long visual topological localization;dynamic elements;long-term visual place recognition;robotics community;mobile autonomous robots;robust life-long localization;cameras;ABLE;image sequence;image matching;},
URL = {http://dx.doi.org/10.1007/s10514-017-9664-7},
} 


@article{21512258 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Navigation Engine Design for Automated Driving Using INS/GNSS/3D LiDAR-SLAM and Integrity Assessment},
journal = {Remote Sensing},
journal = {Remote Sens. (Switzerland)},
author = {Kai-Wei Chiang and Guang-Je Tsai and Yu-Hua Li and You Li and El-Sheimy, N.},
volume = { 12},
number = { 10},
year = {2020/05/},
pages = {1564 (24 pp.) - },
issn = {2072-4292},
address = {Switzerland},
abstract = {Automated driving has made considerable progress recently. The multisensor fusion system is a game changer in making self-driving cars possible. In the near future, multisensor fusion will be necessary to meet the high accuracy needs of automated driving systems. This paper proposes a multisensor fusion design, including an inertial navigation system (INS), a global navigation satellite system (GNSS), and light detection and ranging (LiDAR), to implement 3D simultaneous localization and mapping (INS/GNSS/3D LiDAR-SLAM). The proposed fusion structure enhances the conventional INS/GNSS/odometer by compensating for individual drawbacks such as INS-drift and error-contaminated GNSS. First, a highly integrated INS-aiding LiDAR-SLAM is presented to improve the performance and increase the robustness to adjust to varied environments using the reliable initial values from the INS. Second, the proposed fault detection exclusion (FDE) contributes SLAM to eliminate the failure solutions such as local solution or the divergence of algorithm. Third, the SLAM position velocity acceleration (PVA) model is used to deal with the high dynamic movement. Finally, an integrity assessment benefits the central fusion filter to avoid failure measurements into the update process based on the information from INS-aiding SLAM, which increases the reliability and accuracy. Consequently, our proposed multisensor design can deal with various situations such as long-term GNSS outage, deep urban areas, and highways. The results show that the proposed method can achieve an accuracy of under 1 meter in challenging scenarios, which has the potential to contribute the autonomous system.},
keywords = {fault diagnosis;inertial navigation;intelligent transportation systems;Kalman filters;mobile robots;optical radar;position control;robot vision;satellite navigation;sensor fusion;SLAM (robots);},
note = {INS-drift;error-contaminated GNSS;highly integrated INS-aiding LiDAR-SLAM;SLAM position velocity acceleration model;high dynamic movement;central fusion filter;INS-aiding SLAM;multisensor design;long-term GNSS outage;multisensor fusion system;automated driving systems;multisensor fusion design;inertial navigation system;global navigation satellite system;},
URL = {http://dx.doi.org/10.3390/rs12101564},
} 


@inproceedings{19299148 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Communication constrained cloud-based long-term visual localization in real time},
journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Xiaqing Ding and Yue Wang and Li Tang and Huan Yin and Rong Xiong},
year = {2019//},
pages = {2159 - 66},
address = {Piscataway, NJ, USA},
abstract = {Visual localization is one of the primary capabilities for mobile robots. Long-term visual localization in real time is particularly challenging, in which the robot is required to efficiently localize itself using visual data where appearance may change significantly over time. In this paper, we propose a cloud-based visual localization system targeting at long-term localization in real time. On the robot, we employ two estimators to achieve accurate and real-time performance. One is a sliding-window based visual inertial odometry, which integrates constraints from consecutive observations and self-motion measurements, as well as the constraints induced by localization results from the cloud. This estimator builds a local visual submap as the virtual observation which is then sent to the cloud as new localization constraints. The other one is a delayed state Extended Kalman Filter to fuse the pose of the robot localized from the cloud, the local odometry and the high-frequency inertial measurements. On the cloud, we propose a longer sliding-window based localization method to aggregate the virtual observations for larger field of view, leading to more robust alignment between virtual observations and the map. Under this architecture, the robot can achieve drift-free and real-time localization using onboard resources even in a network with limited bandwidth, high latency and existence of package loss, which enables the autonomous navigation in real-world environment. We evaluate the effectiveness of our system on a dataset with challenging seasonal and illuminative variations. We further validate the robustness of the system under challenging network conditions.},
keywords = {cloud computing;control engineering computing;data visualisation;distance measurement;image filtering;Kalman filters;mobile robots;nonlinear filters;robot vision;SLAM (robots);},
note = {data visualization;cloud-based visual localization system;long-term localization;sliding-window based visual inertial odometry;virtual observation;localization constraints;local odometry;high-frequency inertial measurements;mobile robots;long-term visual localization;extended Kalman filter;},
URL = {http://dx.doi.org/10.1109/IROS40897.2019.8968550},
} 


@inproceedings{21504186 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {NYU-VPR: Long-Term Visual Place Recognition Benchmark with View Direction and Data Anonymization Influences},
journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Diwei Sheng and Yuxiang Chai and Xinru Li and Chen Feng and Jianzhe Lin and Silva, C. and Rizzo, J.-R.},
year = {2021//},
pages = {9773 - 9},
address = {Piscataway, NJ, USA},
abstract = {Visual place recognition (VPR) is critical in not only localization and mapping for autonomous driving vehicles, but also assistive navigation for the visually impaired population. To enable a long-term VPR system on a large scale, several challenges need to be addressed. First, different applications could require different image view directions, such as front views for self-driving cars while side views for the low vision people. Second, VPR in metropolitan scenes can often cause privacy concerns due to the imaging of pedestrian and vehicle identity information, calling for the need for data anonymization before VPR queries and database construction. Both factors could lead to VPR performance variations that are not well understood yet. To study their influences, we present the NYU-VPR dataset that contains more than 200,000 images over a 2km&times;2km area near the New York University campus, taken within the whole year of 2016. We present benchmark results on several popular VPR algorithms showing that side views are significantly more challenging for current VPR methods while the influence of data anonymization is almost negligible, together with our hypothetical explanations and in-depth analysis.},
keywords = {data privacy;image recognition;road vehicles;visual databases;},
note = {image view directions;pedestrian;metropolitan scenes;low vision people;long-term VPR system;visually impaired population;assistive navigation;autonomous driving vehicles;data anonymization influences;view direction;long-term visual place recognition benchmark;NYU-VPR dataset;VPR performance variations;VPR queries;vehicle identity information;},
URL = {http://dx.doi.org/10.1109/IROS51168.2021.9636640},
} 


@article{15339141 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Generic NDT mapping in dynamic environments and its application for lifelong SLAM},
journal = {Robotics and Autonomous Systems},
journal = {Robot. Auton. Syst. (Netherlands)},
author = {Einhorn, E. and Gross, H.-M.},
volume = { 69},
year = {2015/07/},
pages = {28 - 39},
issn = {0921-8890},
address = {Netherlands},
abstract = {In this paper, we present a new, generic approach for Simultaneous Localization and Mapping (SLAM). First of all, we propose an abstraction of the underlying sensor data using Normal Distribution Transform (NDT) maps that are suitable for making our approach independent from the used sensor and the dimension of the generated maps. We present several modifications for the original NDT mapping to handle free-space measurements explicitly. We additionally describe a method to detect and handle dynamic objects such as moving persons. This enables the usage of the proposed approach in highly dynamic environments. In the second part of this paper we describe our graph-based SLAM approach that is designed for lifelong usage. Therefore, the memory and computational complexity is limited by pruning the pose graph in an appropriate way. [All rights reserved Elsevier].},
keywords = {computational complexity;graph theory;mobile robots;normal distribution;object detection;path planning;SLAM (robots);transforms;},
note = {normal distribution transform;NDT mapping;dynamic environment;simultaneous localization and mapping;SLAM;sensor data;free-space measurement;object detection;object handling;computational complexity;pose graph pruning;mobile robotics;},
URL = {http://dx.doi.org/10.1016/j.robot.2014.08.008},
} 


@inproceedings{18372916 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A B-Spline Mapping Framework for Long-Term Autonomous Operations},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Rodrigues, R.T. and Aguiar, A.P. and Pascoal, A.},
year = {2018//},
pages = {3204 - 9},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a 2D B-spline mapping framework for representing unstructured environments in a compact manner. While occupancy-grid and landmark-based maps have been successfully employed by the robotics community in indoor scenarios, outdoor long-term autonomous operations require a more compact representation of the environment. This work tackles this problem by interpolating the data of a high frequency sensor using B-spline curves. Compared to lines and circles, splines are more powerful in the sense that they allow for the description of more complex shapes in the scene. In this work, spline curves are continuously tracked and aligned across multiple sensor readings using lightweight methods, making the proposed framework suitable for robot navigation in outdoor missions. In particular, a Simultaneous Localization and Mapping (SLAM) algorithm specifically tailored for B-spline maps is presented here. The efficacy of the proposed framework is demonstrated by Software-in-the-Loop (SiL) simulations in different scenarios.},
keywords = {image representation;image sensors;mobile robots;navigation;path planning;robot vision;SLAM (robots);splines (mathematics);},
note = {landmark-based maps;robotics community;high frequency sensor;B-spline curves;B-spline maps;mapping algorithm;2D B-spline mapping framework;outdoor long-term autonomous operations;simultaneous localization and mapping;SLAM algorithm;software-in-the-loop simulations;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8594456},
} 


@inproceedings{18392983 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Efficient Long-term Mapping in Dynamic Environments},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Lazaro, M.T. and Capobianco, R. and Grisetti, G.},
year = {2018//},
pages = {153 - 60},
address = {Piscataway, NJ, USA},
abstract = {As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.},
keywords = {graph theory;mobile robots;robot vision;SLAM (robots);},
note = {mapping problem;longterm SLAM datasets;graph coherency;intra-session loop closure detections;out-dated nodes;graph complexity;nonstatic entities;merging procedure;efficient ICP-based alignment;up-to-date state;2D point cloud data;local maps;graph SLAM paradigm;multiple mapping sessions;single mapping sessions;SLAM system;autonomous robots;dynamic environments;long-term robot operation;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8594310},
} 


@article{21039023 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Reference Pose Generation for Long-term Visual Localization via Learned Features and View Synthesis},
journal = {International Journal of Computer Vision},
journal = {Int. J. Comput. Vis. (Germany)},
author = {Zichao Zhang and Sattler, T. and Scaramuzza, D.},
volume = { 129},
number = { 4},
year = {2021//},
pages = {821 - 44},
issn = {0920-5691},
address = {Germany},
abstract = {Visual Localization is one of the key enabling technologies for autonomous driving and augmented reality. High quality datasets with accurate 6 Degree-of-Freedom (DoF) reference poses are the foundation for benchmarking and improving existing methods. Traditionally, reference poses have been obtained via Structure-from-Motion (SfM). However, SfM itself relies on local features which are prone to fail when images were taken under different conditions, e.g., day/night changes. At the same time, manually annotating feature correspondences is not scalable and potentially inaccurate. In this work, we propose a semi-automated approach to generate reference poses based on feature matching between renderings of a 3D model and real images via learned features. Given an initial pose estimate, our approach iteratively refines the pose based on feature matches against a rendering of the model from the current pose estimate. We significantly improve the nighttime reference poses of the popular Aachen Day-Night dataset, showing that state-of-the-art visual localization methods perform better (up to 47%) than predicted by the original reference poses. We extend the dataset with new nighttime test images, provide uncertainty estimates for our new reference poses, and introduce a new evaluation criterion. We will make our reference poses and our framework publicly available upon publication.},
keywords = {augmented reality;computer vision;distance measurement;feature extraction;image matching;image sequences;iterative methods;learning (artificial intelligence);pose estimation;robot vision;SLAM (robots);},
note = {state-of-the-art visual localization;original reference;reference pose generation;long-term visual Localization;Degree-of-Freedom reference poses;benchmarking improving existing methods;local features;feature correspondences;feature matching;initial pose estimate;feature matches;current pose estimate;nighttime reference;popular Aachen Day-Night dataset;},
URL = {http://dx.doi.org/10.1007/s11263-020-01399-8},
} 


@article{18803794 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Augmenting Visual SLAM with Wi-Fi Sensing For Indoor Applications [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Hashemifar, Z.S. and Adhivarahan, C. and Balakrishnan, A. and Dantu, K.},
year = {2019/03/15},
pages = {16 pp. - },
address = {USA},
abstract = {Recent trends have accelerated the development of spatial applications on mobile devices and robots. These include navigation, augmented reality, human-robot interaction, and others. A key enabling technology for such applications is the understanding of the device's location and the map of the surrounding environment. This generic problem, referred to as Simultaneous Localization and Mapping (SLAM), is an extensively researched topic in robotics. However, visual SLAM algorithms face several challenges including perceptual aliasing and high computational cost. These challenges affect the accuracy, efficiency and viability of visual SLAM algorithms, especially for long-term SLAM, and their use in resource-constrained mobile devices. A parallel trend is the ubiquity of Wi-Fi routers for quick Internet access in most urban environments. Most robots and mobile devices are equipped with a Wi-Fi radio as well. We propose a method to utilize Wi-Fi received signal strength to alleviate the challenges faced by visual SLAM algorithms. To demonstrate the utility of this idea, this work makes the following contributions: (i) We propose a generic way to integrate Wi-Fi sensing into visual SLAM algorithms, (ii) We integrate such sensing into three well-known SLAM algorithms, (iii) Using four distinct datasets, we demonstrate the performance of such augmentation in comparison to the original visual algorithms and (iv) We compare our work to Wi-Fi augmented FABMAP algorithm. Overall, we show that our approach can improve the accuracy of visual SLAM algorithms by 11% on average and reduce computation time on average by 15% to 25%.},
keywords = {augmented reality;human-robot interaction;Internet;mobile radio;mobile robots;path planning;robot vision;SLAM (robots);wireless LAN;},
note = {Wi-Fi radio;visual SLAM algorithms;Wi-Fi sensing;original visual algorithms;Wi-Fi Sensing;robots;robotics;long-term SLAM;resource-constrained mobile devices;Wi-Fi routers;},
} 


@inproceedings{19078696 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Lifelong Mapping using Adaptive Local Maps},
journal = {2019 European Conference on Mobile Robots (ECMR). Proceedings},
author = {Banerjee, N. and Lisin, D. and Briggs, J. and Llofriu, M. and Munich, M.E.},
year = {2019//},
pages = {8 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Occupancy mapping enables a mobile robot to make intelligent planning decisions to accomplish its tasks. Adaptive local maps is an algorithm which represents the occupancy information as a set of overlapping local maps anchored to poses in the robot's trajectory. At any time, a global occupancy map can be rendered from the local maps to be used for path planning. The advantage of this approach is that the occupancy information stays consistent despite the changes in the pose estimates resulting from loop closures and localization updates. The disadvantage, however, is that the number of local maps grows over time. For long robot runs, or for multiple runs in the same space, this growth will result in redundant occupancy information, which will in turn increase the time it takes to render the global map, as well as the memory footprint of the system. In this paper, we propose a novel approach for the maintenance of an adaptive local maps system, which intelligently prunes redundant local maps, ensuring the robustness and stability required for lifelong mapping.},
keywords = {mobile robots;path planning;pose estimation;SLAM (robots);},
note = {lifelong mapping;occupancy mapping;mobile robot;intelligent planning decisions;overlapping local maps;global occupancy map;localization updates;long robot runs;redundant occupancy information;adaptive local maps system;redundant local maps;robot trajectory;},
URL = {http://dx.doi.org/10.1109/ECMR.2019.8870347},
} 


@article{19208039 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {View management for lifelong visual maps [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Banerjee, N. and Connolly, R.C. and Lisin, D. and Briggs, J. and Narayana, M. and Munich, M.E.},
year = {2019/08/09},
pages = {8 pp. - },
address = {USA},
abstract = {The time complexity of making observations and loop closures in a graph-based visual SLAM system is a function of the number of views stored. Clever algorithms, such as approximate nearest neighbor search, can make this function sub-linear. Despite this, over time the number of views can still grow to a point at which the speed and/or accuracy of the system becomes unacceptable, especially in computation- and memory-constrained SLAM systems. However, not all views are created equal. Some views are rarely observed, because they have been created in an unusual lighting condition, or from low quality images, or in a location whose appearance has changed. These views can be removed to improve the overall performance of a SLAM system. In this paper, we propose a method for pruning views in a visual SLAM system to maintain its speed and accuracy for long term use.},
keywords = {computational complexity;graph theory;image sensors;mobile robots;robot vision;SLAM (robots);},
note = {loop closures;graph-based visual SLAM system;approximate nearest neighbor search;memory-constrained SLAM systems;view management;visual maps;time complexity;},
} 


@inproceedings{16503801 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Persistent localization and life-long mapping in changing environments using the Frequency Map Enhancement},
journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Krajnik, T. and Pulido Fentanes, J. and Hanheide, M. and Duckett, T.},
year = {2016//},
pages = {4558 - 63},
address = {Piscataway, NJ, USA},
abstract = {We present a lifelong mapping and localisation system for long-term autonomous operation of mobile robots in changing environments. The core of the system is a spatio-temporal occupancy grid that explicitly represents the persistence and periodicity of the individual cells and can predict the probability of their occupancy in the future. During navigation, our robot builds temporally local maps and integrates then into the global spatio-temporal grid. Through re-observation of the same locations, the spatio-temporal grid learns the long-term environment dynamics and gains the ability to predict the future environment states. This predictive ability allows to generate time-specific 2d maps used by the robot's localisation and planning modules. By analysing data from a long-term deployment of the robot in a human-populated environment, we show that the proposed representation improves localisation accuracy and the efficiency of path planning. We also show how to integrate the method into the ROS navigation stack for use by other roboticists.},
keywords = {mobile robots;navigation;operating systems (computers);path planning;robot dynamics;robot vision;SLAM (robots);},
note = {frequency map enhancement;lifelong mapping;localisation system;long-term autonomous operation;mobile robots;spatiotemporal occupancy grid;long-term environment dynamics;robot localisation;path planning;ROS navigation;},
URL = {http://dx.doi.org/10.1109/IROS.2016.7759671},
} 


@article{19955624 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Visual SLAM With Drift-Free Rotation Estimation in Manhattan World},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Jiacheng Liu and Ziyang Meng},
volume = { 5},
number = { 4},
year = {2020/10/},
pages = {6512 - 19},
issn = {2377-3774},
address = {USA},
abstract = {This letter presents an efficient and accurate simultaneous localization and mapping (SLAM) system in man-made environments. The Manhattan world assumption is imposed, with which the global orientation is obtained. The drift-free rotational motion estimation is derived from the structural regularities using line features. In particular, a two-stage vanishing points (VPs) estimation method is developed, which consists of a short-term tracking module to track the clustered line features and a long-term searching module to generate abundant sets of VPs candidates and retrieve the optimal one. A least square problem is constructed and solved to provide refined VPs with the clusters of structural line features every frame. We make full use of the absolute orientation estimation to benefit the whole SLAM process. In particular, we utilize the absolute orientation estimation to increase the localization accuracy in the front end, and formulate a linear batch camera pose refinement problem with the known rotations to improve the real time performance in the back end. Experiments on both synthesized and real-world scenes reveal results with high-precision in the real time camera pose estimation process and high-speed in pose graph optimization process compared with the existing state-of-the-art methods.},
keywords = {cameras;estimation theory;graph theory;motion estimation;optimisation;pose estimation;search problems;SLAM (robots);},
note = {drift-free rotational motion estimation;short-term tracking module;clustered line features;long-term searching module;least square problem;structural line;absolute orientation estimation;estimation process;graph optimization process;visual SLAM process;drift-free rotation estimation;linear batch camera pose refinement problem;simultaneous localization and mapping system;two-stage vanishing point estimation method;refined VP;Manhattan world assumption;man-made environments;},
URL = {http://dx.doi.org/10.1109/LRA.2020.3014648},
} 


@inproceedings{14718688 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Mining visual phrases for long-term visual SLAM},
journal = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014)},
author = {Kanji, T. and Yuuto, C. and Masatoshi, A.},
year = {2014//},
pages = {136 - 42},
address = {Piscataway, NJ, USA},
abstract = {We propose a discriminative and compact scene descriptor for single-view place recognition that facilitates long-term visual SLAM in familiar, semi-dynamic and partially changing environments. In contrast to popular bag-of-words scene descriptors, which rely on a library of vector quantized visual features, our proposed scene descriptor is based on a library of raw image data (such as an available visual experience, images shared by other colleague robots, and publicly available image data on the web) and directly mine it to find visual phrases (VPs) that discriminatively and compactly explain an input query / database image. Our mining approach is motivated by recent success in the field of common pattern discovery-specifically mining of common visual patterns among scenes-and requires only a single library of raw images that can be acquired at different time or day. Experimental results show that even though our scene descriptor is significantly more compact than conventional descriptors it has a relatively higher recognition performance.},
keywords = {data mining;feature extraction;robot vision;SLAM (robots);},
note = {visual phrase mining;long-term visual SLAM;single-view place recognition;simultaneous localization and mapping;bag-of-words scene descriptor;vector quantized visual features;common pattern discovery;recognition performance;},
URL = {http://dx.doi.org/10.1109/IROS.2014.6942552},
} 


@inproceedings{19060928 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Weighted Grid Partitioning for Panel-Based Bathymetric SLAM},
journal = {OCEANS 2019 - Marseille},
author = {Junwoo Jang and Jinwhan Kim},
year = {2019//},
pages = {6 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Bathymetric navigation enables the long-term operation of autonomous underwater vehicles by reducing navigation drift errors with no need for GPS position fixes. In the case that a bathymetric map is not available, the simultaneous localization and mapping (SLAM) algorithm is required, but this increases computational complexity and memory requirement. Panel-based bathymetric SLAM could considerably reduce the computational burden. However, it may suffers from incorrect update when the vehicle does not belong to the updated panel. This study proposes a new update method, called weighted grid partitioning, which considers the probability distribution of a vehicle's location, and is more effective in terms of the map accuracy, computational burden, and memory usage compared to standard update methods. The feasibility of the proposed algorithm is verified through simulations.},
keywords = {autonomous underwater vehicles;bathymetry;Global Positioning System;navigation;oceanographic techniques;SLAM (robots);},
note = {panel-based bathymetric SLAM;bathymetric navigation;long-term operation;autonomous underwater vehicles;navigation drift errors;GPS position fixes;bathymetric map;computational complexity;memory requirement;computational burden;updated panel;standard update methods;weighted grid partitioning;simultaneous localization and mapping algorithm;vehicle location;map accuracy;memory usage;},
URL = {http://dx.doi.org/10.1109/OCEANSE.2019.8867531},
} 


@inproceedings{18096155 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition},
journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Siva, S. and Hao Zhang},
year = {2018//},
pages = {9 pp. - },
address = {Piscataway, NJ, USA},
abstract = {Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first large-scale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.},
keywords = {mobile robots;object recognition;robot vision;sensor fusion;SLAM (robots);},
note = {omnidirectional multisensory perception fusion;long-term place recognition;long-term autonomy;omnidirectional sensors;omnidirectional observation;multidirectional place recognition;omnidirectional multisensory data;appearance variations;Simultaneous Localization and Mapping;},
URL = {http://dx.doi.org/10.1109/ICRA.2018.8461042},
} 


@inproceedings{16553852 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Will it last? Learning stable features for long-term visual localization},
journal = {2016 Fourth International Conference on 3D Vision (3DV). Proceedings},
author = {Dymczyk, M. and Stumm, E. and Nieto, J. and Siegwart, R. and Gilitschenski, I.},
year = {2016//},
pages = {572 - 81},
address = {Los Alamitos, CA, USA},
abstract = {An increasing number of simultaneous localization and mapping (SLAM) systems are using appearance-based localization to improve the quality of pose estimates. However, with the growing time-spans and size of the areas we want to cover, appearance-based maps are often becoming too large to handle and are consisting of features that are not always reliable for localization purposes. This paper presents a method for selecting map features that are persistent over time and thus suited for long-term localization. Our methodology relies on a CNN classifier based on image patches and depth maps for recognizing which features are suitable for life-long matchability. Thus, the classifier not only considers the appearance of a feature but also takes into account its expected lifetime. As a result, our feature selection approach produces more compact maps with a high fraction of temporally-stable features compared to the current state-of-the-art, while rejecting unstable features that typically harm localization. Our approach is validated on indoor and outdoor datasets, that span over a period of several months.},
keywords = {feature selection;image classification;neural nets;SLAM (robots);},
note = {map feature selection;CNN classifier;image patches;depth maps;feature recognition;life-long matchability;long-term visual localization;convolutional neural network;simultaneous localization and mapping systems;SLAM systems;},
URL = {http://dx.doi.org/10.1109/3DV.2016.66},
} 


@article{19096074 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Seamless navigation and mapping using an INS/GNSS/grid-based SLAM semi-tightly coupled integration scheme},
journal = {Information Fusion},
journal = {Inf. Fusion (Netherlands)},
author = {Chiang, K.W. and Tsai, G.J. and Chang, H.W. and Joly, C. and EI-Sheimy, N.},
volume = { 50},
year = {2019/10/},
pages = {181 - 96},
issn = {1566-2535},
address = {Netherlands},
abstract = {Mobile Mapping Systems (MMS) with Inertial Navigation System / Global Navigation Satellite System (INS/GNSS) and mapping sensors have been widely developed in recent years. However current systems and results are still prone to errors, especially in GNSS-denied or multipath environments. To provide robust and stable navigation information, particularly for mapping in long-term GNSS-denied environments, we propose a semi-tightly coupled integration scheme which integrates INS/GNSS with grid-based Simultaneous Localization and Mapping (SLAM). Although traditional SLAM using LiDAR can map the GNSS-denied environment efficiently, it is only in local localization. The proposed integration scheme is based on the Extended Kalman Filter (EKF) with motion constraints. In this scheme, a measurement model for grid-based SLAM is aided by the heading and velocity information. A special innovation of this scheme is the improved fusion of GNSS/INS with the use of grid-based SLAM serves like virtual odometer and virtual compass, thus gaining reliable measurements and error models to maintain good performance during INS-only mode. In addition, the initial values for example position and heading, are given to solve global localization and loop closure problems in SLAM. Finally, a smoothing and multi-resolution map strategy are applied offline to increase the robustness and performance of the proposed grid-based SLAM. Evaluation based on experimental data shows the significant improvement by the proposed semi-tightly coupled integration scheme with low-cost INS/GNSS and LiDAR, which is able to achieve 1-2 m' accuracy in terms of positioning and mapping. An approximately 60% improvement was achieved during long-term GNSS-denied environments using the proposed integration scheme. [All rights reserved Elsevier].},
keywords = {distance measurement;Global Positioning System;inertial navigation;Kalman filters;mobile robots;nonlinear filters;optical radar;position control;satellite navigation;SLAM (robots);},
note = {seamless navigation;integration scheme;Mobile Mapping Systems;robust navigation information;stable navigation information;long-term GNSS-denied environments;grid-based Simultaneous Localization;traditional SLAM;GNSS-denied environment;local localization;error models;INS-only mode;global localization;loop closure problems;multiresolution map strategy;size 1.0 m to 2.0 m;},
URL = {http://dx.doi.org/10.1016/j.inffus.2019.01.004},
} 


@inproceedings{21459638 ,
language = {English},
copyright = {Copyright 2022, The Institution of Engineering and Technology},
title = {Collaborative Visual Inertial SLAM for Multiple Smart Phones},
journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Jialing Liu and Ruyu Liu and Kaiqi Chen and Jianhua Zhang and Dongyan Guo},
year = {2021//},
pages = {11553 - 9},
address = {Piscataway, NJ, USA},
abstract = {The efficiency and accuracy of mapping are crucial in a large scene and long-term AR applications. Multi-agent cooperative SLAM is the precondition of multi-user AR interaction. The cooperation of multiple smart phones has the potential to improve efficiency and robustness of task completion and can complete tasks that a single agent cannot do. However, it depends on robust communication, efficient location detection, robust mapping, and efficient information sharing among agents. We propose a multi-intelligence collaborative monocular visual-inertial SLAM deployed on multiple ios mobile devices with a centralized architecture. Each agent can independently explore the environment, run a visual-inertial odometry module online, and then send all the measurement information to a central server with higher computing resources. The server manages all the information received, detects overlapping areas, merges and optimizes the map, and shares information with the agents when needed. We have verified the performance of the system in public datasets and real environments. The accuracy of mapping and fusion of the proposed system is comparable to VINS-Mono which requires higher computing resources.},
keywords = {augmented reality;distance measurement;groupware;human-robot interaction;iOS (operating system);mobile computing;mobile robots;multi-agent systems;SLAM (robots);smart phones;},
note = {collaborative visual inertial SLAM;multiple smart phones;multiuser AR interaction;robust communication;location detection;robust mapping;information sharing;iOS mobile devices;centralized architecture;visual-inertial odometry module online;},
URL = {http://dx.doi.org/10.1109/ICRA48506.2021.9561946},
} 


@article{19989793 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {Structure-SLAM: Low-Drift Monocular SLAM in Indoor Environments [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Yanyan Li and Brasch, N. and Yida Wang and Navab, N. and Tombari, F.},
year = {2020/08/05},
pages = {8 pp. - },
address = {USA},
abstract = {In this paper a low-drift monocular SLAM method is proposed targeting indoor scenarios, where monocular SLAM often fails due to the lack of textured surfaces. Our approach decouples rotation and translation estimation of the tracking process to reduce the long-term drift in indoor environments. In order to take full advantage of the available geometric information in the scene, surface normals are predicted by a convolutional neural network from each input RGB image in real-time. First, a drift-free rotation is estimated based on lines and surface normals using spherical mean-shift clustering, leveraging the weak Manhattan World assumption. Then translation is computed from point and line features. Finally, the estimated poses are refined with a map-to-frame optimization strategy. The proposed method outperforms the state of the art on common SLAM benchmarks such as ICL-NUIM and TUM RGB-D.},
keywords = {convolutional neural nets;image colour analysis;image texture;indoor environment;optimisation;pose estimation;robot vision;SLAM (robots);},
note = {structure-SLAM;indoor environments;low-drift monocular SLAM method;geometric information;surface normals;convolutional neural network;RGB image;drift-free rotation;estimated poses;SLAM benchmarks;spherical mean-shift clustering;Manhattan World assumption;map-to-frame optimization strategy;ICL-NUIM;TUM RGB-D;},
} 


@article{19313930 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {LCPF: a particle filter lidar SLAM system with loop detection and correction},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Fuyu Nie and Weimin Zhang and Zhuo Yao and Yongliang Shi and Fangxing Li and Qiang Huang},
volume = { 8},
year = {2020//},
pages = {20401 - 12},
issn = {2169-3536},
address = {USA},
abstract = {A globally consistent map is the basis of indoor robot localization and navigation. However, map built by Rao-Blackwellized Particle Filter (RBPF) doesn't have high global consistency which is not suitable for long-term application in large scene. To address the problem, we present an improved RBPF Lidar SLAM system with loop detection and correction named LCPF. The efficiency and accuracy of loop detection depend on the segmentation of submaps. Instead of dividing the submap at fixed number of laser scan like existing method, Dynamic Submap Segmentation is proposed in LCPF. This segmentation algorithm decreases the error inside the submap by splitting the submap where there is high scan match error and later rectifies the error by an improved pose graph optimization between submaps. In order to segment the submap at appropriate point, when to create a new submap is determined by both the accumulation of scan match error and the particle distribution. Furthermore, LCPF uses branch and bound algorithm as basic detector for loop detection and multiple criteria to judge the reliability of a loop. In the criteria, a novel parameter called usable ratio was proposed to measure the useful information that a laser scan containing. Finally, comparisons to existing 2D-Lidar mapping algorithm are performed with a series of open dataset simulations and real robot experiments to demonstrate the effectiveness of LCPF.},
keywords = {graph theory;image segmentation;mobile robots;optical radar;particle filtering (numerical methods);path planning;robot vision;SLAM (robots);},
note = {loop detection;globally consistent map;indoor robot localization;navigation;Rao-Blackwellized particle filter;high global consistency;LCPF;laser scan;segmentation algorithm;high scan match error;particle filter lidar SLAM system;2D-lidar mapping algorithm;dynamic submap segmentation;improved RBPF lidar SLAM system;},
URL = {http://dx.doi.org/10.1109/ACCESS.2020.2968353},
} 


@inproceedings{14630798 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Toward long-term, automated ship hull inspection with visual SLAM, explicit surface optimization, and generic graph-sparsification},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Ozog, P. and Eustice, R.M.},
year = {2014//},
pages = {3832 - 9},
address = {Piscataway, NJ, USA},
abstract = {This paper reports on a method for an autonomous underwater vehicle to perform real-time visual simultaneous localization and mapping (SLAM) on large ship hulls over multiple sessions. Along with a monocular camera, our method uses a piecewise-planar model to explicitly optimize the ship hull surface in our factor-graph framework, and anchor nodes to co-register multiple surveys. To enable realtime performance for long-term SLAM, we use the recent Generic Linear Constraints (GLC) framework to sparsify our factor-graph. This paper analyzes how our single-session SLAM techniques can be used in the GLC framework, and describes a particle filter reacquisition algorithm so that an underwater session can be automatically re-localized to a previously built SLAM graph. We provide real-world experimental results involving automated ship hull inspection, and show that our localization filter out-performs Fast Appearance-Based Mapping (FAB-MAP), a popular place-recognition system. Using our approach, we can automatically align surveys that were taken days, months, and even years apart.},
keywords = {autonomous underwater vehicles;graph theory;inspection;mobile robots;optimisation;particle filtering (numerical methods);robot vision;ships;SLAM (robots);},
note = {place-recognition system;FAB-MAP;fast appearance-based mapping;localization filter;SLAM graph;particle filter reacquisition algorithm;single-session SLAM techniques;GLC framework;generic linear constraint framework;factor-graph framework;piecewise-planar model;monocular camera;visual simultaneous localization and mapping;autonomous underwater vehicle;generic graph-sparsification;explicit ship hull surface optimization;visual SLAM;automated ship hull inspection;},
URL = {http://dx.doi.org/10.1109/ICRA.2014.6907415},
} 


@article{18626899 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using Scan Context Image},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Giseop Kim and Byungjae Park and Ayoung Kim},
volume = { 4},
number = { 2},
year = {2019/04/},
pages = {1948 - 55},
issn = {2377-3774},
address = {USA},
abstract = {In this letter, we present a long-term localization method that effectively exploits the structural information of an environment via an image format. The proposed method presents a robust year-round localization performance even when learned in just a single day. The proposed localizer learns a point cloud descriptor, named Scan Context Image (SCI), and performs robot localization on a grid map by formulating the place recognition problem as place classification using a convolutional neural network. Our method is faster than existing methods proposed for place recognition because it avoids a pairwise comparison between a query and scans in a database. In addition, we provide thorough validations using publicly available long-term datasets, the NCLT dataset and the Oxford RobotCar dataset, and show that the Scan Context Image (SCI) localization attains consistent performance over a year and outperforms existing methods.},
keywords = {image classification;learning (artificial intelligence);mobile robots;neural nets;optical radar;path planning;robot vision;},
note = {long-term LiDAR localization;long-term localization method;structural information;image format;point cloud descriptor;place recognition problem;place classification;convolutional neural network;robot localization;grid map;Oxford RobotCar dataset;scan context image localization;time 1 year;time 1 d;},
URL = {http://dx.doi.org/10.1109/LRA.2019.2897340},
} 


@article{17968518 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Learning of Holism-Landmark Graph Embedding for Place Recognition in Long-Term Autonomy},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Fei Han and El Beleidy, S. and Hua Wang and Cang Ye and Hao Zhang},
volume = { 3},
number = { 4},
year = {2018/10/},
pages = {3669 - 76},
issn = {2377-3774},
address = {USA},
abstract = {Place recognition plays an important role to perform loop closure detection of large-scale, long-term simultaneous localization and mapping in loopy environments. The long-term place recognition problem is challenging because the environment appearance exhibits significant long-term variations across various times of the day, months, and seasons. In this letter, we introduce a novel place representation approach that simultaneously integrates semantic landmarks and holistic information to achieve place recognition in long-term autonomy. First, a graph is constructed for each place. The graph nodes encode all landmarks and the holistic image of the place scene recorded in different scenarios. The edges connecting the nodes indicate that these nodes represent the same landmark or place, even though places and landmarks encoded by the nodes may exhibit different appearances in the long-term periods. Then, a graph embedding is learned to preserve the locality in the feature descriptor space, i.e., finding a projection such that the same landmark and place have the identical representation in the new projected descriptor space, no matter in what scenarios they are recorded. We formulate the embedding learning as an optimization problem and implement a new solver that provides a theoretical convergence guarantee. Extensive evaluations are conducted using large-scale benchmark datasets of place recognition in long-term autonomy, which has shown our approach's promising performance.},
keywords = {graph theory;image coding;image recognition;image representation;learning (artificial intelligence);mobile robots;optimisation;robot vision;SLAM (robots);},
note = {long-term autonomy;holism-landmark graph embedding;long-term place recognition problem;semantic landmarks;graph nodes;place scene;place representation approach;embedding learning;long-term simultaneous localization and mapping;},
URL = {http://dx.doi.org/10.1109/LRA.2018.2856274},
} 


@inproceedings{18112483 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data},
journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Li Sun and Zhi Yan and Mellado, S.M. and Hanheide, M. and Duckett, T.},
year = {2018//},
pages = {5942 - 8},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.},
keywords = {cameras;codecs;image annotation;image coding;learning (artificial intelligence);mobile robots;object detection;pedestrians;pose estimation;robot vision;service robots;SLAM (robots);},
note = {long-term temporal information;sequence-to-sequence LSTM encoder-decoder;on-the-fly prediction;global coordinate system;T-Pose-LSTM model;human trajectory prediction;long-term mobile robot deployments;3DOF pedestrian trajectory prediction learned;Long-Term autonomous mobile robot deployment data;autonomous mobile service robots;monocular camera images;range-finder sensors;3DOF pedestrian trajectory prediction approach;temporal 3DOF-pose long-short-term memory;robust human detection;},
URL = {http://dx.doi.org/10.1109/ICRA.2018.8461228},
} 


@article{16712506 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {SRAL: shared representative appearance learning for long-term visual place recognition},
journal = {IEEE Robotics and Automation Letters},
journal = {IEEE Robot. Autom. Lett. (USA)},
author = {Fei Han and Xue Yang and Yiming Deng and Rentschler, M. and Dejun Yang and Hao Zhang},
volume = { 2},
number = { 2},
year = {2017/04/},
pages = {1172 - 9},
issn = {2377-3774},
address = {USA},
abstract = {Place recognition, or loop closure detection, is an essential component to address the problem of visual simultaneous localization and mapping (SLAM). Long-term navigation of robots in outdoor environments introduces new challenges to enable life-long SLAM, including the strong appearance change resulting from vegetation, weather, and illumination variations across various times of the day, different days, months, or even seasons. In this paper, we propose a new shared representative appearance learning (SRAL) approach to address long-term visual place recognition. Different from previous methods using a single feature modality or a concatenation of multiple features, our SRAL method autonomously learns representative features that are shared in all scene scenarios, and then fuses the features together to represent the long-term appearance of environments observed by a robot during life-long navigation. By formulating SRAL as a regularized optimization problem, we use structured sparsity-inducing norms to model interrelationships of feature modalities. In addition, an optimization algorithm is developed to efficiently solve the formulated optimization problem, which holds a theoretical convergence guarantee. Extensive empirical study was performed to evaluate the SRAL method using large-scale benchmark datasets, including St Lucia, CMU-VL, and Nordland datasets. Experimental results have shown that our SRAL method obtains superior performance for life-long place recognition using individual images, outperforms previous single image-based methods, and is capable of estimating the importance of feature modalities.},
keywords = {feature extraction;image fusion;learning (artificial intelligence);mobile robots;object detection;object recognition;optimisation;path planning;robot vision;SLAM (robots);},
note = {large-scale benchmark datasets;Nordland dataset;CMU-VL dataset;St Lucia dataset;feature modalities;structured sparsity-inducing norms;regularized optimization problem;feature fusion;illumination variation;weather variation;vegetation variation;long-term robot navigation;visual SLAM;visual simultaneous localization-and-mapping;loop closure detection;long-term visual place recognition;shared representative appearance learning;SRAL;},
URL = {http://dx.doi.org/10.1109/LRA.2017.2662061},
} 


@inproceedings{14023877 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Generic 2D/3D SLAM with NDT maps for lifelong application},
journal = {2013 European Conference on Mobile Robots. Proceedings},
author = {Einhorn, E. and Gross, H.-M.},
year = {2013//},
pages = {240 - 7},
address = {Piscataway, NJ, USA},
abstract = {In this paper, we present a new, generic approach for Simultaneous Localization and Mapping (SLAM). First of all, we propose an abstraction of the underlying sensor data using Normal Distribution Transform (NDT) maps that are suitable for making our approach independent from the used sensor and the dimension of the generated maps. We present some modifications for the original NDT mapping to handle free-space measurements explicitly and to enable its usage in dynamic environments with moving obstacles and persons. In the second part of this paper we describe our graph-based SLAM approach that is designed for lifelong usage. Therefore, the memory and computational complexity is limited by pruning the pose graph in an appropriate way.},
keywords = {collision avoidance;computational complexity;graph theory;normal distribution;robot vision;SLAM (robots);transforms;},
note = {generic 2D SLAM;lifelong application;simultaneous localization and mapping;sensor data abstraction;normal distribution transform maps;NDT maps;free-space measurements;dynamic environments;moving obstacles;moving persons;graph-based SLAM approach;computational complexity;pose graph pruning;generic 3D SLAM;},
URL = {http://dx.doi.org/10.1109/ECMR.2013.6698849},
} 


@article{16065531 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Long-term mapping techniques for ship hull inspection and surveillance using an autonomous underwater vehicle},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Ozog, P. and Carlevaris-Bianco, N. and Ayoung Kim and Eustice, R.M.},
volume = { 33},
number = { 3},
year = {2016/05/},
pages = {265 - 89},
issn = {1556-4959},
address = {USA},
abstract = {This paper reports on a system for an autonomous underwater vehicle to perform <i>in situ</i>, multiple session hull inspection using long-term simultaneous localization and mapping (SLAM). Our method assumes very little <i>a priori</i> knowledge, and it does not require the aid of acoustic beacons for navigation, which is a typical mode of navigation in this type of application. Our system combines recent techniques in underwater saliency-informed visual SLAM and a method for representing the ship hull surface as a collection of many locally planar surface features. This methodology produces accurate maps that can be constructed in real-time on consumer-grade computing hardware. A single-session SLAM result is initially used as a prior map for later sessions, where the robot automatically merges the multiple surveys into a common hull-relative reference frame. To perform the relocalization step, we use a particle filter that leverages the locally planar representation of the ship hull surface, and a fast visual descriptor matching algorithm. Finally, we apply the recently developed graph sparsification tool, generic linear constraints, as a way to manage the computational complexity of the SLAM system as the robot accumulates information across multiple sessions. We show results for 20 SLAM sessions for two large vessels over the course of days, months, and even up to three years, with a total path length of approximately 10.2 km.},
keywords = {autonomous underwater vehicles;graph theory;mobile robots;particle filtering (numerical methods);ships;SLAM (robots);telerobotics;video surveillance;},
note = {long term mapping techniques;ship hull inspection;ship hull surveillance;autonomous underwater vehicle;multiple session hull inspection;simultaneous localization and mapping;SLAM;acoustic beacons;consumer grade computing hardware;particle filter;locally planar representation;visual descriptor matching algorithm;generic linear constraints;graph sparsification tool;computational complexity;SLAM system;},
URL = {http://dx.doi.org/10.1002/rob.21582},
} 


@inproceedings{18504194 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {An 879GOPS 243mW 80fps VGA Fully Visual CNN-SLAM Processor for Wide-Range Autonomous Exploration},
journal = {2019 IEEE International Solid- State Circuits Conference - (ISSCC)},
author = {Ziyun Li and Yu Chen and Luyao Gong and Lu Liu and Sylvester, D. and Blaauw, D. and Hun-Seok Kim},
year = {2019//},
pages = {134 - 6},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous localization and mapping (SLAM) estimates an agent's trajectory for all six degrees of freedom (6 DoF) and constructs a 3D map of an unknown surrounding. It is a fundamental kernel that enables head-mounted augmented/virtual reality devices and autonomous navigation of micro aerial vehicles. A noticeable recent trend in visual SLAM is to apply computationand memory-intensive convolutional neural networks (CNNs) that outperform traditional hand-designed feature-based methods [1]. For each video frame, CNN-extracted features are matched with stored keypoints to estimate the agent's 6-DoF pose by solving a perspective-n-points (PnP) non-linear optimization problem (Fig. 7.3.1, left). The agent's long-term trajectory over multiple frames is refined by a bundle adjustment process (BA, Fig. 7.3.1 right), which involves a large-scale (~120 variables) non-linear optimization. Visual SLAM requires massive computation (&gt;250GOP/s) in the CNN-based feature extraction and matching, as well as datadependent dynamic memory access and control flow with high-precision operations, creating significant low-power design challenges. Software implementations are impractical, resulting in 0.2s runtime with a ~3GHz CPU+ GPU system with &gt;100MB memory footprint and &gt;100W power consumption. Prior ASICs have implemented either an incomplete SLAM system [2,3] that lacks estimation of ego-motion or employed a simplified (non-CNN) feature extraction and tracking [2,4,5] that limits SLAM quality and range. A recent ASIC [5] augments visual SLAM with an off-chip high-precision inertial measurement unit (IMU), simplifying the computational complexity, but incurring additional power and cost overhead.},
keywords = {augmented reality;cameras;convolutional neural nets;feature extraction;mobile robots;path planning;pose estimation;power consumption;robot vision;SLAM (robots);},
note = {879GOPS 243mW 80fps;fully visual CNN-SLAM processor;autonomous navigation;microaerial vehicles;video frame;CNN-extracted features;perspective-n-points nonlinear optimization problem;long-term trajectory;bundle adjustment process;large-scale nonlinear optimization;high-precision operations;low-power design challenges;incomplete SLAM system;memory footprint;power consumption;visual SLAM;nonCNN feature extraction;CPU+ GPU system;head-mounted virtual reality devices;head-mounted augmented reality devices;data-dependent dynamic memory access;off-chip high-precision inertial measurement unit;computational complexity;power 243.0 mW;},
URL = {http://dx.doi.org/10.1109/ISSCC.2019.8662397},
} 


@article{18842574 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Learning matchable colorspace transformations for long-term metric visual localization [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Clement, L. and Gridseth, M. and Tomasi, J. and Kelly, J.},
year = {2019/04/01},
pages = {16 pp. - },
address = {USA},
abstract = {Long-term metric localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems in the presence of appearance change caused by lighting, weather or seasonal variations. While experience-based mapping has proven to be an effective technique for enabling visual localization across appearance change, the number of experiences required for reliable long-term localization can be large, and methods for reducing the necessary number of experiences are desired. Taking inspiration from physics-based models of color constancy, we propose a method for learning a nonlinear mapping from RGB to grayscale colorspaces that maximizes the number of feature matches for images captured under varying lighting and weather conditions. Our key insight is that useful image transformations can be learned by approximating conventional non-differentiable localization pipelines with a differentiable learned model that can predict a convenient measure of localization quality, such as the number of feature matches, for a given pair of images. Moreover, we find that the generality of appearance-robust RGB-to-grayscale mappings can be improved by incorporating a learned low-dimensional context feature computed for a specific image pair. Using synthetic and real-world datasets, we show that our method substantially improves feature matching across day-night cycles and presents a viable strategy for significantly improving the efficiency of experience-based visual localization.},
keywords = {feature extraction;image colour analysis;image matching;learning (artificial intelligence);mobile robots;robot vision;},
note = {experience-based visual localization;matchable colorspace transformations;long-term metric localization;autonomous mobile robots;vision-based systems;appearance change;experience-based mapping;physics-based models;nonlinear mapping;weather conditions;nondifferentiable localization pipelines;differentiable learned model;localization quality;appearance-robust RGB-to-grayscale mappings;varying lighting conditions;image transformations;image pair;long-term metric visual localization;color constancy;grayscale colorspaces;RGB colorspaces;image feature matching;learned low-dimensional context feature;},
} 


@article{17684452 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Input Uncertainty Sensitivity Enhanced Nonsingleton Fuzzy Logic Controllers for Long-Term Navigation of Quadrotor UAVs},
journal = {IEEE/ASME Transactions on Mechatronics},
journal = {IEEE/ASME Trans. Mechatron. (USA)},
author = {Changhong Fu and Sarabakha, A. and Kayacan, E. and Wagner, C. and John, R. and Garibaldi, J.M.},
volume = { 23},
number = { 2},
year = {2018/04/},
pages = {725 - 34},
issn = {1083-4435},
address = {USA},
abstract = {Input uncertainty, e.g., noise on the on-board camera and inertial measurement unit, in vision-based control of unmanned aerial vehicles (UAVs) is an inevitable problem. In order to handle input uncertainties as well as further analyze the interaction between the input and the antecedent fuzzy sets (FSs) of nonsingleton fuzzy logic controllers (NSFLCs), an input uncertainty sensitivity enhanced NSFLC has been developed in robot operating system using the C++ programming language. Based on recent advances in nonsingleton inference, the centroid of the intersection of the input and antecedent FSs (Cen-NSFLC) is utilized to calculate the firing strength of each rule instead of the maximum of the intersection used in traditional NSFLC (Tra-NSFLC). An 8-shaped trajectory, consisting of straight and curved lines, is used for the real-time validation of the proposed controllers for a trajectory following problem. An accurate monocular keyframe-based visual-inertial simultaneous localization and mapping (SLAM) approach is used to estimate the position of the quadrotor UAV in GPS-denied unknown environments. The performance of the Cen-NSFLC is compared with a conventional proportional-integral derivative (PID) controller, a singleton FLC and a Tra-NSFLC. All controllers are evaluated for different flight speeds, thus introducing different levels of uncertainty into the control problem. Visual-inertial SLAM-based real-time quadrotor UAV flight tests demonstrate that not only does the Cen-NSFLC achieve the best control performance among the four controllers, but it also shows better control performance when compared to their singleton counterparts. Considering the bias in the use of model-based controllers, e.g., PID, for the control of UAVs, this paper advocates an alternative method, namely Cen-NSFLCs, in uncertain working environments.},
keywords = {aircraft control;autonomous aerial vehicles;C++ language;cameras;control engineering computing;fuzzy control;fuzzy set theory;helicopters;Kalman filters;operating systems (computers);position control;robot vision;sensor fusion;SLAM (robots);three-term control;},
note = {input uncertainty sensitivity enhanced nonsingleton fuzzy logic controllers;inertial measurement unit;unmanned aerial vehicles;input uncertainties;antecedent fuzzy sets;nonsingleton inference;intersection;antecedent FSs;Cen-NSFLC;Tra-NSFLC;quadrotor UAV;proportional-integral derivative controller;visual-inertial SLAM;control performance;visual-inertial simultaneous localization and mapping;long-term navigation;robot operating system;C++ programming language;GPS-denied unknown environments;},
URL = {http://dx.doi.org/10.1109/TMECH.2018.2810947},
} 


@inproceedings{19113227 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {On the Redundancy Detection in Keyframe-Based SLAM},
journal = {2019 International Conference on 3D Vision (3DV)},
author = {Schmuck, P. and Chli, M.},
year = {2019//},
pages = {594 - 603},
address = {Piscataway, NJ, USA},
abstract = {Egomotion and scene estimation is a key component in automating robot navigation, as well as in virtual reality applications for mobile phones or head-mounted displays. It is well known, however, that with long exploratory trajectories and multi-session mapping for long-term autonomy or collaborative applications, the maintenance of the ever-increasing size of these maps quickly becomes a bottleneck. With the explosion of data resulting in increasing runtime of the optimization algorithms ensuring the accuracy of the Simultaneous Localization And Mapping (SLAM) estimates, the large quantity of collected experiences is imposing hard limits on the scalability of such techniques. Considering the keyframe-based paradigm of SLAM techniques, this paper investigates the redundancy inherent in SLAM maps, by quantifying the information of different experiences of the scene as encoded in keyframes. Here we propose and evaluate different information-theoretic and heuristic metrics to remove dispensable scene measurements with minimal impact on the accuracy of the SLAM estimates. Evaluating the proposed metrics in two state-of-the-art centralized collaborative SLAM systems, we provide our key insights into how to identify redundancy in keyframe-based SLAM.},
keywords = {graph theory;information theory;mobile robots;optimisation;robot vision;SLAM (robots);virtual reality;},
note = {virtual reality;mobile phones;head-mounted displays;optimization algorithms;scene measurements;redundancy detection;automating robot navigation;collaborative SLAM systems;information-theoretic;simultaneous localization and mapping;},
URL = {http://dx.doi.org/10.1109/3DV.2019.00071},
} 


@inproceedings{17488948 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Research of large-scale offline map management in visual SLAM},
journal = {2017 4th International Conference on Systems and Informatics (ICSAI)},
author = {Qihui Shen and Hanxu Sun and Ping Ye},
year = {2017//},
pages = {215 - 19},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a novel method of visual simultaneous localization and mapping (SLAM), which is a method of real-time localization and mapping. It is important for a mobile robot to build a map while autonomously navigation. Due to the complexity of the robot work scene, the SLAM method proposed in this paper optimizes map management. It will cost a lot of time and space when a robot long-term works in a same large scene. Therefore, we propose a method in this paper to save a detail map as an offline map in advance. At the same time in order to facilitate the follow-up optimization, the offline map can be divided into several sub-graphs according to the similarity of the scene. Since the segmented offline map has been saved to local system, it can be loaded at any time to localization and obtain the pose of current frame.},
keywords = {mobile robots;robot vision;SLAM (robots);},
note = {large-scale offline map management;visual SLAM;mobile robot;robot work scene;SLAM method;robot long-term works;segmented offline map;local system;visual simultaneous localization and mapping;autonomous navigation;real-time localization and mapping;map management optimization;},
URL = {http://dx.doi.org/10.1109/ICSAI.2017.8248292},
} 


@inproceedings{19283747 ,
language = {English},
copyright = {Copyright 2020, The Institution of Engineering and Technology},
title = {A DenseNet feature-based loop closure method for visual SLAM system},
journal = {2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
author = {Chao Yu and ZuXin Liu and Xin-Jun Liu and Fei Qiao and Yu Wang and Fugui Xie and Qi Wei and Yi Yang},
year = {2019//},
pages = {258 - 65},
address = {Piscataway, NJ, USA},
abstract = {Loop closure is a crucial part in SLAM, especially for large and long-term scenes. Utilizing off-the-shelf networks' features in loop closure becomes a hot spot. However, what kind of network is more suitable in loop closure and how to use their features have not been well-studied. In this paper, DenseNet is introduced in this field according to its own characters. The features of DenseNet preserve both semantic information and structure details and outweigh other popular networks' features significantly. Based on this, a DenseNet feature-based framework, named Dense-Loop, is proposed to address the loop closure problem. Weighted Vector of Locally Aggregated Descriptor (WVLAD) method is used to encode the local descriptors as the final global descriptor, which could resist geometry structure and viewpoint changes. Furthermore, 4 max-pooling by channel and locality-sensitive hashing (LSH) are adopted to accelerate the search process. Extensive experiments are conducted on public datasets and the results demonstrate Dense-Loop could achieve state-of-the-art performance.},
keywords = {convolutional neural nets;robot vision;SLAM (robots);},
note = {convolutional neural network;public datasets;locality-sensitive hashing;max-pooling by channel;geometry structure;weighted vector of locally aggregated descriptor method;off-the-shelf network features;Dense-Loop;loop closure problem;DenseNet feature-based framework;structure details;semantic information;long-term scenes;visual SLAM system;DenseNet feature-based loop closure method;},
URL = {http://dx.doi.org/10.1109/ROBIO49542.2019.8961714},
} 


@inproceedings{18398842 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Long-term Localization of Mobile Robots in Dynamic Changing Environments},
journal = {2018 Chinese Automation Congress (CAC)},
author = {Xiaowei Hu and Jingchuan Wang and Weidong Chen},
year = {2018//},
pages = {384 - 9},
address = {Piscataway, NJ, USA},
abstract = {Long-term localization in dynamic changing environments is still a challenge in robotics. Traditional localization algorithms typically assume that the environment is static. However, in many real-world applications, such as parking lots and industrial plants, there are always dynamic objects (e.g. moving people) and semi-dynamic objects (e.g. parked cars and placed goods). In this paper we address this challenge by introducing a long-term localization algorithm in the environments which combine dynamic objects and semi-dynamic objects. Localizability-based-updating particle filter (LU-P F) algorithm is proposed here. Not only we use localizability matric to build an updating mechanism, but also it is used for localization system. Besides, we propose the dynamic factor as long-memory information to serve as prior knowledge, which improves the robustness of updating process. Experiments in parking lots demonstrate that our approach has better localization results with a more accurate up-to-date map compared to other methods.},
keywords = {mobile robots;particle filtering (numerical methods);road traffic control;},
note = {long-memory information;localizability-based-updating particle filter;long-term localization algorithm;dynamic changing environments;mobile robots;parking lots;},
URL = {http://dx.doi.org/10.1109/CAC.2018.8623046},
} 


@article{18829246 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Topological local-metric framework for mobile robots navigation: a long term perspective},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Li Tang and Yue Wang and Xiaqing Ding and Huan Yin and Rong Xiong and Shoudong Huang},
volume = { 43},
number = { 1},
year = {2019/01/31},
pages = {197 - 211},
issn = {0929-5593},
address = {Germany},
abstract = {Long term mapping and localization are the primary components for mobile robots in real world application deployment, of which the crucial challenge is the robustness and stability. In this paper, we introduce a topological local-metric framework (TLF), aiming at dealing with environmental changes, erroneous measurements and achieving constant complexity. TLF organizes the sensor data collected by the robot in a topological graph, of which the geometry is only encoded in the edge, i.e. the relative poses between adjacent nodes, relaxing the global consistency to local consistency. Therefore the TLF is more robust to unavoidable erroneous measurements from sensor information matching since the error is constrained in the local. Based on TLF, as there is no global coordinate, we further propose the localization and navigation algorithms by switching across multiple local metric coordinates. Besides, a lifelong memorizing mechanism is presented to memorize the environmental changes in the TLF with constant complexity, as no global optimization is required. In experiments, the framework and algorithms are evaluated on 21-session data collected by stereo cameras, which are sensitive to illumination, and compared with the state-of-art global consistent framework. The results demonstrate that TLF can achieve similar localization accuracy with that from global consistent framework, but brings higher robustness with lower cost. The localization performance can also be improved from sessions because of the memorizing mechanism. Finally, equipped with TLF, the robot navigates itself in a 1 km session autonomously.},
keywords = {graph theory;mobile robots;navigation;path planning;},
note = {global consistent framework;topological local-metric framework;TLF;constant complexity;sensor data;stability;robustness;world application deployment;long term perspective;mobile robots navigation;local-metric framework;localization performance;similar localization accuracy;global optimization;environmental changes;multiple local metric coordinates;navigation algorithms;sensor information matching;unavoidable erroneous measurements;local consistency;global consistency;topological graph;size 1.0 km;},
URL = {http://dx.doi.org/10.1007/s10514-018-9724-7},
} 


@inproceedings{17300102 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Robust relocalization based on active loop closure for real-time monocular SLAM},
journal = {Computer Vision Systems. 11th International Conference, ICVS 2017. Revised Selected Papers: LNCS 10528},
author = {Xieyuanli Chen and Huimin Lu and Junhao Xiao and Hui Zhang and Pan Wang},
year = {2017//},
pages = {131 - 43},
address = {Cham, Switzerland},
abstract = {Remarkable performance has been achieved using the state-of-the-art monocular Simultaneous Localization and Mapping (SLAM) algorithms. However, tracking failure is still a challenging problem during the monocular SLAM process, and it seems to be even inevitable when carrying out long-term SLAM in large-scale environments. In this paper, we propose an <i>active loop closure</i> based relocalization system, which enables the monocular SLAM to detect and recover from tracking failures automatically even in previously unvisited areas where no keyframe exists. We test our system by extensive experiments including using the most popular KITTI dataset, and our own dataset acquired by a hand-held camera in outdoor large-scale and indoor small-scale real-world environments where man-made shakes and interruptions were added. The experimental results show that the least recovery time (within 5 ms) and the longest success distance (up to 46 m) were achieved comparing to other relocalization systems. Furthermore, our system is more robust than others, as it can be used in different kinds of situations, i.e., tracking failures caused by the blur, sudden motion and occlusion. Besides robots or autonomous vehicles, our system can also be employed in other applications, like mobile phones, drones, etc.},
keywords = {mobile robots;robot vision;SLAM (robots);},
note = {KITTI dataset;robust relocalization systems;simultaneous localization and mapping;monocular SLAM algorithms;large-scale environments;long-term SLAM;monocular SLAM process;real-time monocular SLAM;active loop closure;tracking failure;recovery time;indoor small-scale real-world environments;hand-held camera;time 5.0 ms;},
URL = {http://dx.doi.org/10.1007/978-3-319-68345-4_12},
} 


@article{18021116 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Long-term online multi-session graph-based SPLAM with memory management},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Labbe, M. and Michaud, F.},
volume = { 42},
number = { 6},
year = {2018/08/},
pages = {1133 - 50},
issn = {0929-5593},
address = {Germany},
abstract = {For long-term simultaneous planning, localization and mapping (SPLAM), a robot should be able to continuously update its map according to the dynamic changes of the environment and the new areas explored. With limited onboard computation capabilities, a robot should also be able to limit the size of the map used for online localization and mapping. This paper addresses these challenges using a memory management mechanism, which identifies locations that should remain in a Working Memory (WM) for online processing from locations that should be transferred to a Long-Term Memory (LTM). When revisiting previously mapped areas that are in LTM, the mechanism can retrieve these locations and place them back in WM for online SPLAM. The approach is tested on a robot equipped with a short-range laser rangefinder and a RGB-D camera, patrolling autonomously 10.5 km in an indoor environment over 11 sessions while having encountered 139 people.},
keywords = {graph theory;laser ranging;mobile robots;path planning;robot vision;SLAM (robots);},
note = {dynamic changes;onboard computation capabilities;online localization;memory management mechanism;Working Memory;WM;online processing;Long-Term Memory;LTM;mapped areas;online SPLAM;multisession graph-based SPLAM;simultaneous planning localization and mapping;RGB-D camera;},
URL = {http://dx.doi.org/10.1007/s10514-017-9682-5},
} 


@article{18658379 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Selective memory: Recalling relevant experience for long-term visual localization},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {MacTavish, K. and Paton, M. and Barfoot, T.D.},
volume = { 35},
number = { 8},
year = {2018/12/},
pages = {1265 - 92},
issn = {1556-4959},
address = {USA},
abstract = {Visual navigation is a key enabling technology for autonomous mobile vehicles. The ability to provide large-scale, long-term navigation using low-cost, low-power vision sensors is appealing for industrial applications. A crucial requirement for long-term navigation systems is the ability to localize in environments whose appearance is constantly changing over time-due to lighting, weather, seasons, and physical changes. This paper presents a multiexperience localization (MEL) system that uses a powerful map representation-storing every visual experience in layers-that does not make assumptions about underlying appearance modalities and generators. Our localization system provides real-time performance by selecting online, a subset of experiences against which to localize. We achieve this task through a novel experience-triage algorithm based on collaborative filtering, which selects experiences relevant to the <i>live view</i>, outperforming competing techniques. Based on classical memory-based recommender systems, this technique also enables landmark-level recommendations, is entirely online, and requires no training data. We demonstrate the capabilities of the MEL system in the context of long-term autonomous path following in unstructured outdoor environments with a challenging 100-day field experiment through day, night, snow, spring, and summer. We furthermore provide offline analysis comparing our system to several state-of-the-art alternatives. We show that the combination of the novel methods presented in this paper enable full use of incredibly rich multiexperience maps, opening the door to robust long-term visual localization.},
keywords = {cartography;image sensors;mobile robots;navigation;path planning;recommender systems;robot vision;},
note = {map representation;visual experience storage;experience-triage algorithm;collaborative filtering;memory-based recommender systems;landmark-level recommendations;unstructured outdoor environments;multiexperience maps;long-term autonomous path;MEL system;appearance modalities;multiexperience localization system;long-term navigation systems;low-power vision sensors;autonomous mobile vehicles;visual navigation;long-term visual localization;selective memory;},
URL = {http://dx.doi.org/10.1002/rob.21838},
} 


@article{16772152 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Survey registration for long-term natural environment monitoring},
journal = {Journal of Field Robotics},
journal = {J. Field Robot. (USA)},
author = {Griffith, S. and Pradalier, C.},
volume = { 34},
number = { 1},
year = {2017/01/},
pages = {188 - 208},
issn = {1556-4959},
address = {USA},
abstract = {This paper presents a survey registration framework to assist in the recurrent inspection of a natural environment. Our framework coarsely aligns surveys at the image-level using visual simultaneous localization and mapping (SLAM), and it registers images at the pixel-level using SIFT Flow, which enables rapid manual inspection. The variation in appearance of natural environments makes data association a primary challenge of this work. We discuss this and other challenges, including 1) alternative approaches for coarsely aligning surveys of a natural environment, 2) how to select which images to compare between two surveys, and 3) strategies to boost image registration accuracy. We evaluate each stage of our approach, emphasizing alignment accuracy and stability with respect to large seasonal variations. Our domain is lakeshore monitoring, in which an autonomous surface vessel surveyed a 1-km lakeshore 33 times in 14 months. Our results show that our framework precisely aligns a significant number of images between surveys captured up to roughly three months apart, often across marked variation in appearance. Using these results, a human was able to spot several changes between surveys that would have otherwise gone unnoticed.},
keywords = {environmental monitoring (geophysics);image registration;SLAM (robots);transforms;},
note = {long term natural environment monitoring;survey registration framework;recurrent inspection;image level using visual simultaneous localization and mapping;SLAM;image registration;SIFT flow;rapid manual inspection;data association;lakeshore monitoring;autonomous surface vessel;},
URL = {http://dx.doi.org/10.1002/rob.21664},
} 


@article{18862549 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Tightly-Coupled Monocular Visual-Odometric SLAM Using Wheels and a MEMS Gyroscope},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Meixiang Quan and Songhao Piao and Minglang Tan and Shi-Sheng Huang},
volume = { 7},
year = {2019//},
pages = {97374 - 89},
issn = {2169-3536},
address = {USA},
abstract = {In this paper, we present a novel tightly coupled probabilistic monocular visual-odometric simultaneous localization and mapping (VOSLAM) algorithm using wheels and a MEMS gyroscope, which can provide accurate, robust, and long-term localization for ground robots. First, we present a novel odometer preintegration theory on manifold; it integrates the wheel encoder measurements and gyroscope measurements to a relative motion constraint that is independent of the linearization point and carefully addresses the uncertainty propagation and gyroscope bias correction. Based on the preintegrated odometer measurement model, we also introduce the odometer error term and tightly integrate it into the visual optimization framework. Then, in order to bootstrap the VOSLAM system, we propose a simple map initialization method. Finally, we present a complete localization mechanism to maximally exploit both sensing cues, which provides different strategies for motion tracking when: 1) both measurements are available; 2) visual measurements are not available; and 3) wheel encoders experience slippage, thereby ensuring the accurate and robust motion tracking. The proposed algorithm is evaluated by performing extensive experiments, and the experimental results demonstrate the superiority of the proposed system.},
keywords = {gyroscopes;image motion analysis;mobile robots;robot vision;SLAM (robots);statistical analysis;trajectory control;},
note = {motion tracking;simple map initialization method;VOSLAM system;visual optimization framework;odometer error term;preintegrated odometer measurement model;gyroscope bias correction;uncertainty propagation;linearization point;relative motion constraint;gyroscope measurements;wheel encoder measurements;odometer preintegration theory;long-term localization;visual-odometric simultaneous localization;MEMS gyroscope;monocular visual-odometric SLAM;},
URL = {http://dx.doi.org/10.1109/ACCESS.2019.2930201},
} 


@inproceedings{18548176 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Approximating Marginalization with Sparse Global Priors for Sliding Window SLAM-Graphs},
journal = {2019 Third IEEE International Conference on Robotic Computing (IRC). Proceedings},
author = {Wilbers, D. and Rumberg, L. and Stachniss, C.},
year = {2019//},
pages = {25 - 31},
address = {Los Alamitos, CA, USA},
abstract = {Most autonomous vehicles rely on some kind of map for localization or navigation. Outdated maps however are a risk to the performance of any map-based localization system applied in autonomous vehicles. It is necessary to update the used maps to ensure stable and long-term operation. We address the problem of computing landmark updates live in the vehicle, which requires efficient use of the computational resources. In particular, we employ a graph-based sliding window approach for simultaneous localization and incremental map refinement. We propose a novel method that approximates sliding window marginalization without inducing fill-in. Our method maintains the exact same sparsity pattern as without performing marginalization, but simultaneously improves the landmark estimates. The main novelty of this work is the derivation of sparse global priors that approximate dense marginalization. In comparison to state-of-the-art work, our approach utilizes global instead of local linearization points, but still minimizes linearization errors. We first approximate marginalization via Kullback-Leibler divergence and then recalculate the mean to compensate linearization errors. We evaluate our approach on simulated and real data from a prototype vehicle and compare our approach to state-of-the-art sliding window marginalization.},
keywords = {approximation theory;error compensation;graph theory;mobile robots;object detection;robot vision;SLAM (robots);},
note = {linearization error compensation;Kullback-Leibler divergence;graph-based sliding window approach;sliding window marginalization;sliding window SLAM-graphs;linearization error minimization;landmark updates;long-term operation;map-based localization system;outdated maps;autonomous vehicles;prototype vehicle;local linearization points;approximate dense marginalization;sparse global priors;landmark estimates;exact same sparsity pattern;incremental map refinement;simultaneous localization;computational resources;},
URL = {http://dx.doi.org/10.1109/IRC.2019.00013},
} 


@article{20882056 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Long-Term Urban Vehicle Localization Using Pole Landmarks Extracted from 3-D Lidar Scans [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Schaefer, A. and Buscher, D. and Vertens, J. and Luft, L. and Burgard, W.},
year = {2019/10/23},
pages = {9 pp. - },
address = {USA},
abstract = {Due to their ubiquity and long-term stability, pole-like objects are well suited to serve as landmarks for vehicle localization in urban environments. In this work, we present a complete mapping and long-term localization system based on pole landmarks extracted from 3-D lidar data. Our approach features a novel pole detector, a mapping module, and an online localization module, each of which are described in detail, and for which we provide an open-source implementation at www.github.com/acschaefer/polex. In extensive experiments, we demonstrate that our method improves on the state of the art with respect to long-term reliability and accuracy: First, we prove reliability by tasking the system with localizing a mobile robot over the course of 15~months in an urban area based on an initial map, confronting it with constantly varying routes, differing weather conditions, seasonal changes, and construction sites. Second, we show that the proposed approach clearly outperforms a recently published method in terms of accuracy. [European Conference on Mobile Robots, Prague, Czech Republic, 2019, pp. 1-7 doi:10.1109/ECMR.2019.8870928].},
keywords = {mobile robots;optical radar;traffic engineering computing;},
note = {pole landmarks extracted;urban environments;complete mapping;novel pole detector;mapping module;online localization module;mobile robot;urban area;initial map;Mobile Robots;},
} 


@article{18452134 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Kai Wang and Yimin Lin and Luowei Wang and Liming Han and Minjie Hua and Xiang Wang and Shiguo Lian and Huang, B.},
year = {2018/12/24},
pages = {7 pp. - },
address = {USA},
abstract = {This paper presents a novel framework for simultaneously implementing localization and segmentation, which are two of the most important vision-based tasks for robotics. While the goals and techniques used for them were considered to be different previously, we show that by making use of the intermediate results of the two modules, their performance can be enhanced at the same time. Our framework is able to handle both the instantaneous motion and long-term changes of instances in localization with the help of the segmentation result, which also benefits from the refined 3D pose information. We conduct experiments on various datasets, and prove that our framework works effectively on improving the precision and robustness of the two tasks and outperforms existing localization and segmentation algorithms.},
keywords = {image segmentation;mobile robots;pose estimation;robot vision;SLAM (robots);},
note = {vision-based tasks;SLAM;instantaneous motion;long-term changes;localization algorithms;segmentation algorithms;refined 3D pose information;semantic segmentation;},
} 


@inproceedings{18372886 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {An Everyday Robotic System that Maintains Local Rules Using Semantic Map Based on Long-Term Episodic Memory},
journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
author = {Furuta, Y. and Okada, K. and Kakiuchi, Y. and Inaba, M.},
year = {2018//},
pages = {7641 - 7},
address = {Piscataway, NJ, USA},
abstract = {To enable robots to work on real home environments, they have to not only consider common knowledge in the global society, but also be aware of existing rules there. Since such &ldquo;local rules&rdquo; are not describable beforehand, robot agents must acquire them through their lives after deployment. To achieve this, we developed a framework that a) lets robots record long-term episodic memories in their deployed environments, b) autonomously builds probabilistic object localization map as structurization of logged data and c) make adapted task plans based on the map. We equipped our framework on PR2 and Fetch robots operating and recording episodic memory for 41 days with semantic common knowledge of the environment. We also conducted demonstrations in which a PR2 robot tidied up a room, showing that the robot agent can successfully plan and execute local-rule-aware home assistive tasks by using our proposed framework.},
keywords = {mobile robots;path planning;},
note = {robot agent;local-rule-aware home assistive tasks;semantic map;long-term episodic memory;home environments;global society;probabilistic object localization map;Fetch robots;semantic common knowledge;PR2 robot;robotic system;time 41.0 d;},
URL = {http://dx.doi.org/10.1109/IROS.2018.8594481},
} 


@article{14405503 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {A speeded-up online incremental vision-based loop-closure detection for long-term SLAM},
journal = {Advanced Robotics},
journal = {Adv. Robot. (USA)},
author = {Kawewong, A. and Tongprasit, N. and Hasegawa, O.},
volume = { 27},
number = { 17},
year = {2013//},
pages = {1325 - 36},
issn = {0169-1864},
address = {USA},
abstract = {An online incremental method of vision-only loop-closure detection for long-term robot navigation is proposed. The method is based on the scheme of direct feature matching which has recently become more efficient than the Bag-of-Words scheme in many challenging environments. The contributions of the paper are the application of hierarchical <i>k</i>-means to speed-up feature matching time and the improvement of the score calculation technique used to determine the loop-closing location. As a result, the presented method runs quickly in long term while retaining high accuracy. The searching cost has been markedly reduced. The proposed method requires no any off-line dictionary generation processes. It can start anywhere at any times. The evaluation has been done on standard well-known datasets being challenging in perceptual aliasing and dynamic changes. The results show that the proposed method offers high precision-recall in large-scale different environments with real-time computation comparing to other vision-only loop-closure detection methods.},
keywords = {feature extraction;image matching;mobile robots;object detection;robot vision;SLAM (robots);},
note = {speeded-up online incremental vision-based loop-closure detection;long-term SLAM;vision-only loop-closure detection;long-term robot navigation;bag-of-words scheme;hierarchical k-means;speed-up feature matching;score calculation technique;loop-closing location;perceptual aliasing;high precision-recall;vision-only loop-closure detection methods;},
URL = {http://dx.doi.org/10.1080/01691864.2013.826410},
} 


@inproceedings{18324318 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Efficient Multi-Drive Map Optimization towards Life-long Localization using Surround View},
journal = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
author = {Sons, M. and Stiller, C.},
year = {2018//},
pages = {2671 - 7},
address = {Piscataway, NJ, USA},
abstract = {Current vision-based localization approaches enable reliable positioning in areas where global navigation satellite systems (GNSS) fail due to multipath and shadowing effects. These approaches require an up-to-date map. It seems promising to update such maps iteratively after passing the mapped area again. However, bundling more and more passes into the existing map leads to unbounded computation and memory complexity. Herein we propose an iterative optimization approach to create highly accurate maps comprising any number of drives with constant computation complexity. The optimization bases on keypoint correspondences matched between the recorded images from multiple drives. First, each new drive is reconstructed separately by a sliding window bundle-adjustment. Thereafter, the estimated trajectory is divided into disjoint clusters. To align the new drive to the current map, we optimize pairs of clusters which are interconnected through loop-closure or inter-drive correspondences. We derive pose differences from all clusters to estimate the final map poses. For global accuracy, we add GNSS measurements from a low cost receiver. We show in our experiments that the approach enables a joint estimate of the trajectories and landmarks from numerous city-scaled passes within several hours on desktop computers.},
keywords = {iterative methods;robot vision;satellite navigation;SLAM (robots);},
note = {sliding window bundle-adjustment;surround view;global navigation satellite systems;iterative optimization approach;vision-based localization;multidrive map optimization;},
URL = {http://dx.doi.org/10.1109/ITSC.2018.8570011},
} 


@inproceedings{18955925 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Updating the visibility of a feature-based map for long-term maintenance},
journal = {2019 IEEE Intelligent Vehicles Symposium (IV)},
author = {Berrio, J.S. and Ward, J. and Worrall, S. and Nebot, E.},
year = {2019//},
pages = {1173 - 9},
address = {Piscataway, NJ, USA},
abstract = {Mobile vehicles operating in urban navigation applications can achieve high integrity localisation with high accuracy by using maps of the surroundings. To accomplish this, the map should always have an accurate representation of the environment. Thus, it is necessary to detect and remove the map components that no longer exist in the current environment. This maintains the map compactness and dependability while simplifying the data association problem. This paper addresses the problem of deletion of transient map components by taking advantage of the geometric connection between the map and agent poses in order to establish and update the visibility of each feature. Once the map is created an initial visibility vector is associated with every map element and updated over time. The visibility of a map element which no longer exists is reduced and ultimately removed from the map. We demonstrate our approach in a 2D feature-based map composed of poles and corners extracted from information provided by a Iidar sensor. The experimental results show the map update using a seven-month data set collected in the University of Sydney campus.},
keywords = {cartography;optical radar;sensor fusion;},
note = {feature-based map;long-term maintenance;mobile vehicles;urban navigation applications;high integrity localisation;data association problem;transient map components;initial visibility vector;map element;2D feature-based map;lidar sensor;},
URL = {http://dx.doi.org/10.1109/IVS.2019.8814189},
} 


@inproceedings{13194899 ,
language = {English},
copyright = {Copyright 2013, The Institution of Engineering and Technology},
title = {Dynamic pose graph SLAM: Long-term mapping in low dynamic environments},
journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2012)},
author = {Walcott-Bryant, A. and Kaess, M. and Johannsson, H. and Leonard, J.J.},
year = {2012//},
pages = {1871 - 8},
address = {Piscataway, NJ, USA},
abstract = {Maintaining a map of an environment that changes over time is a critical challenge in the development of persistently autonomous mobile robots. Many previous approaches to mapping assume a static world. In this work we incorporate the time dimension into the mapping process to enable a robot to maintain an accurate map while operating in dynamical environments. This paper presents Dynamic Pose Graph SLAM (DPG-SLAM), an algorithm designed to enable a robot to remain localized in an environment that changes substantially over time. Using incremental smoothing and mapping (iSAM) as the underlying SLAM state estimation engine, the Dynamic Pose Graph evolves over time as the robot explores new places and revisits previously mapped areas. The approach has been implemented for planar indoor environments, using laser scan matching to derive constraints for SLAM state estimation. Laser scans for the same portion of the environment at different times are compared to perform change detection; when sufficient change has occurred in a location, the dynamic pose graph is edited to remove old poses and scans that no longer match the current state of the world. Experimental results are shown for two real-world dynamic indoor laser data sets, demonstrating the ability to maintain an up-to-date map despite long-term environmental changes.},
keywords = {laser beam applications;mobile robots;optical scanners;SLAM (robots);state estimation;},
note = {long-term mapping;low dynamic environments;environment map maintenance;autonomous mobile robots;dynamical environments;dynamic pose graph SLAM;DPG-SLAM;incremental smoothing and mapping;iSAM;SLAM state estimation engine;planar indoor environments;laser scan matching;change detection;real-world dynamic indoor laser data sets;},
URL = {http://dx.doi.org/10.1109/IROS.2012.6385561},
} 


@article{20176309 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Augmenting visual SLAM with Wi-Fi sensing for indoor applications},
journal = {Autonomous Robots},
journal = {Auton. Robot. (Germany)},
author = {Hashemifar, Z.S. and Adhivarahan, C. and Balakrishnan, A. and Dantu, K.},
volume = { 43},
number = { 8},
year = {2019/12/},
pages = {2245 - 60},
issn = {0929-5593},
address = {Germany},
abstract = {Recent trends have accelerated the development of spatial applications on mobile devices and robots. These include navigation, augmented reality, human-robot interaction, and others. A key enabling technology for such applications is the understanding of the device's location and the map of the surrounding environment. This generic problem, referred to as Simultaneous Localization and Mapping (SLAM), is an extensively researched topic in robotics. However, visual SLAM algorithms face several challenges including perceptual aliasing and high computational cost. These challenges affect the accuracy, efficiency, and viability of visual SLAM algorithms, especially for long-term SLAM, and their use in resource-constrained mobile devices. A parallel trend is the ubiquity of Wi-Fi routers for quick Internet access in most urban environments. Most robots and mobile devices are equipped with a Wi-Fi radio as well. We propose a method to utilize Wi-Fi received signal strength to alleviate the challenges faced by visual SLAM algorithms. To demonstrate the utility of this idea, this work makes the following contributions: (i) We propose a generic way to integrate Wi-Fi sensing into visual SLAM algorithms, (ii) We integrate such sensing into three well-known SLAM algorithms, (iii) Using four distinct datasets, we demonstrate the performance of such augmentation in comparison to the original visual algorithms and (iv) We compare our work to Wi-Fi augmented FABMAP algorithm. Overall, we show that our approach can improve the accuracy of visual SLAM algorithms by 11% on average and reduce computation time on average by 15% to 25%.},
keywords = {augmented reality;indoor radio;Internet;mobile computing;mobile radio;mobile robots;path planning;robot vision;RSSI;SLAM (robots);telecommunication network routing;visual perception;wireless LAN;},
note = {Wi-Fi sensing;robotics;visual SLAM algorithms;resource-constrained mobile devices;Wi-Fi routers;Wi-Fi radio;indoor applications;simultaneous localization and mapping;perceptual aliasing;Wi-Fi received signal strength;Wi-Fi augmented FABMAP algorithm;},
URL = {http://dx.doi.org/10.1007/s10514-019-09874-z},
} 


@inproceedings{18112410 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Long-Term Visual Localization Using Semantically Segmented Images},
journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Stenborg, E. and Toft, C. and Hammarstrand, L.},
year = {2018//},
pages = {6484 - 90},
address = {Piscataway, NJ, USA},
abstract = {Robust cross-seasonal localization is one of the major challenges in long-term visual navigation of autonomous vehicles. In this paper, we exploit recent advances in semantic segmentation of images, i.e., where each pixel is assigned a label related to the type of object it represents, to attack the problem of long-term visual localization. We show that semantically labeled 3D point maps of the environment, together with semantically segmented images, can be efficiently used for vehicle localization without the need for detailed feature descriptors (SIFT, SURF, etc.), Thus, instead of depending on hand-crafted feature descriptors, we rely on the training of an image segmenter. The resulting map takes up much less storage space compared to a traditional descriptor based map. A particle filter based semantic localization solution is compared to one based on SIFT-features, and even with large seasonal variations over the year we perform on par with the larger and more descriptive SIFT-features, and are able to localize with an error below 1 m most of the time.},
keywords = {feature extraction;image segmentation;particle filtering (numerical methods);transforms;},
note = {particle filter based semantic localization solution;SIFT-features;vehicle localization;semantically labeled 3D point maps;autonomous vehicles;long-term visual navigation;robust cross-seasonal localization;semantically segmented images;long-term visual localization;image segmenter;hand-crafted feature descriptors;},
URL = {http://dx.doi.org/10.1109/ICRA.2018.8463150},
} 


@article{16299338 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {University of Michigan North Campus long-term vision and lidar dataset},
journal = {International Journal of Robotics Research},
journal = {Int. J. Robot. Res. (UK)},
author = {Carlevaris-Bianco, N. and Ushani, A.K. and Eustice, R.M.},
volume = { 35},
number = { 9},
year = {2016/08/},
pages = {1023 - 35},
issn = {0278-3649},
address = {UK},
abstract = {This paper documents a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan's North Campus. The dataset consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive sensors for odometry collected using a Segway robot. The dataset was collected to facilitate research focusing on long-term autonomous operation in changing environments. The dataset is composed of 27 sessions spaced approximately biweekly over the course of 15 months. The sessions repeatedly explore the campus, both indoors and outdoors, on varying trajectories, and at different times of the day across all four seasons. This allows the dataset to capture many challenging elements including: moving obstacles (e.g. pedestrians, bicyclists and cars), changing lighting, varying viewpoint, seasonal and weather changes (e.g. falling leaves and snow), and long-term structural changes caused by construction projects. To further facilitate research, we also provide ground-truth pose for all sessions in a single frame of reference.},
keywords = {computer vision;Global Positioning System;optical radar;SLAM (robots);},
note = {University of Michigan north campus long-term vision;robotics research;omnidirectional imagery;planar lidar;3D lidar;GPS;proprioceptive sensors;odometry;Segway robot;computer vision;long-term SLAM;},
URL = {http://dx.doi.org/10.1177/0278364915614638},
} 


@inproceedings{18883620 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {Network Uncertainty Informed Semantic Feature Selection for Visual SLAM},
journal = {2019 16th Conference on Computer and Robot Vision (CRV). Proceedings},
author = {Ganti, P. and Waslander, S.L.},
year = {2019//},
pages = {121 - 8},
address = {Piscataway, NJ, USA},
abstract = {In order to facilitate long-term localization using a visual simultaneous localization and mapping (SLAM) algorithm, careful feature selection can help ensure that reference points persist over long durations and the runtime and storage complexity of the algorithm remain consistent. We present SIVO (Semantically Informed Visual Odometry and Mapping), a novel information-theoretic feature selection method for visual SLAM which incorporates semantic segmentation and neural network uncertainty into the feature selection pipeline. Our algorithm selects points which provide the highest reduction in Shannon entropy between the entropy of the current state and the joint entropy of the state, given the addition of the new feature with the classification entropy of the feature from a Bayesian neural network. Each selected feature significantly reduces the uncertainty of the vehicle state and has been detected to be a static object (building, traffic sign, etc.) repeatedly with a high confidence. This selection strategy generates a sparse map which can facilitate long-term localization. The KITTI odometry dataset is used to evaluate our method, and we also compare our results against ORB_SLAM2. Overall, SIVO performs comparably to the baseline method while reducing the map size by almost 70%.},
keywords = {Bayes methods;data visualisation;feature extraction;image segmentation;mobile robots;neural nets;robot vision;SLAM (robots);},
note = {visual SLAM;semantic segmentation;neural network uncertainty;feature selection pipeline;joint entropy;classification entropy;Bayesian neural network;selection strategy;sparse map;ORB_SLAM2;map size;visual simultaneous localization and mapping;feature selection;information-theoretic feature selection method;Shannon entropy;semantically informed visual odometry and mapping;network uncertainty informed semantic feature selection;},
URL = {http://dx.doi.org/10.1109/CRV.2019.00024},
} 


@inproceedings{16554819 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {OpenABLE: An Open-source Toolbox for Application in Life-Long Visual Localization of Autonomous Vehicles},
journal = {2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)},
author = {Arroyo, R. and Alcantarilla, P.F. and Bergasa, L.M. and Romera, E.},
year = {2016//},
pages = {965 - 70},
address = {Piscataway, NJ, USA},
abstract = {Visual information is a valuable asset in any perception scheme designed for an intelligent transportation system. In this regard, the camera-based recognition of locations provides a higher situational awareness of the environment, which is very useful for varied localization solutions typically needed in long-term autonomous navigation, such as loop closure detection and visual odometry or SLAM correction. In this paper we present OpenABLE, an open-source toolbox contributed to the community with the aim of helping researchers in the application of these kinds of life-long localization algorithms. The implementation follows the philosophy of the topological place recognition method named ABLE, including several new features and improvements. These functionalities allow to match locations using different global image description methods and several configuration options, which enable the users to control varied parameters in order to improve the performance of place recognition depending on their specific problem requisites. The applicability of our toolbox in visual localization purposes for intelligent vehicles is validated in the presented results, jointly with comparisons to the main state-of-the-art methods.},
keywords = {cameras;image matching;intelligent transportation systems;mobile robots;public domain software;robot vision;SLAM (robots);topology;},
note = {OpenABLE;open-source toolbox;visual localization;autonomous vehicle;visual information;intelligent transportation system;ITS;camera-based location recognition;situational awareness;topological place recognition;location matching;simultaneous localization and mapping;SLAM;},
URL = {http://dx.doi.org/10.1109/ITSC.2016.7795672},
} 


@inproceedings{20689891 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Map Updating Revisited for Navigation Map : A mathematical way to perform map updating for autonomous mobile robot},
journal = {2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)},
author = {Hongjian Chen and Zhiqiang Wang and Qing Zhu},
year = {2021//},
pages = {505 - 8},
address = {Piscataway, NJ, USA},
abstract = {Simultaneous localization and mapping, SLAM can product a Map for autonomous robots and self-driving vehicle in navigation. In the actual environment, the scene changes frequently, which makes the old map no long reliable. Therefore, it is necessary to update such a map by an efficient and safely way. In this paper, we review the existing map updating, long-term localization methods and discuss about the challenges in this situation. We present a Map updating method in mathematical way which can update accurately. Our proposed method are tested in five indoor dataset and demonstrated feasibility.},
keywords = {mobile robots;path planning;robot vision;SLAM (robots);},
note = {navigation map;autonomous mobile robot;self-driving vehicle;long-term localization methods;map updating method;simultaneous localization and mapping;SLAM;},
URL = {http://dx.doi.org/10.1109/IPEC51340.2021.9421197},
} 


@inproceedings{18112459 ,
language = {English},
copyright = {Copyright 2018, The Institution of Engineering and Technology},
title = {Elastic LiDAR fusion: dense map-centric continuous-time SLAM},
journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Chanoh Park and Moghadam, P. and Soohwan Kim and Elfes, A. and Fookes, C. and Sridharan, S.},
year = {2018//},
pages = {1206 - 13},
address = {Piscataway, NJ, USA},
abstract = {The concept of continuous-time trajectory representation has brought increased accuracy and efficiency to multi-modal sensor fusion in modern SLAM. However, regardless of these advantages, its offline property caused by the requirement of global batch optimization is critically hindering its relevance for real-time and life-long applications. In this paper, we present a dense map-centric SLAM method based on a continuous-time trajectory to cope with this problem. The proposed system locally functions in a similar fashion to conventional Continuous-Time SLAM (CT-SLAM). However, it removes the need for global trajectory optimization by introducing map deformation. The computational complexity of the proposed approach for loop closure does not depend on the operation time, but only on the size of the space it explored before the loop closure. It is therefore more suitable for long term operation compared to the conventional CT-SLAM. Furthermore, the proposed method reduces uncertainty in the reconstructed dense map by using probabilistic surface element (surfel) fusion. We demonstrate that the proposed method produces globally consistent maps without global batch trajectory optimization, and effectively reduces LiDAR noise by surfel fusion.},
keywords = {image reconstruction;mobile robots;optical radar;optimisation;probability;radar imaging;robot vision;sensor fusion;SLAM (robots);},
note = {dense map-centric continuous-time SLAM;CT-SLAM;computational complexity;surfel fusion;global batch trajectory optimization;probabilistic surface element fusion;map deformation;global trajectory optimization;Continuous-Time SLAM;global batch optimization;multimodal sensor fusion;continuous-time trajectory representation;elastic LiDAR fusion;},
URL = {http://dx.doi.org/10.1109/ICRA.2018.8462915},
} 


@article{17099183 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {FreMEn: Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments},
journal = {IEEE Transactions on Robotics},
journal = {IEEE Trans. Robot. (USA)},
author = {Krajnik, T. and Fentanes, J.P. and Santos, J.M. and Duckett, T.},
volume = { 33},
number = { 4},
year = {2017/08/},
pages = {964 - 77},
issn = {1552-3098},
address = {USA},
abstract = {We present a new approach to long-term mobile robot mapping in dynamic indoor environments. Unlike traditional world models that are tailored to represent static scenes, our approach explicitly models environmental dynamics. We assume that some of the hidden processes that influence the dynamic environment states are periodic and model the uncertainty of the estimated state variables by their frequency spectra. The spectral model can represent arbitrary timescales of environment dynamics with low memory requirements. Transformation of the spectral model to the time domain allows for the prediction of the future environment states, which improves the robot's long-term performance in changing environments. Experiments performed over time periods of months to years demonstrate that the approach can efficiently represent large numbers of observations and reliably predict future environment states. The experiments indicate that the model's predictive capabilities improve mobile robot localization and navigation in changing environments.},
keywords = {mobile robots;state estimation;uncertain systems;},
note = {FreMEn;frequency map enhancement;long-term mobile robot autonomy;long-term mobile robot mapping;dynamic indoor environments;environmental dynamics;uncertainty;estimated state variables;frequency spectra;spectral model;low memory requirements;time domain;},
URL = {http://dx.doi.org/10.1109/TRO.2017.2665664},
} 


@inproceedings{18903889 ,
language = {English},
copyright = {Copyright 2019, The Institution of Engineering and Technology},
title = {A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation},
journal = {2019 International Conference on Robotics and Automation (ICRA)},
author = {Kai Wang and Yimin Lin and Luowei Wang and Liming Han and Minjie Hua and Xiang Wang and Shiguo Lian and Bill Huang},
year = {2019//},
pages = {5224 - 30},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a novel framework for simultaneously implementing localization and segmentation, which are two of the most important vision-based tasks for robotics. While the goals and techniques used for them were considered to be different previously, we show that by making use of the intermediate results of the two modules, their performance can be enhanced at the same time. Our framework is able to handle both the instantaneous motion and long-term changes of instances in localization with the help of the segmentation result, which also benefits from the refined 3D pose information. We conduct experiments on various datasets, and prove that our framework works effectively on improving the precision and robustness of the two tasks and outperforms existing localization and segmentation algorithms.},
keywords = {image motion analysis;image segmentation;mobile robots;pose estimation;robot vision;SLAM (robots);},
note = {segmentation algorithms;semantic segmentation;robotics;refined 3D pose information;vision-based tasks;mutual improvement;unified framework;instantaneous motion change handling;long-term changes;simultaneous localization and segmentation;},
URL = {http://dx.doi.org/10.1109/ICRA.2019.8793499},
} 


@inproceedings{14180382 ,
language = {English},
copyright = {Copyright 2014, The Institution of Engineering and Technology},
title = {Hierarchical SLAM using spectral submap matching with opportunities for long-term operation},
journal = {2013 16th International Conference on Advanced Robotics (ICAR)},
author = {Oberlander, J. and Roennau, A. and Dillmann, R.},
year = {2013//},
pages = {7 pp. - },
address = {Piscataway, NJ, USA},
abstract = {We present a hierarchical SLAM approach which uses spectral registration of local submaps to close loops and to perform global localization after a restart. Using the Fourier-Mellin Transform (FMT), we robustly register occupancy grid representations of local submaps and present methods which improve matching performance. We further show how good match candidates can be reliably detected even from scaled-down versions of the submaps, which significantly reduces the computation time. The spectral registration approach proves useful even in the presence of significant environmental changes due to the fact that it calculates a dense match, incorporating all observed information rather than a sparse set of features.},
keywords = {Fourier transforms;image matching;image registration;robot vision;SLAM (robots);},
note = {matching performance;occupancy grid representations;FMT;Fourier-Mellin transform;global localization;simultaneous localization and mapping;spectral registration approach;spectral submap matching;hierarchical SLAM;},
URL = {http://dx.doi.org/10.1109/ICAR.2013.6766479},
} 


@article{16989841 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Multisensory omni-directional long-term place recognition: Benchmark dataset and analysis [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Mathur, A. and Fei Han and Hao Zhang},
year = {2017/04/18},
pages = {15 pp. - },
address = {USA},
abstract = {Recognizing a previously visited place, also known as place recognition (or loop closure detection) is the key towards fully autonomous mobile robots and self-driving vehicle navigation. Augmented with various Simultaneous Localization and Mapping techniques (SLAM), loop closure detection allows for incremental pose correction and can bolster efficient and accurate map creation. However, repeated and similar scenes (perceptual aliasing) and long term appearance changes (e.g. weather variations) are major challenges for current place recognition algorithms. We introduce a new dataset Multisensory Omnidirectional Long-term Place recognition (MOLP) comprising omnidirectional intensity and disparity images. This dataset presents many of the challenges faced by outdoor mobile robots and current place recognition algorithms. Using MOLP dataset, we formulate the place recognition problem as a regularized sparse convex optimization problem. We conclude that information extracted from intensity image is superior to disparity image in isolating discriminative features for successful long term place recognition. Furthermore, when these discriminative features are extracted from an omnidirectional vision sensor, a robust bidirectional loop closure detection approach is established, allowing mobile robots to close the loop, regardless of the difference in the direction when revisiting a place.},
keywords = {convex programming;feature extraction;image sensors;object detection;object recognition;},
note = {multisensory omnidirectional long-term place recognition;place recognition;loop closure detection;fully autonomous mobile robots;self-driving vehicle navigation;simultaneous localization and mapping techniques;SLAM;perceptual aliasing;appearance change;MOLP;regularized sparse convex optimization problem;discriminative feature isolation;omnidirectional vision sensor;},
} 


@inproceedings{15291608 ,
language = {English},
copyright = {Copyright 2015, The Institution of Engineering and Technology},
title = {Optical flow localisation and appearance mapping (OFLAAM) for long-term navigation},
journal = {2015 International Conference on Unmanned Aircraft Systems (ICUAS). Proceedings},
author = {Pastor-Moreno, D. and Hyo-Sang Shin and Waldock, A.},
year = {2015//},
pages = {980 - 8},
address = {Piscataway, NJ, USA},
abstract = {This paper presents a novel method to use optical flow navigation for long term navigation. Unlike standard SLAM approaches for augmented reality, OFLAAM is designed for Micro Air Vehicles (MAV). It uses a optical flow camera pointing downwards, a IMU and a monocular camera pointing frontwards. That configuration avoids the computational expensive mapping and tracking of the 3D features. It only maps these features in a vocabulary list by a localization module to tackle the optical flow drift and the lose of the navigation estimation. That module, based on the well established algorithm DBoW2, will be also used to close the loop and allow long-term navigation in previously visited areas. The combination of high speed optical flow navigation with a low rate localization algorithm allows fully autonomous navigation for MAV, at the same time it reduces the overall computational load. This framework is implemented in ROS (Robot Operating System) and tested attached to a laptop. A representative scenario is used to validate and analyze the performance of the system.},
keywords = {aircraft navigation;augmented reality;autonomous aerial vehicles;cameras;control engineering computing;image sequences;object tracking;SLAM (robots);},
note = {optical flow localisation and appearance mapping;OFLAAM;long-term navigation;standard SLAM approach;augmented reality;micro air vehicle;MAV;optical flow camera;IMU;monocular camera pointing frontward;computational expensive mapping and tracking;3D feature;localization module;optical flow drift;navigation estimation;algorithm DBoW2;high speed optical flow navigation;low rate localization algorithm;autonomous navigation;ROS;robot operating system;laptop;},
URL = {http://dx.doi.org/10.1109/ICUAS.2015.7152387},
} 


@inproceedings{16055928 ,
language = {English},
copyright = {Copyright 2016, The Institution of Engineering and Technology},
title = {Towards lifelong feature-based mapping in semi-static environments},
journal = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
author = {Rosen, D.M. and Mason, J. and Leonard, J.J.},
year = {2016//},
pages = {1063 - 70},
address = {Piscataway, NJ, USA},
abstract = {The feature-based graphical approach to robotic mapping provides a representationally rich and computationally efficient framework for an autonomous agent to learn a model of its environment. However, this formulation does not naturally support long-term autonomy because it lacks a notion of environmental change; in reality, &ldquo;everything changes and nothing stands still, &rdquo; and any mapping and localization system that aims to support truly persistent autonomy must be similarly adaptive. To that end, in this paper we propose a novel feature-based model of environmental evolution over time. Our approach is based upon the development of an expressive probabilistic generative feature persistence model that describes the survival of abstract semi-static environmental features over time. We show that this model admits a recursive Bayesian estimator, the persistence filter, that provides an exact online method for computing, at each moment in time, an explicit Bayesian belief over the persistence of each feature in the environment. By incorporating this feature persistence estimation into current state-of-the-art graphical mapping techniques, we obtain a flexible, computationally efficient, and information-theoretically rigorous framework for lifelong environmental modeling in an ever-changing world.},
keywords = {adaptive systems;Bayes methods;recursive estimation;robot vision;SLAM (robots);},
note = {lifelong feature-based mapping;feature-based graphical approach;robotic mapping;autonomous agent;localization system;adaptive system;expressive probabilistic generative feature persistence model;abstract semistatic environmental features;recursive Bayesian estimator;persistence filter;explicit Bayesian belief;feature persistence estimation;graphical mapping;lifelong environmental modeling;},
URL = {http://dx.doi.org/10.1109/ICRA.2016.7487237},
} 


@article{17148681 ,
language = {English},
copyright = {Copyright 2017, The Institution of Engineering and Technology},
title = {Neural SLAM [arXiv]},
journal = {arXiv},
journal = {arXiv (USA)},
author = {Jingwei Zhang and Lei Tai and Boedecker, J. and Burgard, W. and Ming Liu},
year = {2017/06/28},
pages = {10 pp., suppl. 1 pp. - },
address = {USA},
abstract = {We present an approach for agents to learn representations of a global map from sensor data, to aid their exploration in new environments. To achieve this, we embed procedures mimicking that of traditional Simultaneous Localization and Mapping (SLAM) into the soft attention based addressing of external memory architectures, in which the external memory acts as an internal representation of the environment. This structure encourages the evolution of SLAM-like behaviors inside a completely differentiable deep neural network. We show that this approach can help reinforcement learning agents to successfully explore new environments where long-term memory is essential. We validate our approach in both challenging grid-world environments and preliminary Gazebo experiments. A video of our experiments can be found at: https://goo.gl/RfiSxo.},
keywords = {neural nets;SLAM (robots);},
note = {neural SLAM;simultaneous localization and mapping;deep neural network;reinforcement learning agents;},
} 


@article{21203267 ,
language = {English},
copyright = {Copyright 2021, The Institution of Engineering and Technology},
title = {Simultaneous Localization and Mapping for Inspection Robots in Water and Sewer Pipe Networks: A Review},
journal = {IEEE Access},
journal = {IEEE Access (USA)},
author = {Aitken, J.M. and Evans, M.H. and Worley, R. and Edwards, S. and Rui Zhang and Dodd, T. and Mihaylova, L. and Anderson, S.R.},
volume = { 9},
year = {2021//},
pages = {140173 - 140198},
issn = {2169-3536},
address = {USA},
abstract = {At the present time, water and sewer pipe networks are predominantly inspected manually. In the near future, smart cities will perform intelligent autonomous monitoring of buried pipe networks, using teams of small robots. These robots, equipped with all necessary computational facilities and sensors (optical, acoustic, inertial, thermal, pressure and others) will be able to inspect pipes whilst navigating, self-localising and communicating information about the pipe condition and faults such as leaks or blockages to human operators for monitoring and decision support. The predominantly manual inspection of pipe networks will be replaced with teams of autonomous inspection robots that can operate for long periods of time over a large spatial scale. Reliable autonomous navigation and reporting of faults at this scale requires effective localization and mapping, which is the estimation of the robot's position and its surrounding environment. This survey presents an overview of state-of-the-art works on robot simultaneous localization and mapping (SLAM) with a focus on water and sewer pipe networks. It considers various aspects of the SLAM problem in pipes, from the motivation, to the water industry requirements, modern SLAM methods, map-types and sensors suited to pipes. Future challenges such as robustness for long term robot operation in pipes are discussed, including how making use of prior knowledge, e.g. geographic information systems (GIS) can be used to build map estimates, and improve multi-robot SLAM in the pipe environment.},
keywords = {inspection;mobile robots;multi-robot systems;navigation;path planning;pipes;robot vision;SLAM (robots);},
note = {map-types and sensors;water industry requirements;water pipe networks;multirobot SLAM;long term robot operation;robot simultaneous localization and mapping;autonomous inspection robots;predominantly manual inspection;intelligent autonomous monitoring;sewer pipe networks;},
URL = {http://dx.doi.org/10.1109/ACCESS.2021.3115981},
} 



