@inproceedings{ali-et-al:2020:3389033,
  author = {A. J. B. Ali and Z. S. Hashemifar and K. Dantu},
  journal = {MobiSys '20: Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services},
  title = {Edge-SLAM: Edge-Assisted Visual Simultaneous Localization and Mapping},
  pages = {325--37},
  doi = {10.1145/3386901.3389033},
  note = {indoor localization;visual simultaneous localization and mapping;processing time;offloading;edge computing resources;ORB-SLAM2;local mapping;loop closure;mobile device constant;edge-SLAM;visual-SLAM system;},
  address = {New York, NY, USA},
  year = {2020},
  abstract = {Localization in urban environments is becoming increasingly important and used in tools such as ARCore [11], ARKit [27] and others. One popular mechanism to achieve accurate indoor localization as well as a map of the space is using Visual Simultaneous Localization and Mapping (Visual-SLAM). However, Visual-SLAM is known to be resource-intensive in memory and processing time. Further, some of the operations grow in complexity over time, making it challenging to run on mobile devices continuously. Edge computing provides additional compute and memory resources to mobile devices to allow offloading of some tasks without the large latencies seen when offloading to the cloud. In this paper, we present Edge-SLAM, a system that uses edge computing resources to offload parts of Visual-SLAM. We use ORB-SLAM2 as a prototypical Visual-SLAM system and modify it to a split architecture between the edge and the mobile device. We keep the tracking computation on the mobile device and move the rest of the computation, i.e., local mapping and loop closure, to the edge. We describe the design choices in this effort and implement them in our prototype. Our results show that our split architecture can allow the functioning of the Visual-SLAM system long-term with limited resources without affecting the accuracy of operation. It also keeps the computation and memory cost on the mobile device constant which would allow for deployment of other end applications that use Visual-SLAM.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  keywords = {indoor radio;mobile computing;mobile robots;path planning;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1145/3386901.3389033},
}

@inproceedings{glover-et-al:2010:5509547,
  author = {A. J. Glover and W. P. Maddern and M. J. Milford and G. F. Wyeth},
  journal = {2010 IEEE International Conference on Robotics and Automation (ICRA 2010)},
  title = {FAB-MAP + RatSLAM: Appearance-based SLAM for multiple times of day},
  pages = {3507--12},
  doi = {10.1109/ROBOT.2010.5509547},
  note = {FAB-MAP + RatSLAM;appearance-based SLAM;probabilistic local feature;data association method;pose cell filtering;lifelong mapping;},
  address = {Piscataway, NJ, USA},
  year = {2010},
  abstract = {Appearance-based mapping and localisation is especially challenging when separate processes of mapping and localisation occur at different times of day. The problem is exacerbated in the outdoors where continuous change in sun angle can drastically affect the appearance of a scene. We confront this challenge by fusing the probabilistic local feature based data association method of FAB-MAP with the pose cell filtering and experience mapping of RatSLAM. We evaluate the effectiveness of our amalgamation of methods using five datasets captured throughout the day from a single camera driven through a network of suburban streets. We show further results when the streets are re-visited three weeks later, and draw conclusions on the value of the system for lifelong mapping.},
  copyright = {Copyright 2010, The Institution of Engineering and Technology},
  keywords = {probability;sensor fusion;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ROBOT.2010.5509547},
}

@article{kawewong-et-al:2013:826410,
  author = {A. Kawewong and N. Tongprasit and O. Hasegawa},
  journal = {Advanced Robotics},
  title = {A speeded-up online incremental vision-based loop-closure detection for long-term SLAM},
  volume = {27},
  number = {17},
  pages = {1325--36},
  doi = {10.1080/01691864.2013.826410},
  note = {speeded-up online incremental vision-based loop-closure detection;long-term SLAM;vision-only loop-closure detection;long-term robot navigation;bag-of-words scheme;hierarchical k-means;speed-up feature matching;score calculation technique;loop-closing location;perceptual aliasing;high precision-recall;vision-only loop-closure detection methods;},
  address = {USA},
  year = {2013},
  abstract = {An online incremental method of vision-only loop-closure detection for long-term robot navigation is proposed. The method is based on the scheme of direct feature matching which has recently become more efficient than the Bag-of-Words scheme in many challenging environments. The contributions of the paper are the application of hierarchical <i>k</i>-means to speed-up feature matching time and the improvement of the score calculation technique used to determine the loop-closing location. As a result, the presented method runs quickly in long term while retaining high accuracy. The searching cost has been markedly reduced. The proposed method requires no any off-line dictionary generation processes. It can start anywhere at any times. The evaluation has been done on standard well-known datasets being challenging in perceptual aliasing and dynamic changes. The results show that the proposed method offers high precision-recall in large-scale different environments with real-time computation comparing to other vision-only loop-closure detection methods.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  issn = {0169-1864},
  keywords = {feature extraction;image matching;mobile robots;object detection;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1080/01691864.2013.826410},
}

@inproceedings{walcott-bryant-et-al:2012:6385561,
  author = {A. Walcott-Bryant and M. Kaess and H. Johannsson and J. J. Leonard},
  journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2012)},
  title = {Dynamic pose graph SLAM: Long-term mapping in low dynamic environments},
  pages = {1871--8},
  doi = {10.1109/IROS.2012.6385561},
  note = {long-term mapping;low dynamic environments;environment map maintenance;autonomous mobile robots;dynamical environments;dynamic pose graph SLAM;DPG-SLAM;incremental smoothing and mapping;iSAM;SLAM state estimation engine;planar indoor environments;laser scan matching;change detection;real-world dynamic indoor laser data sets;},
  address = {Piscataway, NJ, USA},
  year = {2012},
  abstract = {Maintaining a map of an environment that changes over time is a critical challenge in the development of persistently autonomous mobile robots. Many previous approaches to mapping assume a static world. In this work we incorporate the time dimension into the mapping process to enable a robot to maintain an accurate map while operating in dynamical environments. This paper presents Dynamic Pose Graph SLAM (DPG-SLAM), an algorithm designed to enable a robot to remain localized in an environment that changes substantially over time. Using incremental smoothing and mapping (iSAM) as the underlying SLAM state estimation engine, the Dynamic Pose Graph evolves over time as the robot explores new places and revisits previously mapped areas. The approach has been implemented for planar indoor environments, using laser scan matching to derive constraints for SLAM state estimation. Laser scans for the same portion of the environment at different times are compared to perform change detection; when sufficient change has occurred in a location, the dynamic pose graph is edited to remove old poses and scans that no longer match the current state of the world. Experimental results are shown for two real-world dynamic indoor laser data sets, demonstrating the ability to maintain an up-to-date map despite long-term environmental changes.},
  copyright = {Copyright 2013, The Institution of Engineering and Technology},
  keywords = {laser beam applications;mobile robots;optical scanners;SLAM (robots);state estimation;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2012.6385561},
}

@article{bacca-et-al:2013:003,
  author = {B. Bacca and J. Salvi and X. Cufi},
  journal = {Robotics and Autonomous Systems},
  title = {Long-term mapping and localization using feature stability histograms},
  volume = {61},
  number = {12},
  pages = {1539--58},
  doi = {10.1016/j.robot.2013.07.003},
  note = {long-term mapping and localization;feature stability histograms;FSH model;feature management approach;voting schema;reobserved features;human memory model;short-term memory;STM;long-term memory;LTM;local feature stability values;FastSLAM framework;dynamic object filtering;map accuracy;data association effort reduction;mobile robotics;},
  address = {Netherlands},
  year = {2013},
  abstract = {This work proposes a system for long-term mapping and localization based on the Feature Stability Histogram (FSH) model which is an innovative feature management approach able to cope with changing environments. FSH is built using a voting schema, where re-observed features are promoted; otherwise the feature progressively decreases its corresponding FSH value. FSH is inspired by the human memory model. This model introduces concepts of Short-Term Memory (STM), which retains information long enough to use it, and Long-Term Memory (LTM), which retains information for longer periods of time. If the entries in STM are continuously rehearsed, they become part of LTM. However, this work proposes a change in the pipeline of this model, allowing any feature to be part of STM or LTM depending on the feature strength. FSH stores the stability values of local features, stable features are only used for localization and mapping. Experimental validation of the FSH model was conducted using the FastSLAM framework and a long-term dataset collected during a period of one year at different environmental conditions. The experiments carried out include qualitative and quantitative results such as: filtering out dynamic objects, increasing map accuracy, scalability, and reducing the data association effort in long-term runs. [All rights reserved Elsevier].},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  issn = {0921-8890},
  keywords = {mobile robots;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1016/j.robot.2013.07.003},
}

@article{bescos-et-al:2018:2860039,
  author = {B. Bescos and J. M. Facil and J. Civera and J. Neira},
  journal = {IEEE Robotics and Automation Letters},
  title = {DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes},
  volume = {3},
  number = {4},
  pages = {4076--83},
  doi = {10.1109/LRA.2018.2860039},
  note = {deep learning;multiview geometry;monocular datasets;stereo datasets;RGB-D datasets;frame background inpainting;scene rigidity;static map;dynamic object detection;ORB-SLAM2;DynaSLAM;autonomous vehicles;service robotics;visual SLAM system;SLAM algorithms;},
  address = {USA},
  year = {2018},
  abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles. In this letter we present DynaSLAM, a visual SLAM system that, building on ORB-SLAM2, adds the capabilities of dynamic object detection and background inpainting. DynaSLAM is robust in dynamic scenarios for monocular, stereo, and RGB-D configurations. We are capable of detecting the moving objects either by multiview geometry, deep learning, or both. Having a static map of the scene allows inpainting the frame background that has been occluded by such dynamic objects. We evaluate our system in public monocular, stereo, and RGB-D datasets. We study the impact of several accuracy/speed trade-offs to assess the limits of the proposed methodology. DynaSLAM outperforms the accuracy of standard visual SLAM baselines in highly dynamic scenarios. And it also estimates a map of the static parts of the scene, which is a must for long-term applications in real-world environments.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  issn = {2377-3774},
  keywords = {image colour analysis;image restoration;learning (artificial intelligence);mobile robots;object detection;service robots;SLAM (robots);stereo image processing;},
  language = {English},
  url = {http://dx.doi.org/10.1109/LRA.2018.2860039},
}

@inproceedings{liu-et-al:2021:9561126,
  author = {B. Liu and F. Tang and Y. Fu and Y. Yang and Y. Wu},
  journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {A Flexible and Efficient Loop Closure Detection Based on Motion Knowledge},
  pages = {11241--7},
  doi = {10.1109/ICRA48506.2021.9561126},
  note = {novel LCD algorithm;motion knowledge;flexible detection strategy;efficient detection strategy;flexible combinations;efficient combinations;global binary feature;hand-crafted local binary feature;continuous motion model;motion states;visual-inertial odometry system;localization errors;state-of-the-art LCD algorithms;loop closure detection;essential module;long-term explorations;bag-of-words model;low time consumption;},
  address = {Piscataway, NJ, USA},
  year = {2021},
  abstract = {Loop closure detection (LCD) is an essential module for simultaneous localization and mapping (SLAM), which can correct accumulated errors after long-term explorations. The widely used bag-of-words (BoW) model can not satisfy well the requirements of both low time consumption and high accuracy for a mobile platform. In this paper, we propose a novel LCD algorithm based on motion knowledge. We give a flexible and efficient detection strategy and also give flexible and efficient combinations of a global binary feature extracted by convolutional neural network (CNN) and a hand-crafted local binary feature. We take a continuous motion model, grid-based motion statistics (GMS) and motion states as motion knowledge. Furthermore, we fuse the proposed LCD with a visual-inertial odometry (VIO) system to correct localization errors by a pose graph optimization. Comparative experiments with state-of-the-art LCD algorithms on typical datasets have been carried out, and the results demonstrate that our proposed method achieves quite high recall rates and quite high speed at 100% precision. Moreover, experimental results from VIO further validate the effectiveness of the proposed method.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  keywords = {distance measurement;feature extraction;graph theory;mobile robots;neural nets;object detection;pose estimation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA48506.2021.9561126},
}

@article{kim-et-al:2021:3047421,
  author = {C. Kim and S. Cho and M. Sunwoo and P. Resende and B. Bradai and K. Jo},
  journal = {IEEE Access},
  title = {A Geodetic Normal Distribution Map for Long-Term LiDAR Localization on Earth},
  volume = {9},
  pages = {470--84},
  doi = {10.1109/ACCESS.2020.3047421},
  note = {map size compression;PCD map structure;long-term LiDAR localization;point cloud map;map data;geodetic normal distribution map structure;GND map structure;geodetic quad-tree tiling system;map-matching approaches;light detection and ranging sensors;LiDAR sensors;Cartesian voxelization rule;},
  address = {USA},
  year = {2021},
  abstract = {Light detection and ranging (LiDAR) sensors enable a vehicle to estimate a pose by matching their measurements with a point cloud (PCD) map. However, the PCD map structure, widely used in robot fields, has some problems to be applied for mass production in automotive fields. First, the PCD map is too big to store all map data at in-vehicle units or download the map data from a wireless network according to the vehicle location. Second, the PCD map, represented by a single origin in the Cartesian coordinates, causes coordinate conversion errors due to an inaccurate plane-orb projection, when the vehicle estimate the geodetic pose on Earth. To solve two problems, this paper presents a geodetic normal distribution (GND) map structure. The GND map structure supports a geodetic quad-tree tiling system with multiple origins to minimize the coordinate conversion errors. The map data managed by the GND map structure are compressed by using Cartesian probabilistic distributions of points as map features. The truncation errors by heterogeneous coordinates between the geodetic tiling system and Cartesian distributions are compensated by the Cartesian voxelization rule. In order to match the LiDAR measurements with the GND map structure, the paper proposes map-matching approaches based on Monte-Carlo and optimization. The paper performed some experiments to evaluate the map size compression and the long-term localization on Earth: comparison with the PCD map structure, localization in various continents, and long-term localization.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  issn = {2169-3536},
  keywords = {cartography;Monte Carlo methods;optical radar;optimisation;pose estimation;},
  language = {English},
  url = {http://dx.doi.org/10.1109/ACCESS.2020.3047421},
}

@inproceedings{yu-et-al:2019:8961714,
  author = {C. Yu and Z. Liu and X.-J. Liu and F. Qiao and Y. Wang and F. Xie and Q. Wei and Y. Yang},
  journal = {2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  title = {A DenseNet feature-based loop closure method for visual SLAM system},
  pages = {258--65},
  doi = {10.1109/ROBIO49542.2019.8961714},
  note = {convolutional neural network;public datasets;locality-sensitive hashing;max-pooling by channel;geometry structure;weighted vector of locally aggregated descriptor method;off-the-shelf network features;Dense-Loop;loop closure problem;DenseNet feature-based framework;structure details;semantic information;long-term scenes;visual SLAM system;DenseNet feature-based loop closure method;},
  address = {Piscataway, NJ, USA},
  year = {2019},
  abstract = {Loop closure is a crucial part in SLAM, especially for large and long-term scenes. Utilizing off-the-shelf networks' features in loop closure becomes a hot spot. However, what kind of network is more suitable in loop closure and how to use their features have not been well-studied. In this paper, DenseNet is introduced in this field according to its own characters. The features of DenseNet preserve both semantic information and structure details and outweigh other popular networks' features significantly. Based on this, a DenseNet feature-based framework, named Dense-Loop, is proposed to address the loop closure problem. Weighted Vector of Locally Aggregated Descriptor (WVLAD) method is used to encode the local descriptors as the final global descriptor, which could resist geometry structure and viewpoint changes. Furthermore, 4 max-pooling by channel and locality-sensitive hashing (LSH) are adopted to accelerate the search process. Extensive experiments are conducted on public datasets and the results demonstrate Dense-Loop could achieve state-of-the-art performance.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  keywords = {convolutional neural nets;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ROBIO49542.2019.8961714},
}

@article{ball-et-al:2013:9,
  author = {D. Ball and S. Heath and J. Wiles and G. Wyeth and P. Corke and M. Milford},
  journal = {Autonomous Robots},
  title = {OpenRatSLAM: an open source brain-based SLAM system},
  volume = {34},
  number = {3},
  pages = {149--76},
  doi = {10.1007/s10514-012-9317-9},
  note = {OpenRatSLAM;open source brain-based SLAM system;navigation system;neural processes;rodent brain;low resolution monocular image data;web camera;long term robot delivery trial;robot operating system framework;robot abstraction;sensor abstraction;networking;data playback;data visualization;experience map;local view cells;pose cells;visual odometry estimation;messages;parameters;class diagrams;sequence diagrams;parameter tuning strategies;open-source datasets;appearance-based simultaneous localization-and-mapping systems;},
  address = {Netherlands},
  year = {2013},
  abstract = {RatSLAM is a navigation system based on the neural processes underlying navigation in the rodent brain, capable of operating with low resolution monocular image data. Seminal experiments using RatSLAM include mapping an entire suburb with a web camera and a long term robot delivery trial. This paper describes OpenRatSLAM, an open-source version of RatSLAM with bindings to the Robot Operating System framework to leverage advantages such as robot and sensor abstraction, networking, data playback, and visualization. OpenRatSLAM comprises connected ROS nodes to represent RatSLAM's pose cells, experience map, and local view cells, as well as a fourth node that provides visual odometry estimates. The nodes are described with reference to the RatSLAM model and salient details of the ROS implementation such as topics, messages, parameters, class diagrams, sequence diagrams, and parameter tuning strategies. The performance of the system is demonstrated on three publicly available open-source datasets.},
  copyright = {Copyright 2013, The Institution of Engineering and Technology},
  issn = {0929-5593},
  keywords = {control engineering computing;data visualisation;distance measurement;image sensors;mobile robots;operating systems (computers);path planning;public domain software;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1007/s10514-012-9317-9},
}

@article{martini-et-al:2020:s20216002,
  author = {D. D. Martini and M. Gadd and P. Newman},
  journal = {Sensors},
  title = {kRadar++: Coarse-to-Fine FMCW Scanning Radar Localisation},
  volume = {20},
  number = {21},
  pages = {6002(23pp.)--},
  doi = {10.3390/s20216002},
  note = {erroneous metric pose estimation;coarse-to-fine FMCW;two-stage system;topological localisation candidates;radar-only place recognition system;precise pose estimation;spectral landmark-based techniques;scan matching sub-systems;teach-and-repeat systems;extensive radar-focused urban autonomy dataset;rivalling alternative state-of-the-art radar localisation systems;localisation trials;},
  address = {Switzerland},
  year = {2020},
  abstract = {This paper presents a novel two-stage system which integrates topological localisation candidates from a radar-only place recognition system with precise pose estimation using spectral landmark-based techniques. We prove that the-recently available-seminal radar place recognition (RPR) and scan matching sub-systems are complementary in a style reminiscent of the mapping and localisation systems underpinning visual teach-and-repeat (VTR) systems which have been exhibited robustly in the last decade. Offline experiments are conducted on the most extensive radar-focused urban autonomy dataset available to the community with performance comparing favourably with and even rivalling alternative state-of-the-art radar localisation systems. Specifically, we show the long-term durability of the approach and of the sensing technology itself to autonomous navigation. We suggest a range of sensible methods of tuning the system, all of which are suitable for online operation. For both tuning regimes, we achieve, over the course of a month of localisation trials against a single static map, high recalls at high precision, and much reduced variance in erroneous metric pose estimation. As such, this work is a necessary first step towards a radar teach-and-repeat (RTR) system and the enablement of autonomy across extreme changes in appearance or inclement conditions.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  issn = {1424-8220},
  keywords = {mobile robots;path planning;pose estimation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.3390/s20216002},
}

@inproceedings{filliat:2007:364080,
  author = {D. Filliat},
  journal = {2007 IEEE International Conference on Robotics and Automation (IEEE Cat No. 07CH37836D)},
  title = {A visual bag of words method for interactive qualitative localization and mapping},
  pages = {6pp.--},
  doi = {10.1109/ROBOT.2007.364080},
  note = {visual bag of words method;room recognition;user-interactive training;visual categorization;incremental learning;robot vision;map-learning system;animal-like personal robots;humanoid robots;interactive mapping;interactive qualitative localization;},
  address = {Piscataway, NJ, USA},
  year = {2007},
  abstract = {Localization for low cost humanoid or animal-like personal robots has to rely on cheap sensors and has to be robust to user manipulations of the robot. We present a visual localization and map-learning system that relies on vision only and that is able to incrementally learn to recognize the different rooms of an apartment from any robot position. This system is inspired by visual categorization algorithms called bag of words methods that we modified to make fully incremental and to allow a user-interactive training. Our system is able to reliably recognize the room in which the robot is after a short training time and is stable for long term use. Empirical validation on a real robot and on an image database acquired in real environments are presented.},
  copyright = {Copyright 2007, The Institution of Engineering and Technology},
  keywords = {humanoid robots;image recognition;learning (artificial intelligence);robot vision;SLAM (robots);},
  language = {English},
}

@inproceedings{opdenbosch-et-al:2018:00114,
  author = {D. V. Opdenbosch and T. Aykut and N. Alt and E. Steinbach},
  journal = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings},
  title = {Efficient Map Compression for Collaborative Visual SLAM},
  pages = {992--1000},
  doi = {10.1109/WACV.2018.00114},
  note = {swarm robotics;map information;collaborative exploration;communication data rate;swarm participants;environment maps;visual SLAM system;visual information;minimum spanning tree;single map point;inter-feature dependencies;optimal coding order;map sparsification step;map compression;collaborative visual SLAM;linear integer programming problem;},
  address = {Los Alamitos, CA, USA},
  year = {2018},
  abstract = {Swarm robotics is receiving increasing interest, because the collaborative completion of tasks, such as the exploration of unknown environments, leads to improved performance and reduced effort. The ability to exchange map information is an essential requirement for collaborative exploration. When moving to large-scale environments, where the communication data rate between the swarm participants is typically limited, efficient compression algorithms and an approach for discarding less informative parts of the map are key for a successful long-term operation. In this paper, we present a novel compression approach for environment maps obtained from a visual SLAM system. We apply feature coding to the visual information to compress the map efficiently. We make use of a minimum spanning tree to connect all features that serve as observations of a single map point. Thereby, we can exploit inter-feature dependencies and obtain an optimal coding order. Additionally, we add a map sparsification step to keep only useful map points by solving a linear integer programming problem, which preserves the map points that exhibit both good compression properties and high observability. We evaluate the proposed method on a standard dataset and show that our approach outperforms state-of-the-art techniques.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  keywords = {data compression;image coding;integer programming;linear programming;mobile robots;multi-robot systems;robot vision;SLAM (robots);trees (mathematics);},
  language = {English},
  url = {http://dx.doi.org/10.1109/WACV.2018.00114},
}

@inproceedings{einhorn-gross:2013:6698849,
  author = {E. Einhorn and H.-M. Gross},
  journal = {2013 European Conference on Mobile Robots. Proceedings},
  title = {Generic 2D/3D SLAM with NDT maps for lifelong application},
  pages = {240--7},
  doi = {10.1109/ECMR.2013.6698849},
  note = {generic 2D SLAM;lifelong application;simultaneous localization and mapping;sensor data abstraction;normal distribution transform maps;NDT maps;free-space measurements;dynamic environments;moving obstacles;moving persons;graph-based SLAM approach;computational complexity;pose graph pruning;generic 3D SLAM;},
  address = {Piscataway, NJ, USA},
  year = {2013},
  abstract = {In this paper, we present a new, generic approach for Simultaneous Localization and Mapping (SLAM). First of all, we propose an abstraction of the underlying sensor data using Normal Distribution Transform (NDT) maps that are suitable for making our approach independent from the used sensor and the dimension of the generated maps. We present some modifications for the original NDT mapping to handle free-space measurements explicitly and to enable its usage in dynamic environments with moving obstacles and persons. In the second part of this paper we describe our graph-based SLAM approach that is designed for lifelong usage. Therefore, the memory and computational complexity is limited by pruning the pose graph in an appropriate way.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  keywords = {collision avoidance;computational complexity;graph theory;normal distribution;robot vision;SLAM (robots);transforms;},
  language = {English},
  url = {http://dx.doi.org/10.1109/ECMR.2013.6698849},
}

@article{einhorn-gross:2015:008,
  author = {E. Einhorn and H.-M. Gross},
  journal = {Robotics and Autonomous Systems},
  title = {Generic NDT mapping in dynamic environments and its application for lifelong SLAM},
  volume = {69},
  pages = {28--39},
  doi = {10.1016/j.robot.2014.08.008},
  note = {normal distribution transform;NDT mapping;dynamic environment;simultaneous localization and mapping;SLAM;sensor data;free-space measurement;object detection;object handling;computational complexity;pose graph pruning;mobile robotics;},
  address = {Netherlands},
  year = {2015},
  abstract = {In this paper, we present a new, generic approach for Simultaneous Localization and Mapping (SLAM). First of all, we propose an abstraction of the underlying sensor data using Normal Distribution Transform (NDT) maps that are suitable for making our approach independent from the used sensor and the dimension of the generated maps. We present several modifications for the original NDT mapping to handle free-space measurements explicitly. We additionally describe a method to detect and handle dynamic objects such as moving persons. This enables the usage of the proposed approach in highly dynamic environments. In the second part of this paper we describe our graph-based SLAM approach that is designed for lifelong usage. Therefore, the memory and computational complexity is limited by pruning the pose graph in an appropriate way. [All rights reserved Elsevier].},
  copyright = {Copyright 2015, The Institution of Engineering and Technology},
  issn = {0921-8890},
  keywords = {computational complexity;graph theory;mobile robots;normal distribution;object detection;path planning;SLAM (robots);transforms;},
  language = {English},
  url = {http://dx.doi.org/10.1016/j.robot.2014.08.008},
}

@article{cao-et-al:2021:2962416,
  author = {F. Cao and F. Yan and S. Wang and Y. Zhuang and W. Wang},
  journal = {IEEE Transactions on Industrial Electronics},
  title = {Season-Invariant and Viewpoint-Tolerant LiDAR Place Recognition in GPS-Denied Environments},
  volume = {68},
  number = {1},
  pages = {563--74},
  doi = {10.1109/TIE.2019.2962416},
  note = {LiDAR-based methods;sequence-based temporal consistency check;efficient place recognition;three-dimensional point clouds;compact cylindrical image model;severe seasonal changes;long-term robust localization;novel LiDAR-based place recognition system;structural changes;illumination changes;article studies light detection;existing place recognition methods;viewpoint shifts;GPS-denied environments;viewpoint-tolerant LiDAR place recognition;season-invariant;},
  address = {USA},
  year = {2021},
  abstract = {Place recognition remains a challenging problem under various perceptual conditions, e.g., all weather, times of day, seasons, and viewpoint shifts. Different from most of the existing place recognition methods using pure vision, this article studies light detection and ranging (LiDAR) based approaches. Point clouds have some benefits for place recognition since they do not suffer from illumination changes. On the other hand, they are dramatically affected by structural changes from different viewpoints or across seasons. In this article, a novel LiDAR-based place recognition system is proposed to achieve long-term robust localization, even under severe seasonal changes and viewpoint shifts. To improve the efficiency, a compact cylindrical image model is designed to convert three-dimensional point clouds to two-dimensional images representing the prominent geometric relationships of scenes. The contexts (buildings, trees, road structures, etc.) of scenes are utilized for efficient place recognition. A sequence-based temporal consistency check is also introduced for postverification. Extensive real experiments on three datasets (Oxford RobotCar [1], NCLT [2], and DUT-AS) show that the proposed system outperforms both state-of-the-art visual and LiDAR-based methods, verifying its robust performance in challenging scenarios.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  issn = {0278-0046},
  keywords = {feature extraction;image classification;image sensors;mobile robots;object recognition;optical radar;robot vision;SLAM (robots);visual databases;},
  language = {English},
  url = {http://dx.doi.org/10.1109/TIE.2019.2962416},
}

@article{cao-et-al:2018:2815956,
  author = {F. Cao and Y. Zhuang and H. Zhang and W. Wang},
  journal = {IEEE Sensors Journal},
  title = {Robust Place Recognition and Loop Closing in Laser-Based SLAM for UGVs in Urban Environments},
  volume = {18},
  number = {10},
  pages = {4242--52},
  doi = {10.1109/JSEN.2018.2815956},
  note = {long-term autonomy;unmanned ground vehicles;indoor environments;outdoor environments;visual sensors lack adaptability;poor changing illumination;dynamically changing illumination;place recognition algorithm;loop closure detection;image model;2-D images;real-time place recognition;robust place recognition;loop closing;urban environments;laser-based SLAM;3-D-laser-based place recognition algorithm;simultaneous localization-and-mapping;3D laser points;query BA image matching;ORB features extraction;visual bag-of-words approach;speed normalization algorithm;3D geometry-based verification algorithm;self-developed UGV platforms;},
  address = {USA},
  year = {2018},
  abstract = {Robust place recognition plays a key role for the long-term autonomy of unmanned ground vehicles (UGVs) working in indoor or outdoor environments. Although most of the state-of-the-art that approaches for place recognition are vision-based, visual sensors lack adaptability in environments with poor or dynamically changing illumination. In this paper, a 3-D-laser-based place recognition algorithm is proposed to accomplish loop closure detection for simultaneous localization and mapping. An image model named bearing angle (BA) is adopted to convert 3-D laser points to 2-D images, and then ORB features extracted from BA images are utilized to perform scene matching. Since the computational cost for matching a query BA image with all the BA images in a database is too high to meet the requirement of performing real-time place recognition, a visual bag of words approach is used to improve search efficiency. Furthermore, a speed normalization algorithm and a 3-D geometry-based verification algorithm are proposed to complete the proposed place recognition algorithm. Experiments were conducted on two self-developed UGV platforms to verify the performance of the proposed method.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  issn = {1530-437X},
  keywords = {computational geometry;feature extraction;image matching;image sensors;mobile robots;object detection;object recognition;remotely operated vehicles;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/JSEN.2018.2815956},
}

@article{dayoub-et-al:2011:013,
  author = {F. Dayoub and G. Cielniak and T. Duckett},
  journal = {Robotics and Autonomous Systems},
  title = {Long-term experiments with an adaptive spherical view representation for navigation in changing environments},
  volume = {59},
  number = {5},
  pages = {285--95},
  doi = {10.1016/j.robot.2011.02.013},
  note = {adaptive spherical view representation;changing environment navigation;real world environments;mobile robots;hybrid metric topological map;human memory;spherical metric representation;3D geometry;},
  address = {Netherlands},
  year = {2011},
  abstract = {Real-world environments such as houses and offices change over time, meaning that a mobile robot's map will become out of date. In this work, we introduce a method to update the reference views in a hybrid metric-topological map so that a mobile robot can continue to localize itself in a changing environment. The updating mechanism, based on the multi-store model of human memory, incorporates a spherical metric representation of the observed visual features for each node in the map, which enables the robot to estimate its heading and navigate using multi-view geometry, as well as representing the local 3D geometry of the environment. A series of experiments demonstrate the persistence performance of the proposed system in real changing environments, including analysis of the long-term stability. [All rights reserved Elsevier].},
  copyright = {Copyright 2012, The Institution of Engineering and Technology},
  issn = {0921-8890},
  keywords = {mobile robots;path planning;},
  language = {English},
  url = {http://dx.doi.org/10.1016/j.robot.2011.02.013},
}

@article{han-et-al:2018:3,
  author = {F. Han and H. Wang and G. Huang and H. Zhang},
  journal = {Autonomous Robots},
  title = {Sequence-based sparse optimization methods for long-term loop closure detection in visual SLAM},
  volume = {42},
  number = {7},
  pages = {1323--35},
  doi = {10.1007/s10514-018-9736-3},
  note = {Robust Multimodal Sequence-based method;appearance variation problems;critical strong perceptual aliasing;life-long SLAM;long-term period;previous visited places;current place;long-term loop closure detection;Sequence-based sparse optimization methods;long-term SLAM;existing loop closure detection methods;formulated optimization problem;query place;template places;robust place recognition problem;different places;ROMS method;long-term visual SLAM;robust loop closure detection;},
  address = {Germany},
  year = {2018},
  abstract = {Loop closure detection is one of the most important module in Simultaneously Localization and Mapping (SLAM) because it enables to find the global topology among different places. A loop closure is detected when the current place is recognized to match the previous visited places. When the SLAM is executed throughout a long-term period, there will be additional challenges for the loop closure detection. The illumination, weather, and vegetation conditions can often change significantly during the life-long SLAM, resulting in the critical strong perceptual aliasing and appearance variation problems in loop closure detection. In order to address this problem, we propose a new Robust Multimodal Sequence-based (ROMS) method for robust loop closure detection in long-term visual SLAM. A sequence of images is used as the representation of places in our ROMS method, where each image in the sequence is encoded by multiple feature modalites so that different places can be recognized discriminatively. We formulate the robust place recognition problem as a convex optimization problem with structured sparsity regularization due to the fact that only a small set of template places can match the query place. In addition, we also develop a new algorithm to solve the formulated optimization problem efficiently, which guarantees to converge to the global optima theoretically. Our ROMS method is evaluated through extensive experiments on three large-scale benchmark datasets, which record scenes ranging from different times of the day, months, and seasons. Experimental results demonstrate that our ROMS method outperforms the existing loop closure detection methods in long-term SLAM, and achieves the state-of-the-art performance.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  issn = {0929-5593},
  keywords = {mobile robots;optimisation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1007/s10514-018-9736-3},
}

@article{han-et-al:2018:2856274,
  author = {F. Han and S. E. Beleidy and H. Wang and C. Ye and H. Zhang},
  journal = {IEEE Robotics and Automation Letters},
  title = {Learning of Holism-Landmark Graph Embedding for Place Recognition in Long-Term Autonomy},
  volume = {3},
  number = {4},
  pages = {3669--76},
  doi = {10.1109/LRA.2018.2856274},
  note = {long-term autonomy;holism-landmark graph embedding;long-term place recognition problem;semantic landmarks;graph nodes;place scene;place representation approach;embedding learning;long-term simultaneous localization and mapping;},
  address = {USA},
  year = {2018},
  abstract = {Place recognition plays an important role to perform loop closure detection of large-scale, long-term simultaneous localization and mapping in loopy environments. The long-term place recognition problem is challenging because the environment appearance exhibits significant long-term variations across various times of the day, months, and seasons. In this letter, we introduce a novel place representation approach that simultaneously integrates semantic landmarks and holistic information to achieve place recognition in long-term autonomy. First, a graph is constructed for each place. The graph nodes encode all landmarks and the holistic image of the place scene recorded in different scenarios. The edges connecting the nodes indicate that these nodes represent the same landmark or place, even though places and landmarks encoded by the nodes may exhibit different appearances in the long-term periods. Then, a graph embedding is learned to preserve the locality in the feature descriptor space, i.e., finding a projection such that the same landmark and place have the identical representation in the new projected descriptor space, no matter in what scenarios they are recorded. We formulate the embedding learning as an optimization problem and implement a new solver that provides a theoretical convergence guarantee. Extensive evaluations are conducted using large-scale benchmark datasets of place recognition in long-term autonomy, which has shown our approach's promising performance.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  issn = {2377-3774},
  keywords = {graph theory;image coding;image recognition;image representation;learning (artificial intelligence);mobile robots;optimisation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/LRA.2018.2856274},
}

@article{han-et-al:2017:2662061,
  author = {F. Han and X. Yang and Y. Deng and M. Rentschler and D. Yang and H. Zhang},
  journal = {IEEE Robotics and Automation Letters},
  title = {SRAL: shared representative appearance learning for long-term visual place recognition},
  volume = {2},
  number = {2},
  pages = {1172--9},
  doi = {10.1109/LRA.2017.2662061},
  note = {large-scale benchmark datasets;Nordland dataset;CMU-VL dataset;St Lucia dataset;feature modalities;structured sparsity-inducing norms;regularized optimization problem;feature fusion;illumination variation;weather variation;vegetation variation;long-term robot navigation;visual SLAM;visual simultaneous localization-and-mapping;loop closure detection;long-term visual place recognition;shared representative appearance learning;SRAL;},
  address = {USA},
  year = {2017},
  abstract = {Place recognition, or loop closure detection, is an essential component to address the problem of visual simultaneous localization and mapping (SLAM). Long-term navigation of robots in outdoor environments introduces new challenges to enable life-long SLAM, including the strong appearance change resulting from vegetation, weather, and illumination variations across various times of the day, different days, months, or even seasons. In this paper, we propose a new shared representative appearance learning (SRAL) approach to address long-term visual place recognition. Different from previous methods using a single feature modality or a concatenation of multiple features, our SRAL method autonomously learns representative features that are shared in all scene scenarios, and then fuses the features together to represent the long-term appearance of environments observed by a robot during life-long navigation. By formulating SRAL as a regularized optimization problem, we use structured sparsity-inducing norms to model interrelationships of feature modalities. In addition, an optimization algorithm is developed to efficiently solve the formulated optimization problem, which holds a theoretical convergence guarantee. Extensive empirical study was performed to evaluate the SRAL method using large-scale benchmark datasets, including St Lucia, CMU-VL, and Nordland datasets. Experimental results have shown that our SRAL method obtains superior performance for life-long place recognition using individual images, outperforms previous single image-based methods, and is capable of estimating the importance of feature modalities.},
  copyright = {Copyright 2017, The Institution of Engineering and Technology},
  issn = {2377-3774},
  keywords = {feature extraction;image fusion;learning (artificial intelligence);mobile robots;object detection;object recognition;optimisation;path planning;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/LRA.2017.2662061},
}

@inproceedings{pomerleau-et-al:2014:6907397,
  author = {F. Pomerleau and P. Krusi and F. Colas and P. Furgale and R. Siegwart},
  journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Long-term 3D map maintenance in dynamic environments},
  pages = {3712--19},
  doi = {10.1109/ICRA.2014.6907397},
  note = {long-term 3D map maintenance;mobile robotics;single-session geometric maps;simultaneous localization and mapping;SLAM;semantic layer;road network information;motion planning;long-term localization and mapping;3D laser scanner;dynamic point velocity estimation;static scene geometry representation;online mapping and localization system;ETH Zurich campus;},
  address = {Piscataway, NJ, USA},
  year = {2014},
  abstract = {New applications of mobile robotics in dynamic urban areas require more than the single-session geometric maps that have dominated simultaneous localization and mapping (SLAM) research to date; maps must be updated as the environment changes and include a semantic layer (such as road network information) to aid motion planning in dynamic environments. We present an algorithm for long-term localization and mapping in real time using a three-dimensional (3D) laser scanner. The system infers the static or dynamic state of each 3D point in the environment based on repeated observations. The velocity of each dynamic point is estimated without requiring object models or explicit clustering of the points. At any time, the system is able to produce a most-likely representation of underlying static scene geometry. By storing the time history of velocities, we can infer the dominant motion patterns within the map. The result is an online mapping and localization system specifically designed to enable long-term autonomy within highly dynamic environments. We validate the approach using data collected around the campus of ETH Zurich over seven months and several kilometers of navigation. To the best of our knowledge, this is the first work to unify long-term map update with tracking of dynamic objects.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  keywords = {image representation;mobile robots;path planning;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA.2014.6907397},
}

@article{tipaldi-et-al:2013:0278364913502830,
  author = {G. D. Tipaldi and D. Meyer-Delius and W. Burgard},
  journal = {International Journal of Robotics Research},
  title = {Lifelong localization in changing environments},
  volume = {32},
  number = {14},
  pages = {1662--78},
  doi = {10.1177/0278364913502830},
  note = {lifelong localization;changing environments;robot localization systems;realworld settings;learned maps;sound probabilistic approach;Rao-Blackwellized particle filter;hidden Markov model;dynamic environments;MonteCarlo localization approach;mobile robots;},
  address = {UK},
  year = {2013},
  abstract = {Robot localization systems typically assume that the environment is static, ignoring the dynamics inherent in most realworld settings. Corresponding scenarios include households, offices, warehouses and parking lots, where the location of certain objects such as goods, furniture or cars can change over time. These changes typically lead to inconsistent observations with respect to previously learned maps and thus decrease the localization accuracy or even prevent the robot from globally localizing itself. In this paper we present a soundprobabilistic approach to lifelong localization in changing environments using a combination ofa Rao-Blackwellizedparticlefilter with a hidden Markov model. By exploiting several properties of this model, we obtain a highly efficient map management approach for dynamic environments, which makes it feasible to run our algorithm online. Extensive experiments with a real robot in a dynamically changing environment demonstrate that our algorithm reliably adapts to changes in the environment and also outperforms the popular MonteCarlo localization approach.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  issn = {0278-3649},
  keywords = {mobile robots;Monte Carlo methods;particle filtering (numerical methods);},
  language = {English},
  url = {http://dx.doi.org/10.1177/0278364913502830},
}

@inproceedings{huang-et-al:2013:6698835,
  author = {G. Huang and M. Kaess and J. J. Leonard},
  journal = {2013 European Conference on Mobile Robots. Proceedings},
  title = {Consistent sparsification for graph optimization},
  pages = {150--7},
  doi = {10.1109/ECMR.2013.6698835},
  note = {graph optimization sparsification;standard pose-graph formulation;simultaneous localization and mapping;SLAM;memory resources;consistent graph sparsification scheme;graph node marginalization;graph node sparsification;consistent relative constraints;consistent l<sub>1</sub>-regularized minimization problem;synthetic data;real data;graph edge sparsification;},
  address = {Piscataway, NJ, USA},
  year = {2013},
  abstract = {In a standard pose-graph formulation of simultaneous localization and mapping (SLAM), due to the continuously increasing numbers of nodes (states) and edges (measurements), the graph may grow prohibitively too large for long-term navigation. This motivates us to systematically reduce the pose graph amenable to available processing and memory resources. In particular, in this paper we introduce a consistent graph sparsification scheme: (i) sparsifying nodes via marginalization of old nodes, while retaining all the information (consistent relative constraints) - which is conveyed in the discarded measurements - about the remaining nodes after marginalization; and (ii) sparsifying edges by formulating and solving a consistent &lscr;1-regularized minimization problem, which automatically promotes the sparsity of the graph. The proposed approach is validated on both synthetic and real data.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  keywords = {graph theory;minimisation;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ECMR.2013.6698835},
}

@article{kim-et-al:2019:2897340,
  author = {G. Kim and B. Park and A. Kim},
  journal = {IEEE Robotics and Automation Letters},
  title = {1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using Scan Context Image},
  volume = {4},
  number = {2},
  pages = {1948--55},
  doi = {10.1109/LRA.2019.2897340},
  note = {long-term LiDAR localization;long-term localization method;structural information;image format;point cloud descriptor;place recognition problem;place classification;convolutional neural network;robot localization;grid map;Oxford RobotCar dataset;scan context image localization;time 1 year;time 1 d;},
  address = {USA},
  year = {2019},
  abstract = {In this letter, we present a long-term localization method that effectively exploits the structural information of an environment via an image format. The proposed method presents a robust year-round localization performance even when learned in just a single day. The proposed localizer learns a point cloud descriptor, named Scan Context Image (SCI), and performs robot localization on a grid map by formulating the place recognition problem as place classification using a convolutional neural network. Our method is faster than existing methods proposed for place recognition because it avoids a pairwise comparison between a query and scans in a database. In addition, we provide thorough validations using publicly available long-term datasets, the NCLT dataset and the Oxford RobotCar dataset, and show that the Scan Context Image (SCI) localization attains consistent performance over a year and outperforms existing methods.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  issn = {2377-3774},
  keywords = {image classification;learning (artificial intelligence);mobile robots;neural nets;optical radar;path planning;robot vision;},
  language = {English},
  url = {http://dx.doi.org/10.1109/LRA.2019.2897340},
}

@inproceedings{kurz-et-al:2021:9636530,
  author = {G. Kurz and M. Holoch and P. Biber},
  journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Geometry-based Graph Pruning for Lifelong SLAM},
  pages = {3313--20},
  doi = {10.1109/IROS51168.2021.9636530},
  note = {mapped locations;robot trajectory;real-world long-term datasets;geometry-based graph pruning;lifelong SLAM;},
  address = {Piscataway, NJ, USA},
  year = {2021},
  abstract = {Lifelong SLAM considers long-term operation of a robot where already mapped locations are revisited many times in changing environments. As a result, traditional graph-based SLAM approaches eventually become extremely slow due to the continuous growth of the graph and the loss of sparsity. Both problems can be addressed by a graph pruning algorithm. It carefully removes vertices and edges to keep the graph size reasonable while preserving the information needed to provide good SLAM results. We propose a novel method that considers geometric criteria for choosing the vertices to be pruned. It is efficient, easy to implement, and leads to a graph with evenly spread vertices that remain part of the robot trajectory. Furthermore, we present a novel approach of marginalization that is more robust to wrong loop closures than existing methods. The proposed algorithm is evaluated on two publicly available real-world long-term datasets and compared to the unpruned case as well as ground truth. We show that even on a long dataset (25h), our approach manages to keep the graph sparse and the speed high while still providing good accuracy (40 times speed up, 6cm map error compared to unpruned case).},
  copyright = {Copyright 2022, The Institution of Engineering and Technology},
  keywords = {geometry;graph theory;SLAM (robots);trajectory control;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS51168.2021.9636530},
}

@inproceedings{singh-et-al:2021:9564866,
  author = {G. Singh and M. Wu and S.-K. Lam and D. V. Minh},
  journal = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
  title = {Hierarchical Loop Closure Detection for Long-term Visual SLAM with Semantic-Geometric Descriptors},
  pages = {2909--16},
  doi = {10.1109/ITSC48978.2021.9564866},
  note = {loop closure detection capability;long-term SLAM applications;conventional feature descriptors;loop closure detection methods;Mapping systems;Modern visual Simultaneous Localization;semantic-geometric descriptors;hierarchical loop closure detection;lowest query time;highest loop closure detection accuracy;recent state-of-the-art methods;accurate loop closure detection;local location semantics;global location semantics;novel hierarchical place recognition method;long-term SLAM scenarios;available features;local semantic vocabulary trees;semantic-geometric location descriptors;semantic visual SLAM framework;long-term visual SLAM;},
  address = {Piscataway, NJ, USA},
  year = {2021},
  abstract = {Modern visual Simultaneous Localization and Mapping (SLAM) systems rely on loop closure detection methods for correcting drifts in maps and poses. Existing loop closure detection methods mainly employ conventional feature descriptors to create vocabulary for describing places using bag-of-words (BOW). Such methods do not perform well in long-term SLAM applications as the scene content may change over time due to the presence of dynamic objects, even though the locations are revisited with the same viewpoint. This work enhances the loop closure detection capability of long-term visual SLAM by reducing the number of false matches through the use of location semantics. We extend a semantic visual SLAM framework to build compact global semantic-geometric location descriptors and local semantic vocabulary trees, by leveraging on the already available features and semantics. The local semantic vocabulary trees support incremental vocabulary learning, which is well-suited for long-term SLAM scenarios where the scenes encountered are not known beforehand. A novel hierarchical place recognition method that leverages the global and local location semantics is proposed to enable fast and accurate loop closure detection. The proposed method outperforms recent state-of-the-art methods (i.e., FABMAP2, SeqSLAM, iBOW-LCD, and HTMap) on all datasets considered (i.e., KITTI, Synthia, and CBD), with highest loop closure detection accuracy and lowest query time.},
  copyright = {Copyright 2022, The Institution of Engineering and Technology},
  keywords = {feature extraction;image matching;mobile robots;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ITSC48978.2021.9564866},
}

@article{hu-et-al:2022:1003907,
  author = {H. Hu and H. Wang and Z. Liu and W. Chen},
  journal = {IEEE/CAA Journal of Automatica Sinica},
  title = {Domain-Invariant Similarity Activation Map Contrastive Learning for Retrieval-Based Long-Term Visual Localization},
  volume = {9},
  number = {2},
  pages = {313--28},
  doi = {10.1109/JAS.2021.1003907},
  note = {general architecture;domain-invariant features;multidomain image translation;novel gradient-weighted similarity activation mapping loss;finer localization;adaptive triplet loss;coarse-to-fine image retrieval pipeline;Grad-SAM loss;CMU-Seasons dataset;strong generalization ability;RobotCar dataset using models;state-of-the-art image-based localization baselines;night-time images;domain-invariant similarity activation map contrastive learning;retrieval-based long-term;mobile robot;autonomous driving;efficient technique;image-based localization methods;drastic variability;illumination changes;retrieval-based visual localization;},
  address = {USA},
  year = {2022},
  abstract = {Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  issn = {2329-9266},
  keywords = {feature extraction;image classification;image retrieval;learning (artificial intelligence);mobile robots;object recognition;SLAM (robots);unsupervised learning;visual databases;},
  language = {English},
  url = {http://dx.doi.org/10.1109/JAS.2021.1003907},
}

@inproceedings{johannsson-et-al:2013:6630556,
  author = {H. Johannsson and M. Kaess and M. Fallon and J. J. Leonard},
  journal = {2013 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Temporally scalable visual SLAM using a reduced pose graph},
  pages = {54--61},
  doi = {10.1109/ICRA.2013.6630556},
  note = {temporally scalable visual SLAM;reduced pose graph;pose graph representation;online binocular visual SLAM system;indoor mapping;accelerometer data;},
  address = {Piscataway, NJ, USA},
  year = {2013},
  abstract = {In this paper, we demonstrate a system for temporally scalable visual SLAM using a reduced pose graph representation. Unlike previous visual SLAM approaches that maintain static keyframes, our approach uses new measurements to continually improve the map, yet achieves efficiency by avoiding adding redundant frames and not using marginalization to reduce the graph. To evaluate our approach, we present results using an online binocular visual SLAM system that uses place recognition for both robustness and multi-session operation. Additionally, to enable large-scale indoor mapping, our system automatically detects elevator rides based on accelerometer data. We demonstrate long-term mapping in a large multi-floor building, using approximately nine hours of data collected over the course of six months. Our results illustrate the capability of our visual SLAM system to map a large are over extended period of time.},
  copyright = {Copyright 2013, The Institution of Engineering and Technology},
  keywords = {graph theory;pose estimation;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA.2013.6630556},
}

@inproceedings{kretzschmar-et-al:2011:6048060,
  author = {H. Kretzschmar and C. Stachniss and G. Grisetti},
  journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
  title = {Efficient information-theoretic graph pruning for graph-based SLAM with laser range finders},
  pages = {865--71},
  doi = {10.1109/IROS.2011.6048060},
  note = {information-theoretic graph pruning;graph-based SLAM;laser range finders;pose graph;data acquisition;Chow-Liu trees;sparse graph;},
  address = {Piscataway, NJ, USA},
  year = {2011},
  abstract = {In graph-based SLAM, the pose graph encodes the poses of the robot during data acquisition as well as spatial constraints between them. The size of the pose graph has a substantial influence on the runtime and the memory requirements of a SLAM system, which hinders long-term mapping. In this paper, we address the problem of efficient information-theoretic compression of pose graphs. Our approach estimates the expected information gain of laser measurements with respect to the resulting occupancy grid map. It allows for restricting the size of the pose graph depending on the information that the robot acquires about the environment or based on a given memory limit, which results in an any-space SLAM system. When discarding laser scans, our approach marginalizes out the corresponding pose nodes from the graph. To avoid a densely connected pose graph, which would result from exact marginalization, we propose an approximation to marginalization that is based on local Chow-Liu trees and maintains a sparse graph. Real world experiments suggest that our approach effectively reduces the growth of the pose graph while minimizing the loss of information in the resulting grid map.},
  copyright = {Copyright 2011, The Institution of Engineering and Technology},
  keywords = {data acquisition;graph theory;laser ranging;SLAM (robots);trees (mathematics);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2011.6048060},
}

@inproceedings{thomas-et-al:2021:9561701,
  author = {H. Thomas and B. Agro and M. Gridseth and J. Zhang and T. D. Barfoot},
  journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor Navigation},
  pages = {14047--14053},
  doi = {10.1109/ICRA48506.2021.9561701},
  note = {training pool;network predictions;common localization techniques;lidar segmentation;autonomous indoor navigation;self-supervised learning approach;semantic segmentation;lidar frames;deep point cloud segmentation architecture;human annotation;annotation process;SLAM;ray-tracing algorithms;multiple navigation sessions;permanent structures;disentangle short-term;long-term movable objects;new sessions;semantic labels;session;semantically filtered point clouds;},
  address = {Piscataway, NJ, USA},
  year = {2021},
  abstract = {We present a self-supervised learning approach for the semantic segmentation of lidar frames. Our method is used to train a deep point cloud segmentation architecture without any human annotation. The annotation process is automated with the combination of simultaneous localization and mapping (SLAM) and ray-tracing algorithms. By performing multiple navigation sessions in the same environment, we are able to identify permanent structures, such as walls, and disentangle short-term and long-term movable objects, such as people and tables, respectively. New sessions can then be performed using a network trained to predict these semantic labels. We demonstrate the ability of our approach to improve itself over time, from one session to the next. With semantically filtered point clouds, our robot can navigate through more complex scenarios, which, when added to the training pool, help to improve our network predictions. We provide insights into our network predictions and show that our approach can also improve the performances of common localization techniques.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  keywords = {image segmentation;indoor communication;learning (artificial intelligence);mobile robots;optical radar;path planning;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA48506.2021.9561701},
}

@article{yin-et-al:2020:2905046,
  author = {H. Yin and Y. Wang and X. Ding and L. Tang and S. Huang and R. Xiong},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  title = {3D LiDAR-based global localization using siamese neural network},
  volume = {21},
  number = {4},
  pages = {1380--92},
  doi = {10.1109/TITS.2019.2905046},
  note = {3D point clouds;mobile vehicles;outdoor scenarios;autonomous vehicles;localization failures;reduced dimension scan representations;neural networks;global prior map;feature learning method;3D light detection;siamese network;place recognition problem;similarity modeling problem;dimension reduced representations;learned representations;global poses;localization framework;localization step;localization approach;3D LiDAR-based global localization;siamese neural network;},
  address = {USA},
  year = {2020},
  abstract = {Global localization in 3D point clouds is a challenging task for mobile vehicles in outdoor scenarios, which requires the vehicle to localize itself correctly in a given map without prior knowledge of its pose. This is a critical component of autonomous vehicles or robots on the road for handling localization failures. In this paper, based on reduced dimension scan representations learned from neural networks, a solution to global localization is proposed by achieving place recognition first and then metric pose estimation in the global prior map. Specifically, we present a semi-handcrafted feature learning method for 3D Light detection and ranging (LiDAR) point clouds using artificial statistics and siamese network, which transforms the place recognition problem into a similarity modeling problem. Additionally, the sensor data using dimension reduced representations require less storage space and make the searching easier. With the learned representations by networks and the global poses, a prior map is built and used in the localization framework. In the localization step, position only observations obtained by place recognition are used in a particle filter algorithm to achieve precise pose estimation. To demonstrate the effectiveness of our place recognition and localization approach, KITTI benchmark and our multi-session datasets are employed for comparison with other geometric-based algorithms. The results show that our system can achieve both high accuracy and efficiency for long-term autonomy.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  issn = {1524-9050},
  keywords = {feature extraction;image filtering;image representation;learning (artificial intelligence);mobile robots;neural nets;optical radar;particle filtering (numerical methods);pose estimation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/TITS.2019.2905046},
}

@article{zhang-et-al:2018:1729881418780178,
  author = {H. Zhang and X. Chen and H. Lu and J. Xiao},
  journal = {International Journal of Advanced Robotic Systems},
  title = {Distributed and collaborative monocular simultaneous localization and mapping for multi-robot systems in large-scale environments},
  volume = {15},
  number = {3},
  pages = {173--92},
  doi = {10.1177/1729881418780178},
  note = {local map;large-scale global map;multirobot relative poses;long-term monocular simultaneous localization;robots;map merging method;vision-based multirobot;mapping architecture;collaborative simultaneous localization;robot operating system messages;incremental maps;mapping algorithm;monocular vision;multirobot system;mapping system;collaborative monocular simultaneous localization;large-scale environments;},
  address = {UK},
  year = {2018},
  abstract = {In this article, we propose a distributed and collaborative monocular simultaneous localization and mapping system for the multi-robot system in large-scale environments, where monocular vision is the only exteroceptive sensor. Each robot estimates its pose and reconstructs the environment simultaneously using the same monocular simultaneous localization and mapping algorithm. Meanwhile, they share the results of their incremental maps by streaming keyframes through the robot operating system messages and the wireless network. Subsequently, each robot in the group can obtain the global map with high efficiency. To build the collaborative simultaneous localization and mapping architecture, two novel approaches are proposed. One is a robust relocalization method based on active loop closure, and the other is a vision-based multi-robot relative pose estimating and map merging method. The former is used to solve the problem of tracking failures when robots carry out long-term monocular simultaneous localization and mapping in large-scale environments, while the latter uses the appearance-based place recognition method to determine multi-robot relative poses and build the large-scale global map by merging each robot's local map. Both KITTI data set and our own data set acquired by a handheld camera are used to evaluate the proposed system. Experimental results show that the proposed distributed multi-robot collaborative monocular simultaneous localization and mapping system can be used in both indoor small-scale and outdoor large-scale environments.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  issn = {1729-8814},
  keywords = {cameras;mobile robots;multi-robot systems;pose estimation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1177/1729881418780178},
}

@article{biswas-veloso:2013:0278364913503892,
  author = {J. Biswas and M. M. Veloso},
  journal = {International Journal of Robotics Research},
  title = {Localization and navigation of the CoBots over long-term deployments},
  volume = {32},
  number = {14},
  pages = {1679--94},
  doi = {10.1177/0278364913503892},
  note = {motion commands;graph representation;topological policy;hierarchical planner;laser rangefinder observations;2D-vector map;fast sampling plane filtering algorithm;analytically computed state space derivatives;sensor observations;particle filter;CGR algorithm;corrective gradient refinement algorithm;navigation algorithms;localization algorithms;indoor service mobile robots;multifloor buildings;multiple collaborative robots;long-term deployments;},
  address = {UK},
  year = {2013},
  abstract = {For the last three years, we have developed and researched multiple collaborative robots, CoBots, which have been autonomously traversing our multi-floor buildings. We pursue the goal of long-term autonomy for indoor service mobile robots as the ability for them to be deployed indefinitely while they perform tasks in an evolving environment. The CoBots include several levels of autonomy, and in this paper we focus on their localization and navigation algorithms. We present the Corrective Gradient Refinement (CGR) algorithm, which refines the proposal distribution of the particle filter used for localization with sensor observations using analytically computed state space derivatives on a vector map. We also present the Fast Sampling Plane Filtering algorithm that extracts planar regions from depth images in real time. These planar regions are then projected onto the 2D vector map of the building, and along with the laser rangefinder observations, used with CGR for localization. For navigation, we present a hierarchical planner, which computes a topological policy using a graph representation of the environment, computes motion commands based on the topological policy, and then modifies the motion commands to side-step perceived obstacles. We started logging the deployments of the CoBots one and a halfyears ago, and have since collected logs of the CoBots traversing more than 130 km over 1082 deployments and a total run time of 182 h, which we publish as a dataset consisting of more than 10 million laser scans. The logs show that although there have been continuous changes in the environment, the robots are robust to most of them, and there exist only afew locations where changes in the environment cause increased uncertainty in localization.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  issn = {0278-3649},
  keywords = {mobile robots;multi-robot systems;navigation;particle filtering (numerical methods);path planning;service robots;topology;},
  language = {English},
  url = {http://dx.doi.org/10.1177/0278364913503892},
}

@article{biswas-veloso:2017:005,
  author = {J. Biswas and M. M. Veloso},
  journal = {Robotics and Autonomous Systems},
  title = {Episodic non-Markov localization},
  volume = {87},
  pages = {162--76},
  doi = {10.1016/j.robot.2016.09.005},
  note = {episodic nonMarkov localization;EnML;mobile robot localization;autonomous service mobile robots;depth cameras;laser rangefinders;episodes;pose estimate limiting;robot trajectory;timestep detection;nonlinear functional optimization;maximum likelihood estimation;varying graphical network representation;dynamic Bayesian network;static map;unmapped moving objects;dynamic features;short-term features;permanent mapped objects;long-term features;unmapped static objects;Markov observation independence;},
  address = {Netherlands},
  year = {2017},
  abstract = {Markov localization and its variants are widely used for mobile robot localization. These methods assume Markov independence of observations, implying that the observations can be entirely explained by a map. However, in real human environments, robots frequently make unexpected observations due to unmapped static objects like chairs and tables, and dynamic objects like humans. We therefore introduce Episodic non-Markov Localization (EnML), which reasons about the world as consisting of three classes of objects: long-term features corresponding to permanent mapped objects, short-term features corresponding to unmapped static objects, and dynamic features corresponding to unmapped moving objects. Long-term features are represented by a static map, while short-term features are detected and tracked in real-time. To reason about unexpected observations and their correlations across poses, we augment the Dynamic Bayesian Network for Markov localization to include varying edges and nodes, resulting in a novel Varying Graphical Network representation. The maximum likelihood estimate of the belief is incrementally computed by non-linear functional optimization. By detecting timesteps along the robot's trajectory where unmapped observations prior to such time steps are unrelated to those afterwards, EnML limits the history of observations and pose estimates to &ldquo;episodes&rdquo; over which the belief is computed. We demonstrate EnML using different types of sensors including laser rangefinders and depth cameras, and over multiple datasets, comparing it with alternative approaches. We further include results of a team of indoor autonomous service mobile robots traversing hundreds of kilometers using EnML. [All rights reserved Elsevier].},
  copyright = {Copyright 2017, The Institution of Engineering and Technology},
  issn = {0921-8890},
  keywords = {belief networks;laser ranging;Markov processes;maximum likelihood estimation;mobile robots;optimisation;sensor placement;trajectory control;},
  language = {English},
  url = {http://dx.doi.org/10.1016/j.robot.2016.09.005},
}

@article{coulin-et-al:2022:3136241,
  author = {J. Coulin and R. Guillemard and V. Gay-Bellile and C. Joly and A. D. L. Fortelle},
  journal = {IEEE Robotics and Automation Letters},
  title = {Tightly-Coupled Magneto-Visual-Inertial Fusion for Long Term Localization in Indoor Environment},
  volume = {7},
  number = {2},
  pages = {952--9},
  doi = {10.1109/LRA.2021.3136241},
  note = {magneto-Visual-Inertial fusion;long term localization;indoor environment;visual data;inertial data;magnetic data;long-term localization;state-of-the-art Visual-Inertial SLAM solutions;reuse visual map;MultiState Constraint Kalman Filter;magnetic map;environment appearance;localization accuracy;},
  address = {USA},
  year = {2022},
  abstract = {We propose in this letter a tightly-coupled fusion of visual, inertial and magnetic data for long-term localization in indoor environment. Unlike state-of-the-art Visual-Inertial SLAM (VISLAM) solutions that reuse visual map to prevent drift, we present in this letter an extension of the Multi-State Constraint Kalman Filter (MSCKF) that takes advantage of a magnetic map. It makes our solution more robust to variations of the environment appearance. The experimental results demonstrate that the localization accuracy of the proposed approach is almost the same over time periods longer than a year.},
  copyright = {Copyright 2022, The Institution of Engineering and Technology},
  issn = {2377-3766},
  keywords = {inertial navigation;Kalman filters;mobile robots;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/LRA.2021.3136241},
}

@inproceedings{oberlander-et-al:2013:6766479,
  author = {J. Oberlander and A. Roennau and R. Dillmann},
  journal = {2013 16th International Conference on Advanced Robotics (ICAR)},
  title = {Hierarchical SLAM using spectral submap matching with opportunities for long-term operation},
  pages = {7pp.--},
  doi = {10.1109/ICAR.2013.6766479},
  note = {matching performance;occupancy grid representations;FMT;Fourier-Mellin transform;global localization;simultaneous localization and mapping;spectral registration approach;spectral submap matching;hierarchical SLAM;},
  address = {Piscataway, NJ, USA},
  year = {2013},
  abstract = {We present a hierarchical SLAM approach which uses spectral registration of local submaps to close loops and to perform global localization after a restart. Using the Fourier-Mellin Transform (FMT), we robustly register occupancy grid representations of local submaps and present methods which improve matching performance. We further show how good match candidates can be reliably detected even from scaled-down versions of the submaps, which significantly reduces the computation time. The spectral registration approach proves useful even in the presence of significant environmental changes due to the fact that it calculates a dense match, incorporating all observed information rather than a sparse set of features.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  keywords = {Fourier transforms;image matching;image registration;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICAR.2013.6766479},
}

@article{saarinen-et-al:2013:0278364913499415,
  author = {J. P. Saarinen and H. Andreasson and T. Stoyanov and A. J. Lilienthal},
  journal = {International Journal of Robotics Research},
  title = {3D normal distributions transform occupancy maps: An efficient representation for mapping in dynamic environments},
  volume = {32},
  number = {14},
  pages = {1627--44},
  doi = {10.1177/0278364913499415},
  note = {3D normal distributions transform occupancy maps;dynamic environments;autonomous vehicles;industrial environments;3D world models;3D spatial representation;online real-world mapping;normal distributions transform maps;occupancy grid maps;NDT-OM;NDT maps;exact recursive update equations;multiresolution maps;occupancy update equations;probabilistic sensor model;milkfactory;preprogrammed manipulators;},
  address = {USA},
  year = {2013},
  abstract = {In order to enable long-term operation of autonomous vehicles in industrial environments numerous challenges need to be addressed. A basic requirement for many applications is the creation and maintenance of consistent 3D world models. This article proposes a novel 3D spatial representation for online real-world mapping, building upon two known representations: normal distributions transform (NDT) maps and occupancy grid maps. The proposed normal distributions transform occupancy map (NDT-OM) combines the advantages of both representations; compactness of NDT maps and robustness of occupancy maps. One key contribution in this article is that we formulate an exact recursive updates for NDT-OMs. We show that the recursive update equations provide natural support for multi-resolution maps. Next, we describe a modification of the recursive update equations that allows adaptation in dynamic environments. As a second key contribution we introduce NDT-OMs and formulate the occupancy update equations that allow to build consistent maps in dynamic environments. The update of the occupancy values are based on an efficient probabilistic sensor model that is specially formulated for NDT-OMs. In several experiments with a total of 17 hours of data from a milkfactory we demonstrate that NDT-OMs enable real-time performance in large-scale, long-term industrial setups.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  issn = {0278-3649},
  keywords = {control engineering computing;dairy products;industrial robots;mobile robots;normal distribution;production engineering computing;production facilities;robot dynamics;robot vision;SLAM (robots);solid modelling;transforms;},
  language = {English},
  url = {http://dx.doi.org/10.1177/0278364913499415},
}

@inproceedings{berrio-et-al:2019:8814289,
  author = {J. S. Berrio and J. Ward and S. Worrall and E. Nebot},
  journal = {2019 IEEE Intelligent Vehicles Symposium (IV)},
  title = {Identifying robust landmarks in feature-based maps},
  pages = {1166--72},
  doi = {10.1109/IVS.2019.8814289},
  note = {feature-based maps;urban environment;automated vehicle;global map reference frame;optimal path planning;safe navigation;dynamic objects;geometric distribution;map maintenance;landmarks identification;autonomous vehicle;},
  address = {Piscataway, NJ, USA},
  year = {2019},
  abstract = {To operate in an urban environment, an automated vehicle must be capable of accurately estimating its position within a global map reference frame. This is necessary for optimal path planning and safe navigation. To accomplish this over an extended period of time, the global map requires long term maintenance. This includes the addition of newly observable features and the removal of transient features belonging to dynamic objects. The latter is especially important for the long-term use of the map as matching against a map with features that no longer exist can result in incorrect data associations, and consequently erroneous localisation. This paper addresses the problem of removing features from the map that correspond to objects that are no longer observable/present in the environment. This is achieved by assigning a single score which depends on the geometric distribution and characteristics when the features are re-detected (or not) on different occasions. Our approach not only eliminates ephemeral features, but can also be used as a reduction algorithm for highly dense maps. We tested our approach using half a year of weekly drives over the same 500 metre section of road in an urban environment. The results presented demonstrate the validity of the long term approach to map maintenance.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  keywords = {feature extraction;geometry;mobile robots;object detection;optimisation;path planning;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IVS.2019.8814289},
}

@inproceedings{zhu-et-al:2018:8500686,
  author = {J. Zhu and Y. Ai and B. Tian and D. Cao and S. Scherer},
  journal = {2018 IEEE Intelligent Vehicles Symposium (IV)},
  title = {Visual Place Recognition in Long-term and Large-scale Environment based on CNN Feature},
  pages = {1679--85},
  doi = {10.1109/IVS.2018.8500686},
  note = {visual place recognition;large-scale environment;CNN feature;intelligent vehicles;intelligent vehicle localization;visual description;place images;hand-crafted feature;description method;convolutional neural network;pre-trained network model;place sequence;image descriptors;Hamming distance;place matching;},
  address = {Piscataway, NJ, USA},
  year = {2018},
  abstract = {With the universal application of camera in intelligent vehicles, visual place recognition has become a major problem in intelligent vehicle localization. The traditional solution is to make visual description of place images using hand-crafted feature for matching places, but this description method is not very good for extreme variability, especially for seasonal transformation. In this paper, we propose a new method based on convolutional neural network (CNN), by putting images into the pre-trained network model to get automatically learned image descriptors, and through some operations of pooling, fusion and binarization to optimize them, then the similarity result of place recognition is presented with the Hamming distance of the place sequence. In the experimental part, we compare our method with some state-of-the-art algorithms, FABMAP, ABLE-M and SeqSLAM, to illustrate its advantages. The experimental results show that our method based on CNN achieves better performance than other methods on the representative public datasets.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  keywords = {convolution;feedforward neural nets;image matching;image recognition;intelligent transportation systems;learning (artificial intelligence);SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IVS.2018.8500686},
}

@inproceedings{ikeda-tanaka:2010:5509579,
  author = {K. Ikeda and K. Tanaka},
  journal = {2010 IEEE International Conference on Robotics and Automation (ICRA 2010)},
  title = {Visual robot localization using compact binary landmarks},
  pages = {4397--403},
  doi = {10.1109/ROBOT.2010.5509579},
  note = {visual robot localization;compact binary landmarks;mobile robot localization;information sharing networks;landmark database;visual landmarks;compact binary code;semantic hashing technique;web-scale image retrieval;binary representation;semantic gap;saliency evaluation;high-speed car-like robot;},
  address = {Piscataway, NJ, USA},
  year = {2010},
  abstract = {This paper is concerned with the problem of mobile robot localization using a novel compact representation of visual landmarks. With recent progress in lifelong map-learning as well as in information sharing networks, compact representation of a large-size landmark database has become crucial. In this paper, we propose a compact binary code (e.g. 32bit code) landmark representation by employing the semantic hashing technique from web-scale image retrieval. We show how well such a binary representation achieves compactness of a landmark database while maintaining efficiency of the localization system. In our contribution, we investigate the cost-performance, the semantic gap, the saliency evaluation using the presented techniques as well as challenge to further reduce the resources (#bits) per landmark. Experiments using a high-speed car-like robot show promising results.},
  copyright = {Copyright 2010, The Institution of Engineering and Technology},
  keywords = {control engineering computing;image retrieval;mobile robots;path planning;robot vision;},
  language = {English},
  url = {http://dx.doi.org/10.1109/ROBOT.2010.5509579},
}

@inproceedings{konolige-bowman:2009:5354121,
  author = {K. Konolige and J. Bowman},
  journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2009)},
  title = {Towards lifelong visual maps},
  pages = {1156--63},
  doi = {10.1109/IROS.2009.5354121},
  note = {visual maps;SLAM mapping system;FastSLAM;occlusion;localization failures;mobile robots;},
  address = {Piscataway, NJ, USA},
  year = {2009},
  abstract = {The typical SLAM mapping system assumes a static environment and constructs a map that is then used without regard for ongoing changes. Most SLAM systems, such as FastSLAM, also require a single connected run to create a map. In this paper we present a system of visual mapping, using only input from a stereo camera, that continually updates an optimized metric map in large indoor spaces with movable objects: people, furniture, partitions, etc. The system can be stopped and restarted at arbitrary disconnected points, is robust to occlusion and localization failures, and efficiently maintains alternative views of a dynamic environment. It operates completely online at a 30 Hz frame rate.},
  copyright = {Copyright 2010, The Institution of Engineering and Technology},
  keywords = {image sensors;mobile robots;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2009.5354121},
}

@article{mactavish-et-al:2018:21838,
  author = {K. MacTavish and M. Paton and T. D. Barfoot},
  journal = {Journal of Field Robotics},
  title = {Selective memory: Recalling relevant experience for long-term visual localization},
  volume = {35},
  number = {8},
  pages = {1265--92},
  doi = {10.1002/rob.21838},
  note = {map representation;visual experience storage;experience-triage algorithm;collaborative filtering;memory-based recommender systems;landmark-level recommendations;unstructured outdoor environments;multiexperience maps;long-term autonomous path;MEL system;appearance modalities;multiexperience localization system;long-term navigation systems;low-power vision sensors;autonomous mobile vehicles;visual navigation;long-term visual localization;selective memory;},
  address = {USA},
  year = {2018},
  abstract = {Visual navigation is a key enabling technology for autonomous mobile vehicles. The ability to provide large-scale, long-term navigation using low-cost, low-power vision sensors is appealing for industrial applications. A crucial requirement for long-term navigation systems is the ability to localize in environments whose appearance is constantly changing over time-due to lighting, weather, seasons, and physical changes. This paper presents a multiexperience localization (MEL) system that uses a powerful map representation-storing every visual experience in layers-that does not make assumptions about underlying appearance modalities and generators. Our localization system provides real-time performance by selecting online, a subset of experiences against which to localize. We achieve this task through a novel experience-triage algorithm based on collaborative filtering, which selects experiences relevant to the <i>live view</i>, outperforming competing techniques. Based on classical memory-based recommender systems, this technique also enables landmark-level recommendations, is entirely online, and requires no training data. We demonstrate the capabilities of the MEL system in the context of long-term autonomous path following in unstructured outdoor environments with a challenging 100-day field experiment through day, night, snow, spring, and summer. We furthermore provide offline analysis comparing our system to several state-of-the-art alternatives. We show that the combination of the novel methods presented in this paper enable full use of incredibly rich multiexperience maps, opening the door to robust long-term visual localization.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  issn = {1556-4959},
  keywords = {cartography;image sensors;mobile robots;navigation;path planning;recommender systems;robot vision;},
  language = {English},
  url = {http://dx.doi.org/10.1002/rob.21838},
}

@inproceedings{pirker-et-al:2011:6048253,
  author = {K. Pirker and M. Ruther and H. Bischof},
  journal = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011)},
  title = {CD SLAM - Continuous localization and mapping in a dynamic world},
  pages = {3990--7},
  doi = {10.1109/IROS.2011.6048253},
  note = {CD SLAM;continuous localization and mapping;perpetual localization and mapping;memory consumption;repetitive scene elements;dynamic scene elements;robust data association;visual SLAM method;visibility-dependent map filtering;keyframe organization;complex map representation;mobile robot;},
  address = {Piscataway, NJ, USA},
  year = {2011},
  abstract = {When performing large-scale perpetual localization and mapping one faces problems like memory consumption or repetitive and dynamic scene elements requiring robust data association. We propose a visual SLAM method which handles short- and long-term scene dynamics in large environments using a single camera only. Through visibility-dependent map filtering and efficient keyframe organization we reach a considerable performance gain only through incorporation of a slightly more complex map representation. Experiments on a large, mixed indoor/outdoor dataset over a time period of two weeks demonstrate the scalability and robustness of our approach.},
  copyright = {Copyright 2011, The Institution of Engineering and Technology},
  keywords = {filtering theory;mobile robots;robot dynamics;robot vision;sensor fusion;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2011.6048253},
}

@inproceedings{wang-et-al:2019:8793499,
  author = {K. Wang and Y. Lin and L. Wang and L. Han and M. Hua and X. Wang and S. Lian and B. Huang},
  journal = {2019 International Conference on Robotics and Automation (ICRA)},
  title = {A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation},
  pages = {5224--30},
  doi = {10.1109/ICRA.2019.8793499},
  note = {segmentation algorithms;semantic segmentation;robotics;refined 3D pose information;vision-based tasks;mutual improvement;unified framework;instantaneous motion change handling;long-term changes;simultaneous localization and segmentation;},
  address = {Piscataway, NJ, USA},
  year = {2019},
  abstract = {This paper presents a novel framework for simultaneously implementing localization and segmentation, which are two of the most important vision-based tasks for robotics. While the goals and techniques used for them were considered to be different previously, we show that by making use of the intermediate results of the two modules, their performance can be enhanced at the same time. Our framework is able to handle both the instantaneous motion and long-term changes of instances in localization with the help of the segmentation result, which also benefits from the refined 3D pose information. We conduct experiments on various datasets, and prove that our framework works effectively on improving the precision and robustness of the two tasks and outperforms existing localization and segmentation algorithms.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  keywords = {image motion analysis;image segmentation;mobile robots;pose estimation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA.2019.8793499},
}

@article{clement-et-al:2020:2967659,
  author = {L. Clement and M. Gridseth and J. Tomasi and J. Kelly},
  journal = {IEEE Robotics and Automation Letters},
  title = {Learning matchable image transformations for long-term metric visual localization},
  volume = {5},
  number = {2},
  pages = {1492--9},
  doi = {10.1109/LRA.2020.2967659},
  note = {matchable image transformations;long-term metric visual localization;long-term metric self-localization;autonomous mobile robots;vision-based systems;appearance change;seasonal variations;experience-based mapping;appearance gap;reliable metric localization;color constancy theory;RGB-to-grayscale mapping;inlier feature;weather conditions;single-experience localization pipeline;target nondifferentiable localization pipeline;low-dimensional context feature;cross-appearance feature matching;localization performance;day-night cycles;continuous metric localization;single mapping experience;experience-based localization;dramatically reduced data requirements;},
  address = {USA},
  year = {2020},
  abstract = {Long-term metric self-localization is an essential capability of autonomous mobile robots, but remains challenging for vision-based systems due to appearance changes caused by lighting, weather, or seasonal variations. While experience-based mapping has proven to be an effective technique for bridging the `appearance gap,' the number of experiences required for reliable metric localization over days or months can be very large, and methods for reducing the necessary number of experiences are needed for this approach to scale. Taking inspiration from color constancy theory, we learn a nonlinear RGB-to-grayscale mapping that explicitly maximizes the number of inlier feature matches for images captured under different lighting and weather conditions, and use it as a pre-processing step in a conventional single-experience localization pipeline to improve its robustness to appearance change. We train this mapping by approximating the target non-differentiable localization pipeline with a deep neural network, and find that incorporating a learned low-dimensional context feature can further improve cross-appearance feature matching. Using synthetic and real-world datasets, we demonstrate substantial improvements in localization performance across day-night cycles, enabling continuous metric localization over a 30-hour period using a single mapping experience, and allowing experience-based localization to scale to long deployments with dramatically reduced data requirements.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  issn = {2377-3774},
  keywords = {feature extraction;image colour analysis;image matching;learning (artificial intelligence);mobile robots;neural nets;object recognition;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/LRA.2020.2967659},
}

@inproceedings{murphy-sibley:2014:6907022,
  author = {L. Murphy and G. Sibley},
  journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Incremental unsupervised topological place discovery},
  pages = {1312--18},
  doi = {10.1109/ICRA.2014.6907022},
  note = {incremental unsupervised topological place discovery;online place discovery-and-recognition engine;memory footprint reduction;place recognition systems;precision improvement;recall improvement;cluster topics;covisibility graph;relative simultaneous localization-and-mapping engine;dynamic visual words vocabulary;density based clustering;visual topics estimation;indoor robot sequence;standard outdoor robot sequence;static camera;},
  address = {Piscataway, NJ, USA},
  year = {2014},
  abstract = {This paper describes an online place discovery and recognition engine that fuses information over time to create topologically distinct places. A key motivation is the recognition that a single image may be a poor exemplar of what constitutes a place. Images are not `places' nor are they `documents'. Instead, by treating image-sequences as a multimodal distribution over topics - and by discovering topics incrementally and online - it is possible to both reduce the memory footprint of place recognition systems, and to improve precision and recall. Distinctive key-places are represented by a cluster topics found from the covisibility graph of a relative simultaneous localization and mapping engine - key-places inherently span many images. A dynamic vocabulary of visual words and density based clustering is used to continually estimate a set of visual topics, changes in which drive the place-recognition process. The system is evaluated using an indoor robot sequence, a standard outdoor robot sequence and a long-term sequence from a static camera. Experiments demonstrate qualitatively distinct themes associated with discovered places - from common place types such as `hallway', or `desk-area', to temporal concepts such as `dusk', `dawn' or `mid-day'. Compared to traditional image-based place-recognition, this reduces the information that must be stored without reducing place-recognition performance.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  keywords = {graph theory;image sensors;image sequences;object recognition;pattern clustering;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA.2014.6907022},
}

@inproceedings{sun-et-al:2021:9635886,
  author = {L. Sun and M. Taher and C. Wild and C. Zhao and Y. Zhang and F. Majer and Z. Yan and T. Krajnik and T. Prescott and T. Duckett},
  journal = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Robust and Long-term Monocular Teach and Repeat Navigation using a Single-experience Map},
  pages = {2635--42},
  doi = {10.1109/IROS51168.2021.9635886},
  note = {proportional-integral control;visual error;day-to-night navigation;single-experience map;navigation experiments;outdoor environments;long-term operation;deep-learned descriptors;high illumination variance;self-supervised descriptor;autonomous navigation;robust monocular teach;long-term monocular teach;robust monocular visual teach-and-repeat navigation system;DarkPoint;vast array;},
  address = {Piscataway, NJ, USA},
  year = {2021},
  abstract = {This paper presents a robust monocular visual teach-and-repeat (VT&amp;R) navigation system for long-term operation in outdoor environments. The approach leverages deep-learned descriptors to deal with the high illumination variance of the real world. In particular, a tailored self-supervised descriptor, DarkPoint, is proposed for autonomous navigation in outdoor environments. We seamlessly integrate the localisation with control, in which proportional-integral control is used to eliminate the visual error with the pitfall of the unknown depth. Consequently, our approach achieves day-to-night navigation using a single-experience map and is able to repeat complex and fast manoeuvres. To verify our approach, we performed a vast array of navigation experiments in various outdoor environments, where both navigation accuracy and robustness of the proposed system are investigated. The experimental results show that our approach is superior to the baseline method with regards to accuracy and robustness.},
  copyright = {Copyright 2022, The Institution of Engineering and Technology},
  keywords = {deep learning (artificial intelligence);distance measurement;mobile robots;radionavigation;robot vision;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS51168.2021.9635886},
}

@article{tang-et-al:2019:7,
  author = {L. Tang and Y. Wang and X. Ding and H. Yin and R. Xiong and S. Huang},
  journal = {Autonomous Robots},
  title = {Topological local-metric framework for mobile robots navigation: a long term perspective},
  volume = {43},
  number = {1},
  pages = {197--211},
  doi = {10.1007/s10514-018-9724-7},
  note = {global consistent framework;topological local-metric framework;TLF;constant complexity;sensor data;stability;robustness;world application deployment;long term perspective;mobile robots navigation;local-metric framework;localization performance;similar localization accuracy;global optimization;environmental changes;multiple local metric coordinates;navigation algorithms;sensor information matching;unavoidable erroneous measurements;local consistency;global consistency;topological graph;size 1.0 km;},
  address = {Germany},
  year = {2019},
  abstract = {Long term mapping and localization are the primary components for mobile robots in real world application deployment, of which the crucial challenge is the robustness and stability. In this paper, we introduce a topological local-metric framework (TLF), aiming at dealing with environmental changes, erroneous measurements and achieving constant complexity. TLF organizes the sensor data collected by the robot in a topological graph, of which the geometry is only encoded in the edge, i.e. the relative poses between adjacent nodes, relaxing the global consistency to local consistency. Therefore the TLF is more robust to unavoidable erroneous measurements from sensor information matching since the error is constrained in the local. Based on TLF, as there is no global coordinate, we further propose the localization and navigation algorithms by switching across multiple local metric coordinates. Besides, a lifelong memorizing mechanism is presented to memorize the environmental changes in the TLF with constant complexity, as no global optimization is required. In experiments, the framework and algorithms are evaluated on 21-session data collected by stereo cameras, which are sensitive to illumination, and compared with the state-of-art global consistent framework. The results demonstrate that TLF can achieve similar localization accuracy with that from global consistent framework, but brings higher robustness with lower cost. The localization performance can also be improved from sessions because of the memorizing mechanism. Finally, equipped with TLF, the robot navigates itself in a 1 km session autonomously.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  issn = {0929-5593},
  keywords = {graph theory;mobile robots;navigation;path planning;},
  language = {English},
  url = {http://dx.doi.org/10.1007/s10514-018-9724-7},
}

@inproceedings{wang-et-al:2020:9468884,
  author = {L. Wang and W. Chen and J. Wang},
  journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Long-Term Localization With Time Series Map Prediction for Mobile Robots in Dynamic Environments},
  pages = {7pp.--},
  doi = {10.1109/IROS45743.2020.9468884},
  note = {prediction error;long-term localization performance;dynamic environments;time series Map Prediction;mobile robot;historical information;high-precision localization;long-term localization approach;ARMA-based Map Prediction;average model;time series modeling method;environmental map modeling;},
  address = {Piscataway, NJ, USA},
  year = {2020},
  abstract = {In many applications of mobile robot, the environment is constantly changing. How to use historical information to analysis environmental changes and generate a map corresponding with current environment is important to achieve high-precision localization. Inspired by predictive mechanism of brain, this paper presents a long-term localization approach named ArmMPU (ARMA-based Map Prediction and Update) based on time series modeling and prediction. Autoregressive moving average model (ARMA), a kind of time series modeling method, is employed for environmental map modeling and prediction, then predicted map and filtered observation are fused to fix the prediction error. The simulation and experiment results show that the proposed method improves long-term localization performance in dynamic environments.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  keywords = {autoregressive moving average processes;mobile robots;robot dynamics;time series;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS45743.2020.9468884},
}

@inproceedings{wu-wu:2019:8968599,
  author = {L. Wu and Y. Wu},
  journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Deep Supervised Hashing with Similar Hierarchy for Place Recognition},
  pages = {3781--6},
  doi = {10.1109/IROS40897.2019.8968599},
  note = {deep supervised hashing;long-term simultaneous localization;deep learning;similar hierarchy loss function;high quality hash codes;place recognition;simultaneous localization and mapping;SLAM;convolution neural network;CNN;CPU;},
  address = {Piscataway, NJ, USA},
  year = {2019},
  abstract = {Place recognition as one of the most significant requirements for long-term simultaneous localization and mapping (SLAM) has been developed rapidly in recent years. Also, deep learning is proved to be more capable than traditional methods to extract features under some complex environments. However, in real-world environments, there are many challenging problems such as viewpoint changes and illumination changes. The existing deep learning-based place recognition in extracting feature phases and matching process is both time-consuming. Moreover, features extracted from convolution neural network (CNN) are floating-point type with high dimension. In this paper, we propose deep supervised hashing for place recognition, where we design a similar hierarchy loss function to learn a model. The model can distinguish the similar images more accurately which is well suitable to place recognition. Besides the model can learn high quality hash codes by maximizing the likelihood of triplet labels. Experiments on several benchmark datasets for place recognition show that our approach is robust to viewpoints, illuminations and season changes with high accuracy. Furthermore, the trained model can extract features and match in real time on CPU with less memory consumption.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  keywords = {convolutional neural nets;feature extraction;image matching;object recognition;robot vision;SLAM (robots);supervised learning;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS40897.2019.8968599},
}

@article{bosse-zlot:2009:009,
  author = {M. Bosse and R. Zlot},
  journal = {Robotics and Autonomous Systems},
  title = {Keypoint design and evaluation for place recognition in 2D lidar maps},
  volume = {57},
  number = {12},
  pages = {1211--24},
  doi = {10.1016/j.robot.2009.07.009},
  note = {keypoint design;place recognition;2D lidar maps;robot navigation;sublinear time search;arbitrary keypoint descriptors;k-nearest neighbors search;match verification algorithms;mapping problems;SLAM;},
  address = {Netherlands},
  year = {2009},
  abstract = {We address the place recognition problem, which we define as the problem of establishing whether an observed location has been previously seen, and if so, determining the transformation aligning the current observations to an existing map. In the contexts of robot navigation and mapping, place recognition amounts to globally localizing a robot or map segment without being given any prior estimate. An efficient method of solving this problem involves first selecting a set of keypoints in the scene which store an encoding of their local region, and then utilizing a sublinear-time search into a database of keypoints previously generated from the global map to identify places with common features. We present an algorithm to embed arbitrary keypoint descriptors in a reduced-dimension metric space, in order to frame the problem as an efficient nearest neighbor search. Given that there are a multitude of possibilities for keypoint design, we propose a general methodology for comparing keypoint location selection heuristics and descriptor models that describe the region around the keypoint. With respect to selecting keypoint locations, we introduce a metric that encodes how likely it is that the keypoint will be found in the presence of noise and occlusions during mapping passes. Metrics for keypoint descriptors are used to assess the distinguishability between the distributions of matches and non-matches and the probability the correct match will be found in an approximate k-nearest neighbors search. Verification of the test outcomes is done by comparing the various keypoint designs on a kilometers-scale place recognition problem. We apply our design evaluation methodology to three keypoint selection heuristics and six keypoint descriptor models. A full place recognition system is presented, including a series of match verification algorithms which effectively filter out false positives. Results from city-scale and long-term mapping problems illustrate our approach for both offline and online SLAM, map merging, and global localization and demonstrate that our algorithm is able to produce accurate maps over trajectories of hundreds of kilometers. [All rights reserved Elsevier].},
  copyright = {Copyright 2010, The Institution of Engineering and Technology},
  issn = {0921-8890},
  keywords = {optical radar;pattern recognition;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1016/j.robot.2009.07.009},
}

@inproceedings{dymczyk-et-al:2016:66,
  author = {M. Dymczyk and E. Stumm and J. Nieto and R. Siegwart and I. Gilitschenski},
  journal = {2016 Fourth International Conference on 3D Vision (3DV). Proceedings},
  title = {Will it last? Learning stable features for long-term visual localization},
  pages = {572--81},
  doi = {10.1109/3DV.2016.66},
  note = {map feature selection;CNN classifier;image patches;depth maps;feature recognition;life-long matchability;long-term visual localization;convolutional neural network;simultaneous localization and mapping systems;SLAM systems;},
  address = {Los Alamitos, CA, USA},
  year = {2016},
  abstract = {An increasing number of simultaneous localization and mapping (SLAM) systems are using appearance-based localization to improve the quality of pose estimates. However, with the growing time-spans and size of the areas we want to cover, appearance-based maps are often becoming too large to handle and are consisting of features that are not always reliable for localization purposes. This paper presents a method for selecting map features that are persistent over time and thus suited for long-term localization. Our methodology relies on a CNN classifier based on image patches and depth maps for recognizing which features are suitable for life-long matchability. Thus, the classifier not only considers the appearance of a feature but also takes into account its expected lifetime. As a result, our feature selection approach produces more compact maps with a high fraction of temporally-stable features compared to the current state-of-the-art, while rejecting unstable features that typically harm localization. Our approach is validated on indoor and outdoor datasets, that span over a period of several months.},
  copyright = {Copyright 2017, The Institution of Engineering and Technology},
  keywords = {feature selection;image classification;neural nets;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/3DV.2016.66},
}

@inproceedings{dymczyk-et-al:2015:7139575,
  author = {M. Dymczyk and S. Lynen and T. Cieslewski and M. Bosse and R. Siegwart and P. Furgale},
  journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
  title = {The gist of maps - summarizing experience for lifelong localization},
  pages = {2767--73},
  doi = {10.1109/ICRA.2015.7139575},
  note = {online place recognition;offline map maintenance;summary map;reduced map representation;scoring function;sampling policy;vision-based localization;visual navigation;lifelong localization;},
  address = {Piscataway, NJ, USA},
  year = {2015},
  abstract = {Robust, scalable place recognition is a core competency for many robotic applications. However, when revisiting places over and over, many state-of-the-art approaches exhibit reduced performance in terms of computation and memory complexity and in terms of accuracy. For successful deployment of robots over long time scales, we must develop algorithms that get better with repeated visits to the same environment, while still working within a fixed computational budget. This paper presents and evaluates an algorithm that alternates between online place recognition and offline map maintenance with the goal of producing the best performance with a fixed map size. At the core of the algorithm is the concept of a Summary Map, a reduced map representation that includes only the landmarks that are deemed most useful for place recognition. To assign landmarks to the map, we use a scoring function that ranks the utility of each landmark and a sampling policy that selects the landmarks for each place. The Summary Map can then be used by any descriptor-based inference method for constant-complexity online place recognition. We evaluate a number of scoring functions and sampling policies and show that it is possible to build and maintain maps of a constant size and that place-recognition performance improves over multiple visits.},
  copyright = {Copyright 2015, The Institution of Engineering and Technology},
  keywords = {cartography;image recognition;image sampling;},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA.2015.7139575},
}

@inproceedings{dymczyk-et-al:2016:7759673,
  author = {M. Dymczyk and T. Schneider and I. Gilitschenski and R. Siegwart and E. Stumm},
  journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Erasing bad memories: Agent-side summarization for long-term mapping},
  pages = {4572--9},
  doi = {10.1109/IROS.2016.7759673},
  note = {agent-side summarization;long-term mapping;pose estimation;robotic applications;autonomous navigation;collaboration;multiagent data collection;localization map;data transfers;multisession mapping;landmark selection method;learned feature coefficients;data flow reduction;},
  address = {Piscataway, NJ, USA},
  year = {2016},
  abstract = {Precisely estimating the pose of an agent in a global reference frame is a crucial goal that unlocks a multitude of robotic applications, including autonomous navigation and collaboration. In order to achieve this, current state-of-the-art localization approaches collect data provided by one or more agents and create a single, consistent localization map, maintained over time. However, with the introduction of lengthier sorties and the growing size of the environments, data transfers between the backend server where the global map is stored and the agents are becoming prohibitively large. While some existing methods partially address this issue by building compact summary maps, the data transfer from the agents to the backend can still easily become unmanageable. In this paper, we propose a method that is designed to reduce the amount of data that needs to be transferred from the agent to the backend, functioning in large-scale, multi-session mapping scenarios. Our approach is based upon a landmark selection method that exploits information coming from multiple, possibly weak and correlated, landmark utility predictors; fused using learned feature coefficients. Such a selection yields a drastic reduction in data transfer while maintaining localization performance and the ability to efficiently summarize environments over time. We evaluate our approach on a data set that was autonomously collected in a dynamic indoor environment over a period of several months.},
  copyright = {Copyright 2016, The Institution of Engineering and Technology},
  keywords = {data flow computing;multi-agent systems;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2016.7759673},
}

@inproceedings{gadd-newman:2016:7759843,
  author = {M. Gadd and P. Newman},
  journal = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Checkout my map: version control for fleetwide visual localisation},
  pages = {5729--36},
  doi = {10.1109/IROS.2016.7759843},
  note = {robots;autonomous vehicle fleets;structural dynamics;visual map supporting;EBN;experience-based navigation;visual localisation;version control;},
  address = {Piscataway, NJ, USA},
  year = {2016},
  abstract = {This paper is about underpinning long-term operations of fleets of vehicles using visual localisation. In particular it examines ways in which vehicles, considered as independent agents, can share, update and leverage each others' visual experiences in a mutually beneficial way. We draw on our previous work in Experience-based Navigation (EBN) [1], in which a visual map supporting multiple representations of the same place is built, yielding real-time localisation capability for a solitary vehicle. We now consider how any number of such agents might operate in concert via data sharing policies that are germane to the shared task of lifelong localisation. We rapidly construct considerable maps by the conjoining of work distributed to asynchronous processes, and share expertise amongst the team by the selective dispensing of mission-specific map contents. We demonstrate and evaluate our system against 100km of data collected in North Oxford over a period of a month featuring diverse deviation in appearance due to atmospheric, lighting, and structural dynamics. We show that our framework is capable of creating maps in a fraction of the time required by single-agent EBN, with no significant loss in localisation robustness, and is able to furnish robots on real-world forays with maps which require much less storage.},
  copyright = {Copyright 2016, The Institution of Engineering and Technology},
  keywords = {configuration management;mobile robots;navigation;robot dynamics;robot vision;SLAM (robots);vehicle dynamics;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2016.7759843},
}

@article{labbe-michaud:2019:21831,
  author = {M. Labbe and F. Michaud},
  journal = {Journal of Field Robotics},
  title = {RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation},
  volume = {36},
  number = {2},
  pages = {416--46},
  doi = {10.1002/rob.21831},
  note = {RTAB-Map;open-source lidar;visual simultaneous localization;mapping library;long-term online operation;open-source library;real-time appearance-based mapping;appearance-based loop closure detection approach;SLAM approach;SLAM approaches;visual- lidar-based;visual SLAM configurations;lidar SLAM configurations;},
  address = {USA},
  year = {2019},
  abstract = {Distributed as an open-source library since 2013, real-time appearance-based mapping (RTAB-Map) started as an appearance-based loop closure detection approach with memory management to deal with large-scale and long-term online operation. It then grew to implement simultaneous localization and mapping (SLAM) on various robots and mobile platforms. As each application brings its own set of constraints on sensors, processing capabilities, and locomotion, it raises the question of which SLAM approach is the most appropriate to use in terms of cost, accuracy, computation power, and ease of integration. Since most of SLAM approaches are either visual- or lidar-based, comparison is difficult. Therefore, we decided to extend RTAB-Map to support both visual and lidar SLAM, providing in one package a tool allowing users to implement and compare a variety of 3D and 2D solutions for a wide range of applications with different robots and sensors. This paper presents this extended version of RTAB-Map and its use in comparing, both quantitatively and qualitatively, a large selection of popular real-world datasets (e.g., KITTI, EuRoC, TUM RGB-D, MIT Stata Center on PR2 robot), outlining strengths, and limitations of visual and lidar SLAM configurations from a practical perspective for autonomous navigation applications.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  issn = {1556-4959},
  keywords = {image colour analysis;mobile robots;object detection;optical radar;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1002/rob.21831},
}

@article{mazuran-et-al:2016:0278364915581629,
  author = {M. Mazuran and W. Burgard and G. D. Tipaldi},
  journal = {International Journal of Robotics Research},
  title = {Nonlinear factor recovery for long-term SLAM},
  volume = {35},
  number = {1-3},
  pages = {50--72},
  doi = {10.1177/0278364915581629},
  note = {nonlinear factor recovery;long-term SLAM;graph-based simultaneous localization and mapping approach;computational cost;marginal distribution;Kullback-Leibler divergence;NFR;convex optimization problem;nonlinear measurement function;tree-based sparse approximations;binary factors;mobile robotics;},
  address = {UK},
  year = {2016},
  abstract = {For long-term operations, graph-based simultaneous localization and mapping (SLAM) approaches require nodes to be marginalized in order to control the computational cost. In this paper, we present a method to recover a set of nonlinear factors that best represents the marginal distribution in terms of Kullback-Leibler divergence. The proposed method, which we call <i>nonlinear factor recovery</i> (NFR), estimates both the mean and the information matrix of the set of nonlinear factors, where the recovery of the latter is equivalent to solving a convex optimization problem. NFR is able to provide either the dense distribution or a sparse approximation of it. In contrast to previous algorithms, our method does not necessarily require a global linearization point and can be used with any nonlinear measurement function. Moreover, we are not restricted to only using tree-based sparse approximations and binary factors, but we can include any topology and correlations between measurements. Experiments performed on several publicly available datasets demonstrate that our method outperforms the state of the art with respect to the Kullback-Leibler divergence and the sparsity of the solution.},
  copyright = {Copyright 2016, The Institution of Engineering and Technology},
  issn = {0278-3649},
  keywords = {approximation theory;convex programming;costing;mobile robots;SLAM (robots);trees (mathematics);},
  language = {English},
  url = {http://dx.doi.org/10.1177/0278364915581629},
}

@inproceedings{mohan-et-al:2015:7139966,
  author = {M. Mohan and D. Galvez-Lopez and C. Monteleoni and G. Sibley},
  journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
  title = {Environment selection and hierarchical place recognition},
  pages = {5487--94},
  doi = {10.1109/ICRA.2015.7139966},
  note = {environment selection;hierarchical place recognition algorithm;multiple independent large maps;image decomposition;query image;similarity criterion;co-occurrent features;co-occurrence matrices;image collection;inverted indices hierarchy;loop detection;},
  address = {Piscataway, NJ, USA},
  year = {2015},
  abstract = {As robots continue to create long-term maps, the amount of information that they need to handle increases over time. In terms of place recognition, this implies that the number of images being considered may increase until exceeding the computational resources of the robot. In this paper we consider a scenario where, given multiple independent large maps, possibly from different cities or locations, a robot must effectively and in real time decide whether it can localize itself in one of those known maps. Since the number of images to be handled by such a system is likely to be extremely large, we find that it is beneficial to decompose the set of images into independent groups or environments. This raises a new question: Given a query image, how do we select the best environment? This paper proposes a similarity criterion that can be used to solve this problem. It is based on the observation that, if each environment is described in terms of its co-occurrent features, similarity between environments can be established by comparing their co-occurrence matrices. We show that this leads to a novel place recognition algorithm that divides the collection of images into environments and arranges them in a hierarchy of inverted indices. By selecting first the relevant environment for the operating robot, we can reduce the number of images to perform the actual loop detection, reducing the execution time while preserving the accuracy. The practicality of this approach is shown through experimental results on several large datasets covering a combined distance of more than 750Km.},
  copyright = {Copyright 2015, The Institution of Engineering and Technology},
  keywords = {cartography;image recognition;matrix algebra;robot vision;},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA.2015.7139966},
}

@inproceedings{lazaro-et-al:2018:8594310,
  author = {M. T. Lazaro and R. Capobianco and G. Grisetti},
  journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Efficient Long-term Mapping in Dynamic Environments},
  pages = {153--60},
  doi = {10.1109/IROS.2018.8594310},
  note = {mapping problem;longterm SLAM datasets;graph coherency;intra-session loop closure detections;out-dated nodes;graph complexity;nonstatic entities;merging procedure;efficient ICP-based alignment;up-to-date state;2D point cloud data;local maps;graph SLAM paradigm;multiple mapping sessions;single mapping sessions;SLAM system;autonomous robots;dynamic environments;long-term robot operation;},
  address = {Piscataway, NJ, USA},
  year = {2018},
  abstract = {As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  keywords = {graph theory;mobile robots;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2018.8594310},
}

@article{carlevaris-bianco-et-al:2014:2347571,
  author = {N. Carlevaris-Bianco and M. Kaess and R. M. Eustice},
  journal = {IEEE Transactions on Robotics},
  title = {Generic Node Removal for Factor-Graph SLAM},
  volume = {30},
  number = {6},
  pages = {1371--85},
  doi = {10.1109/TRO.2014.2347571},
  note = {generic node removal;factor-graph SLAM;generic factor-based method;simultaneous localization and mapping;generic linear constraints;monocular vision;Kullback-Leibler divergence;GLC method;computational complexity;},
  address = {USA},
  year = {2014},
  abstract = {This paper reports on a generic factor-based method for node removal in factor-graph simultaneous localization and mapping (SLAM), which we call generic linear constraints (GLCs). The need for a generic node removal tool is motivated by long-term SLAM applications, whereby nodes are removed in order to control the computational cost of graph optimization. GLC is able to produce a new set of linearized factors over the elimination clique that can represent either the true marginalization (i.e., dense GLC) or a sparse approximation of the true marginalization using a ChowLiu tree (i.e., sparse GLC). The proposed algorithm improves upon commonly used methods in two key ways: First, it is not limited to graphs with strictly full-state relative-pose factors and works equally well with other low-rank factors, such as those produced by monocular vision. Second, the new factors are produced in such a way that accounts for measurement correlation, which is a problem encountered in other methods that rely strictly upon pairwise measurement composition. We evaluate the proposed method over multiple real-world SLAM graphs and show that it outperforms other recently proposed methods in terms of Kullback-Leibler divergence. Additionally, we experimentally demonstrate that the proposed GLC method provides a principled and flexible tool to control the computational complexity of long-term graph SLAM, with results shown for 34.9 h of real-world indoor-outdoor data covering 147.4 km collected over 27 mapping sessions spanning a period of 15 months.},
  copyright = {Copyright 2015, The Institution of Engineering and Technology},
  issn = {1552-3098},
  keywords = {computational complexity;graph theory;mobile robots;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/TRO.2014.2347571},
}

@article{piasco-et-al:2021:6,
  author = {N. Piasco and D. Sidibe and V. Gouet-Brunet and C. Demonceaux},
  journal = {International Journal of Computer Vision},
  title = {Improving Image Description with Auxiliary Modality for Visual Localization in Challenging Conditions},
  volume = {129},
  number = {1},
  pages = {185--202},
  doi = {10.1007/s11263-020-01363-6},
  note = {principal difficulty;long-term localization;dynamic changes;outdoor environments;outdoor large scale image-based localization;cross-weather;learned global image descriptor;scene geometry information;query image;localization accuracy;localization performances;visual appearance;visual clues;geometric clues;monocular images;cross-season localization;night images;daytime images;multimodal descriptors;image description;auxiliary modality;visual localization;image indexing;lifelong localization;robot navigation;autonomous driving heritage valorization;cultural heritage valorization;},
  address = {Germany},
  year = {2021},
  abstract = {Image indexing for lifelong localization is a key component for a large panel of applications, including robot navigation, autonomous driving or cultural heritage valorization. The principal difficulty in long-term localization arises from the dynamic changes that affect outdoor environments. In this work, we propose a new approach for outdoor large scale image-based localization that can deal with challenging scenarios like cross-season, cross-weather and day/night localization. The key component of our method is a new learned global image descriptor, that can effectively benefit from scene geometry information during training. At test time, our system is capable of inferring the depth map related to the query image and use it to increase localization accuracy. We show through extensive evaluation that our method can improve localization performances, especially in challenging scenarios when the visual appearance of the scene has changed. Our method is able to leverage both visual and geometric clues from monocular images to create discriminative descriptors for cross-season localization and effective matching of images acquired at different time periods. Our method can also use weakly annotated data to localize night images across a reference dataset of daytime images. Finally we extended our method to reflectance modality and we compare multi-modal descriptors respectively based on geometry, material reflectance and a combination of both.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  issn = {0920-5691},
  keywords = {computer vision;feature extraction;image classification;image colour analysis;image matching;image recognition;image representation;image retrieval;learning (artificial intelligence);mobile robots;object detection;object recognition;path planning;robot vision;},
  language = {English},
  url = {http://dx.doi.org/10.1007/s11263-020-01363-6},
}

@inproceedings{vysotska-et-al:2015:7139576,
  author = {O. Vysotska and T. Naseer and L. Spinello and W. Burgard and C. Stachniss},
  journal = {2015 IEEE International Conference on Robotics and Automation (ICRA). Proceedings},
  title = {Efficient and effective matching of image sequences under substantial appearance changes exploiting GPS priors},
  pages = {2774--9},
  doi = {10.1109/ICRA.2015.7139576},
  note = {image sequences matching;GPS priors;image data;visual image matching;sequence information;seasonal changes;},
  address = {Piscataway, NJ, USA},
  year = {2015},
  abstract = {The ability to localize a robot is an important capability and matching of observations under substantial changes is a prerequisite for robust long-term operation. This paper investigates the problem of efficiently coping with seasonal changes in image data. We present an extension of a recent approach [15] to visual image matching using sequence information. Our extension allows for exploiting GPS priors in the matching process to overcome the main computational bottleneck of the previous method and to handle loops within the image sequences. We present an experimental evaluation using real world data containing substantial seasonal changes and show that our approach outperforms the previous method in case a noisy GPS pose prior is available.},
  copyright = {Copyright 2015, The Institution of Engineering and Technology},
  keywords = {Global Positioning System;image matching;image sequences;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA.2015.7139576},
}

@article{biber-duckett:2009:0278364908096286,
  author = {P. Biber and T. Duckett},
  journal = {International Journal of Robotics Research},
  title = {Experimental analysis of sample-based maps for long-term SLAM},
  volume = {28},
  number = {1},
  pages = {20--33},
  doi = {10.1177/0278364908096286},
  note = {sample-based maps;long-term SLAM;experimental analysis;mobile service robots;stability-plasticity dilemma;sample-based representation;robust statistics;},
  address = {USA},
  year = {2009},
  abstract = {This paper presents a system for long-term SLAM (simultaneous localization and mapping) by mobile service robots and its experimental evaluation in a real dynamic environment. To deal with the stability-plasticity dilemma (the trade-off between adaptation to new patterns and preservation of old patterns), the environment is represented by multiple timescales simultaneously (five in our experiments). A sample-based representation is proposed, where older memories fade at different rates depending on the timescale and robust statistics are used to interpret the samples. The dynamics of this representation are analyzed in a five-week experiment, measuring the relative influence of short- and long-term memories over time and further demonstrating the robustness of the approach.},
  copyright = {Copyright 2009, The Institution of Engineering and Technology},
  issn = {0278-3649},
  keywords = {mobile robots;robot dynamics;service robots;SLAM (robots);stability;},
  language = {English},
  url = {http://dx.doi.org/10.1177/0278364908096286},
}

@inproceedings{egger-et-al:2018:8593854,
  author = {P. Egger and P. V. K. Borges and G. Catt and A. Pfrunder and R. Siegwart and R. Dube},
  journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization},
  pages = {3430--7},
  doi = {10.1109/IROS.2018.8593854},
  note = {local views;sliding window fashion;matching current;old features;map representation;local maps;off-road environments;single localization failure;distinctive features;coined PoseMap;dynamic environments;robotic systems;long-term localization;multienvironment 3D LiDAR localization;frequency 8.0 Hz;time 18.0 month;},
  address = {Piscataway, NJ, USA},
  year = {2018},
  abstract = {Reliable long-term localization is key for robotic systems in dynamic environments. In this paper, we propose a novel approach for long-term localization using 3D LiDARs, coined PoseMap. In essence, we extract distinctive features from range measurements and bundle these into local views along with observation poses. The sensor's trajectory is then estimated in a sliding window fashion by matching current and old features and minimizing the distances in-between. The map representation facilitates finding a suitable set of old features, by selecting the closest local map(s) for matching. Similarly to a visibility analysis, this procedure provides a suitable set of features for localization but at a fraction of the computational cost. PoseMap also allows for updates and extensions of the map at any time by replacing and adding local maps when necessary. We evaluate our approach using two platforms both equipped with a 3D LiDAR and an IMU, demonstrating localization at 8 Hz and robustness to changes in the environment such as moving vehicles and changing vegetation. PoseMap was implemented on an autonomous vehicle allowing it to drive autonomously over a period of 18 months through a mix of industrial and unstructured off-road environments, covering more than 100 kms without a single localization failure.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  keywords = {feature extraction;mobile robots;optical radar;path planning;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2018.8593854},
}

@inproceedings{ganti-waslander:2019:00024,
  author = {P. Ganti and S. L. Waslander},
  journal = {2019 16th Conference on Computer and Robot Vision (CRV). Proceedings},
  title = {Network Uncertainty Informed Semantic Feature Selection for Visual SLAM},
  pages = {121--8},
  doi = {10.1109/CRV.2019.00024},
  note = {visual SLAM;semantic segmentation;neural network uncertainty;feature selection pipeline;joint entropy;classification entropy;Bayesian neural network;selection strategy;sparse map;ORB_SLAM2;map size;visual simultaneous localization and mapping;feature selection;information-theoretic feature selection method;Shannon entropy;semantically informed visual odometry and mapping;network uncertainty informed semantic feature selection;},
  address = {Piscataway, NJ, USA},
  year = {2019},
  abstract = {In order to facilitate long-term localization using a visual simultaneous localization and mapping (SLAM) algorithm, careful feature selection can help ensure that reference points persist over long durations and the runtime and storage complexity of the algorithm remain consistent. We present SIVO (Semantically Informed Visual Odometry and Mapping), a novel information-theoretic feature selection method for visual SLAM which incorporates semantic segmentation and neural network uncertainty into the feature selection pipeline. Our algorithm selects points which provide the highest reduction in Shannon entropy between the entropy of the current state and the joint entropy of the state, given the addition of the new feature with the classification entropy of the feature from a Bayesian neural network. Each selected feature significantly reduces the uncertainty of the vehicle state and has been detected to be a static object (building, traffic sign, etc.) repeatedly with a high confidence. This selection strategy generates a sparse map which can facilitate long-term localization. The KITTI odometry dataset is used to evaluate our method, and we also compare our results against ORB_SLAM2. Overall, SIVO performs comparably to the baseline method while reducing the map size by almost 70%.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  keywords = {Bayes methods;data visualisation;feature extraction;image segmentation;mobile robots;neural nets;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/CRV.2019.00024},
}

@inproceedings{gao-zhang:2020:9196906,
  author = {P. Gao and H. Zhang},
  journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Long-term Place Recognition through Worst-case Graph Matching to Integrate Landmark Appearances and Spatial Relationships},
  pages = {1070--6},
  doi = {10.1109/ICRA40945.2020.9196906},
  note = {robotics applications;simultaneously localization and mapping;spatial relationship similarities;spatial cues;visual cues;old landmarks;long-term environment changes;landmark information;integrate landmark appearances;worst-case graph matching;place recognition performance;long-term place recognition;worst appearance similarity;similar appearances;worst-case scenario;graph matching problem;visual appearances;angular spatial relationships;graph representation;},
  address = {Piscataway, NJ, USA},
  year = {2020},
  abstract = {Place recognition is an important component for simultaneously localization and mapping in a variety of robotics applications. Recently, several approaches using landmark information to represent a place showed promising performance to address long-term environment changes. However, previous approaches do not explicitly consider changes of the landmarks, i,e., old landmarks may disappear and new ones often appear over time. In addition, representations used in these approaches to represent landmarks are limited, based upon visual or spatial cues only. In this paper, we introduce a novel worst-case graph matching approach that integrates spatial relationships of landmarks with their appearances for long-term place recognition. Our method designs a graph representation to encode distance and angular spatial relationships as well as visual appearances of landmarks in order to represent a place. Then, we formulate place recognition as a graph matching problem under the worst-case scenario. Our approach matches places by computing the similarities of distance and angular spatial relationships of the landmarks that have the least similar appearances (i.e., worst-case). If the worst appearance similarity of landmarks is small, two places are identified to be not the same, even though their graph representations have high spatial relationship similarities. We evaluate our approach over two public benchmark datasets for long-term place recognition, including St. Lucia and CMU-VL. The experimental results have validated that our approach obtains the state-of-the-art place recognition performance, with a changing number of landmarks.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  keywords = {graph theory;image matching;mobile robots;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA40945.2020.9196906},
}

@article{muhlfellner-et-al:2016:21595,
  author = {P. Muhlfellner and M. Burki and M. Bosse and W. Derendarz and R. Philippsen and P. Furgale},
  journal = {Journal of Field Robotics},
  title = {Summary Maps for Lifelong Visual Localization},
  volume = {33},
  number = {5},
  pages = {561--90},
  doi = {10.1002/rob.21595},
  note = {summary maps;lifelong visual localization;robot vision;handle environments;weather conditions;lighting conditions;changing environments;online localization;offline map building;map summary;multiple visually varied datasets;natural environments;visual information;},
  address = {USA},
  year = {2016},
  abstract = {Robots that use vision for localization need to handle environments that are subject to seasonal and structural change, and operate under changing lighting and weather conditions. We present a framework for lifelong localization and mapping designed to provide robust and metrically accurate online localization in these kinds of changing environments. Our system iterates between offline map building, map summary, and online localization. The offline mapping fuses data from multiple visually varied datasets, thus dealing with changing environments by incorporating new information. Before passing these data to the online localization system, the map is summarized, selecting only the landmarks that are deemed useful for localization. This <i>Summary Map</i> enables online localization that is accurate and robust to the variation of visual information in natural environments while still being computationally efficient. We present a number of summary policies for selecting useful features for localization from the multisession map, and we explore the tradeoff between localization performance and computational complexity. The system is evaluated on 77 recordings, with a total length of 30 kilometers, collected outdoors over 16 months. These datasets cover all seasons, various times of day, and changing weather such as sunshine, rain, fog, and snow. We show that it is possible to build consistent maps that span data collected over an entire year, and cover day-to-night transitions. Simple statistics computed on landmark observations are enough to produce a Summary Map that enables robust and accurate localization over a wide range of seasonal, lighting, and weather conditions.},
  copyright = {Copyright 2016, The Institution of Engineering and Technology},
  issn = {1556-4959},
  keywords = {robot vision;robust control;},
  language = {English},
  url = {http://dx.doi.org/10.1002/rob.21595},
}

@article{neubert-et-al:2015:005,
  author = {P. Neubert and N. Sunderhauf and P. Protzel},
  journal = {Robotics and Autonomous Systems},
  title = {Superpixel-based appearance change prediction for long-term navigation across seasons},
  volume = {69},
  pages = {15--27},
  doi = {10.1016/j.robot.2014.08.005},
  note = {superpixel-based appearance change prediction;long-term navigation;robotic systems;weather conditions;place recognition;visual appearance;learned knowledge;SP-ACP;SeqSLAM;BRIEF-Gist;Nordland dataset;},
  address = {Netherlands},
  year = {2015},
  abstract = {Changing environments pose a serious problem to current robotic systems aiming at long term operation under varying seasons or local weather conditions. This paper is built on our previous work where we propose to learn to <i>predict</i> the changes in an environment. Our key insight is that the occurring scene changes are in part systematic, repeatable and therefore predictable. The goal of our work is to support existing approaches to place recognition by learning how the visual appearance of an environment changes over time and by using this learned knowledge to predict its appearance under different environmental conditions. We describe the general idea of appearance change prediction (ACP) and investigate properties of our novel implementation based on vocabularies of superpixels (SP-ACP). Our previous work showed that the proposed approach significantly improves the performance of SeqSLAM and BRIEF-Gist for place recognition on a subset of the Nordland dataset under extremely different environmental conditions in summer and winter. This paper deepens the understanding of the proposed SP-ACP system and evaluates the influence of its parameters. We present the results of a large-scale experiment on the complete 10 h Nordland dataset and appearance change predictions between different combinations of seasons. [All rights reserved Elsevier].},
  copyright = {Copyright 2015, The Institution of Engineering and Technology},
  issn = {0921-8890},
  keywords = {image recognition;knowledge based systems;mobile robots;navigation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1016/j.robot.2014.08.005},
}

@article{ozog-et-al:2016:21582,
  author = {P. Ozog and N. Carlevaris-Bianco and A. Kim and R. M. Eustice},
  journal = {Journal of Field Robotics},
  title = {Long-term mapping techniques for ship hull inspection and surveillance using an autonomous underwater vehicle},
  volume = {33},
  number = {3},
  pages = {265--89},
  doi = {10.1002/rob.21582},
  note = {long term mapping techniques;ship hull inspection;ship hull surveillance;autonomous underwater vehicle;multiple session hull inspection;simultaneous localization and mapping;SLAM;acoustic beacons;consumer grade computing hardware;particle filter;locally planar representation;visual descriptor matching algorithm;generic linear constraints;graph sparsification tool;computational complexity;SLAM system;},
  address = {USA},
  year = {2016},
  abstract = {This paper reports on a system for an autonomous underwater vehicle to perform <i>in situ</i>, multiple session hull inspection using long-term simultaneous localization and mapping (SLAM). Our method assumes very little <i>a priori</i> knowledge, and it does not require the aid of acoustic beacons for navigation, which is a typical mode of navigation in this type of application. Our system combines recent techniques in underwater saliency-informed visual SLAM and a method for representing the ship hull surface as a collection of many locally planar surface features. This methodology produces accurate maps that can be constructed in real-time on consumer-grade computing hardware. A single-session SLAM result is initially used as a prior map for later sessions, where the robot automatically merges the multiple surveys into a common hull-relative reference frame. To perform the relocalization step, we use a particle filter that leverages the locally planar representation of the ship hull surface, and a fast visual descriptor matching algorithm. Finally, we apply the recently developed graph sparsification tool, generic linear constraints, as a way to manage the computational complexity of the SLAM system as the robot accumulates information across multiple sessions. We show results for 20 SLAM sessions for two large vessels over the course of days, months, and even up to three years, with a total path length of approximately 10.2 km.},
  copyright = {Copyright 2016, The Institution of Engineering and Technology},
  issn = {1556-4959},
  keywords = {autonomous underwater vehicles;graph theory;mobile robots;particle filtering (numerical methods);ships;SLAM (robots);telerobotics;video surveillance;},
  language = {English},
  url = {http://dx.doi.org/10.1002/rob.21582},
}

@inproceedings{schmuck-chli:2019:00071,
  author = {P. Schmuck and M. Chli},
  journal = {2019 International Conference on 3D Vision (3DV)},
  title = {On the Redundancy Detection in Keyframe-Based SLAM},
  pages = {594--603},
  doi = {10.1109/3DV.2019.00071},
  note = {virtual reality;mobile phones;head-mounted displays;optimization algorithms;scene measurements;redundancy detection;automating robot navigation;collaborative SLAM systems;information-theoretic;simultaneous localization and mapping;},
  address = {Piscataway, NJ, USA},
  year = {2019},
  abstract = {Egomotion and scene estimation is a key component in automating robot navigation, as well as in virtual reality applications for mobile phones or head-mounted displays. It is well known, however, that with long exploratory trajectories and multi-session mapping for long-term autonomy or collaborative applications, the maintenance of the ever-increasing size of these maps quickly becomes a bottleneck. With the explosion of data resulting in increasing runtime of the optimization algorithms ensuring the accuracy of the Simultaneous Localization And Mapping (SLAM) estimates, the large quantity of collected experiences is imposing hard limits on the scalability of such techniques. Considering the keyframe-based paradigm of SLAM techniques, this paper investigates the redundancy inherent in SLAM maps, by quantifying the information of different experiences of the scene as encoded in keyframes. Here we propose and evaluate different information-theoretic and heuristic metrics to remove dispensable scene measurements with minimal impact on the accuracy of the SLAM estimates. Evaluating the proposed metrics in two state-of-the-art centralized collaborative SLAM systems, we provide our key insights into how to identify redundancy in keyframe-based SLAM.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  keywords = {graph theory;information theory;mobile robots;optimisation;robot vision;SLAM (robots);virtual reality;},
  language = {English},
  url = {http://dx.doi.org/10.1109/3DV.2019.00071},
}

@inproceedings{yin-et-al:2018:8593562,
  author = {P. Yin and L. Xu and Z. Liu and L. Li and H. Salman and Y. He and W. Xu and H. Wang and H. Choset},
  journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Stabilize an Unsupervised Feature Learning for LiDAR-based Place Recognition},
  pages = {1162--7},
  doi = {10.1109/IROS.2018.8593562},
  note = {Generative Adversarial Network;adversarial feature;place recognition;global geometry map;Conditional Entropy Reduction module;unsupervised place feature;local 2D maps;dynamic octree mapping module;core modules;LiDAR inputs;end-to-end feature;geometry matching;traditional methods;LiDAR-based place recognition;unsupervised feature learning;feature size;place recognition task;North Campus Long-Term LiDAR dataset;feature learning process;place feature learning;},
  address = {Piscataway, NJ, USA},
  year = {2018},
  abstract = {Place recognition is one of the major challenges for the LiDAR-based effective localization and mapping task. Traditional methods are usually relying on geometry matching to achieve place recognition, where a global geometry map need to be restored. In this paper, we accomplish the place recognition task based on an end-to-end feature learning framework with the LiDAR inputs. This method consists of two core modules, a dynamic octree mapping module that generates local 2D maps with the consideration of the robot's motion; and an unsupervised place feature learning module which is an improved adversarial feature learning network with additional assistance for the long-term place recognition requirement. More specially, in place feature learning, we present an additional Generative Adversarial Network with a designed Conditional Entropy Reduction module to stabilize the feature learning process in an unsupervised manner. We evaluate the proposed method on the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental results show that the proposed method outperforms state-of-the-art in place recognition tasks under long-term applications. What's more, the feature size and inference efficiency in the proposed method are applicable in real-time performance on practical robotic platforms.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  keywords = {entropy;feature extraction;geometry;image matching;image recognition;learning (artificial intelligence);mobile robots;octrees;optical radar;robot vision;unsupervised learning;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2018.8593562},
}

@article{meng-et-al:2021:3062647,
  author = {Q. Meng and H. Guo and X. Zhao and D. Cao and H. Chen},
  journal = {IEEE/ASME Transactions on Mechatronics},
  title = {Loop-Closure Detection With a Multiresolution Point Cloud Histogram Mode in Lidar Odometry and Mapping for Intelligent Vehicles},
  volume = {26},
  number = {3},
  pages = {1307--17},
  doi = {10.1109/TMECH.2021.3062647},
  note = {control tasks;lidar simultaneous localization;integrates motion compensation;ground information removal functions;real-time environment map;loop-closure detection method;multiresolution point cloud histogram mode;map information;urban conditions;driving loops;lidar SLAM system;accurate positioning information;effective positioning information;intelligent vehicles;loop-closure detection algorithm;lidar odometry;precise positioning;basic condition;decision making;},
  address = {USA},
  year = {2021},
  abstract = {Precise positioning is the basic condition for intelligent vehicles to complete perception, decision making and control tasks. In response to this challenge, in this article, lidar simultaneous localization and mapping (SLAM) is taken as the research object, and a SLAM system is designed that integrates motion compensation and ground information removal functions, and can construct a real-time environment map and determine its own position on the map while the vehicle is driving. A loop-closure detection method with a multiresolution point cloud histogram mode is proposed, which can effectively detect whether the vehicle passes through the same position and perform optimization to obtain globally consistent pose and map information in the urban conditions with more driving loops. We conduct experiments on the well-known KITTI dataset and compare the results with those of state-of-the-art systems. The experiments confirm that the lidar SLAM system designed in this article can provide accurate and effective positioning information for intelligent vehicles. The proposed loop-closure detection algorithm has an excellent real-time performance and accuracy, which can guarantee the long-term driving operation of these vehicles.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  issn = {1083-4435},
  keywords = {distance measurement;mobile robots;motion compensation;object detection;optical radar;position control;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/TMECH.2021.3062647},
}

@article{arroyo-et-al:2018:7,
  author = {R. Arroyo and P. F. Alcantarilla and L. M. Bergasa and E. Romera},
  journal = {Autonomous Robots},
  title = {Are you ABLE to perform a life-long visual topological localization?},
  volume = {42},
  number = {3},
  pages = {665--85},
  doi = {10.1007/s10514-017-9664-7},
  note = {life-long visual topological localization;dynamic elements;long-term visual place recognition;robotics community;mobile autonomous robots;robust life-long localization;cameras;ABLE;image sequence;image matching;},
  address = {Germany},
  year = {2018},
  abstract = {Visual topological localization is a process typically required by varied mobile autonomous robots, but it is a complex task if long operating periods are considered. This is because of the appearance variations suffered in a place: dynamic elements, illumination or weather. Due to these problems, long-term visual place recognition across seasons has become a challenge for the robotics community. For this reason, we propose an innovative method for a robust and efficient life-long localization using cameras. In this paper, we describe our approach (ABLE), which includes three different versions depending on the type of images: monocular, stereo and panoramic. This distinction makes our proposal more adaptable and effective, because it allows to exploit the extra information that can be provided by each type of camera. Besides, we contribute a novel methodology for identifying places, which is based on a fast matching of global binary descriptors extracted from sequences of images. The presented results demonstrate the benefits of using ABLE, which is compared to the most representative state-of-the-art algorithms in long-term conditions.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  issn = {0929-5593},
  keywords = {image matching;image sequences;mobile robots;robot dynamics;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1007/s10514-017-9664-7},
}

@article{mur-artal-et-al:2015:2463671,
  author = {R. Mur-Artal and J. M. M. Montiel and J. D. Tardos},
  journal = {IEEE Transactions on Robotics},
  title = {ORB-SLAM: A Versatile and Accurate Monocular SLAM System},
  volume = {31},
  number = {5},
  pages = {1147--63},
  doi = {10.1109/TRO.2015.2463671},
  note = {ORB-SLAM system;feature-based monocular simultaneous localization and mapping system;survival of the fittest strategy;},
  address = {USA},
  year = {2015},
  abstract = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
  copyright = {Copyright 2015, The Institution of Engineering and Technology},
  issn = {1552-3098},
  keywords = {SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/TRO.2015.2463671},
}

@article{paul-newman:2013:0278364913509859,
  author = {R. Paul and P. Newman},
  journal = {International Journal of Robotics Research},
  title = {Self-help: Seeking out perplexing images for ever improving topological mapping},
  volume = {32},
  number = {14},
  pages = {1742--66},
  doi = {10.1177/0278364913509859},
  note = {perplexing images;topological mapping;robot;navigation performance;targeted data retrieval;introspection;life-long learning;place recognition;topic-based probabilistic representation;background images;mobile robot;outdoor workspaces;data collection;},
  address = {USA},
  year = {2013},
  abstract = {In this work, we present a novel approach that allows a robot to improve its own navigation performance through introspection and then targeted data retrieval. It is a step in the direction of life-long learning and adaptation and is motivated by the desire to build robots that have plastic competencies which are not baked in. They should react to and benefitfrom use. We consider a particular instantiation of this problem in the context of place recognition. Based on a topic-based probabilistic representation for images, we use a measure of perplexity to evaluate how well a working set of background images explain the robot's online view of the world. Offline, the robot then searches an external resource to seek out additional background images that bolster its ability to localize in its environment when used next. In this way the robot adapts and improves performance through use. We demonstrate this approach using data collected from a mobile robot operating in outdoor workspaces.},
  copyright = {Copyright 2014, The Institution of Engineering and Technology},
  issn = {0278-3649},
  keywords = {image recognition;image representation;mobile robots;navigation;path planning;probability;robot vision;},
  language = {English},
  url = {http://dx.doi.org/10.1177/0278364913509859},
}

@article{griffith-pradalier:2017:21664,
  author = {S. Griffith and C. Pradalier},
  journal = {Journal of Field Robotics},
  title = {Survey registration for long-term natural environment monitoring},
  volume = {34},
  number = {1},
  pages = {188--208},
  doi = {10.1002/rob.21664},
  note = {long term natural environment monitoring;survey registration framework;recurrent inspection;image level using visual simultaneous localization and mapping;SLAM;image registration;SIFT flow;rapid manual inspection;data association;lakeshore monitoring;autonomous surface vessel;},
  address = {USA},
  year = {2017},
  abstract = {This paper presents a survey registration framework to assist in the recurrent inspection of a natural environment. Our framework coarsely aligns surveys at the image-level using visual simultaneous localization and mapping (SLAM), and it registers images at the pixel-level using SIFT Flow, which enables rapid manual inspection. The variation in appearance of natural environments makes data association a primary challenge of this work. We discuss this and other challenges, including 1) alternative approaches for coarsely aligning surveys of a natural environment, 2) how to select which images to compare between two surveys, and 3) strategies to boost image registration accuracy. We evaluate each stage of our approach, emphasizing alignment accuracy and stability with respect to large seasonal variations. Our domain is lakeshore monitoring, in which an autonomous surface vessel surveyed a 1-km lakeshore 33 times in 14 months. Our results show that our framework precisely aligns a significant number of images between surveys captured up to roughly three months apart, often across marked variation in appearance. Using these results, a human was able to spot several changes between surveys that would have otherwise gone unnoticed.},
  copyright = {Copyright 2017, The Institution of Engineering and Technology},
  issn = {1556-4959},
  keywords = {environmental monitoring (geophysics);image registration;SLAM (robots);transforms;},
  language = {English},
  url = {http://dx.doi.org/10.1002/rob.21664},
}

@inproceedings{hochdorfer-et-al:2009:5339626,
  author = {S. Hochdorfer and M. Lutz and C. Schlegel},
  journal = {2009 IEEE International Conference on Technologies for Practical Robot Applications. TePRA 2009},
  title = {Lifelong localization of a mobile service-robot in everyday indoor environments using omnidirectional vision},
  pages = {161--6},
  doi = {10.1109/TEPRA.2009.5339626},
  note = {lifelong localization;mobile service-robot;indoor environments;omnidirectional vision;simultaneous localization and mapping;processing power;memory resources;},
  address = {Piscataway, NJ, USA},
  year = {2009},
  abstract = {SLAM (Simultaneous Localization and Mapping) mechanisms are a key component towards advanced service robotics applications. Currently, a major hurdle on the way to lifelong localization is the handling of the ever growing amount of landmarks over time. Therefore, the required resources in terms of memory and processing power are also growing over time.},
  copyright = {Copyright 2010, The Institution of Engineering and Technology},
  keywords = {mobile robots;pattern clustering;service robots;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/TEPRA.2009.5339626},
}

@inproceedings{luthardt-et-al:2018:8569323,
  author = {S. Luthardt and V. Willert and J. Adamy},
  journal = {2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
  title = {LLama-SLAM: Learning High-quality Visual Landmarks for Long-term Mapping and Localization},
  pages = {2645--52},
  doi = {10.1109/ITSC.2018.8569323},
  note = {LLama-SLAM;autonomous driving;stereo image streams;GNSS measurement;statistics;high-quality visual landmark learning;long-term mapping and localization;GNSS;visual long-term landmarks;r advanced driver assistance systems;},
  address = {Piscataway, NJ, USA},
  year = {2018},
  abstract = {The precise localization of vehicles is an important requirement for autonomous driving or advanced driver assistance systems. Using common GNSS the ego position can be measured but not with the reliability and precision necessary. An alternative approach to achieve precise localization is the usage of visual landmarks observed by a camera mounted in the vehicle. However, this raises the necessity of reliable visual landmarks that are easily recognizable and persistent. We propose a novel SLAM algorithm that focuses on learning and mapping such visual long-term landmarks (LLamas). The algorithm therefore processes stereo image streams from several recording sessions in the same spatial area. The key part within LLama-SLAM is the assessment of the landmarks with quality values that are inferred as viewpoint dependent probabilities from observation statistics. By adding solely landmarks of high quality to the final LLama Map, it can be kept compact while still allowing reliable localization. Due to the long-term evaluation of the GNSS measurement during the sessions, the landmarks can be positioned precisely in a global referenced coordinate system. For a first assessment of the algorithm's capabilities, we present some experimental results from the mapping process combining three sessions recorded over two months on the same route.},
  copyright = {Copyright 2019, The Institution of Engineering and Technology},
  keywords = {control engineering computing;driver information systems;mobile robots;probability;road traffic control;robot vision;SLAM (robots);statistics;stereo image processing;},
  language = {English},
  url = {http://dx.doi.org/10.1109/ITSC.2018.8569323},
}

@article{nuske-et-al:Sept.2009:20306,
  author = {S. Nuske and J. Roberts and G. Wyeth},
  journal = {Journal of Field Robotics},
  title = {Robust outdoor visual localization using a three-dimensional-edge map},
  volume = {26},
  number = {9},
  pages = {728--56},
  doi = {10.1002/rob.20306},
  note = {robust outdoor visual localization;autonomous vehicles;industrial applications;camera images;robust operation;3D edge map localization systems;single pose hypothesis;multihypothesis particle filter;intelligent exposure control algorithm;},
  address = {USA},
  year = {Sept.2009},
  abstract = {Visual localization systems that are practical for autonomous vehicles in outdoor industrial applications must perform reliably in a wide range of conditions. Changing outdoor conditions cause difficulty by drastically altering the information available in the camera images. To confront the problem, we have developed a visual localization system that uses a surveyed three-dimensional (3D)-edge map of permanent structures in the environment. The map has the invariant properties necessary to achieve long-term robust operation. Previous 3D-edge map localization systems usually maintain a single pose hypothesis, making it difficult to initialize without an accurate prior pose estimate and also making them susceptible to misalignment with unmapped edges detected in the camera image. A multihypothesis particle filter is employed here to perform the initialization procedure with significant uncertainty in the vehicle's initial pose. A novel observation function for the particle filter is developed and evaluated against two existing functions. The new function is shown to further improve the abilities of the particle filter to converge given a very coarse estimate of the vehicle's initial pose. An intelligent exposure control algorithm is also developed that improves the quality of the pertinent information in the image. Results gathered over an entire sunny day and also during rainy weather illustrate that the localization system can operate in a wide range of outdoor conditions. The conclusion is that an invariant map, a robust multihypothesis localization algorithm, and an intelligent exposure control algorithm all combine to enable reliable visual localization through challenging outdoor conditions. &copy; 2009 Wiley Periodicals, Inc.},
  copyright = {Copyright 2010, The Institution of Engineering and Technology},
  issn = {1556-4959},
  keywords = {artificial intelligence;cameras;mobile robots;particle filtering (numerical methods);robust control;},
  language = {English},
  url = {http://dx.doi.org/10.1002/rob.20306},
}

@inproceedings{siva-zhang:2018:8461042,
  author = {S. Siva and H. Zhang},
  journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Omnidirectional Multisensory Perception Fusion for Long-Term Place Recognition},
  pages = {9pp.--},
  doi = {10.1109/ICRA.2018.8461042},
  note = {omnidirectional multisensory perception fusion;long-term place recognition;long-term autonomy;omnidirectional sensors;omnidirectional observation;multidirectional place recognition;omnidirectional multisensory data;appearance variations;Simultaneous Localization and Mapping;},
  address = {Piscataway, NJ, USA},
  year = {2018},
  abstract = {Over the recent years, long-term place recognition has attracted an increasing attention to detect loops for largescale Simultaneous Localization and Mapping (SLAM) in loopy environments during long-term autonomy. Almost all existing methods are designed to work with traditional cameras with a limited field of view. Recent advances in omnidirectional sensors offer a robot an opportunity to perceive the entire surrounding environment. However, no work has existed thus far to research how omnidirectional sensors can help long-term place recognition, especially when multiple types of omnidirectional sensory data are available. In this paper, we propose a novel approach to integrate observations obtained from multiple sensors from different viewing angles in the omnidirectional observation in order to perform multi-directional place recognition in longterm autonomy. Our approach also answers two new questions when omnidirectional multisensory data is available for place recognition, including whether it is possible to recognize a place with long-term appearance variations when robots approach it from various directions, and whether observations from various viewing angles are the same informative. To evaluate our approach and hypothesis, we have collected the first large-scale dataset that consists of omnidirectional multisensory (intensity and depth) data collected in urban and suburban environments across a year. Experimental results have shown that our approach is able to achieve multi-directional long-term place recognition, and identifies the most discriminative viewing angles from the omnidirectional observation.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  keywords = {mobile robots;object recognition;robot vision;sensor fusion;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA.2018.8461042},
}

@inproceedings{siva-et-al:2020:9340992,
  author = {S. Siva and Z. Nahman and H. Zhang},
  journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Voxel-based Representation Learning for Place Recognition Based on 3D Point Clouds},
  pages = {8351--7},
  doi = {10.1109/IROS45743.2020.9340992},
  note = {VBRL;place recognition;3D point cloud data;voxel-based representation learning;3D point clouds;voxel representations;3D point cloud input;feature learning;},
  address = {Piscataway, NJ, USA},
  year = {2020},
  abstract = {Place recognition is a critical component towards addressing the key problem of Simultaneous Localization and Mapping (SLAM). Most existing methods use visual images; whereas, place recognition using 3D point clouds, especially based on the voxel representations, has not been well addressed yet. In this paper, we introduce the novel approach of voxel-based representation learning (VBRL) that uses 3D point clouds to recognize places with long-term environment variations. VBRL splits a 3D point cloud input into voxels and uses multi-modal features extracted from these voxels to perform place recognition. Additionally, VBRL uses structured sparsity-inducing norms to learn representative voxels and feature modalities that are important to match places under long-term changes. Both place recognition, and voxel and feature learning are integrated into a unified regularized optimization formulation. As the sparsity-inducing norms are non-smooth, it is hard to solve the formulated optimization problem. Thus, we design a new iterative optimization algorithm, which has a theoretical convergence guarantee. Experimental results have shown that VBRL performs place recognition well using 3D point cloud data and is capable of learning the importance of voxels and feature modalities.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  keywords = {feature extraction;image representation;image sensors;iterative methods;learning (artificial intelligence);mobile robots;optimisation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS45743.2020.9340992},
}

@article{yang-et-al:2020:s20082432,
  author = {S. Yang and G. Fan and L. Bai and C. Zhao and D. Li},
  journal = {Sensors},
  title = {SGC-VSLAM: A Semantic and Geometric Constraints VSLAM for Dynamic Indoor Environments},
  volume = {20},
  number = {8},
  pages = {2432(20pp.)--},
  doi = {10.3390/s20082432},
  note = {SGC-VSLAM;dynamic indoor environments;autonomous mobile robots;state-of-the-art VSLAM;strong scene rigidity assumption;independent dynamic objects;semantic constraints VSLAM;geometric constraints VSLAM;dynamic detection;static point cloud map construction modules;dynamic feature detection method;semantic bounding box;truly dynamic features;ORB-SLAM2 system;high-dynamic scenarios;},
  address = {Switzerland},
  year = {2020},
  abstract = {As one of the core technologies for autonomous mobile robots, Visual Simultaneous Localization and Mapping (VSLAM) has been widely researched in recent years. However, most state-of-the-art VSLAM adopts a strong scene rigidity assumption for analytical convenience, which limits the utility of these algorithms for real-world environments with independent dynamic objects. Hence, this paper presents a semantic and geometric constraints VSLAM (SGC-VSLAM), which is built on the RGB-D mode of ORB-SLAM2 with the addition of dynamic detection and static point cloud map construction modules. In detail, a novel improved quadtree-based method was adopted for SGC-VSLAM to enhance the performance of the feature extractor in ORB-SLAM (Oriented FAST and Rotated BRIEF-SLAM). Moreover, a new dynamic feature detection method called semantic and geometric constraints was proposed, which provided a robust and fast way to filter dynamic features. The semantic bounding box generated by YOLO v3 (You Only Look Once, v3) was used to calculate a more accurate fundamental matrix between adjacent frames, which was then used to filter all of the truly dynamic features. Finally, a static point cloud was estimated by using a new drawing key frame selection strategy. Experiments on the public TUM RGB-D (Red-Green-Blue Depth) dataset were conducted to evaluate the proposed approach. This evaluation revealed that the proposed SGC-VSLAM can effectively improve the positioning accuracy of the ORB-SLAM2 system in high-dynamic scenarios and was also able to build a map with the static parts of the real environment, which has long-term application value for autonomous mobile robots.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  issn = {1424-8220},
  keywords = {feature extraction;image colour analysis;mobile robots;quadtrees;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.3390/s20082432},
}

@inproceedings{zhu-et-al:2021:9561584,
  author = {S. Zhu and X. Zhang and S. Guo and J. Li and H. Liu},
  journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Lifelong Localization in Semi-Dynamic Environment},
  pages = {14389--14395},
  doi = {10.1109/ICRA48506.2021.9561584},
  note = {lifelong localization;semidynamic environment;nonstatic environments;static objects;highly dynamic objects;localization failure;semidynamic scenarios;lower dynamics;semantic mapping;semidynamic objects;mainstream object detection algorithms;localization algorithms;object detection algorithm;semantic map;localization method;nonstatic objects;invalid observation;localization fluctuation;nonstatic scenarios;},
  address = {Piscataway, NJ, USA},
  year = {2021},
  abstract = {Mapping and localization in non-static environments are fundamental problems in robotics. Most of previous methods mainly focus on static and highly dynamic objects in the environment, which may suffer from localization failure in semi-dynamic scenarios without considering objects with lower dynamics, such as parked cars and stopped pedestrians. In this paper, we introduce semantic mapping and lifelong localization approaches to recognize semi-dynamic objects in non-static environments. We also propose a generic framework that can integrate mainstream object detection algorithms with mapping and localization algorithms. The mapping method combines an object detection algorithm and a SLAM algorithm to detect semi-dynamic objects and constructs a semantic map that only contains semi-dynamic objects in the environment. During navigation, the localization method can classify observation corresponding to static and non-static objects respectively and evaluate whether those semi-dynamic objects have moved, to reduce the weight of invalid observation and localization fluctuation. Real-world experiments show that the proposed method can improve the localization accuracy of mobile robots in non-static scenarios.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  keywords = {mobile robots;object detection;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA48506.2021.9561584},
}

@article{an-et-al:2016:0,
  author = {S.-Y. An and L.-K. Lee and S.-Y. Oh},
  journal = {Autonomous Robots},
  title = {Ceiling vision-based active SLAM framework for dynamic and wide-open environments},
  volume = {40},
  number = {2},
  pages = {291--324},
  doi = {10.1007/s10514-015-9453-0},
  note = {long-term navigation;DEL;dynamic edge link;image space;visual node descriptor;model-free landmark;service robot;simultaneous localization and mapping;indoor environment;wide-open environments;dynamic environments;ceiling vision-based active SLAM framework;},
  address = {Germany},
  year = {2016},
  abstract = {A typical indoor environment can be divided into three categories; office (or room), hallway, and wide-open space such as lobby and hall. There have been numerous approaches for solving simultaneous localization and mapping (SLAM) problem in office (or room) and hallway. However, direct application of the existing approaches to wide-open space may be failed, because it has some distinguished features compared to other indoor places. To solve this problem, this paper proposes a new ceiling vision-based active SLAM framework, with an emphasis on practical deployment of service robot for commercial use in dynamically changing and wide-open environments by adopting the ceiling vision. First, for defining ceiling feature which can be extracted regardless of complexity of ceiling pattern we introduce a model-free landmark, i.e., visual node descriptor, which consists of edge points and their orientations in image space. Second, a recursive `explore and exploit' is proposed for autonomous mapping. It is recursively performed by spreading out mapped area gradually while the robot is actively localized in the map. It can improve map accuracy due to frequent small loop closing. Third, a dynamic edge link (DEL) is proposed to cope with environmental changes in the map. Owing to DEL, we do not need to filter out corrupted sensor data and to distinguish moving object from static one. Also, a self-repairing map mechanism is introduced to deal with unexpected installation or removal of inner structures. We therefore achieve long-term navigation. Several simulations and real experiments in various places show that the proposed active SLAM framework could build a topologically consistent map, and demonstrated that it can be applied well to real environments such as wide-open space in a city hall and railway station.},
  copyright = {Copyright 2016, The Institution of Engineering and Technology},
  issn = {0929-5593},
  keywords = {mobile robots;navigation;robot vision;service robots;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1007/s10514-015-9453-0},
}

@article{krajnik-et-al:2017:2665664,
  author = {T. Krajnik and J. P. Fentanes and J. M. Santos and T. Duckett},
  journal = {IEEE Transactions on Robotics},
  title = {FreMEn: Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in Changing Environments},
  volume = {33},
  number = {4},
  pages = {964--77},
  doi = {10.1109/TRO.2017.2665664},
  note = {FreMEn;frequency map enhancement;long-term mobile robot autonomy;long-term mobile robot mapping;dynamic indoor environments;environmental dynamics;uncertainty;estimated state variables;frequency spectra;spectral model;low memory requirements;time domain;},
  address = {USA},
  year = {2017},
  abstract = {We present a new approach to long-term mobile robot mapping in dynamic indoor environments. Unlike traditional world models that are tailored to represent static scenes, our approach explicitly models environmental dynamics. We assume that some of the hidden processes that influence the dynamic environment states are periodic and model the uncertainty of the estimated state variables by their frequency spectra. The spectral model can represent arbitrary timescales of environment dynamics with low memory requirements. Transformation of the spectral model to the time domain allows for the prediction of the future environment states, which improves the robot's long-term performance in changing environments. Experiments performed over time periods of months to years demonstrate that the approach can efficiently represent large numbers of observations and reliably predict future environment states. The experiments indicate that the model's predictive capabilities improve mobile robot localization and navigation in changing environments.},
  copyright = {Copyright 2017, The Institution of Engineering and Technology},
  issn = {1552-3098},
  keywords = {mobile robots;state estimation;uncertain systems;},
  language = {English},
  url = {http://dx.doi.org/10.1109/TRO.2017.2665664},
}

@inproceedings{naseer-et-al:2015:7324181,
  author = {T. Naseer and B. Suger and M. Ruhnke and W. Burgard},
  journal = {2015 European Conference on Mobile Robots (ECMR). Proceedings},
  title = {Vision-based Markov localization across large perceptual changes},
  pages = {6pp.--},
  doi = {10.1109/ECMR.2015.7324181},
  note = {vision-based Markov localization;mobile robot autonomous operation;substantial perceptual changes;low frequency camera;image sequence;discrete Bayes filter;sensor model;probability distribution;},
  address = {Piscataway, NJ, USA},
  year = {2015},
  abstract = {Recently, there has been significant progress towards lifelong, autonomous operation of mobile robots, especially in the field of localization and mapping. One important challenge in this context is visual localization under substantial perceptual changes, for example, coming from different seasons. In this paper, we present an approach to localize a mobile robot with a low frequency camera with respect to an image sequence, recorded previously within a different season. Our approach uses a discrete Bayes filter and a sensor model based on whole image descriptors. Thereby it exploits sequential information to model the dynamics of the system. Since we compute a probability distribution over the whole state space, our approach can handle more complex trajectories that may include same season loop-closures as well as fragmented sub-sequences. Throughout an extensive experimental evaluation on challenging datasets, we demonstrate that our approach outperforms state-of-the-art techniques.},
  copyright = {Copyright 2015, The Institution of Engineering and Technology},
  keywords = {Bayes methods;image filtering;image sequences;Markov processes;mobile robots;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ECMR.2015.7324181},
}

@inproceedings{naseer-et-al:2017:7989305,
  author = {T. Naseer and G. L. Oliveira and T. Brox and W. Burgard},
  journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Semantics-aware visual localization under challenging perceptual conditions},
  pages = {2614--20},
  doi = {10.1109/ICRA.2017.7989305},
  note = {semantics-aware visual localization;visual place recognition;long-term visual navigation;robot localization;image feature descriptions;deep convolutional neural networks;spatially inconsistent image matches;nonperfect image matches;discriminative holistic image representation;dense scene description;salient scene description;image segmentation;perceptual scene dynamics;structural scene dynamics;Freiburg;},
  address = {Piscataway, NJ, USA},
  year = {2017},
  abstract = {Visual place recognition under difficult perceptual conditions remains a challenging problem due to changing weather conditions, illumination and seasons. Long-term visual navigation approaches for robot localization should be robust to these dynamics of the environment. Existing methods typically leverage feature descriptions of whole images or image regions from Deep Convolutional Neural Networks. Some approaches also exploit sequential information to alleviate the problem of spatially inconsistent and non-perfect image matches. In this paper, we propose a novel approach for learning a discriminative holistic image representation which exploits the image content to create a dense and salient scene description. These salient descriptions are learnt over a variety of datasets under large perceptual changes. Such an approach enables us to precisely segment the regions of an image which are geometrically stable over large time lags. We combine features from these salient regions and an off-the-shelf holistic representation to form a more robust scene descriptor. We also introduce a semantically labeled dataset which captures extreme perceptual and structural scene dynamics over the course of 3 years. We evaluated our approach with extensive experiments on data collected over several kilometers in Freiburg and show that our learnt image representation outperforms off-the-shelf features from the deep networks and hand-crafted features.},
  copyright = {Copyright 2017, The Institution of Engineering and Technology},
  keywords = {convolution;image matching;image representation;image segmentation;neural nets;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA.2017.7989305},
}

@inproceedings{qin-et-al:2020:9340939,
  author = {T. Qin and T. Chen and Y. Chen and Q. Su},
  journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot},
  pages = {5939--45},
  doi = {10.1109/IROS45743.2020.9340939},
  note = {AVP-SLAM;semantic visual mapping;autonomous vehicles;parking lot;autonomous valet parking;GPS-denied parking lots;accurate localization ability;traditional visual-based methods;appearance changes;robust semantic features;localize vehicles;traditional features;global visual semantic map;autonomous parking application;},
  address = {Piscataway, NJ, USA},
  year = {2020},
  abstract = {Autonomous valet parking is a specific application for autonomous vehicles. In this task, vehicles need to navigate in narrow, crowded and GPS-denied parking lots. Accurate localization ability is of great importance. Traditional visual-based methods suffer from tracking lost due to texture-less regions, repeated structures, and appearance changes. In this paper, we exploit robust semantic features to build the map and localize vehicles in parking lots. Semantic features contain guide signs, parking lines, speed bumps, etc, which typically appear in parking lots. Compared with traditional features, these semantic features are long-term stable and robust to the perspective and illumination change. We adopt four surround-view cameras to increase the perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system generates a global visual semantic map. This map is further used to localize vehicles at the centimeter level. We analyze the accuracy and recall of our system and compare it against other methods in real experiments. Furthermore, we demonstrate the practicability of the proposed system by the autonomous parking application.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  keywords = {cameras;data visualisation;driver information systems;feature extraction;Global Positioning System;mobile robots;object detection;road vehicles;robot vision;SLAM (robots);traffic engineering computing;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS45743.2020.9340939},
}

@inproceedings{taisho-kanji:2016:7866383,
  author = {T. Taisho and T. Kanji},
  journal = {2016 IEEE International Conference on Robotics and Biomimetics (ROBIO). Proceedings},
  title = {Mining DCNN landmarks for long-term visual SLAM},
  pages = {570--6},
  doi = {10.1109/ROBIO.2016.7866383},
  note = {DCNN landmarks mining;visual SLAM;discriminative visual features;deep convolutional neural networks;visual landmark mining;visual word semantic representation;cross-domain visual place recognition;},
  address = {Piscataway, NJ, USA},
  year = {2016},
  abstract = {Long-term visual SLAM, in familiar, semi-dynamic, and partially changing environments is an important area of research in robotics. The main problem we faced is the question of how to describe a scene discriminatively and compactly-both of which are necessary in order to cope with changes in appearance and a large amount of visual information. In this study, we address the above issues by mining visual experience. Our strategy is to mine a library of raw visual images, termed visual experience, to find the relevant visual patterns to effectively explain the input scene. From a practical point of view, our work offers three main contributions over the previous work. First, it is the first application of discriminative visual features from deep convolutional neural networks (DCNN) to the task of visual landmark mining. Second, we show how to interpret a high-dimensional DCNN feature to a compact semantic representation of visual word. Third, we show that our approach can turn the scene description task with any feature (including the DCNN feature) into the task of mining visual experience. Experiments on a challenging cross-domain visual place recognition validate efficacy of the proposed approach.},
  copyright = {Copyright 2017, The Institution of Engineering and Technology},
  keywords = {data mining;feedforward neural nets;object recognition;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ROBIO.2016.7866383},
}

@article{ali-et-al:2021:3100882,
  author = {W. Ali and P. Liu and R. Ying and Z. Gong},
  journal = {IEEE Sensors Journal},
  title = {A Life-Long SLAM Approach Using Adaptable Local Maps Based on Rasterized LIDAR Images},
  volume = {21},
  number = {19},
  pages = {21740--9},
  doi = {10.1109/JSEN.2021.3100882},
  note = {life-long SLAM system;dynamic environment;robust mapping strategy;efficient mapping strategy;computational requirements;rasterized images;map management system;loop closure system;lower computational requirements;long-term operation;adaptable local maps;rasterized LIDAR;real-time autonomous robot applications;dynamic space;},
  address = {USA},
  year = {2021},
  abstract = {Most real-time autonomous robot applications require a robot to traverse through a dynamic space for a long time. In some cases, a robot needs to work in the same environment. Such applications give rise to the problem of a life-long SLAM system. Life-long SLAM presents two main challenges i.e. the tracking should not fail in a dynamic environment and the need for a robust and efficient mapping strategy. The system should update maps with new information; while also keeping track of older observations. But, mapping for a long time can require higher computational requirements. In this paper, we propose a solution to the problem of life-long SLAM. We represent the global map as a set of rasterized images of local maps along with a map management system responsible for updating local maps and keeping track of older values. We also present an efficient approach of using the bag of visual words method for loop closure detection and relocalization. We evaluate the performance of our system on the KITTI dataset and an indoor dataset. Our loop closure system reported recall and precision of above 90 percent. The computational cost of our system is much lower as compared to state-of-the-art methods. Our method reports lower computational requirements even for long-term operation.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  issn = {1530-437X},
  keywords = {mobile robots;object detection;optical radar;path planning;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/JSEN.2021.3100882},
}

@inproceedings{ding-et-al:2019:8968550,
  author = {X. Ding and Y. Wang and L. Tang and H. Yin and R. Xiong},
  journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Communication constrained cloud-based long-term visual localization in real time},
  pages = {2159--66},
  doi = {10.1109/IROS40897.2019.8968550},
  note = {data visualization;cloud-based visual localization system;long-term localization;sliding-window based visual inertial odometry;virtual observation;localization constraints;local odometry;high-frequency inertial measurements;mobile robots;long-term visual localization;extended Kalman filter;},
  address = {Piscataway, NJ, USA},
  year = {2019},
  abstract = {Visual localization is one of the primary capabilities for mobile robots. Long-term visual localization in real time is particularly challenging, in which the robot is required to efficiently localize itself using visual data where appearance may change significantly over time. In this paper, we propose a cloud-based visual localization system targeting at long-term localization in real time. On the robot, we employ two estimators to achieve accurate and real-time performance. One is a sliding-window based visual inertial odometry, which integrates constraints from consecutive observations and self-motion measurements, as well as the constraints induced by localization results from the cloud. This estimator builds a local visual submap as the virtual observation which is then sent to the cloud as new localization constraints. The other one is a delayed state Extended Kalman Filter to fuse the pose of the robot localized from the cloud, the local odometry and the high-frequency inertial measurements. On the cloud, we propose a longer sliding-window based localization method to aggregate the virtual observations for larger field of view, leading to more robust alignment between virtual observations and the map. Under this architecture, the robot can achieve drift-free and real-time localization using onboard resources even in a network with limited bandwidth, high latency and existence of package loss, which enables the autonomous navigation in real-world environment. We evaluate the effectiveness of our system on a dataset with challenging seasonal and illuminative variations. We further validate the robustness of the system under challenging network conditions.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  keywords = {cloud computing;control engineering computing;data visualisation;distance measurement;image filtering;Kalman filters;mobile robots;nonlinear filters;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS40897.2019.8968550},
}

@article{ding-et-al:2020:2942760,
  author = {X. Ding and Y. Wang and R. Xiong and D. Li and L. Tang and H. Yin and L. Zhao},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  title = {Persistent Stereo Visual Localization on Cross-Modal Invariant Map},
  volume = {21},
  number = {11},
  pages = {4646--58},
  doi = {10.1109/TITS.2019.2942760},
  note = {persistent stereo visual localization;cross-modal invariant map;autonomous mobile vehicles;low-cost equipment;stereo camera;laser data;cross-modal invariance;map learning algorithm;multisession visual;generative map model;laser map points;local bundle adjustment;online sliding-window based visual odometry;public KITTI datasets;self-collected datasets;},
  address = {USA},
  year = {2020},
  abstract = {Autonomous mobile vehicles are expected to perform persistent and accurate localization with low-cost equipment. To achieve this goal, we propose a stereo camera based visual localization method using a modified laser map, which takes the advantage of both the low cost of camera, and high geometric precision of laser data to achieve long-term performance. Considering that LiDAR and camera give measurements of the same environment in different modalities, the cross-modal invariance is investigated to modify the laser map for visual localization. Specifically, a map learning algorithm is introduced to sample the robust subsets in laser maps that are useful for visual localization using multi-session visual and laser data. Further, a generative map model is derived to describe this cross-modal invariance, based on which two types of measurements are defined to model the laser map points as appropriate visual observations. Tightly coupling these measurements within the local bundle adjustment during online sliding-window based visual odometry, the vehicle can achieve robust localization even one year after the map was built. The effectiveness of the proposed method is evaluated on both the public KITTI datasets and self-collected datasets in our campus, which include seasonal, illumination and object variations. On all experimental localization sessions, our method provides satisfactory results, even when the direction is opposite to that in the mapping session, verifying the superior performance of the laser map based visual localization method.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  issn = {1524-9050},
  keywords = {cameras;distance measurement;invariance;learning (artificial intelligence);mobile robots;path planning;road traffic control;robot vision;SLAM (robots);stereo image processing;traffic engineering computing;},
  language = {English},
  url = {http://dx.doi.org/10.1109/TITS.2019.2942760},
}

@article{xu-et-al:2021:3060741,
  author = {X. Xu and H. Yin and Z. Chen and Y. Li and Y. Wang and R. Xiong},
  journal = {IEEE Robotics and Automation Letters},
  title = {DiSCO: Differentiable Scan Context With Orientation},
  volume = {6},
  number = {2},
  pages = {2791--8},
  doi = {10.1109/LRA.2021.3060741},
  note = {map database;robot navigation;global localization;global optimal relative orientation;differentiable phase correlation;place descriptor;current scan;down-stream local optimal metric pose estimation;similar place;named Differentiable Scan Context;LiDAR-based place recognition method;appearance change;LiDAR scan;},
  address = {USA},
  year = {2021},
  abstract = {Global localization is essential for robot navigation, of which the first step is to retrieve a query from the map database. This problem is called place recognition. In recent years, LiDAR scan based place recognition has drawn attention as it is robust against the appearance change. In this letter, we propose a LiDAR-based place recognition method, named Differentiable Scan Context with Orientation (DiSCO), which simultaneously finds the scan at a similar place and estimates their relative orientation. The orientation can further be used as the initial value for the down-stream local optimal metric pose estimation, improving the pose estimation especially when a large orientation between the current scan and retrieved scan exists. Our key idea is to transform the feature into the frequency domain. We utilize the magnitude of the spectrum as the place descriptor, which is theoretically rotation-invariant. In addition, based on the differentiable phase correlation, we can efficiently estimate the global optimal relative orientation using the spectrum. With such structural constraints, the network can be learned in an end-to-end manner, and the backbone is fully shared by the two tasks, achieving better interpretability and lightweight. Finally, DiSCO is validated on three datasets with long-term outdoor conditions, showing better performance than the compared methods. Codes are released at https://github.com/MaverickPeter/DiSCO-pytorch.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  issn = {2377-3766},
  keywords = {feature extraction;image recognition;mobile robots;object recognition;optical radar;pose estimation;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/LRA.2021.3060741},
}

@inproceedings{latif-et-al:2012:6385879,
  author = {Y. Latif and C. Cadena and J. Neira},
  journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2012)},
  title = {Realizing, reversing, recovering: Incremental robust loop closing over time using the iRRR algorithm},
  pages = {4211--17},
  doi = {10.1109/IROS.2012.6385879},
  note = {incremental robust loop;iRRR algorithm;robust autonomous robot applications;mapping systems;localization systems;mobile robot;realizing;reversing;recovering;},
  address = {Piscataway, NJ, USA},
  year = {2012},
  abstract = {The ability to reconsider information over time allows to detect failures and is crucial for long term robust autonomous robot applications. This applies to loop closure decisions in localization and mapping systems. This paper describes a method to analyze all available information up to date in order to robustly remove past incorrect loop closures from the optimization process. The main novelties of our algorithm are: 1. incrementally reconsidering loop closures and 2. handling multi-session, spatially related or unrelated experiments. We validate our proposal in real multi-session experiments showing better results than those obtained by state of the art methods.},
  copyright = {Copyright 2013, The Institution of Engineering and Technology},
  keywords = {mobile robots;robust control;},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS.2012.6385879},
}

@article{latif-et-al:2017:016,
  author = {Y. Latif and G. Huang and J. Leonard and J. Neira},
  journal = {Robotics and Autonomous Systems},
  title = {Sparse optimization for robust and efficient loop closing},
  volume = {93},
  pages = {13--26},
  doi = {10.1016/j.robot.2017.03.016},
  note = {fast convex optimization techniques;-minimization problem;convex &lscr;;loop-closure detection;previous images;loop-closing event;key insight;long-term visual navigation;revisits;efficient loop closing;robust loop closing;sparse optimization;corresponding images;loop-closure decision;single globally optimal match;offline dictionary learning;real-time robot navigation;loop closures;},
  address = {Netherlands},
  year = {2017},
  abstract = {It is essential for a robot to be able to detect revisits or <i>loop closures</i> for long-term visual navigation. A key insight explored in this work is that the loop-closing event inherently occurs sparsely, i.e., the image currently being taken matches with only a small subset (if any) of previous images. Based on this observation, we formulate the problem of loop-closure detection as a <i>sparse, convex</i>&lscr;<sub>1</sub>-minimization problem. By leveraging fast convex optimization techniques, we are able to efficiently find loop closures, thus enabling real-time robot navigation. This novel formulation requires no offline dictionary learning, as required by most existing approaches, and thus allows <i>online incremental</i> operation. Our approach ensures a <i>unique</i> hypothesis by choosing only a single globally optimal match when making a loop-closure decision. Furthermore, the proposed formulation enjoys a <i>flexible</i> representation with <i>no</i> restriction imposed on how images should be represented, while requiring only that the representations are &ldquo;close&rdquo; to each other when the corresponding images are visually similar. The proposed algorithm is validated extensively using real-world datasets. [All rights reserved Elsevier].},
  copyright = {Copyright 2017, The Institution of Engineering and Technology},
  issn = {0921-8890},
  keywords = {convex programming;image representation;learning (artificial intelligence);minimisation;mobile robots;optimisation;path planning;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1016/j.robot.2017.03.016},
}

@inproceedings{song-et-al:2019:8967749,
  author = {Y. Song and D. Zhu and J. Li and Y. Tian and M. Li},
  journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title = {Learning Local Feature Descriptor with Motion Attribute For Vision-based Localization},
  pages = {3794--801},
  doi = {10.1109/IROS40897.2019.8967749},
  note = {local feature point;fully convolutional network;MD-Net;motion attribute estimation;feature description;local feature descriptor;vision-based localization algorithm;camera-based localization;open-loop localization;re-localization;loop closure detection;},
  address = {Piscataway, NJ, USA},
  year = {2019},
  abstract = {In recent years, camera-based localization has been widely used for robotic applications, and most proposed algorithms rely on local features extracted from recorded images. For better performance, the features used for open-loop localization are required to be short-term globally static, and the ones used for re-localization or loop closure detection need to be long-term static. Therefore, the motion attribute of a local feature point could be exploited to improve localization performance, e.g., the feature points extracted from moving persons or vehicles can be excluded from these systems due to their unsteadiness. In this paper, we design a fully convolutional network (FCN), named MD-Net, to perform motion attribute estimation and feature description simultaneously. MD-Net has a shared backbone network to extract features from the input image and two network branches to complete each sub-task. With MD-Net, we can obtain the motion attribute while avoiding increasing much more computation. Experimental results demonstrate that the proposed method can learn distinct local feature descriptor along with motion attribute only using an FCN, by outperforming competing methods by a wide margin. We also show that the proposed algorithm can be integrated into a vision-based localization algorithm to improve estimation accuracy significantly.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  keywords = {cameras;convolutional neural nets;feature extraction;learning (artificial intelligence);robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IROS40897.2019.8967749},
}

@inproceedings{yue-et-al:2020:9197072,
  author = {Y. Yue and C. Yang and J. Zhang and M. Wen and Z. Wu and H. Zhang and D. Wang},
  journal = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Day and Night Collaborative Dynamic Mapping in Unstructured Environment Based on Multimodal Sensors},
  pages = {2981--7},
  doi = {10.1109/ICRA40945.2020.9197072},
  note = {dynamic collaborative mapping;multimodal environmental perception;heterogeneous sensor fusion model;local 3D maps;night rainforest;3D map fusion missions;multimodal sensors;long-term operation;collaborative robots;dynamic environment;dynamic objects;},
  address = {Piscataway, NJ, USA},
  year = {2020},
  abstract = {Enabling long-term operation during day and night for collaborative robots requires a comprehensive understanding of the unstructured environment. Besides, in the dynamic environment, robots must be able to recognize dynamic objects and collaboratively build a global map. This paper proposes a novel approach for dynamic collaborative mapping based on multimodal environmental perception. For each mission, robots first apply heterogeneous sensor fusion model to detect humans and separate them to acquire static observations. Then, the collaborative mapping is performed to estimate the relative position between robots and local 3D maps are integrated into a globally consistent 3D map. The experiment is conducted in the day and night rainforest with moving people. The results show the accuracy, robustness, and versatility in 3D map fusion missions.},
  copyright = {Copyright 2020, The Institution of Engineering and Technology},
  keywords = {groupware;image fusion;mobile robots;multi-robot systems;path planning;sensor fusion;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/ICRA40945.2020.9197072},
}

@article{pan-et-al:2019:s19194252,
  author = {Z. Pan and H. Chen and S. Li and Y. Liu},
  journal = {Sensors},
  title = {ClusterMap Building and Relocalization in Urban Environments for Unmanned Vehicles},
  volume = {19},
  number = {19},
  pages = {4252(22pp.)--},
  doi = {10.3390/s19194252},
  note = {ClusterMap building;urban environments;unmanned vehicles;map building;map-based relocalization techniques;high-density laser range finders;relocalization problems;long-term applications;map format;perceived point clouds;point clusters;dynamic objects;location descriptor;cluster descriptors;local maps;global maps;high-density point clouds;high-precision segmentation algorithms;consistent ClusterMap;scale problem;mapping solution;mobile vehicle;3D visual-LIDAR simultaneous localization and mapping solution;visual information;KITTI dataset;},
  address = {Switzerland},
  year = {2019},
  abstract = {Map building and map-based relocalization techniques are important for unmanned vehicles operating in urban environments. The existing approaches require expensive high-density laser range finders and suffer from relocalization problems in long-term applications. This study proposes a novel map format called the ClusterMap, on the basis of which an approach to achieving relocalization is developed. The ClusterMap is generated by segmenting the perceived point clouds into different point clusters and filtering out clusters belonging to dynamic objects. A location descriptor associated with each cluster is designed for differentiation. The relocalization in the global map is achieved by matching cluster descriptors between local and global maps. The solution does not require high-density point clouds and high-precision segmentation algorithms. In addition, it prevents the effects of environmental changes on illumination intensity, object appearance, and observation direction. A consistent ClusterMap without any scale problem is built by utilizing a 3D visual-LIDAR simultaneous localization and mapping solution by fusing LIDAR and visual information. Experiments on the KITTI dataset and our mobile vehicle illustrates the effectiveness of the proposed approach.},
  copyright = {Copyright 2021, The Institution of Engineering and Technology},
  issn = {1424-8220},
  keywords = {image segmentation;laser ranging;mobile robots;optical radar;path planning;pattern clustering;remotely operated vehicles;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.3390/s19194252},
}

@inproceedings{xin-et-al:2017:8310121,
  author = {Z. Xin and X. Cui and J. Zhang and Y. Yang and Y. Wang},
  journal = {2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA)},
  title = {Visual place recognition with CNNs: from global to partial},
  pages = {6pp.--},
  doi = {10.1109/IPTA.2017.8310121},
  note = {long-term mobile robot autonomy;partial reranking process;potential places;global image representations;partial image representations;recognition precision;pre-trained convolutional neural networks;visual place recognition pipeline;CNN;},
  address = {Piscataway, NJ, USA},
  year = {2017},
  abstract = {Visual place recognition is one of the most challenging problems in computer vision, due to the large diversities that real-world places can represent. Recently, visual place recognition has become a key part of loop closure detection and topological localization in long-term mobile robot autonomy. In this work, we build up a novel visual place recognition pipeline composed of a first filtering stage followed by a partial reranking process. In the filtering stage, image-wise features are utilized to find a small set of potential places. Afterwards, stable region-wise landmarks are extracted for more accurate matching in the partial reranking process. All global and partial image representations are derived from pre-trained Convolutional Neural Networks (CNNs), and the landmarks are extracted by object proposal techniques. Moreover, a new similarity measurement is provided by considering both spatial and scale distribution of landmarks. Compared with current methods only considering scale distribution, the presented similarity measurement can benefit recognition precision and robustness effectively. Experiments with varied viewpoints and environmental conditions demonstrate that the proposed method achieves superior performance against state-of-the-art methods.},
  copyright = {Copyright 2018, The Institution of Engineering and Technology},
  keywords = {computer vision;feature extraction;feedforward neural nets;image matching;image recognition;image representation;learning (artificial intelligence);mobile robots;object detection;object recognition;robot vision;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/IPTA.2017.8310121},
}

@article{du-et-al:2022:3028218,
  author = {Z.-J. Du and S.-S. Huang and T.-J. Mu and Q. Zhao and R. R. Martin and K. Xu},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  title = {Accurate Dynamic SLAM Using CRF-Based Long-Term Consistency},
  volume = {28},
  number = {4},
  pages = {1745--57},
  doi = {10.1109/TVCG.2020.3028218},
  note = {efficient initial camera;estimation method;distinguishing dynamic;conditional random fields;accurate camera trajectory estimation;highly dynamic environments;accurate dynamic SLAM;CRF-based long-term consistency;world dynamic 3D reconstruction;reality applications;novel RGB-D SLAM approach;dynamic components;short time-span;consecutive frames;accurate dynamic 3D landmark detection method;long-term observations;},
  address = {USA},
  year = {2022},
  abstract = {Accurate camera pose estimation is essential and challenging for real world dynamic 3D reconstruction and augmented reality applications. In this article, we present a novel RGB-D SLAM approach for accurate camera pose tracking in dynamic environments. Previous methods detect dynamic components only across a short time-span of consecutive frames. Instead, we provide a more accurate dynamic 3D landmark detection method, followed by the use of long-term consistency via conditional random fields, which leverages long-term observations from multiple frames. Specifically, we first introduce an efficient initial camera pose estimation method based on distinguishing dynamic from static points using graph-cut RANSAC. These static/dynamic labels are used as priors for the unary potential in the conditional random fields, which further improves the accuracy of dynamic 3D landmark detection. Evaluation using the TUM and Bonn RGB-D dynamic datasets shows that our approach significantly outperforms state-of-the-art methods, providing much more accurate camera trajectory estimation in a variety of highly dynamic environments. We also show that dynamic 3D reconstruction can benefit from the camera poses estimated by our RGB-D SLAM approach.},
  copyright = {Copyright 2022, The Institution of Engineering and Technology},
  issn = {1941-0506},
  keywords = {augmented reality;cameras;feature extraction;graph theory;image reconstruction;image sequences;motion estimation;pose estimation;SLAM (robots);},
  language = {English},
  url = {http://dx.doi.org/10.1109/TVCG.2020.3028218},
}
