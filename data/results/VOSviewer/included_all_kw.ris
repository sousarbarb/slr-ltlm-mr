TY  - CPAPER
AU  - Ali, A J B
AU  - Hashemifar, Z S
AU  - Dantu, K
AD  - University at Buffalo, Buffalo, United States
TI  - Edge-SLAM: Edge-assisted visual simultaneous localization and mapping
SP  - 325-337
PY  - 2020
DA  - 2020
PB  - Association for Computing Machinery, Inc
AB  - Localization in urban environments is becoming increasingly important and
      used in tools such as ARCore [11], ARKit [27] and others. One popular
      mechanism to achieve accurate indoor localization as well as a map of the
      space is using Visual Simultaneous Localization and Mapping (Visual-SLAM).
      However, Visual-SLAM is known to be resource-intensive in memory and
      processing time. Further, some of the operations grow in complexity over
      time, making it challenging to run on mobile devices continuously. Edge
      computing provides additional compute and memory resources to mobile
      devices to allow offloading of some tasks without the large latencies seen
      when offloading to the cloud. In this paper, we present Edge-SLAM, a
      system that uses edge computing resources to offload parts of Visual-SLAM.
      We use ORB-SLAM2 as a prototypical Visual-SLAM system and modify it to a
      split architecture between the edge and the mobile device. We keep the
      tracking computation on the mobile device and move the rest of the
      computation, i.e., local mapping and loop closure, to the edge. We
      describe the design choices in this effort and implement them in our
      prototype. Our results show that our split architecture can allow the
      functioning of the Visual-SLAM system long-term with limited resources
      without affecting the accuracy of operation. It also keeps the computation
      and memory cost on the mobile device constant which would allow for
      deployment of other end applications that use Visual-SLAM. © 2020 ACM.
DO  - 10.1145/3386901.3389033
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088125321&doi=10.1145%2f3386901.3389033&partnerID=40&md5=9805a788a84734c5039d93ffd0d88b95
UR  - http://dx.doi.org/10.1145/3386901.3389033
KW  - edge computing
KW  - localization
KW  - mapping
KW  - mobile systems
KW  - split architecture
KW  - visual simultaneous localization and mapping
KW  - memory architecture
KW  - slam robotics
KW  - computing resource
KW  - indoor localization
KW  - local mapping
KW  - memory resources
KW  - processing time
KW  - split architectures
KW  - urban environments
KW  - visual simultaneous localization and mappings
KW  - indoor positioning systems
N1  - cited By 11; Conference of 18th ACM International Conference on Mobile
      Systems, Applications, and Services, MobiSys 2020 ; Conference Date: 15
      June 2020 Through 19 June 2020; Conference Code:160682
ER  - 

TY  - JOUR
AU  - Davison, A J
AU  - Murray, D W
AD  - IEEE, United Kingdom; Robotics Research Group, Department of Engineering
      Science, University of Oxford, Oxford, OX1 3PJ, United Kingdom
TI  - Simultaneous localization and map-building using active vision
T2  - IEEE Transactions on Pattern Analysis and Machine Intelligence
VL  - 24
IS  - 7
SP  - 865-880
PY  - 2002
DA  - 2002
AB  - An active approach to sensing can provide the focused measurement
      capability over a wide field of view which allows correctly formulated
      Simultaneous Localization and Map-Building (SLAM) to be implemented with
      vision, permitting repeatable long-term localization using only naturally
      occurring, automatically-detected features. In this paper, we present the
      first example of a general system for autonomous localization using active
      vision, enabled here by a high-performance stereo head, addressing such
      issues as uncertainty-based measurement selection, automatic
      map-maintenance, and goal-directed steering. We present varied real-time
      experiments in a complex environment.
SN  - 0162-8828
DO  - 10.1109/TPAMI.2002.1017615
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036647318&doi=10.1109%2fTPAMI.2002.1017615&partnerID=40&md5=982a6c37043ef2f32ed09699824f95e3
UR  - http://dx.doi.org/10.1109/TPAMI.2002.1017615
KW  - active vision
KW  - mobile robots
KW  - simultaneous localization and map-building
KW  - active visual sensing
KW  - electromechanical stereo head
KW  - high resolution omni-directional data
KW  - odometry
KW  - algorithms
KW  - cameras
KW  - closed loop control systems
KW  - degrees of freedom (mechanics)
KW  - electromechanical devices
KW  - feature extraction
KW  - kalman filtering
KW  - kinematics
KW  - mathematical models
KW  - matrix algebra
KW  - probability density function
KW  - computer vision
N1  - cited By 404
ER  - 

TY  - CPAPER
AU  - Glover, A J
AU  - Maddern, W P
AU  - Milford, M J
AU  - Wyeth, G F
AD  - School of Information Technology and Electrical Engineering, University of
      Queensland, St Lucia, QLD 4072, Australia; Queensland Brain Institute,
      University of Queensland, St Lucia, QLD 4072, Australia
TI  - FAB-MAP + RatSLAM: Appearance-based SLAM for multiple times of day
SP  - 3507-3512
PY  - 2010
DA  - 2010
AB  - Appearance-based mapping and localisation is especially challenging when
      separate processes of mapping and localisation occur at different times of
      day. The problem is exacerbated in the outdoors where continuous change in
      sun angle can drastically affect the appearance of a scene. We confront
      this challenge by fusing the probabilistic local feature based data
      association method of FAB-MAP with the pose cell filtering and experience
      mapping of RatSLAM. We evaluate the effectiveness of our amalgamation of
      methods using five datasets captured throughout the day from a single
      camera driven through a network of suburban streets. We show further
      results when the streets are re-visited three weeks later, and draw
      conclusions on the value of the system for lifelong mapping. ©2010 IEEE.
DO  - 10.1109/ROBOT.2010.5509547
C1  - Anchorage, AK
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955801323&doi=10.1109%2fROBOT.2010.5509547&partnerID=40&md5=f77cf7db9cd04c0c21f52c4ac46f8d40
UR  - http://dx.doi.org/10.1109/ROBOT.2010.5509547
KW  - appearance based
KW  - data association
KW  - data sets
KW  - experience mappings
KW  - local feature
KW  - localisation
KW  - single cameras
KW  - suburban streets
KW  - sun angle
KW  - data processing
KW  - mapping
KW  - metals
KW  - roads and streets
KW  - robotics
N1  - cited By 168; Conference of 2010 IEEE International Conference on Robotics
      and Automation, ICRA 2010 ; Conference Date: 3 May 2010 Through 7 May
      2010; Conference Code:81416
ER  - 

TY  - JOUR
AU  - Kawewong, A
AU  - Tongprasit, N
AU  - Hasegawa, O
AD  - Faculty of Engineering, Department of Computer Engineering, Chiang Mai
      University, 239 Huay Kaew Rd., Chiang Mai, 50200, Muang District,
      Thailand; Faculty of Science, Material Science Research Center, Chiang Mai
      University, 239 Huay Kaew Rd., Chiang Mai, 50200, Muang District,
      Thailand; Imaging Science and Engineering Laboratory, Department of
      Computational Intelligence and Systems Science, Tokyo Institute of
      Technology, 4259-J3 Nagatsuta, Midori-ku, Yokohama, 226-5803, Japan
TI  - A speeded-up online incremental vision-based loop-closure detection for
      long-term SLAM
T2  - Advanced Robotics
VL  - 27
IS  - 17
SP  - 1325-1336
PY  - 2013
DA  - 2013
AB  - An online incremental method of vision-only loop-closure detection for
      long-term robot navigation is proposed. The method is based on the scheme
      of direct feature matching which has recently become more efficient than
      the Bag-of-Words scheme in many challenging environments. The
      contributions of the paper are the application of hierarchical k-means to
      speed-up feature matching time and the improvement of the score
      calculation technique used to determine the loop-closing location. As a
      result, the presented method runs quickly in long term while retaining
      high accuracy. The searching cost has been markedly reduced. The proposed
      method requires no any off-line dictionary generation processes. It can
      start anywhere at any times. The evaluation has been done on standard
      well-known datasets being challenging in perceptual aliasing and dynamic
      changes. The results show that the proposed method offers high
      precision-recall in large-scale different environments with real-time
      computation comparing to other vision-only loop-closure detection methods.
      © 2013 Taylor & Francis and The Robotics Society of Japan.
SN  - 0169-1864
DO  - 10.1080/01691864.2013.826410
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885606487&doi=10.1080%2f01691864.2013.826410&partnerID=40&md5=135539eb8d56fc796786b85600adaaed
UR  - http://dx.doi.org/10.1080/01691864.2013.826410
KW  - place localization
KW  - robotics navigation
KW  - simultaneous localization and mapping
KW  - vision-based loop-closure detection
KW  - calculation techniques
KW  - hierarchical k-means
KW  - loop closure
KW  - off-line dictionaries
KW  - perceptual aliasing
KW  - real-time computations
KW  - mathematical techniques
KW  - robots
KW  - robotics
N1  - cited By 4
ER  - 

TY  - JOUR
AU  - Schaefer, A
AU  - Büscher, D
AU  - Vertens, J
AU  - Luft, L
AU  - Burgard, W
AD  - Department of Computer Science, University of Freiburg, Germany
TI  - Long-term vehicle localization in urban environments based on pole
      landmarks extracted from 3-D lidar scans
T2  - Robotics and Autonomous Systems
VL  - 136
SP  - 103709
PY  - 2021
DA  - 2021
PB  - Elsevier B.V.
AB  - Due to their ubiquity and long-term stability, pole-like objects are well
      suited to serve as landmarks for vehicle localization in urban
      environments. In this work, we present a complete mapping and long-term
      localization system based on pole landmarks extracted from 3-D lidar data.
      Our approach features a novel pole detector, a mapping module, and an
      online localization module, each of which are described in detail, and for
      which we provide an open-source implementation (Schaefer and Büscher,
      0000). In extensive experiments, we demonstrate that our method improves
      on the state of the art with respect to long-term reliability and
      accuracy: First, we prove reliability by tasking the system with
      localizing a mobile robot over the course of 15 months in an urban area
      based on an initial map, confronting it with constantly varying routes,
      differing weather conditions, seasonal changes, and construction sites.
      Second, we show that the proposed approach clearly outperforms a recently
      published method in terms of accuracy. © 2020 Elsevier B.V.
SN  - 0921-8890
DO  - 10.1016/j.robot.2020.103709
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097641390&doi=10.1016%2fj.robot.2020.103709&partnerID=40&md5=5d5ddb6bedf322f6767044bee0046c6b
UR  - http://dx.doi.org/10.1016/j.robot.2020.103709
KW  - autonomous driving
KW  - feature extraction
KW  - landmark
KW  - lidar
KW  - localization
KW  - mapping
KW  - pole
KW  - agricultural robots
KW  - optical radar
KW  - poles
KW  - complete mappings
KW  - construction sites
KW  - localization system
KW  - long term stability
KW  - on-line localization
KW  - open source implementation
KW  - urban environments
KW  - vehicle localization
KW  - urban growth
N1  - cited By 5
ER  - 

TY  - CPAPER
AU  - Walcott-Bryant, A
AU  - Kaess, M
AU  - Johannsson, H
AU  - Leonard, J J
AD  - Massachusetts Institute of Technology, Cambridge, MA 02139, United States
TI  - Dynamic pose graph SLAM: Long-term mapping in low dynamic environments
SP  - 1871-1878
PY  - 2012
DA  - 2012
AB  - Maintaining a map of an environment that changes over time is a critical
      challenge in the development of persistently autonomous mobile robots.
      Many previous approaches to mapping assume a static world. In this work we
      incorporate the time dimension into the mapping process to enable a robot
      to maintain an accurate map while operating in dynamical environments.
      This paper presents Dynamic Pose Graph SLAM (DPG-SLAM), an algorithm
      designed to enable a robot to remain localized in an environment that
      changes substantially over time. Using incremental smoothing and mapping
      (iSAM) as the underlying SLAM state estimation engine, the Dynamic Pose
      Graph evolves over time as the robot explores new places and revisits
      previously mapped areas. The approach has been implemented for planar
      indoor environments, using laser scan matching to derive constraints for
      SLAM state estimation. Laser scans for the same portion of the environment
      at different times are compared to perform change detection; when
      sufficient change has occurred in a location, the dynamic pose graph is
      edited to remove old poses and scans that no longer match the current
      state of the world. Experimental results are shown for two real-world
      dynamic indoor laser data sets, demonstrating the ability to maintain an
      up-to-date map despite long-term environmental changes. © 2012 IEEE.
DO  - 10.1109/IROS.2012.6385561
C1  - Vilamoura, Algarve
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872347968&doi=10.1109%2fIROS.2012.6385561&partnerID=40&md5=ecb49adc23454d108fe45bcb6e9f2888
UR  - http://dx.doi.org/10.1109/IROS.2012.6385561
KW  - autonomous mobile robot
KW  - change detection
KW  - critical challenges
KW  - dynamic environments
KW  - dynamical environment
KW  - environmental change
KW  - indoor environment
KW  - laser data
KW  - laser scans
KW  - mapping process
KW  - time dimension
KW  - image matching
KW  - intelligent robots
KW  - intelligent systems
KW  - lasers
KW  - mapping
KW  - state estimation
KW  - robotics
N1  - cited By 93; Conference of 25th IEEE/RSJ International Conference on
      Robotics and Intelligent Systems, IROS 2012 ; Conference Date: 7 October
      2012 Through 12 October 2012; Conference Code:94955
ER  - 

TY  - JOUR
AU  - Bacca, B
AU  - Salvi, J
AU  - Cufi, X
TI  - Long-term mapping and localization using feature stability histograms
T2  - Robotics and Autonomous Systems
VL  - 61
IS  - 12
SP  - 1539-1558
PY  - 2013
DA  - 2013
CY  - Netherlands
AB  - This work proposes a system for long-term mapping and localization based
      on the Feature Stability Histogram (FSH) model which is an innovative
      feature management approach able to cope with changing environments. FSH
      is built using a voting schema, where re-observed features are promoted;
      otherwise the feature progressively decreases its corresponding FSH value.
      FSH is inspired by the human memory model. This model introduces concepts
      of Short-Term Memory (STM), which retains information long enough to use
      it, and Long-Term Memory (LTM), which retains information for longer
      periods of time. If the entries in STM are continuously rehearsed, they
      become part of LTM. However, this work proposes a change in the pipeline
      of this model, allowing any feature to be part of STM or LTM depending on
      the feature strength. FSH stores the stability values of local features,
      stable features are only used for localization and mapping. Experimental
      validation of the FSH model was conducted using the FastSLAM framework and
      a long-term dataset collected during a period of one year at different
      environmental conditions. The experiments carried out include qualitative
      and quantitative results such as: filtering out dynamic objects,
      increasing map accuracy, scalability, and reducing the data association
      effort in long-term runs. [All rights reserved Elsevier].
SN  - 0921-8890
DO  - 10.1016/j.robot.2013.07.003
UR  - http://dx.doi.org/10.1016/j.robot.2013.07.003
KW  - mobile robots
KW  - slam (robots)
N1  - long-term mapping and localization;feature stability histograms;FSH
      model;feature management approach;voting schema;reobserved features;human
      memory model;short-term memory;STM;long-term memory;LTM;local feature
      stability values;FastSLAM framework;dynamic object filtering;map
      accuracy;data association effort reduction;mobile robotics;
ER  - 

TY  - JOUR
AU  - Bescos, B
AU  - Facil, J M
AU  - Civera, J
AU  - Neira, J
AD  - Instituto de Investigación en Ingeniería de Aragón (I3A), Universidad de
      Zaragoza, Zaragoza, Spain
TI  - DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes
T2  - IEEE Robotics and Automation Letters
VL  - 3
IS  - 4
SP  - 4076-4083
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - The assumption of scene rigidity is typical in SLAM algorithms. Such a
      strong assumption limits the use of most visual SLAM systems in populated
      real-world environments, which are the target of several relevant
      applications like service robotics or autonomous vehicles. In this letter
      we present DynaSLAM, a visual SLAM system that, building on ORB-SLAM2,
      adds the capabilities of dynamic object detection and background
      inpainting. DynaSLAM is robust in dynamic scenarios for monocular, stereo,
      and RGB-D configurations. We are capable of detecting the moving objects
      either by multiview geometry, deep learning, or both. Having a static map
      of the scene allows inpainting the frame background that has been occluded
      by such dynamic objects. We evaluate our system in public monocular,
      stereo, and RGB-D datasets. We study the impact of several accuracy/speed
      trade-offs to assess the limits of the proposed methodology. DynaSLAM
      outperforms the accuracy of standard visual SLAM baselines in highly
      dynamic scenarios. And it also estimates a map of the static parts of the
      scene, which is a must for long-term applications in real-world
      environments. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2018.2860039
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062528288&doi=10.1109%2fLRA.2018.2860039&partnerID=40&md5=a816f645991a250a0cbe53c7ef70d1d9
UR  - http://dx.doi.org/10.1109/LRA.2018.2860039
KW  - localization
KW  - slam
KW  - visual-based navigation
KW  - deep learning
KW  - economic and social effects
KW  - robotics
KW  - robots
KW  - autonomous vehicles
KW  - background inpainting
KW  - dynamic scenarios
KW  - multi-view geometry
KW  - real world environments
KW  - service robotics
KW  - object detection
N1  - cited By 243
ER  - 

TY  - CPAPER
AU  - Liu, B
AU  - Tang, F
AU  - Fu, Y
AU  - Yang, Y
AU  - Wu, Y
TI  - A Flexible and Efficient Loop Closure Detection Based on Motion Knowledge
SP  - 11241-11247
PY  - 2021
DA  - 2021
AB  - Loop closure detection (LCD) is an essential module for simultaneous
      localization and mapping (SLAM), which can correct accumulated errors
      after long-term explorations. The widely used bag-of-words (BoW) model can
      not satisfy well the requirements of both low time consumption and high
      accuracy for a mobile platform. In this paper, we propose a novel LCD
      algorithm based on motion knowledge. We give a flexible and efficient
      detection strategy and also give flexible and efficient combinations of a
      global binary feature extracted by convolutional neural network (CNN) and
      a hand-crafted local binary feature. We take a continuous motion model,
      grid-based motion statistics (GMS) and motion states as motion knowledge.
      Furthermore, we fuse the proposed LCD with a visual-inertial odometry
      (VIO) system to correct localization errors by a pose graph optimization.
      Comparative experiments with state-of-the-art LCD algorithms on typical
      datasets have been carried out, and the results demonstrate that our
      proposed method achieves quite high recall rates and quite high speed at
      100% precision. Moreover, experimental results from VIO further validate
      the effectiveness of the proposed method.
DO  - 10.1109/ICRA48506.2021.9561126
C1  - Piscataway, NJ, USA
UR  - http://dx.doi.org/10.1109/ICRA48506.2021.9561126
KW  - distance measurement
KW  - feature extraction
KW  - graph theory
KW  - mobile robots
KW  - neural nets
KW  - object detection
KW  - pose estimation
KW  - robot vision
KW  - slam (robots)
N1  - novel LCD algorithm;motion knowledge;flexible detection strategy;efficient
      detection strategy;flexible combinations;efficient combinations;global
      binary feature;hand-crafted local binary feature;continuous motion
      model;motion states;visual-inertial odometry system;localization
      errors;state-of-the-art LCD algorithms;loop closure detection;essential
      module;long-term explorations;bag-of-words model;low time consumption;
ER  - 

TY  - JOUR
AU  - Kim, C
AU  - Cho, S
AU  - Sunwoo, M
AU  - Resende, P
AU  - Bradai, B
AU  - Jo, K
AD  - Department of Automotive Engineering, Hanyang University, Seoul, South
      Korea; Valeo Driving Assistance Research Center, Bobigny Cedex, France;
      Department of Smart Vehicle Engineering, Konkuk University, Seoul, 05029,
      South Korea
TI  - A Geodetic Normal Distribution Map for Long-Term LiDAR Localization on
      Earth
T2  - IEEE Access
VL  - 9
SP  - 470-484
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Light detection and ranging (LiDAR) sensors enable a vehicle to estimate a
      pose by matching their measurements with a point cloud (PCD) map. However,
      the PCD map structure, widely used in robot fields, has some problems to
      be applied for mass production in automotive fields. First, the PCD map is
      too big to store all map data at in-vehicle units or download the map data
      from a wireless network according to the vehicle location. Second, the PCD
      map, represented by a single origin in the Cartesian coordinates, causes
      coordinate conversion errors due to an inaccurate plane-orb projection,
      when the vehicle estimate the geodetic pose on Earth. To solve two
      problems, this paper presents a geodetic normal distribution (GND) map
      structure. The GND map structure supports a geodetic quad-tree tiling
      system with multiple origins to minimize the coordinate conversion errors.
      The map data managed by the GND map structure are compressed by using
      Cartesian probabilistic distributions of points as map features. The
      truncation errors by heterogeneous coordinates between the geodetic tiling
      system and Cartesian distributions are compensated by the Cartesian
      voxelization rule. In order to match the LiDAR measurements with the GND
      map structure, the paper proposes map-matching approaches based on
      Monte-Carlo and optimization. The paper performed some experiments to
      evaluate the map size compression and the long-term localization on Earth:
      comparison with the PCD map structure, localization in various continents,
      and long-term localization. © 2013 IEEE.
SN  - 2169-3536
DO  - 10.1109/ACCESS.2020.3047421
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098756455&doi=10.1109%2fACCESS.2020.3047421&partnerID=40&md5=789605919db35cb0d2d39b4bb7661e8f
UR  - http://dx.doi.org/10.1109/ACCESS.2020.3047421
KW  - map compression
KW  - normal distribution map
KW  - registration
KW  - world-scale map management
KW  - errors
KW  - geodesy
KW  - normal distribution
KW  - vehicles
KW  - cartesian coordinate
KW  - coordinate conversion
KW  - lidar measurements
KW  - light detection and ranging
KW  - mass production
KW  - probabilistic distribution
KW  - truncation errors
KW  - vehicle location
KW  - optical radar
N1  - cited By 1
ER  - 

TY  - JOUR
AU  - Qin, C
AU  - Zhang, Y
AU  - Liu, Y
AU  - Coleman, S
AU  - Kerr, D
AU  - Lv, G
AD  - College of Information Science and Engineering, Northeastern University,
      Shenyang, China; Intelligent Systems Research Centre, University of
      Ulster, Derry, United Kingdom
TI  - Appearance-invariant place recognition by adversarially learning
      disentangled representation
T2  - Robotics and Autonomous Systems
VL  - 131
SP  - 103561
PY  - 2020
DA  - 2020
PB  - Elsevier B.V.
AB  - Place recognition is an essential component to address the problem of
      visual navigation and SLAM. The long-term place recognition is challenging
      as the environment exhibits significant variations across different times
      of the days, months, and seasons. In this paper, we view appearance
      changes as multiple domains and propose a Feature Disentanglement Network
      (FDNet) based on a convolutional auto-encoder and adversarial learning to
      extract two independent deep features — content and appearance. In our
      network, the content feature is learned which only retains the content
      information of images through the competition with the discriminators and
      content encoder. Besides, we utilize the triplets loss to make the
      appearance feature encode the appearance information. The generated
      content features are directly used to measure the similarity of images
      without dimensionality reduction operations. We use datasets that contain
      extreme appearance changes to carry out experiments, which show how
      meaningful recall at 100% precision can be achieved by our proposed method
      where existing state-of-art approaches often get worse performance. © 2020
      Elsevier B.V.
SN  - 0921-8890
DO  - 10.1016/j.robot.2020.103561
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085276237&doi=10.1016%2fj.robot.2020.103561&partnerID=40&md5=6bf815af2793164437e3aac295ae84a4
UR  - http://dx.doi.org/10.1016/j.robot.2020.103561
KW  - adversarial learning
KW  - changing environment
KW  - representation disentanglement
KW  - visual place recognition
KW  - agricultural robots
KW  - dimensionality reduction
KW  - signal encoding
KW  - auto encoders
KW  - content information
KW  - multiple domains
KW  - place recognition
KW  - visual navigation
KW  - arts computing
N1  - cited By 3
ER  - 

TY  - CPAPER
AU  - Yu, C
AU  - Liu, Z
AU  - Liu, X-J
AU  - Qiao, F
AU  - Wang, Y
AU  - Xie, F
AU  - Wei, Q
AU  - Yang, Y
AD  - Tsinghua University, Department of Electronic Engineering, Beijing, China;
      Beihang University, School of Opto-electronics Engineering, Beijing,
      China; Tsinghua University, Department of Mechanical Engineering, Beijing,
      China
TI  - A DenseNet feature-based loop closure method for visual SLAM system
SP  - 258-265
PY  - 2019
DA  - 2019
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Loop closure is a crucial part in SLAM, especially for large and long-term
      scenes. Utilizing off-the-shelf networks' features in loop closure becomes
      a hot spot. However, what kind of network is more suitable in loop closure
      and how to use their features have not been well-studied. In this paper,
      DenseNet is introduced in this field according to its own characters. The
      features of DenseNet preserve both semantic information and structure
      details and outweigh other popular networks' features significantly. Based
      on this, a DenseNet feature-based framework, named Dense-Loop, is proposed
      to address the loop closure problem. Weighted Vector of Locally Aggregated
      Descriptor (WVLAD) method is used to encode the local descriptors as the
      final global descriptor, which could resist geometry structure and
      viewpoint changes. Furthermore, 4 max-pooling by channel and
      locality-sensitive hashing (LSH) are adopted to accelerate the search
      process. Extensive experiments are conducted on public datasets and the
      results demonstrate Dense-Loop could achieve state-of-the-art performance.
      © 2019 IEEE.
DO  - 10.1109/ROBIO49542.2019.8961714
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079054757&doi=10.1109%2fROBIO49542.2019.8961714&partnerID=40&md5=51a7e255a4e9359036f6ef2d66e35930
UR  - http://dx.doi.org/10.1109/ROBIO49542.2019.8961714
KW  - convolutional neural network
KW  - densenet
KW  - loop closure
KW  - slam
KW  - biomimetics
KW  - neural networks
KW  - semantics
KW  - geometry structure
KW  - locality sensitive hashing
KW  - semantic information
KW  - state-of-the-art performance
KW  - robotics
N1  - cited By 1; Conference of 2019 IEEE International Conference on Robotics
      and Biomimetics, ROBIO 2019 ; Conference Date: 6 December 2019 Through 8
      December 2019; Conference Code:156997
ER  - 

TY  - JOUR
AU  - Ball, D
AU  - Heath, S
AU  - Wiles, J
AU  - Wyeth, G
AU  - Corke, P
AU  - Milford, M
AD  - School of Electrical Engineering and Computer Science, Queensland
      University of Technology, Brisbane, Australia; School of Information
      Technology and Electrical Engineering, University of Queensland, Brisbane,
      Australia
TI  - OpenRatSLAM: An open source brain-based SLAM system
T2  - Autonomous Robots
VL  - 34
IS  - 3
SP  - 149-176
PY  - 2013
DA  - 2013
AB  - RatSLAM is a navigation system based on the neural processes underlying
      navigation in the rodent brain, capable of operating with low resolution
      monocular image data. Seminal experiments using RatSLAM include mapping an
      entire suburb with a web camera and a long term robot delivery trial. This
      paper describes OpenRatSLAM, an open-source version of RatSLAM with
      bindings to the Robot Operating System framework to leverage advantages
      such as robot and sensor abstraction, networking, data playback, and
      visualization. OpenRatSLAM comprises connected ROS nodes to represent
      RatSLAM's pose cells, experience map, and local view cells, as well as a
      fourth node that provides visual odometry estimates. The nodes are
      described with reference to the RatSLAM model and salient details of the
      ROS implementation such as topics, messages, parameters, class diagrams,
      sequence diagrams, and parameter tuning strategies. The performance of the
      system is demonstrated on three publicly available open-source datasets. ©
      2013 Springer Science+Business Media New York.
SN  - 0929-5593
DO  - 10.1007/s10514-012-9317-9
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877875441&doi=10.1007%2fs10514-012-9317-9&partnerID=40&md5=27be1e46694136eaac6e79c3d3f4bfc3
UR  - http://dx.doi.org/10.1007/s10514-012-9317-9
KW  - appearance-based
KW  - brain-based
KW  - hippocampus
KW  - mapping
KW  - navigation
KW  - open-source
KW  - openratslam
KW  - ratslam
KW  - ros
KW  - slam
KW  - appearance based
KW  - data visualization
KW  - navigation systems
KW  - open systems
KW  - robots
KW  - brain mapping
N1  - cited By 84
ER  - 

TY  - JOUR
AU  - Martini, D D
AU  - Gadd, M
AU  - Newman, P
AD  - Department of Engineering Science, Oxford Robotics Institute, University
      of Oxford, Oxford, OX1 3PJ, United Kingdom
TI  - kRadar++: Coarse-to-fine FMCW scanning radar localisation
T2  - Sensors (Switzerland)
VL  - 20
IS  - 21
SP  - 1-23
PY  - 2020
DA  - 2020
PB  - MDPI AG
AB  - This paper presents a novel two-stage system which integrates topological
      localisation candidates from a radar-only place recognition system with
      precise pose estimation using spectral landmark-based techniques. We prove
      that the-recently available-seminal radar place recognition (RPR) and scan
      matching sub-systems are complementary in a style reminiscent of the
      mapping and localisation systems underpinning visual teach-and-repeat
      (VTR) systems which have been exhibited robustly in the last decade.
      Offline experiments are conducted on the most extensive radar-focused
      urban autonomy dataset available to the community with performance
      comparing favourably with and even rivalling alternative state-of-the-art
      radar localisation systems. Specifically, we show the long-term durability
      of the approach and of the sensing technology itself to autonomous
      navigation. We suggest a range of sensible methods of tuning the system,
      all of which are suitable for online operation. For both tuning regimes,
      we achieve, over the course of a month of localisation trials against a
      single static map, high recalls at high precision, and much reduced
      variance in erroneous metric pose estimation. As such, this work is a
      necessary first step towards a radar teach-and-repeat (RTR) system and the
      enablement of autonomy across extreme changes in appearance or inclement
      conditions. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.
SN  - 1424-8220
DO  - 10.3390/s20216002
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094161841&doi=10.3390%2fs20216002&partnerID=40&md5=f4c1d9437039b382f0f7c7990f5edcdf
UR  - http://dx.doi.org/10.3390/s20216002
KW  - autonomous vehicles
KW  - deep learning
KW  - localisation
KW  - mapping
KW  - place recognition
KW  - radar
KW  - air navigation
KW  - frequency modulation
KW  - alternative state
KW  - autonomous navigation
KW  - inclement conditions
KW  - localisation systems
KW  - long term durability
KW  - online operations
KW  - sensing technology
KW  - article
KW  - male
KW  - recall
KW  - telecommunication
N1  - cited By 6
ER  - 

TY  - CPAPER
AU  - Filliat, D
AD  - ENSTA, 32 boulevard Victor, 75015 Paris, France
TI  - A visual bag of words method for interactive qualitative localization and
      mapping
SP  - 3921-3926
PY  - 2007
DA  - 2007
AB  - Localization for low cost humanoid or animal-like personal robots has to
      rely on cheap sensors and has to be robust to user manipulations of the
      robot. We present a visual localization and map-learning system that
      relies on vision only and that is able to incrementally learn to recognize
      the different rooms of an apartment from any robot position. This system
      is inspired by visual categorization algorithms called bag of words
      methods that we modified to make fully incremental and to allow a
      user-interactive training. Our system is able to reliably recognize the
      room in which the robot is after a short training time and is stable for
      long term use. Empirical validation on a real robot and on an image
      database acquired in real environments are presented. © 2007 IEEE.
DO  - 10.1109/ROBOT.2007.364080
C1  - Rome
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348999309&doi=10.1109%2fROBOT.2007.364080&partnerID=40&md5=c563ec3e9406b19170de8d8d5ea13710
UR  - http://dx.doi.org/10.1109/ROBOT.2007.364080
KW  - algorithms
KW  - database systems
KW  - image analysis
KW  - learning systems
KW  - manipulators
KW  - user interfaces
KW  - image databases
KW  - interactive qualitative localization
KW  - real robots
KW  - visual localization
KW  - anthropomorphic robots
N1  - cited By 181; Conference of 2007 IEEE International Conference on Robotics
      and Automation, ICRA'07 ; Conference Date: 10 April 2007 Through 14 April
      2007; Conference Code:70639
ER  - 

TY  - CPAPER
AU  - Opdenbosch, D V
AU  - Aykut, T
AU  - Oelsch, M
AU  - Alt, N
AU  - Steinbach, E
AD  - Department of Media Technology, Technical University of Munich, Germany
TI  - Efficient Map Compression for Collaborative Visual SLAM
VL  - 2018-January
SP  - 992-1000
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Swarm robotics is receiving increasing interest, because the collaborative
      completion of tasks, such as the exploration of unknown environments,
      leads to improved performance and reduced effort. The ability to exchange
      map information is an essential requirement for collaborative exploration.
      When moving to large-scale environments, where the communication data rate
      between the swarm participants is typically limited, efficient compression
      algorithms and an approach for discarding less informative parts of the
      map are key for a successful long-term operation. In this paper, we
      present a novel compression approach for environment maps obtained from a
      visual SLAM system. We apply feature coding to the visual information to
      compress the map efficiently. We make use of a minimum spanning tree to
      connect all features that serve as observations of a single map point.
      Thereby, we can exploit inter-feature dependencies and obtain an optimal
      coding order. Additionally, we add a map sparsification step to keep only
      useful map points by solving a linear integer programming problem, which
      preserves the map points that exhibit both good compression properties and
      high observability. We evaluate the proposed method on a standard dataset
      and show that our approach outperforms state-of-the-art techniques. © 2018
      IEEE.
DO  - 10.1109/WACV.2018.00114
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050991898&doi=10.1109%2fWACV.2018.00114&partnerID=40&md5=2bc76d07f1cc70ca5b98cb894bbb7c9c
UR  - http://dx.doi.org/10.1109/WACV.2018.00114
KW  - integer programming
KW  - robotics
KW  - swarm intelligence
KW  - compression algorithms
KW  - compression approach
KW  - compression properties
KW  - exploration of unknown environments
KW  - inter-feature dependencies
KW  - linear integer programming
KW  - minimum spanning trees
KW  - state-of-the-art techniques
KW  - computer vision
N1  - cited By 10; Conference of 18th IEEE Winter Conference on Applications of
      Computer Vision, WACV 2018 ; Conference Date: 12 March 2018 Through 15
      March 2018; Conference Code:136280
ER  - 

TY  - JOUR
AU  - Derner, E
AU  - Gomez, C
AU  - Hernandez, A C
AU  - Barber, R
AU  - Babuška, R
AD  - Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical
      University in Prague, Czech Republic; Department of Control Engineering,
      Faculty of Electrical Engineering, Czech Technical University in Prague,
      Czech Republic; Robotics Lab, Department of Systems Engineering and
      Automation, Carlos III University of Madrid, Spain; Cognitive Robotics,
      Delft University of Technology, Netherlands
TI  - Change detection using weighted features for image-based localization
T2  - Robotics and Autonomous Systems
VL  - 135
SP  - 103676
PY  - 2021
DA  - 2021
PB  - Elsevier B.V.
AB  - Autonomous mobile robots are becoming increasingly important in many
      industrial and domestic environments. Dealing with unforeseen situations
      is a difficult problem that must be tackled to achieve long-term robot
      autonomy. In vision-based localization and navigation methods, one of the
      major issues is the scene dynamics. The autonomous operation of the robot
      may become unreliable if the changes occurring in dynamic environments are
      not detected and managed. Moving chairs, opening and closing doors or
      windows, replacing objects and other changes make many conventional
      methods fail. To deal with these challenges, we present a novel method for
      change detection based on weighted local visual features. The core idea of
      the algorithm is to distinguish the valuable information in stable regions
      of the scene from the potentially misleading information in the regions
      that are changing. We evaluate the change detection algorithm in a visual
      localization framework based on feature matching by performing a series of
      long-term localization experiments in various real-world environments. The
      results show that the change detection method yields an improvement in the
      localization accuracy, compared to the baseline method without change
      detection. In addition, an experimental evaluation on a public long-term
      localization data set with more than 10000 images reveals that the
      proposed method outperforms two alternative localization methods on images
      recorded several months after the initial mapping. © 2020 Elsevier B.V.
SN  - 0921-8890
DO  - 10.1016/j.robot.2020.103676
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095916049&doi=10.1016%2fj.robot.2020.103676&partnerID=40&md5=83df5997d76e7d54030c7b47d281605f
UR  - http://dx.doi.org/10.1016/j.robot.2020.103676
KW  - change detection
KW  - image-based localization
KW  - long-term autonomy
KW  - mobile robotics
KW  - agricultural robots
KW  - industrial robots
KW  - autonomous mobile robot
KW  - autonomous operations
KW  - change detection algorithms
KW  - experimental evaluation
KW  - image-based localizations
KW  - misleading informations
KW  - real world environments
KW  - vision based localization
KW  - feature extraction
N1  - cited By 2
ER  - 

TY  - CPAPER
AU  - Einhorn, E
AU  - Gross, H-M
AD  - Ilmenau University of Technology, Germany
TI  - Generic 2D/3D SLAM with NDT maps for lifelong application
SP  - 240-247
PY  - 2013
DA  - 2013
PB  - IEEE Computer Society
AB  - In this paper, we present a new, generic approach for Simultaneous
      Localization and Mapping (SLAM). First of all, we propose an abstraction
      of the underlying sensor data using Normal Distribution Transform (NDT)
      maps that are suitable for making our approach independent from the used
      sensor and the dimension of the generated maps. We present some
      modifications for the original NDT mapping to handle free-space
      measurements explicitly and to enable its usage in dynamic environments
      with moving obstacles and persons. In the second part of this paper we
      describe our graph-based SLAM approach that is designed for lifelong
      usage. Therefore, the memory and computational complexity is limited by
      pruning the pose graph in an appropriate way. © 2013 IEEE.
DO  - 10.1109/ECMR.2013.6698849
C1  - Barcelona
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893247078&doi=10.1109%2fECMR.2013.6698849&partnerID=40&md5=ebfcb5183c974f0abbc693fc7dea0282
UR  - http://dx.doi.org/10.1109/ECMR.2013.6698849
KW  - mathematical techniques
KW  - mobile robots
KW  - normal distribution
KW  - sensors
KW  - dynamic environments
KW  - free space measurements
KW  - generic approach
KW  - graph-based
KW  - moving obstacles
KW  - sensor data
KW  - simultaneous localization and mapping
KW  - slam approach
KW  - robotics
N1  - cited By 24; Conference of 2013 6th European Conference on Mobile Robots,
      ECMR 2013 ; Conference Date: 25 September 2013 Through 27 September 2013;
      Conference Code:102443
ER  - 

TY  - JOUR
AU  - Einhorn, E
AU  - Gross, H-M
AD  - Ilmenau University of Technology, Germany
TI  - Generic NDT mapping in dynamic environments and its application for
      lifelong SLAM
T2  - Robotics and Autonomous Systems
VL  - 69
IS  - 1
SP  - 28-39
PY  - 2015
DA  - 2015
PB  - Elsevier B.V.
AB  - In this paper, we present a new, generic approach for Simultaneous
      Localization and Mapping (SLAM). First of all, we propose an abstraction
      of the underlying sensor data using Normal Distribution Transform (NDT)
      maps that are suitable for making our approach independent from the used
      sensor and the dimension of the generated maps. We present several
      modifications for the original NDT mapping to handle free-space
      measurements explicitly. We additionally describe a method to detect and
      handle dynamic objects such as moving persons. This enables the usage of
      the proposed approach in highly dynamic environments. In the second part
      of this paper we describe our graph-based SLAM approach that is designed
      for lifelong usage. Therefore, the memory and computational complexity is
      limited by pruning the pose graph in an appropriate way. © 2014 Elsevier
      B.V. All rights reserved.
SN  - 0921-8890
DO  - 10.1016/j.robot.2014.08.008
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933278205&doi=10.1016%2fj.robot.2014.08.008&partnerID=40&md5=ca03d93bc7621dee02a22f8ac3cab631
UR  - http://dx.doi.org/10.1016/j.robot.2014.08.008
KW  - 2d and 3d mapping
KW  - detection and tracking of moving objects
KW  - lifelong slam
KW  - map registration
KW  - mobile robots
KW  - normal distribution transform
KW  - occupancy mapping
KW  - graphic methods
KW  - normal distribution
KW  - object detection
KW  - robotics
KW  - 3-d mapping
KW  - dynamic environments
KW  - free space measurements
KW  - generic approach
KW  - its applications
KW  - simultaneous localization and mapping
KW  - mapping
N1  - cited By 31
ER  - 

TY  - JOUR
AU  - Boniardi, F
AU  - Caselitz, T
AU  - Kümmerle, R
AU  - Burgard, W
AD  - University of Freiburg, Georges-Köhler-Allee 80, Freiburg i. Br., 79110,
      Germany; KUKA, Zugspitzstraße 140, Augsburg, 86165, Germany
TI  - A pose graph-based localization system for long-term navigation in CAD
      floor plans
T2  - Robotics and Autonomous Systems
VL  - 112
SP  - 84-97
PY  - 2019
DA  - 2019
PB  - Elsevier B.V.
AB  - Accurate localization is an essential technology for flexible automation.
      Industrial applications require mobile platforms to be precisely localized
      in complex environments, often subject to continuous changes and
      reconfiguration. Most of the approaches use precomputed maps both for
      localization and for interfacing robots with workers and operators. This
      results in increased deployment time and costs as mapping experts are
      required to setup the robotic systems in factory facilities. Moreover,
      such maps need to be updated whenever significant changes in the
      environment occur in order to be usable within commanding tools. To
      overcome those limitations, in this work we present a robust and highly
      accurate method for long-term LiDAR-based indoor localization that uses
      CAD-based architectural floor plans. The system leverages a combination of
      graph-based mapping techniques and Bayes filtering to maintain a sparse
      and up-to-date globally consistent map that represents the latest
      configuration of the environment. This map is aligned to the CAD drawing
      using prior constraints and is exploited for relative localization, thus
      allowing the robot to estimate its current pose with respect to the global
      reference frame of the floor plan. Furthermore, the map helps in limiting
      the disturbances caused by structures and clutter not represented in the
      drawing. Several long-term experiments in changing real-world environments
      show that our system outperforms common state-of-the-art localization
      methods in terms of accuracy and robustness while remaining memory and
      computationally efficient. © 2018 Elsevier B.V.
SN  - 0921-8890
DO  - 10.1016/j.robot.2018.11.003
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057496266&doi=10.1016%2fj.robot.2018.11.003&partnerID=40&md5=4257f50d17c298217cd42b7eacaabf4a
UR  - http://dx.doi.org/10.1016/j.robot.2018.11.003
KW  - adaptive systems
KW  - localization
KW  - mapping
KW  - mobile robotics
KW  - slam
KW  - computer aided design
KW  - floors
KW  - robotics
KW  - robots
KW  - architectural floor plans
KW  - computationally efficient
KW  - long-term experiments
KW  - mobile robotic
KW  - real world environments
KW  - relative localization
KW  - indoor positioning systems
N1  - cited By 21
ER  - 

TY  - JOUR
AU  - Cao, F
AU  - Yan, F
AU  - Wang, S
AU  - Zhuang, Y
AU  - Wang, W
TI  - Season-Invariant and Viewpoint-Tolerant LiDAR Place Recognition in
      GPS-Denied Environments
T2  - IEEE Transactions on Industrial Electronics
VL  - 68
IS  - 1
SP  - 563-574
PY  - 2021
DA  - 2021
CY  - USA
AB  - Place recognition remains a challenging problem under various perceptual
      conditions, e.g., all weather, times of day, seasons, and viewpoint
      shifts. Different from most of the existing place recognition methods
      using pure vision, this article studies light detection and ranging
      (LiDAR) based approaches. Point clouds have some benefits for place
      recognition since they do not suffer from illumination changes. On the
      other hand, they are dramatically affected by structural changes from
      different viewpoints or across seasons. In this article, a novel
      LiDAR-based place recognition system is proposed to achieve long-term
      robust localization, even under severe seasonal changes and viewpoint
      shifts. To improve the efficiency, a compact cylindrical image model is
      designed to convert three-dimensional point clouds to two-dimensional
      images representing the prominent geometric relationships of scenes. The
      contexts (buildings, trees, road structures, etc.) of scenes are utilized
      for efficient place recognition. A sequence-based temporal consistency
      check is also introduced for postverification. Extensive real experiments
      on three datasets (Oxford RobotCar [1], NCLT [2], and DUT-AS) show that
      the proposed system outperforms both state-of-the-art visual and
      LiDAR-based methods, verifying its robust performance in challenging
      scenarios.
SN  - 0278-0046
DO  - 10.1109/TIE.2019.2962416
UR  - http://dx.doi.org/10.1109/TIE.2019.2962416
KW  - feature extraction
KW  - image classification
KW  - image sensors
KW  - mobile robots
KW  - object recognition
KW  - optical radar
KW  - robot vision
KW  - slam (robots)
KW  - visual databases
N1  - LiDAR-based methods;sequence-based temporal consistency check;efficient
      place recognition;three-dimensional point clouds;compact cylindrical image
      model;severe seasonal changes;long-term robust localization;novel
      LiDAR-based place recognition system;structural changes;illumination
      changes;article studies light detection;existing place recognition
      methods;viewpoint shifts;GPS-denied environments;viewpoint-tolerant LiDAR
      place recognition;season-invariant;
ER  - 

TY  - JOUR
AU  - Cao, F
AU  - Zhuang, Y
AU  - Zhang, H
AU  - Wang, W
AD  - School of Control Science and Engineering, Dalian University of
      Technology, Dalian, 116024, China; Department of Computing Science,
      University of Alberta, Edmonton, AB T6G 2R3, Canada; Research Center of
      Information and Control, Dalian University of Technology, Dalian, 116024,
      China
TI  - Robust Place Recognition and Loop Closing in Laser-Based SLAM for UGVs in
      Urban Environments
T2  - IEEE Sensors Journal
VL  - 18
IS  - 10
SP  - 4242-4252
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Robust place recognition plays a key role for the long-term autonomy of
      unmanned ground vehicles (UGVs) working in indoor or outdoor environments.
      Although most of the state-of-the-art that approaches for place
      recognition are vision-based, visual sensors lack adaptability in
      environments with poor or dynamically changing illumination. In this
      paper, a 3-D-laser-based place recognition algorithm is proposed to
      accomplish loop closure detection for simultaneous localization and
      mapping. An image model named bearing angle (BA) is adopted to convert 3-D
      laser points to 2-D images, and then ORB features extracted from BA images
      are utilized to perform scene matching. Since the computational cost for
      matching a query BA image with all the BA images in a database is too high
      to meet the requirement of performing real-time place recognition, a
      visual bag of words approach is used to improve search efficiency.
      Furthermore, a speed normalization algorithm and a 3-D geometry-based
      verification algorithm are proposed to complete the proposed place
      recognition algorithm. Experiments were conducted on two self-developed
      UGV platforms to verify the performance of the proposed method. ©
      2001-2012 IEEE.
SN  - 1530-437X
DO  - 10.1109/JSEN.2018.2815956
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043760869&doi=10.1109%2fJSEN.2018.2815956&partnerID=40&md5=8c1971c247d620b88401adc5e757a97f
UR  - http://dx.doi.org/10.1109/JSEN.2018.2815956
KW  - laser scanning
KW  - place recognition
KW  - simultaneous localization and mapping (slam)
KW  - unmanned ground vehicles (ugvs)
KW  - ground vehicles
KW  - image enhancement
KW  - indoor positioning systems
KW  - query processing
KW  - robotics
KW  - computational costs
KW  - normalization algorithms
KW  - simultaneous localization and mapping
KW  - state-of-the-art approach
KW  - unmanned ground vehicles
KW  - verification algorithms
KW  - intelligent vehicle highway systems
N1  - cited By 25
ER  - 

TY  - JOUR
AU  - Dayoub, F
AU  - Cielniak, G
AU  - Duckett, T
AD  - School of Computer Science, University of Lincoln, LN6 7TS Lincoln, United
      Kingdom
TI  - Long-term experiments with an adaptive spherical view representation for
      navigation in changing environments
T2  - Robotics and Autonomous Systems
VL  - 59
IS  - 5
SP  - 285-295
PY  - 2011
DA  - 2011
AB  - Real-world environments such as houses and offices change over time,
      meaning that a mobile robot's map will become out of date. In this work,
      we introduce a method to update the reference views in a hybrid
      metric-topological map so that a mobile robot can continue to localize
      itself in a changing environment. The updating mechanism, based on the
      multi-store model of human memory, incorporates a spherical metric
      representation of the observed visual features for each node in the map,
      which enables the robot to estimate its heading and navigate using
      multi-view geometry, as well as representing the local 3D geometry of the
      environment. A series of experiments demonstrate the persistence
      performance of the proposed system in real changing environments,
      including analysis of the long-term stability. © 2011 Elsevier B.V. All
      rights reserved.
SN  - 0921-8890
DO  - 10.1016/j.robot.2011.02.013
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955627458&doi=10.1016%2fj.robot.2011.02.013&partnerID=40&md5=bcc7120431a0b179391f91aa1f14307c
UR  - http://dx.doi.org/10.1016/j.robot.2011.02.013
KW  - mobile robot navigation
KW  - omnidirectional vision
KW  - persistent mapping
KW  - 3d geometry
KW  - changing environment
KW  - human memory
KW  - long term stability
KW  - long-term experiments
KW  - multi-view geometry
KW  - real world environments
KW  - topological map
KW  - visual feature
KW  - experiments
KW  - mobile robots
KW  - multi agent systems
KW  - navigation systems
KW  - three dimensional
KW  - navigation
N1  - cited By 44
ER  - 

TY  - JOUR
AU  - Han, F
AU  - Wang, H
AU  - Huang, G
AU  - Zhang, H
AD  - Department of Computer Science, Colorado School of Mines, Golden, CO
      80401, United States; Department of Mechanical Engineering, University of
      Delaware, Newark, DE 19716, United States
TI  - Sequence-based sparse optimization methods for long-term loop closure
      detection in visual SLAM
T2  - Autonomous Robots
VL  - 42
IS  - 7
SP  - 1323-1335
PY  - 2018
DA  - 2018
PB  - Springer New York LLC
AB  - Loop closure detection is one of the most important module in
      Simultaneously Localization and Mapping (SLAM) because it enables to find
      the global topology among different places. A loop closure is detected
      when the current place is recognized to match the previous visited places.
      When the SLAM is executed throughout a long-term period, there will be
      additional challenges for the loop closure detection. The illumination,
      weather, and vegetation conditions can often change significantly during
      the life-long SLAM, resulting in the critical strong perceptual aliasing
      and appearance variation problems in loop closure detection. In order to
      address this problem, we propose a new Robust Multimodal Sequence-based
      (ROMS) method for robust loop closure detection in long-term visual SLAM.
      A sequence of images is used as the representation of places in our ROMS
      method, where each image in the sequence is encoded by multiple feature
      modalites so that different places can be recognized discriminatively. We
      formulate the robust place recognition problem as a convex optimization
      problem with structured sparsity regularization due to the fact that only
      a small set of template places can match the query place. In addition, we
      also develop a new algorithm to solve the formulated optimization problem
      efficiently, which guarantees to converge to the global optima
      theoretically. Our ROMS method is evaluated through extensive experiments
      on three large-scale benchmark datasets, which record scenes ranging from
      different times of the day, months, and seasons. Experimental results
      demonstrate that our ROMS method outperforms the existing loop closure
      detection methods in long-term SLAM, and achieves the state-of-the-art
      performance. © 2018, Springer Science+Business Media, LLC, part of
      Springer Nature.
SN  - 0929-5593
DO  - 10.1007/s10514-018-9736-3
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045744764&doi=10.1007%2fs10514-018-9736-3&partnerID=40&md5=b781b775b2bd25ecfd433c09429b9718
UR  - http://dx.doi.org/10.1007/s10514-018-9736-3
KW  - long-term autonomy
KW  - long-term place recognition
KW  - loop closure detection
KW  - visual slam
KW  - artificial intelligence
KW  - robots
KW  - convex optimization problems
KW  - localization and mappings
KW  - loop closure
KW  - place recognition
KW  - state-of-the-art performance
KW  - structured sparsities
KW  - convex optimization
N1  - cited By 15
ER  - 

TY  - JOUR
AU  - Han, F
AU  - Beleidy, S E
AU  - Wang, H
AU  - Ye, C
AU  - Zhang, H
AD  - Department of Computer Science, Colorado School of Mines, Golden, CO
      80401, United States; Department of Computer Science, Virginia
      Commonwealth University, Richmond, VA 23284, United States
TI  - Learning of Holism-Landmark graph embedding for place recognition in
      Long-Term autonomy
T2  - IEEE Robotics and Automation Letters
VL  - 3
IS  - 4
SP  - 3669-3676
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Place recognition plays an important role to perform loop closure
      detection of large-scale, long-term simultaneous localization and mapping
      in loopy environments. The long-term place recognition problem is
      challenging because the environment appearance exhibits significant
      long-term variations across various times of the day, months, and seasons.
      In this letter, we introduce a novel place representation approach that
      simultaneously integrates semantic landmarks and holistic information to
      achieve place recognition in long-term autonomy. First, a graph is
      constructed for each place. The graph nodes encode all landmarks and the
      holistic image of the place scene recorded in different scenarios. The
      edges connecting the nodes indicate that these nodes represent the same
      landmark or place, even though places and landmarks encoded by the nodes
      may exhibit different appearances in the long-term periods. Then, a graph
      embedding is learned to preserve the locality in the feature descriptor
      space, i.e., finding a projection such that the same landmark and place
      have the identical representation in the new projected descriptor space,
      no matter in what scenarios they are recorded. We formulate the embedding
      learning as an optimization problem and implement a new solver that
      provides a theoretical convergence guarantee. Extensive evaluations are
      conducted using large-scale benchmark datasets of place recognition in
      long-term autonomy, which has shown our approach's promising performance.
      © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2018.2856274
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063309114&doi=10.1109%2fLRA.2018.2856274&partnerID=40&md5=2308c44339df12a6013c88ec92d15094
UR  - http://dx.doi.org/10.1109/LRA.2018.2856274
KW  - localization
KW  - recognition
KW  - slam
KW  - visual learning
KW  - benchmarking
KW  - edge detection
KW  - mapping
KW  - optimization
KW  - robustness (control systems)
KW  - semantics
KW  - convergence
KW  - image edge detection
KW  - simultaneous localization and mapping
KW  - robotics
N1  - cited By 5
ER  - 

TY  - JOUR
AU  - Han, F
AU  - Yang, X
AU  - Deng, Y
AU  - Rentschler, M
AU  - Yang, D
AU  - Zhang, H
AD  - Division of Computer Science, Colorado School of Mines, GoldenCO 80401,
      United States; Department of Electrical and Computer Engineering, Michigan
      State University, East Lansing, MI 48824, United States; Department of
      Mechanical Engineering, University of Colorado BoulderCO 80309, United
      States
TI  - SRAL: Shared Representative Appearance Learning for Long-Term Visual Place
      Recognition
T2  - IEEE Robotics and Automation Letters
VL  - 2
IS  - 2
SP  - 1172-1179
PY  - 2017
DA  - 2017
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Place recognition, or loop closure detection, is an essential component to
      address the problem of visual simultaneous localization and mapping
      (SLAM). Long-term navigation of robots in outdoor environments introduces
      new challenges to enable life-long SLAM, including the strong appearance
      change resulting from vegetation, weather, and illumination variations
      across various times of the day, different days, months, or even seasons.
      In this paper, we propose a new shared representative appearance learning
      (SRAL) approach to address long-term visual place recognition. Different
      from previous methods using a single feature modality or a concatenation
      of multiple features, our SRAL method autonomously learns representative
      features that are shared in all scene scenarios, and then fuses the
      features together to represent the long-term appearance of environments
      observed by a robot during life-long navigation. By formulating SRAL as a
      regularized optimization problem, we use structured sparsity-inducing
      norms to model interrelationships of feature modalities. In addition, an
      optimization algorithm is developed to efficiently solve the formulated
      optimization problem, which holds a theoretical convergence guarantee.
      Extensive empirical study was performed to evaluate the SRAL method using
      large-scale benchmark datasets, including St Lucia, CMU-VL, and Nordland
      datasets. Experimental results have shown that our SRAL method obtains
      superior performance for life-long place recognition using individual
      images, outperforms previous single image-based methods, and is capable of
      estimating the importance of feature modalities. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2017.2662061
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050632561&doi=10.1109%2fLRA.2017.2662061&partnerID=40&md5=988aa6c2178d0ccd73bdcd235fd8505f
UR  - http://dx.doi.org/10.1109/LRA.2017.2662061
KW  - long-term place recognition
KW  - loop closure detection
KW  - simultaneous localization and mapping (slam)
KW  - visual learning
KW  - mapping
KW  - optimization
KW  - robotics
KW  - robots
KW  - illumination variation
KW  - loop closure
KW  - optimization algorithms
KW  - place recognition
KW  - regularized optimization problems
KW  - simultaneous localization and mapping
KW  - visual simultaneous localization and mappings
KW  - optical character recognition
N1  - cited By 34
ER  - 

TY  - CPAPER
AU  - Nobre, F
AU  - Heckman, C
AU  - Ozog, P
AU  - Wolcott, R W
AU  - Walls, J M
AD  - Boulder Autonomous Robotics and Perception Group, University of Colorado,
      United States; Toyota Research Institute, Japan
TI  - Online probabilistic change detection in feature-based maps
SP  - 3661-3668
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Sparse feature-based maps provide a compact representation of the
      environment that admit efficient algorithms, for example simultaneous
      localization and mapping. These representations typically assume a static
      world and therefore contain static map features. However, since the world
      contains dynamic elements, determining when map features no longer
      correspond to the environment is essential for long-term utility. This
      work develops a feature-based model of the environment which evolves over
      time through feature persistence. Moreover, we augment the
      state-of-the-art sparse mapping model with a correlative structure that
      captures spatio-temporal properties, e.g. that nearby features frequently
      have similar persistence. We show that such relationships, typically
      addressed through an ad hoc formalism focusing only on feature
      repeatability, are crucial to evaluate through a probabilistically
      principled approach. The joint posterior over feature persistence can be
      computed efficiently and used to improve online data association decisions
      for localization. The proposed algorithms are validated in numerical
      simulation and using publicly available data sets. © 2018 IEEE.
DO  - 10.1109/ICRA.2018.8461111
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063135084&doi=10.1109%2fICRA.2018.8461111&partnerID=40&md5=ab5d2b6619fbe3e96c1f67afb0b970de
UR  - http://dx.doi.org/10.1109/ICRA.2018.8461111
KW  - mapping
KW  - change detection
KW  - compact representation
KW  - dynamic elements
KW  - feature based modeling
KW  - simultaneous localization and mapping
KW  - sparse features
KW  - spatio-temporal properties
KW  - state of the art
KW  - robotics
N1  - cited By 6; Conference of 2018 IEEE International Conference on Robotics
      and Automation, ICRA 2018 ; Conference Date: 21 May 2018 Through 25 May
      2018; Conference Code:139796
ER  - 

TY  - CPAPER
AU  - Pomerleau, F
AU  - Krüsi, P
AU  - Colas, F
AU  - Furgale, P
AU  - Siegwart, R
AD  - Autonomous Systems Lab, ETH Zurich, Zurich, 8092, Switzerland
TI  - Long-term 3D map maintenance in dynamic environments
SP  - 3712-3719
PY  - 2014
DA  - 2014
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - New applications of mobile robotics in dynamic urban areas require more
      than the single-session geometric maps that have dominated simultaneous
      localization and mapping (SLAM) research to date; maps must be updated as
      the environment changes and include a semantic layer (such as road network
      information) to aid motion planning in dynamic environments. We present an
      algorithm for long-term localization and mapping in real time using a
      three-dimensional (3D) laser scanner. The system infers the static or
      dynamic state of each 3D point in the environment based on repeated
      observations. The velocity of each dynamic point is estimated without
      requiring object models or explicit clustering of the points. At any time,
      the system is able to produce a most-likely representation of underlying
      static scene geometry. By storing the time history of velocities, we can
      infer the dominant motion patterns within the map. The result is an online
      mapping and localization system specifically designed to enable long-term
      autonomy within highly dynamic environments. We validate the approach
      using data collected around the campus of ETH Zurich over seven months and
      several kilometers of navigation. To the best of our knowledge, this is
      the first work to unify long-term map update with tracking of dynamic
      objects. © 2014 IEEE.
DO  - 10.1109/ICRA.2014.6907397
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929207776&doi=10.1109%2fICRA.2014.6907397&partnerID=40&md5=a1da6d69b80262e555a2c0cb57af3c43
UR  - http://dx.doi.org/10.1109/ICRA.2014.6907397
KW  - dynamic obstacles
KW  - icp
KW  - kd-tree
KW  - long-term mapping
KW  - registration
KW  - robot
KW  - scan matching
KW  - slam
KW  - air navigation
KW  - collision avoidance
KW  - mapping
KW  - motion planning
KW  - robotics
KW  - robots
KW  - semantics
KW  - urban planning
KW  - k-d tree
KW  - robot programming
N1  - cited By 79; Conference of 2014 IEEE International Conference on Robotics
      and Automation, ICRA 2014 ; Conference Date: 31 May 2014 Through 7 June
      2014; Conference Code:107395
ER  - 

TY  - JOUR
AU  - Tipaldi, G D
AU  - Meyer-Delius, D
AU  - Burgard, W
AD  - Department of Computer Science, University of Freiburg, Freiburg, Germany;
      KUKA Laboratories GmbH, Augsburg, Germany
TI  - Lifelong localization in changing environments
T2  - International Journal of Robotics Research
VL  - 32
IS  - 14
SP  - 1662-1678
PY  - 2013
DA  - 2013
AB  - Robot localization systems typically assume that the environment is
      static, ignoring the dynamics inherent in most real-world settings.
      Corresponding scenarios include households, offices, warehouses and
      parking lots, where the location of certain objects such as goods,
      furniture or cars can change over time. These changes typically lead to
      inconsistent observations with respect to previously learned maps and thus
      decrease the localization accuracy or even prevent the robot from globally
      localizing itself. In this paper we present a sound probabilistic approach
      to lifelong localization in changing environments using a combination of a
      Rao-Blackwellized particle filter with a hidden Markov model. By
      exploiting several properties of this model, we obtain a highly efficient
      map management approach for dynamic environments, which makes it feasible
      to run our algorithm online. Extensive experiments with a real robot in a
      dynamically changing environment demonstrate that our algorithm reliably
      adapts to changes in the environment and also outperforms the popular
      Monte-Carlo localization approach. © The Author(s) 2013.
SN  - 0278-3649
DO  - 10.1177/0278364913502830
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892620468&doi=10.1177%2f0278364913502830&partnerID=40&md5=32b07f9c5be711731c83f908f4ca52fa
UR  - http://dx.doi.org/10.1177/0278364913502830
KW  - cognitive robotics
KW  - learning and adaptive systems
KW  - localization
KW  - mapping
KW  - mobile and distributed robotics slam
KW  - learning and adaptive system
KW  - localization accuracy
KW  - monte carlo localization
KW  - probabilistic approaches
KW  - rao-blackwellized particle filter
KW  - algorithms
KW  - hidden markov models
KW  - robot applications
KW  - robotics
N1  - cited By 66
ER  - 

TY  - CPAPER
AU  - Huang, G
AU  - Kaess, M
AU  - Leonard, J J
AD  - Computer Science and Artificial Intelligence Laboratory, Massachusetts
      Institute of Technology, Cambridge, MA 02139, United States
TI  - Consistent sparsification for graph optimization
SP  - 150-157
PY  - 2013
DA  - 2013
PB  - IEEE Computer Society
AB  - In a standard pose-graph formulation of simultaneous localization and
      mapping (SLAM), due to the continuously increasing numbers of nodes
      (states) and edges (measurements), the graph may grow prohibitively too
      large for long-term navigation. This motivates us to systematically reduce
      the pose graph amenable to available processing and memory resources. In
      particular, in this paper we introduce a consistent graph sparsification
      scheme: i) sparsifying nodes via marginalization of old nodes, while
      retaining all the information (consistent relative constraints) - which is
      conveyed in the discarded measurements - about the remaining nodes after
      marginalization; and ii) sparsifying edges by formulating and solving a
      consistent ℓ1-regularized minimization problem, which automatically
      promotes the sparsity of the graph. The proposed approach is validated on
      both synthetic and real data. © 2013 IEEE.
DO  - 10.1109/ECMR.2013.6698835
C1  - Barcelona
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893301601&doi=10.1109%2fECMR.2013.6698835&partnerID=40&md5=d3ed7f443662be6219eb636619411593
UR  - http://dx.doi.org/10.1109/ECMR.2013.6698835
KW  - mathematical techniques
KW  - mobile robots
KW  - robotics
KW  - graph optimization
KW  - graph sparsification
KW  - marginalization
KW  - memory resources
KW  - minimization problems
KW  - simultaneous localization and mapping
KW  - sparsification
KW  - synthetic and real data
KW  - graph theory
N1  - cited By 51; Conference of 2013 6th European Conference on Mobile Robots,
      ECMR 2013 ; Conference Date: 25 September 2013 Through 27 September 2013;
      Conference Code:102443
ER  - 

TY  - JOUR
AU  - Kim, G
AU  - Park, B
AU  - Kim, A
AD  - Department of Civil and Environmental Engineering, KAIST, Daejeon, 34141,
      South Korea; Intelligent Robot System Research Group, ETRI, Daejeon,
      34129, South Korea
TI  - 1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using
      Scan Context Image
T2  - IEEE Robotics and Automation Letters
VL  - 4
IS  - 2
SP  - 1948-1955
PY  - 2019
DA  - 2019
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - In this letter, we present a long-term localization method that
      effectively exploits the structural information of an environment via an
      image format. The proposed method presents a robust year-round
      localization performance even when learned in just a single day. The
      proposed localizer learns a point cloud descriptor, named Scan Context
      Image (SCI), and performs robot localization on a grid map by formulating
      the place recognition problem as place classification using a
      convolutional neural network. Our method is faster than existing methods
      proposed for place recognition because it avoids a pairwise comparison
      between a query and scans in a database. In addition, we provide thorough
      validations using publicly available long-term datasets, the NCLT dataset
      and the Oxford RobotCar dataset, and show that the Scan Context Image
      (SCI) localization attains consistent performance over a year and
      outperforms existing methods. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2019.2897340
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062621439&doi=10.1109%2fLRA.2019.2897340&partnerID=40&md5=f984d40ba54e543247d76c41300d14fd
UR  - http://dx.doi.org/10.1109/LRA.2019.2897340
KW  - localization
KW  - range sensing
KW  - slam
KW  - neural networks
KW  - robot applications
KW  - consistent performance
KW  - convolutional neural network
KW  - localization performance
KW  - pair-wise comparison
KW  - structural information
KW  - query processing
N1  - cited By 47
ER  - 

TY  - CPAPER
AU  - Kurz, G
AU  - Holoch, M
AU  - Biber, P
AD  - Corporate Research, Robert Bosch GmbH, Germany
TI  - Geometry-based Graph Pruning for Lifelong SLAM
SP  - 3313-3320
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Lifelong SLAM considers long-term operation of a robot where already
      mapped locations are revisited many times in changing environments. As a
      result, traditional graph-based SLAM approaches eventually become
      extremely slow due to the continuous growth of the graph and the loss of
      sparsity. Both problems can be addressed by a graph pruning algorithm. It
      carefully removes vertices and edges to keep the graph size reasonable
      while preserving the information needed to provide good SLAM results. We
      propose a novel method that considers geometric criteria for choosing the
      vertices to be pruned. It is efficient, easy to implement, and leads to a
      graph with evenly spread vertices that remain part of the robot
      trajectory. Furthermore, we present a novel approach of marginalization
      that is more robust to wrong loop closures than existing methods. The
      proposed algorithm is evaluated on two publicly available real-world
      long-term datasets and compared to the unpruned case as well as ground
      truth. We show that even on a long dataset (25h), our approach manages to
      keep the graph sparse and the speed high while still providing good
      accuracy (40 times speed up, 6cm map error compared to unpruned case). ©
      2021 IEEE.
DO  - 10.1109/IROS51168.2021.9636530
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124373627&doi=10.1109%2fIROS51168.2021.9636530&partnerID=40&md5=102902e7b40ce9738cb94735d6e987a0
UR  - http://dx.doi.org/10.1109/IROS51168.2021.9636530
KW  - graph theory
KW  - robotics
KW  - changing environment
KW  - graph sizes
KW  - graph-based
KW  - loop closure
KW  - marginalization
KW  - novel methods
KW  - pruning algorithms
KW  - real-world
KW  - robot trajectory
KW  - slam approach
KW  - graphic methods
N1  - cited By 0; Conference of 2021 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2021 ; Conference Date: 27 September
      2021 Through 1 October 2021; Conference Code:175617
ER  - 

TY  - CPAPER
AU  - Singh, G
AU  - Wu, M
AU  - Lam, S-K
AU  - Minh, D V
TI  - Hierarchical Loop Closure Detection for Long-term Visual SLAM with
      Semantic-Geometric Descriptors
VL  - 2021-September
SP  - 2909-2916
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Modern visual Simultaneous Localization and Mapping (SLAM) systems rely on
      loop closure detection methods for correcting drifts in maps and poses.
      Existing loop closure detection methods mainly employ conventional feature
      descriptors to create vocabulary for describing places using bag-of-words
      (BOW). Such methods do not perform well in long-term SLAM applications as
      the scene content may change over time due to the presence of dynamic
      objects, even though the locations are revisited with the same viewpoint.
      This work enhances the loop closure detection capability of long-term
      visual SLAM by reducing the number of false matches through the use of
      location semantics. We extend a semantic visual SLAM framework to build
      compact global semantic-geometric location descriptors and local semantic
      vocabulary trees, by leveraging on the already available features and
      semantics. The local semantic vocabulary trees support incremental
      vocabulary learning, which is well-suited for long-term SLAM scenarios
      where the scenes encountered are not known beforehand. A novel
      hierarchical place recognition method that leverages the global and local
      location semantics is proposed to enable fast and accurate loop closure
      detection. The proposed method outperforms recent state-of-the-art methods
      (i.e., FABMAP2, SeqSLAM, iBOW-LCD, and HTMap) on all datasets considered
      (i.e., KITTI, Synthia, and CBD), with highest loop closure detection
      accuracy and lowest query time. © 2021 IEEE.
DO  - 10.1109/ITSC48978.2021.9564866
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118448425&doi=10.1109%2fITSC48978.2021.9564866&partnerID=40&md5=b3c9ea9859b2444fbc603add6249ccb1
UR  - http://dx.doi.org/10.1109/ITSC48978.2021.9564866
KW  - forestry
KW  - location
KW  - robotics
KW  - detection methods
KW  - feature descriptors
KW  - geometric descriptor
KW  - local semantics
KW  - localisation systems
KW  - loop closure
KW  - mapping systems
KW  - simultaneous localization and mapping
KW  - visual simultaneous localization and mappings
KW  - vocabulary tree
KW  - semantics
N1  - cited By 0; Conference of 2021 IEEE International Intelligent
      Transportation Systems Conference, ITSC 2021 ; Conference Date: 19
      September 2021 Through 22 September 2021; Conference Code:173112
ER  - 

TY  - JOUR
AU  - Hu, H
AU  - Wang, H
AU  - Liu, Z
AU  - Chen, W
AD  - Shanghai Jiao Tong University, Department of Automation, Shanghai, 200240,
      China; Institute of Medical Robotics, Key Laboratory of System Control and
      Information Processing, Ministry of Education, Key Laboratory of Marine
      Intelligent Equipment and System of Ministry of Education, Shanghai Jiao
      Tong University, Department of Automation, Shanghai, 200240, China;
      University of Cambridge, Department of Computer Science and Technology,
      Cambridge, CB3 0FD, United Kingdom
TI  - Domain-Invariant Similarity Activation Map Contrastive Learning for
      Retrieval-Based Long-Term Visual Localization
T2  - IEEE/CAA Journal of Automatica Sinica
VL  - 9
IS  - 2
SP  - 313-328
PY  - 2022
DA  - 2022
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Visual localization is a crucial component in the application of mobile
      robot and autonomous driving. Image retrieval is an efficient and
      effective technique in image-based localization methods. Due to the
      drastic variability of environmental conditions, e.g., illumination
      changes, retrieval-based visual localization is severely affected and
      becomes a challenging problem. In this work, a general architecture is
      first formulated probabilistically to extract domain-invariant features
      through multi-domain image translation. Then, a novel gradient-weighted
      similarity activation mapping loss (Grad-SAM) is incorporated for finer
      localization with high accuracy. We also propose a new adaptive triplet
      loss to boost the contrastive learning of the embedding in a
      self-supervised manner. The final coarse-to-fine image retrieval pipeline
      is implemented as the sequential combination of models with and without
      Grad-SAM loss. Extensive experiments have been conducted to validate the
      effectiveness of the proposed approach on the CMU-Seasons dataset. The
      strong generalization ability of our approach is verified with the
      RobotCar dataset using models pre-trained on urban parts of the
      CMU-Seasons dataset. Our performance is on par with or even outperforms
      the state-of-the-art image-based localization baselines in medium or high
      precision, especially under challenging environments with illumination
      variance, vegetation, and night-time images. Moreover, real-site
      experiments have been conducted to validate the efficiency and
      effectiveness of the coarse-to-fine strategy for localization. © 2014
      Chinese Association of Automation.
SN  - 2329-9266
DO  - 10.1109/JAS.2021.1003907
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101742423&doi=10.1109%2fJAS.2021.1003907&partnerID=40&md5=aa955cfc805ad33f1d0b31589541d169
UR  - http://dx.doi.org/10.1109/JAS.2021.1003907
KW  - deep representation learning
KW  - place recognition
KW  - visual localization
KW  - chemical activation
KW  - coarse-to-fine strategy
KW  - environmental conditions
KW  - general architectures
KW  - generalization ability
KW  - illumination changes
KW  - image-based localizations
KW  - sequential combination
KW  - image retrieval
N1  - cited By 1
ER  - 

TY  - CPAPER
AU  - Johannsson, H
AU  - Kaess, M
AU  - Fallon, M
AU  - Leonard, J J
AD  - Computer Science and Artificial Intelligence Laboratory, Massachusetts
      Institute of Technology, Cambridge, United States
TI  - Temporally scalable visual SLAM using a reduced pose graph
SP  - 54-61
PY  - 2013
DA  - 2013
AB  - In this paper, we demonstrate a system for temporally scalable visual SLAM
      using a reduced pose graph representation. Unlike previous visual SLAM
      approaches that maintain static keyframes, our approach uses new
      measurements to continually improve the map, yet achieves efficiency by
      avoiding adding redundant frames and not using marginalization to reduce
      the graph. To evaluate our approach, we present results using an online
      binocular visual SLAM system that uses place recognition for both
      robustness and multi-session operation. Additionally, to enable
      large-scale indoor mapping, our system automatically detects elevator
      rides based on accelerometer data. We demonstrate long-term mapping in a
      large multi-floor building, using approximately nine hours of data
      collected over the course of six months. Our results illustrate the
      capability of our visual SLAM system to map a large are over extended
      period of time. © 2013 IEEE.
DO  - 10.1109/ICRA.2013.6630556
C1  - Karlsruhe
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887298986&doi=10.1109%2fICRA.2013.6630556&partnerID=40&md5=a677df3b28e54abbd5e76f1624d20411
UR  - http://dx.doi.org/10.1109/ICRA.2013.6630556
KW  - accelerometer data
KW  - graph representation
KW  - key-frames
KW  - marginalization
KW  - place recognition
KW  - visual slam
KW  - robotics
N1  - cited By 75; Conference of 2013 IEEE International Conference on Robotics
      and Automation, ICRA 2013 ; Conference Date: 6 May 2013 Through 10 May
      2013; Conference Code:100673
ER  - 

TY  - JOUR
AU  - Karaoguz, H
AU  - Bozma, H I
AD  - Karaoguz, H (Corresponding Author), Bogazici Univ, Intelligent Syst Lab,
      Elect & Elect Engn, Istanbul, Turkey. Karaoguz, Hakan; Bozma, H. Isil,
      Bogazici Univ, Intelligent Syst Lab, Elect & Elect Engn, Istanbul, Turkey.
TI  - An integrated model of autonomous topological spatial cognition
T2  - AUTONOMOUS ROBOTS
VL  - 40
IS  - 8
SP  - 1379-1402
PY  - 2016
DA  - 2016
PB  - SPRINGER
CY  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
AB  - This paper is focused on endowing a mobile robot with topological spatial
      cognition. We propose an integrated model-where the concept of a `place'
      is defined as a collection of appearances or locations sharing common
      perceptual signatures or physical boundaries. In this model, as the robot
      navigates, places are detected in a systematic manner via monitoring
      coherency in the incoming visual data while pruning out uninformative or
      scanty data. Detected places are then either recognized or learned along
      with mapping as necessary. The novelties of the model are twofold: First,
      it explicitly incorporates a long-term spatial memory where the knowledge
      of learned places and their spatial relations are retained in place and
      map memories respectively. Second, the processing modules operate together
      so that the robot is able to build its spatial memory in an organized,
      incremental and unsupervised manner. Thus, the robot's long-term spatial
      memory evolves completely on its own while learned knowledge is organized
      based on appearance-related similarities in a manner that is amenable for
      higher-level semantic reasoning, As such, the proposed model constitutes a
      step forward towards having robots that are capable of interacting with
      their environments in an autonomous manner.
SN  - 0929-5593
DO  - 10.1007/s10514-015-9514-4
UR  - http://dx.doi.org/10.1007/s10514-015-9514-4
KW  - spatial cognition
KW  - long-term spatial memory
KW  - place recognition
KW  - semantic maps
KW  - loop closure
KW  - large-scale
KW  - fab-map
KW  - localization
KW  - vision
KW  - space
ER  - 

TY  - JOUR
AU  - Karaoguz, H
AU  - Bozma, H I
AD  - Karaoguz, H (Corresponding Author), Kungliga Tekn Hgsk Stockholm,
      Stockholm, Sweden. Karaoguz, Hakan, Kungliga Tekn Hgsk Stockholm,
      Stockholm, Sweden. Bozma, H. Isil, Bogazici Univ, Fac Elect & Elect Engn,
      Istanbul, Turkey.
TI  - Merging of appearance-based place knowledge among multiple robots
T2  - AUTONOMOUS ROBOTS
VL  - 44
IS  - 6
SP  - 1009-1027
PY  - 2020
DA  - 2020
PB  - SPRINGER
CY  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
AB  - If robots can merge the appearance-based place knowledge of other robots
      with their own, they can relate to these places even if they have not
      previously visited them. We have investigated this problem using robots
      with compatible visual sensing capabilities and with each robot having its
      individual long-term place memory. Here, each place refers to a spatial
      region as defined by a collection of appearances and in the place memory,
      the knowledge is organized in a tree hierarchy. In the proposed merging
      approach, the hierarchical organization plays a key role-as it corresponds
      to a nested sequence of hyperspheres in the appearance space. The merging
      proceeds by considering the extent of overlap of the respective nested
      hyperspheres-starting with the largest covering hypersphere. Thus,
      differing from related work, knowledge is merged in as large chunks as
      possible while the hierarchical structure is preserved accordingly. As
      such, the merging scales better as the extent of knowledge to be merged
      increases. This is demonstrated in an extensive set of multirobot
      experiments where robots share their knowledge and then use their merged
      knowledge when visiting these places.
SN  - 0929-5593
DO  - 10.1007/s10514-020-09911-2
UR  - http://dx.doi.org/10.1007/s10514-020-09911-2
KW  - place recognition
KW  - multi-robot
KW  - unsupervised learning
KW  - large-scale
KW  - slam
KW  - maps
ER  - 

TY  - JOUR
AU  - Kretzschmar, H
AU  - Stachniss, C
AD  - Department of Computer Science, University of Freiburg, Geroges
      Köhler-Allee 79, Freiburg 79110, Germany
TI  - Information-theoretic compression of pose graphs for laser-based SLAM
T2  - International Journal of Robotics Research
VL  - 31
IS  - 11
SP  - 1219-1230
PY  - 2012
DA  - 2012
AB  - In graph-based simultaneous localization and mapping (SLAM), the pose
      graph grows over time as the robot gathers information about the
      environment. An ever growing pose graph, however, prevents long-term
      mapping with mobile robots. In this paper, we address the problem of
      efficient information-theoretic compression of pose graphs. Our approach
      estimates the mutual information between the laser measurements and the
      map to discard the measurements that are expected to provide only a small
      amount of information. Our method subsequently marginalizes out the nodes
      from the pose graph that correspond to the discarded laser measurements.
      To maintain a sparse pose graph that allows for efficient map
      optimization, our approach applies an approximate marginalization
      technique that is based on Chow-Liu trees. Our contributions allow the
      robot to effectively restrict the size of the pose graph. Alternatively,
      the robot is able to maintain a pose graph that does not grow unless the
      robot explores previously unobserved parts of the environment. Real-world
      experiments demonstrate that our approach to pose graph compression is
      well suited for long-term mobile robot mapping. © 2012 The Author(s).
SN  - 0278-3649
DO  - 10.1177/0278364912455072
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866286835&doi=10.1177%2f0278364912455072&partnerID=40&md5=435ef5d0fe3199d837c8d6347c9717d9
UR  - http://dx.doi.org/10.1177/0278364912455072
KW  - compression
KW  - long-term
KW  - mutual information
KW  - pose graph
KW  - slam
KW  - amount of information
KW  - graph-based
KW  - laser measurements
KW  - marginalization
KW  - mutual informations
KW  - real world experiment
KW  - robot mapping
KW  - simultaneous localization and mapping
KW  - compaction
KW  - information theory
KW  - mathematical techniques
KW  - mobile robots
KW  - robotics
KW  - trees (mathematics)
N1  - cited By 76
ER  - 

TY  - CPAPER
AU  - Kretzschmar, H
AU  - Stachniss, C
AU  - Grisetti, G
AD  - University of Freiburg, Department of Computer Science, Freiburg, Germany;
      Sapienza University of Rome, Department of Systems and Computer Science,
      Rome, Italy
TI  - Efficient information-theoretic graph pruning for graph-based SLAM with
      laser range finders
SP  - 865-871
PY  - 2011
DA  - 2011
AB  - In graph-based SLAM, the pose graph encodes the poses of the robot during
      data acquisition as well as spatial constraints between them. The size of
      the pose graph has a substantial influence on the runtime and the memory
      requirements of a SLAM system, which hinders long-term mapping. In this
      paper, we address the problem of efficient information-theoretic
      compression of pose graphs. Our approach estimates the expected
      information gain of laser measurements with respect to the resulting
      occupancy grid map. It allows for restricting the size of the pose graph
      depending on the information that the robot acquires about the environment
      or based on a given memory limit, which results in an any-space SLAM
      system. When discarding laser scans, our approach marginalizes out the
      corresponding pose nodes from the graph. To avoid a densely connected pose
      graph, which would result from exact marginalization, we propose an
      approximation to marginalization that is based on local Chow-Liu trees and
      maintains a sparse graph. Real world experiments suggest that our approach
      effectively reduces the growth of the pose graph while minimizing the loss
      of information in the resulting grid map. © 2011 IEEE.
DO  - 10.1109/IROS.2011.6048060
C1  - San Francisco, CA
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455169163&doi=10.1109%2fIROS.2011.6048060&partnerID=40&md5=bf7e84780c60fe957fb2fa454b90d5ac
UR  - http://dx.doi.org/10.1109/IROS.2011.6048060
KW  - expected informations
KW  - graph-based
KW  - grid map
KW  - laser measurements
KW  - laser range finders
KW  - laser scans
KW  - marginalization
KW  - memory requirements
KW  - occupancy grid map
KW  - real world experiment
KW  - runtimes
KW  - sparse graphs
KW  - spatial constraints
KW  - information theory
KW  - intelligent robots
KW  - robotics
KW  - trees (mathematics)
N1  - cited By 41; Conference of 2011 IEEE/RSJ International Conference on
      Intelligent Robots and Systems: Celebrating 50 Years of Robotics, IROS'11
      ; Conference Date: 25 September 2011 Through 30 September 2011; Conference
      Code:87712
ER  - 

TY  - JOUR
AU  - Kretzschmar, H
AU  - Grisetti, G
AU  - Stachniss, C
AD  - Department of Computer Science, University of Freiburg,
      Georges-Koehler-Allee 79, Freiburg, 79110, Germany
TI  - Lifelong Map Learning for Graph-based SLAM in Static Environments
T2  - KI - Kunstliche Intelligenz
VL  - 24
IS  - 3
SP  - 199-206
PY  - 2010
DA  - 2010
PB  - Springer Science and Business Media Deutschland GmbH
AB  - In this paper, we address the problem of lifelong map learning in static
      environments with mobile robots using the graph-based formulation of the
      simultaneous localization and mapping problem. The pose graph, which
      stores the poses of the robot and spatial constraints between them, is the
      central data structure in graph-based SLAM. The size of the pose graph has
      a direct influence on the runtime and the memory complexity of the SLAM
      system and typically grows over time. A robot that performs lifelong
      mapping in a bounded environment has to limit the memory and computational
      complexity of its mapping system. We present a novel approach to prune the
      pose graph so that it only grows when the robot acquires relevant new
      information about the environment in terms of expected information gain.
      As a result, our approach scales with the size of the environment and not
      with the length of the trajectory, which is an important prerequisite for
      lifelong map learning. The experiments presented in this paper illustrate
      the properties of our method using real robots. © 2010, Springer-Verlag.
SN  - 0933-1875
DO  - 10.1007/s13218-010-0034-2
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651515765&doi=10.1007%2fs13218-010-0034-2&partnerID=40&md5=6949e57ee281b8072eb6ad7690e3eaf6
UR  - http://dx.doi.org/10.1007/s13218-010-0034-2
KW  - expected information gain
KW  - mapping
KW  - slam
KW  - robotics
KW  - robots
KW  - expected informations
KW  - graph-based
KW  - information gain
KW  - map learning
KW  - runtimes
KW  - simultaneous localization and mapping problems
KW  - spatial constraints
KW  - static environment
N1  - cited By 14
ER  - 

TY  - CPAPER
AU  - Thomas, H
AU  - Agro, B
AU  - Gridseth, M
AU  - Zhang, J
AU  - Barfoot, T D
AD  - University of Toronto Institute for Aerospace Studies (UTIAS), 4925
      Dufferin StON, Canada; Apple Inc.
TI  - Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor
      Navigation
VL  - 2021-May
SP  - 14047-14053
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - We present a self-supervised learning approach for the semantic
      segmentation of lidar frames. Our method is used to train a deep point
      cloud segmentation architecture without any human annotation. The
      annotation process is automated with the combination of simultaneous
      localization and mapping (SLAM) and ray-tracing algorithms. By performing
      multiple navigation sessions in the same environment, we are able to
      identify permanent structures, such as walls, and disentangle short-term
      and long-term movable objects, such as people and tables, respectively.
      New sessions can then be performed using a network trained to predict
      these semantic labels. We demonstrate the ability of our approach to
      improve itself over time, from one session to the next. With semantically
      filtered point clouds, our robot can navigate through more complex
      scenarios, which, when added to the training pool, help to improve our
      network predictions. We provide insights into our network predictions and
      show that our approach can also improve the performances of common
      localization techniques. © 2021 IEEE
DO  - 10.1109/ICRA48506.2021.9561701
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114148382&doi=10.1109%2fICRA48506.2021.9561701&partnerID=40&md5=319ae684d1faab8fe3c8c5e2f0af2aa9
UR  - http://dx.doi.org/10.1109/ICRA48506.2021.9561701
N1  - cited By 0; Conference of 2021 IEEE International Conference on Robotics
      and Automation, ICRA 2021 ; Conference Date: 30 May 2021 Through 5 June
      2021; Conference Code:177050
ER  - 

TY  - JOUR
AU  - Yin, H
AU  - Xu, X
AU  - Wang, Y
AU  - Xiong, R
AD  - Institute of Cyber-Systems and Control, College of Control Science and
      Engineering, Zhejiang University, Hangzhou, China
TI  - Radar-to-Lidar: Heterogeneous Place Recognition via Joint Learning
T2  - Frontiers in Robotics and AI
VL  - 8
SP  - 661199
PY  - 2021
DA  - 2021
PB  - Frontiers Media S.A.
AB  - Place recognition is critical for both offline mapping and online
      localization. However, current single-sensor based place recognition still
      remains challenging in adverse conditions. In this paper, a heterogeneous
      measurement based framework is proposed for long-term place recognition,
      which retrieves the query radar scans from the existing lidar (Light
      Detection and Ranging) maps. To achieve this, a deep neural network is
      built with joint training in the learning stage, and then in the testing
      stage, shared embeddings of radar and lidar are extracted for
      heterogeneous place recognition. To validate the effectiveness of the
      proposed method, we conducted tests and generalization experiments on the
      multi-session public datasets and compared them to other competitive
      methods. The experimental results indicate that our model is able to
      perform multiple place recognitions: lidar-to-lidar (L2L), radar-to-radar
      (R2R), and radar-to-lidar (R2L), while the learned model is trained only
      once. We also release the source code publicly:
      https://github.com/ZJUYH/radar-to-lidar-place-recognition. © Copyright ©
      2021 Yin, Xu, Wang and Xiong.
SN  - 2296-9144
DO  - 10.3389/frobt.2021.661199
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107035661&doi=10.3389%2ffrobt.2021.661199&partnerID=40&md5=c02d4e877a05b938257508b780e718e8
UR  - http://dx.doi.org/10.3389/frobt.2021.661199
KW  - deep neural network
KW  - heterogeneous measurements
KW  - lidar
KW  - mobile robot
KW  - place recognition
KW  - radar
N1  - cited By 2
ER  - 

TY  - JOUR
AU  - Yin, H
AU  - Wang, Y
AU  - Ding, X
AU  - Tang, L
AU  - Huang, S
AU  - Xiong, R
AD  - State Key Laboratory of Industrial Control and Technology, Zhejiang
      University, Hangzhou, 310058, China; Institute of Cyber-Systems and
      Control, Zhejiang University, Hangzhou, 310058, China; Joint Centre for
      Robotics Research between, Zhejiang University, Hangzhou, 310058, China;
      University of Technology Sydney, Sydney, NSW 2007, Australia; Center for
      Autonomous Systems (CAS), University of Technology Sydney, Sydney, NSW
      2007, Australia
TI  - 3D LiDAR-Based Global Localization Using Siamese Neural Network
T2  - IEEE Transactions on Intelligent Transportation Systems
VL  - 21
IS  - 4
SP  - 1380-1392
PY  - 2020
DA  - 2020
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Global localization in 3D point clouds is a challenging task for mobile
      vehicles in outdoor scenarios, which requires the vehicle to localize
      itself correctly in a given map without prior knowledge of its pose. This
      is a critical component of autonomous vehicles or robots on the road for
      handling localization failures. In this paper, based on reduced dimension
      scan representations learned from neural networks, a solution to global
      localization is proposed by achieving place recognition first and then
      metric pose estimation in the global prior map. Specifically, we present a
      semi-handcrafted feature learning method for 3D Light detection and
      ranging (LiDAR) point clouds using artificial statistics and siamese
      network, which transforms the place recognition problem into a similarity
      modeling problem. Additionally, the sensor data using dimension reduced
      representations require less storage space and make the searching easier.
      With the learned representations by networks and the global poses, a prior
      map is built and used in the localization framework. In the localization
      step, position only observations obtained by place recognition are used in
      a particle filter algorithm to achieve precise pose estimation. To
      demonstrate the effectiveness of our place recognition and localization
      approach, KITTI benchmark and our multi-session datasets are employed for
      comparison with other geometric-based algorithms. The results show that
      our system can achieve both high accuracy and efficiency for long-term
      autonomy. © 2000-2011 IEEE.
SN  - 1524-9050
DO  - 10.1109/TITS.2019.2905046
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082761003&doi=10.1109%2fTITS.2019.2905046&partnerID=40&md5=e5e8a88c8f1bc504ae85a5c44ab35115
UR  - http://dx.doi.org/10.1109/TITS.2019.2905046
KW  - global localization
KW  - mobile vehicles
KW  - place recognition
KW  - siamese network
KW  - digital storage
KW  - learning systems
KW  - road vehicles
KW  - critical component
KW  - feature learning
KW  - light detection and ranging
KW  - mobile vehicle
KW  - reduced representation
KW  - similarity models
KW  - optical radar
N1  - cited By 18
ER  - 

TY  - JOUR
AU  - Zhang, H
AU  - Chen, X
AU  - Lu, H
AU  - Xiao, J
AD  - Department of Automation, National University of Defense Technology,
      Changsha, China
TI  - Distributed and collaborative monocular simultaneous localization and
      mapping for multi-robot systems in large-scale environments
T2  - International Journal of Advanced Robotic Systems
VL  - 15
IS  - 3
PY  - 2018
DA  - 2018
PB  - SAGE Publications Inc.
AB  - In this article, we propose a distributed and collaborative monocular
      simultaneous localization and mapping system for the multi-robot system in
      large-scale environments, where monocular vision is the only exteroceptive
      sensor. Each robot estimates its pose and reconstructs the environment
      simultaneously using the same monocular simultaneous localization and
      mapping algorithm. Meanwhile, they share the results of their incremental
      maps by streaming keyframes through the robot operating system messages
      and the wireless network. Subsequently, each robot in the group can obtain
      the global map with high efficiency. To build the collaborative
      simultaneous localization and mapping architecture, two novel approaches
      are proposed. One is a robust relocalization method based on active loop
      closure, and the other is a vision-based multi-robot relative pose
      estimating and map merging method. The former is used to solve the problem
      of tracking failures when robots carry out long-term monocular
      simultaneous localization and mapping in large-scale environments, while
      the latter uses the appearance-based place recognition method to determine
      multi-robot relative poses and build the large-scale global map by merging
      each robot’s local map. Both KITTI data set and our own data set acquired
      by a handheld camera are used to evaluate the proposed system.
      Experimental results show that the proposed distributed multi-robot
      collaborative monocular simultaneous localization and mapping system can
      be used in both indoor small-scale and outdoor large-scale environments. ©
      2018, The Author(s) 2018.
SN  - 1729-8806
DO  - 10.1177/1729881418780178
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049965041&doi=10.1177%2f1729881418780178&partnerID=40&md5=6faa9727a648f164c51e1d9cf2506a6d
UR  - http://dx.doi.org/10.1177/1729881418780178
KW  - large-scale slam
KW  - monocular slam
KW  - multi-robot collaborative slam
KW  - relocalization
KW  - conformal mapping
KW  - industrial robots
KW  - merging
KW  - multipurpose robots
KW  - robot learning
KW  - robotics
KW  - vision
KW  - exteroceptive sensor
KW  - multirobots
KW  - re-localization
KW  - robot operating system
KW  - simultaneous localization and mapping
KW  - simultaneous localization and mapping algorithms
KW  - indoor positioning systems
N1  - cited By 10
ER  - 

TY  - JOUR
AU  - Biswas, J
AU  - Veloso, M M
AD  - Robotics Institute, School of Computer Science, Carnegie Mellon
      University, 5000 Forbes Avenue, Pittsburgh, PA 15213-3890, United States;
      Computer Science Department, School of Computer Science, Carnegie Mellon
      University, Pittsburgh, PA, United States
TI  - Localization and navigation of the CoBots over long-term deployments
T2  - International Journal of Robotics Research
VL  - 32
IS  - 14
SP  - 1679-1694
PY  - 2013
DA  - 2013
AB  - For the last three years, we have developed and researched multiple
      collaborative robots, CoBots, which have been autonomously traversing our
      multi-floor buildings. We pursue the goal of long-term autonomy for indoor
      service mobile robots as the ability for them to be deployed indefinitely
      while they perform tasks in an evolving environment. The CoBots include
      several levels of autonomy, and in this paper we focus on their
      localization and navigation algorithms. We present the Corrective Gradient
      Refinement (CGR) algorithm, which refines the proposal distribution of the
      particle filter used for localization with sensor observations using
      analytically computed state space derivatives on a vector map. We also
      present the Fast Sampling Plane Filtering algorithm that extracts planar
      regions from depth images in real time. These planar regions are then
      projected onto the 2D vector map of the building, and along with the laser
      rangefinder observations, used with CGR for localization. For navigation,
      we present a hierarchical planner, which computes a topological policy
      using a graph representation of the environment, computes motion commands
      based on the topological policy, and then modifies the motion commands to
      side-step perceived obstacles. We started logging the deployments of the
      CoBots one and a half years ago, and have since collected logs of the
      CoBots traversing more than 130 km over 1082 deployments and a total run
      time of 182 h, which we publish as a dataset consisting of more than 10
      million laser scans. The logs show that although there have been
      continuous changes in the environment, the robots are robust to most of
      them, and there exist only a few locations where changes in the
      environment cause increased uncertainty in localization. © The Author(s)
      2013.
SN  - 0278-3649
DO  - 10.1177/0278364913503892
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892596433&doi=10.1177%2f0278364913503892&partnerID=40&md5=b4ec65b2b0b206ce6f3d978fe1a1d270
UR  - http://dx.doi.org/10.1177/0278364913503892
KW  - autonomous robots
KW  - indoor mobile robots
KW  - localization
KW  - long-term autonomy
KW  - navigation
KW  - graph representation
KW  - hierarchical planners
KW  - localization and navigation
KW  - proposal distribution
KW  - service mobile robots
KW  - algorithms
KW  - distributed computer systems
KW  - motion planning
KW  - robots
KW  - target tracking
KW  - topology
KW  - vector spaces
KW  - mobile robots
N1  - cited By 52
ER  - 

TY  - JOUR
AU  - Biswas, J
AU  - Veloso, M M
AD  - Computer Science Department, Carnegie Mellon UniversityPA, United States;
      College of Information and Computer Sciences, University of Massachusetts,
      Amherst, United States
TI  - Episodic non-Markov localization
T2  - Robotics and Autonomous Systems
VL  - 87
SP  - 162-176
PY  - 2017
DA  - 2017
PB  - Elsevier B.V.
AB  - Markov localization and its variants are widely used for mobile robot
      localization. These methods assume Markov independence of observations,
      implying that the observations can be entirely explained by a map.
      However, in real human environments, robots frequently make unexpected
      observations due to unmapped static objects like chairs and tables, and
      dynamic objects like humans. We therefore introduce Episodic non-Markov
      Localization (EnML), which reasons about the world as consisting of three
      classes of objects: long-term features corresponding to permanent mapped
      objects, short-term features corresponding to unmapped static objects, and
      dynamic features corresponding to unmapped moving objects. Long-term
      features are represented by a static map, while short-term features are
      detected and tracked in real-time. To reason about unexpected observations
      and their correlations across poses, we augment the Dynamic Bayesian
      Network for Markov localization to include varying edges and nodes,
      resulting in a novel Varying Graphical Network representation. The maximum
      likelihood estimate of the belief is incrementally computed by non-linear
      functional optimization. By detecting timesteps along the robot's
      trajectory where unmapped observations prior to such time steps are
      unrelated to those afterwards, EnML limits the history of observations and
      pose estimates to “episodes” over which the belief is computed. We
      demonstrate EnML using different types of sensors including laser
      rangefinders and depth cameras, and over multiple datasets, comparing it
      with alternative approaches. We further include results of a team of
      indoor autonomous service mobile robots traversing hundreds of kilometers
      using EnML. © 2016 Elsevier B.V.
SN  - 0921-8890
DO  - 10.1016/j.robot.2016.09.005
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997124583&doi=10.1016%2fj.robot.2016.09.005&partnerID=40&md5=fc49a61c300fea3a8e182df6c1e8c3b3
UR  - http://dx.doi.org/10.1016/j.robot.2016.09.005
KW  - localization
KW  - long-term autonomy
KW  - mapping
KW  - bayesian networks
KW  - functions
KW  - maximum likelihood
KW  - maximum likelihood estimation
KW  - mobile robots
KW  - range finders
KW  - robot applications
KW  - dynamic bayesian networks
KW  - laser range finders
KW  - markov localizations
KW  - maximum likelihood estimate
KW  - mobile robot localization
KW  - service mobile robots
KW  - robots
N1  - cited By 9
ER  - 

TY  - JOUR
AU  - Coulin, J
AU  - Guillemard, R
AU  - Gay-Bellile, V
AU  - Joly, C
AU  - Fortelle, A D L
AD  - Université Paris-Saclay, CEA-List, Palaiseau, 91120, France; MINES
      ParisTech, PSL University, Center for Robotics, Paris, 75006, France
TI  - Tightly-Coupled Magneto-Visual-Inertial Fusion for Long Term Localization
      in Indoor Environment
T2  - IEEE Robotics and Automation Letters
VL  - 7
IS  - 2
SP  - 952-959
PY  - 2022
DA  - 2022
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - We propose in this letter a tightly-coupled fusion of visual, inertial and
      magnetic data for long-term localization in indoor environment. Unlike
      state-of-the-art Visual-Inertial SLAM (VISLAM) solutions that reuse visual
      map to prevent drift, we present in this letter an extension of the
      Multi-State Constraint Kalman Filter (MSCKF) that takes advantage of a
      magnetic map. It makes our solution more robust to variations of the
      environment appearance. The experimental results demonstrate that the
      localization accuracy of the proposed approach is almost the same over
      time periods longer than a year. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2021.3136241
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121789491&doi=10.1109%2fLRA.2021.3136241&partnerID=40&md5=0d1f2802e3001409ccf19523ed40252f
UR  - http://dx.doi.org/10.1109/LRA.2021.3136241
KW  - indoor magnetic field
KW  - localization
KW  - msckf
KW  - sensor fusion
KW  - visual-inertial slam
KW  - bibliographies
KW  - robotics
KW  - bibt e x
KW  - code
KW  - documentation
KW  - ieee
KW  - indoor environment
KW  - l a t e x
KW  - localisation
KW  - style
KW  - template
KW  - tightly-coupled
KW  - indoor positioning systems
N1  - cited By 0
ER  - 

TY  - CPAPER
AU  - Li, J
AU  - Eustice, R M
AU  - Johnson-Roberson, M
AD  - Li, J (Corresponding Author), Univ Michigan, Dept Elect Engn & Comp Sci,
      Ann Arbor, MI 48109 USA. Li, Jie, Univ Michigan, Dept Elect Engn & Comp
      Sci, Ann Arbor, MI 48109 USA. Eustice, Ryan M.; Johnson-Roberson, Matthew,
      Univ Michigan, Dept Naval Architecture & Marine Engn, Ann Arbor, MI 48109
      USA.
TI  - High-Level Visual Features for Underwater Place Recognition
T2  - 2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)
J2  - IEEE International Conference on Robotics and Automation ICRA
SP  - 3652-3659
PY  - 2015
DA  - 2015
PB  - IEEE COMPUTER SOC
AB  - This paper reports on a method to perform robust visual relocalization
      between temporally separated sets of underwater images gathered by a
      robot. The place recognition and relocalization problem is more
      challenging in the underwater environment mainly due to three factors: 1)
      changes in illumination; 2) long-term changes in the visual appearance of
      features because of phenomena like biofouling on man-made structures and
      growth or movement in natural features; and 3) low density of visually
      salient features for image matching. To address these challenges, a
      patch-based feature matching approach is proposed, which uses image
      segmentation and local intensity contrast to locate salient patches and
      HOG description to make correspondences between patches. Compared to
      traditional point-based features that are sensitive to dramatic appearance
      changes underwater, patch-based features are able to encode higher level
      information such as shape or structure which tends to persist across years
      in underwater environments. The algorithm is evaluated on real data, from
      multiple years, collected by a Hovering Autonomous Underwater Vehicle for
      ship hull inspection. Results in relocalization performance across
      missions from different years are compared to other traditional methods.
DO  - 10.1109/ICRA.2015.7139706
C1  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
UR  - http://dx.doi.org/10.1109/ICRA.2015.7139706
KW  - simultaneous localization
KW  - navigation
KW  - slam
N1  - IEEE International Conference on Robotics and Automation (ICRA), Seattle,
      WA, MAY 26-30, 2015
ER  - 

TY  - JOUR
AU  - Santos, J M
AU  - Krajnik, T
AU  - Fentanes, T J P A D
AD  - Santos, JM (Corresponding Author), Univ Lincoln, Lincoln Ctr Autonomous
      Syst, Lincoln LN6 7TS, England. Santos, Joao Machado; Krajnik, Tomas;
      Fentanes, Jaime Pulido; Duckett, Tom, Univ Lincoln, Lincoln Ctr Autonomous
      Syst, Lincoln LN6 7TS, England.
TI  - Lifelong Information- Driven Exploration to Complete and Refine 4-D
      Spatio-Temporal Maps
T2  - IEEE ROBOTICS AND AUTOMATION LETTERS
VL  - 1
IS  - 2
SP  - 684-691
PY  - 2016
DA  - 2016
PB  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
CY  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
AB  - This letter presents an exploration method that allows mobile robots to
      build and maintain spatio-temporal models of changing environments. The
      assumption of a perpetually changing world adds a temporal dimension to
      the exploration problem, making spatio-temporal exploration a
      never-ending, lifelong learning process. We address the problem by
      application of information-theoretic exploration methods to
      spatio-temporal models that represent the uncertainty of environment
      states as probabilistic functions of time. This allows to predict the
      potential information gain to be obtained by observing a particular area
      at a given time, and consequently, to decide which locations to visit and
      the best times to go there. To validate the approach, a mobile robot was
      deployed continuously over 5 consecutive business days in a busy office
      environment. The results indicate that the robot's ability to spot
      environmental changes improved as it refined its knowledge of the world
      dynamics.
SN  - 2377-3766
DO  - 10.1109/LRA.2016.2516594
UR  - http://dx.doi.org/10.1109/LRA.2016.2516594
KW  - mapping
KW  - service robots
KW  - mobile robots
KW  - environments
KW  - localization
KW  - navigation
ER  - 

TY  - CPAPER
AU  - Oberländer, J
AU  - Roennau, A
AU  - Dillmann, R
AD  - Department of Interactive Diagnosis and Service Systems (IDS), FZI
      Research Center for Information Technology, 76131 Karlsruhe, Germany;
      Humanoids and Intelligence Systems Lab, Institute for Anthropomatics,
      Karlsruhe Institute of Technology, 76128 Karlsruhe, Germany
TI  - Hierarchical SLAM using spectral submap matching with opportunities for
      long-term operation
SP  - 1-7
PY  - 2013
DA  - 2013
PB  - IEEE Computer Society
AB  - We present a hierarchical SLAM approach which uses spectral registration
      of local submaps to close loops and to perform global localization after a
      restart. Using the Fourier-Mellin Transform (FMT), we robustly register
      occupancy grid representations of local submaps and present methods which
      improve matching performance. We further show how good match candidates
      can be reliably detected even from scaled-down versions of the submaps,
      which significantly reduces the computation time. The spectral
      registration approach proves useful even in the presence of significant
      environmental changes due to the fact that it calculates a dense match,
      incorporating all observed information rather than a sparse set of
      features. © 2013 IEEE.
DO  - 10.1109/ICAR.2013.6766479
C1  - Montevideo
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899407505&doi=10.1109%2fICAR.2013.6766479&partnerID=40&md5=cf224e6cd8a1b3f7d54704b29766ce26
UR  - http://dx.doi.org/10.1109/ICAR.2013.6766479
KW  - human computer interaction
KW  - computation time
KW  - environmental change
KW  - fourier-mellin transforms
KW  - global localization
KW  - matching performance
KW  - occupancy grids
KW  - scaled-down versions
KW  - slam approach
KW  - robotics
N1  - cited By 11; Conference of 2013 16th International Conference on Advanced
      Robotics, ICAR 2013 ; Conference Date: 25 November 2013 Through 29
      November 2013; Conference Code:104529
ER  - 

TY  - JOUR
AU  - Oh, J
AU  - Eoh, G
AD  - Department of Robotics, Kwangwoon University, Seoul, 01897, South Korea;
      Industrial AI Research Center, Chungbuk National University, Cheongju,
      28116, South Korea
TI  - Variational Bayesian approach to condition-invariant feature extraction
      for visual place recognition
T2  - Applied Sciences (Switzerland)
VL  - 11
IS  - 19
SP  - 8976
PY  - 2021
DA  - 2021
PB  - MDPI
AB  - As mobile robots perform long-term operations in large-scale environments,
      coping with perceptual changes becomes an important issue recently. This
      paper introduces a stochastic variational inference and learning
      architecture that can extract condition-invariant features for visual
      place recognition in a changing environment. Under the assumption that a
      latent representation of the variational autoencoder can be divided into
      condition-invariant and condition-sensitive features, a new structure of
      the variation autoencoder is proposed and a variational lower bound is
      derived to train the model. After training the model, condition-invariant
      features are extracted from test images to calculate the similarity
      matrix, and the places can be recognized even in severe environmental
      changes. Experiments were conducted to verify the proposed method, and the
      experimental results showed that our assumption was reasonable and
      effective in recognizing places in changing environments. © 2021 by the
      authors. Licensee MDPI, Basel, Switzerland.
SN  - 2076-3417
DO  - 10.3390/app11198976
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116389822&doi=10.3390%2fapp11198976&partnerID=40&md5=000d76d9f79bb71dfb08f8c5394e1fce
UR  - http://dx.doi.org/10.3390/app11198976
KW  - auto-encoder
KW  - deep learning
KW  - localization
KW  - mobile robots
KW  - place recognition
KW  - slam
N1  - cited By 0
ER  - 

TY  - JOUR
AU  - Saarinen, J P
AU  - Andreasson, H
AU  - Stoyanov, T
AU  - Lilienthal, A J
TI  - 3D normal distributions transform occupancy maps: An efficient
      representation for mapping in dynamic environments
T2  - International Journal of Robotics Research
VL  - 32
IS  - 14
SP  - 1627-1644
PY  - 2013
DA  - 2013
CY  - USA
AB  - In order to enable long-term operation of autonomous vehicles in
      industrial environments numerous challenges need to be addressed. A basic
      requirement for many applications is the creation and maintenance of
      consistent 3D world models. This article proposes a novel 3D spatial
      representation for online real-world mapping, building upon two known
      representations: normal distributions transform (NDT) maps and occupancy
      grid maps. The proposed normal distributions transform occupancy map
      (NDT-OM) combines the advantages of both representations; compactness of
      NDT maps and robustness of occupancy maps. One key contribution in this
      article is that we formulate an exact recursive updates for NDT-OMs. We
      show that the recursive update equations provide natural support for
      multi-resolution maps. Next, we describe a modification of the recursive
      update equations that allows adaptation in dynamic environments. As a
      second key contribution we introduce NDT-OMs and formulate the occupancy
      update equations that allow to build consistent maps in dynamic
      environments. The update of the occupancy values are based on an efficient
      probabilistic sensor model that is specially formulated for NDT-OMs. In
      several experiments with a total of 17 hours of data from a milkfactory we
      demonstrate that NDT-OMs enable real-time performance in large-scale,
      long-term industrial setups.
SN  - 0278-3649
DO  - 10.1177/0278364913499415
UR  - http://dx.doi.org/10.1177/0278364913499415
KW  - control engineering computing
KW  - dairy products
KW  - industrial robots
KW  - mobile robots
KW  - normal distribution
KW  - production engineering computing
KW  - production facilities
KW  - robot dynamics
KW  - robot vision
KW  - slam (robots)
KW  - solid modelling
KW  - transforms
N1  - 3D normal distributions transform occupancy maps;dynamic
      environments;autonomous vehicles;industrial environments;3D world
      models;3D spatial representation;online real-world mapping;normal
      distributions transform maps;occupancy grid maps;NDT-OM;NDT maps;exact
      recursive update equations;multiresolution maps;occupancy update
      equations;probabilistic sensor model;milkfactory;preprogrammed
      manipulators;
ER  - 

TY  - JOUR
AU  - Pérez, J
AU  - Caballero, F
AU  - Merino, L
AD  - University Pablo de Olavide, Seville, Spain; University of Seville,
      Seville, Spain
TI  - Enhanced Monte Carlo Localization with Visual Place Recognition for Robust
      Robot Localization
T2  - Journal of Intelligent and Robotic Systems: Theory and Applications
VL  - 80
IS  - 3-4
SP  - 641-656
PY  - 2015
DA  - 2015
PB  - Kluwer Academic Publishers
AB  - This paper proposes extending Monte Carlo Localization methods with visual
      place recognition information in order to build a robust robot
      localization system. This system is aimed to work in crowded and
      non-planar scenarios, where 2D laser rangefinders may not always be enough
      to match the robot position within the map. Thus, visual place recognition
      will be used in order to obtain robot position clues that can be used to
      detect when the robot is lost and also to reset its positions to the right
      one. The paper presents experimental results based on datasets gathered
      with a real robot in challenging scenarios. © 2015, Springer
      Science+Business Media Dordrecht.
SN  - 0921-0296
DO  - 10.1007/s10846-015-0198-y
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945480250&doi=10.1007%2fs10846-015-0198-y&partnerID=40&md5=28fa10437be093bb3ca9e4b093a1233b
UR  - http://dx.doi.org/10.1007/s10846-015-0198-y
KW  - crowded environment
KW  - long-term localization
KW  - monte carlo localization
KW  - robust localization
KW  - range finders
KW  - robot applications
KW  - robots
KW  - place recognition
KW  - real robot
KW  - robot positions
KW  - robust robots
KW  - monte carlo methods
N1  - cited By 15
ER  - 

TY  - CPAPER
AU  - Berrio, J S
AU  - Ward, J
AU  - Worrall, S
AU  - Nebot, E
AD  - Australian Centre for Field Robotics (ACFR), University of SydneyNSW,
      Australia
TI  - Identifying robust landmarks in feature-based maps
VL  - 2019-June
SP  - 1166-1172
PY  - 2019
DA  - 2019
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - To operate in an urban environment, an automated vehicle must be capable
      of accurately estimating its position within a global map reference frame.
      This is necessary for optimal path planning and safe navigation. To
      accomplish this over an extended period of time, the global map requires
      long term maintenance. This includes the addition of newly observable
      features and the removal of transient features belonging to dynamic
      objects. The latter is especially important for the long-term use of the
      map as matching against a map with features that no longer exist can
      result in incorrect data associations, and consequently erroneous
      localisation. This paper addresses the problem of removing features from
      the map that correspond to objects that are no longer observable/present
      in the environment. This is achieved by assigning a single score which
      depends on the geometric distribution and characteristics when the
      features are re-detected (or not) on different occasions. Our approach not
      only eliminates ephemeral features, but can also be used as a reduction
      algorithm for highly dense maps. We tested our approach using half a year
      of weekly drives over the same 500 metre section of road in an urban
      environment. The results presented demonstrate the validity of the long
      term approach to map maintenance. © 2019 IEEE.
DO  - 10.1109/IVS.2019.8814289
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072290600&doi=10.1109%2fIVS.2019.8814289&partnerID=40&md5=b9240fab7189359c3acbe8cff456cdb3
UR  - http://dx.doi.org/10.1109/IVS.2019.8814289
KW  - digital storage
KW  - intelligent vehicle highway systems
KW  - motion planning
KW  - probability distributions
KW  - urban planning
KW  - automated vehicles
KW  - geometric distribution
KW  - long-term maintenances
KW  - optimal path planning
KW  - reduction algorithms
KW  - safe navigations
KW  - transient features
KW  - urban environments
KW  - vehicles
N1  - cited By 4; Conference of 30th IEEE Intelligent Vehicles Symposium, IV
      2019 ; Conference Date: 9 June 2019 Through 12 June 2019; Conference
      Code:151325
ER  - 

TY  - JOUR
AU  - Berrio, J S
AU  - Worrall, S
AU  - Shan, M
AU  - Nebot, E
AD  - Australian Centre for Field Robotics (ACFR), The University of Sydney,
      Sydney, NSW 2006, Australia (e-mail: j.berrio@acfr.usyd.edu.au);
      Australian Centre for Field Robotics (ACFR), The University of Sydney,
      Sydney, NSW 2006, Australia.
TI  - Long-Term Map Maintenance Pipeline for Autonomous Vehicles
T2  - IEEE Transactions on Intelligent Transportation Systems
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - For autonomous vehicles to operate persistently in a typical urban
      environment, it is essential to have high accuracy position information.
      This requires a mapping and localisation system that can adapt to changes
      over time. A localisation approach based on a single-survey map will not
      be suitable for long-term operation as it does not incorporate variations
      in the environment. In this paper, we present new algorithms to maintain a
      featured-based map. A map maintenance pipeline is proposed that can
      continuously update a map with the most relevant features taking advantage
      of the changes in the surroundings. Our pipeline detects and removes
      transient features based on their geometrical relationships with the
      vehicle's pose. Newly identified features became part of a new feature map
      and are assessed by the pipeline as candidates for the localisation map.
      By purging out-of-date features and adding newly detected features, we
      continually update the prior map to more accurately represent the most
      recent environment. We have validated our approach using the USyd Campus
      Dataset, which includes more than 18 months of data. The results presented
      demonstrate that our maintenance pipeline produces a resilient map which
      can provide sustained localisation performance over time. IEEE
SN  - 1524-9050
DO  - 10.1109/TITS.2021.3094485
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110835631&doi=10.1109%2fTITS.2021.3094485&partnerID=40&md5=31d95a0a9abb9d0a523480e40ef5f5f9
UR  - http://dx.doi.org/10.1109/TITS.2021.3094485
KW  - autonomous vehicles
KW  - feature extraction
KW  - feature-based map
KW  - long-term localisation
KW  - maintenance engineering
KW  - map update.
KW  - pipelines
KW  - task analysis
KW  - transient analysis
KW  - visualization
KW  - maintenance
KW  - geometrical relationship
KW  - high-accuracy
KW  - localisation
KW  - localisation systems
KW  - position information
KW  - relevant features
KW  - transient features
KW  - typical urban
N1  - cited By 0
ER  - 

TY  - CPAPER
AU  - Zhu, J
AU  - Ai, Y
AU  - Tian, B
AU  - Cao, D
AU  - Scherer, S
TI  - Visual Place Recognition in Long-term and Large-scale Environment based on
      CNN Feature
SP  - 1679-1685
PY  - 2018
DA  - 2018
AB  - With the universal application of camera in intelligent vehicles, visual
      place recognition has become a major problem in intelligent vehicle
      localization. The traditional solution is to make visual description of
      place images using hand-crafted feature for matching places, but this
      description method is not very good for extreme variability, especially
      for seasonal transformation. In this paper, we propose a new method based
      on convolutional neural network (CNN), by putting images into the
      pre-trained network model to get automatically learned image descriptors,
      and through some operations of pooling, fusion and binarization to
      optimize them, then the similarity result of place recognition is
      presented with the Hamming distance of the place sequence. In the
      experimental part, we compare our method with some state-of-the-art
      algorithms, FABMAP, ABLE-M and SeqSLAM, to illustrate its advantages. The
      experimental results show that our method based on CNN achieves better
      performance than other methods on the representative public datasets.
DO  - 10.1109/IVS.2018.8500686
C1  - Piscataway, NJ, USA
UR  - http://dx.doi.org/10.1109/IVS.2018.8500686
KW  - convolution
KW  - feedforward neural nets
KW  - image matching
KW  - image recognition
KW  - intelligent transportation systems
KW  - learning (artificial intelligence)
KW  - slam (robots)
N1  - visual place recognition;large-scale environment;CNN feature;intelligent
      vehicles;intelligent vehicle localization;visual description;place
      images;hand-crafted feature;description method;convolutional neural
      network;pre-trained network model;place sequence;image descriptors;Hamming
      distance;place matching;
ER  - 

TY  - JOUR
AU  - Tsintotas, K A
AU  - Bampis, L
AU  - Gasteratos, A
AD  - Laboratory of Robotics and Automation, Department of Production and
      Management Engineering, School of Engineering, Democritus University of
      Thrace, Xanthi, GR-67132, Greece
TI  - Modest-vocabulary loop-closure detection with incremental bag of tracked
      words
T2  - Robotics and Autonomous Systems
VL  - 141
SP  - 103782
PY  - 2021
DA  - 2021
PB  - Elsevier B.V.
AB  - A key feature in the context of simultaneous localization and mapping is
      loop-closure detection, a process determining whether the current robot's
      environment perception coincides with previous observation. However, in
      long-term operations, both computational efficiency and memory
      requirements involved in an autonomous robot operation in uncontrolled
      environments, are of particular importance. The majority of approaches
      scale linearly with the environment's size in terms of storage and query
      time. The article at hand presents an efficient appearance-based
      loop-closure detection pipeline, which encodes the traversed trajectory by
      a low amount of unique visual words generated on-line through feature
      tracking. The incrementally constructed visual vocabulary is referred to
      as the “Bag of Tracked Words.” A nearest-neighbor voting scheme is
      utilized to query the database and assign probabilistic scores to all
      visited locations. Exploiting the inherent temporal coherency in the
      loop-closure task, the produced scores are processed through a Bayesian
      filter to estimate the belief state about the robot's location on the map.
      Also, a geometrical verification step ensures consistency between image
      matches. Management is also applied to the resulting vocabulary to reduce
      its growth rate and constraint the system's computational complexity while
      improving its voting distinctiveness. The proposed approach's performance
      is experimentally evaluated on several publicly available and challenging
      datasets, including hand-held, car-mounted, aerial, and ground
      trajectories. Results demonstrate the method's adaptability, which retains
      high operational frequency in environments of up to 13 km and high recall
      rates for perfect precision, outperforming other state-of-the-art
      techniques. The system's effectiveness is owed to the reduced vocabulary
      size, which is at least one order of magnitude smaller than other
      contemporary approaches. An open research-oriented source code has been
      made publicly available, which is dubbed as “BoTW-LCD.” © 2021 Elsevier
      B.V.
SN  - 0921-8890
DO  - 10.1016/j.robot.2021.103782
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104623871&doi=10.1016%2fj.robot.2021.103782&partnerID=40&md5=23ceef64ccee234a9474a6f306c0aef9
UR  - http://dx.doi.org/10.1016/j.robot.2021.103782
KW  - loop-closure detection
KW  - mapping
KW  - recognition
KW  - slam
KW  - visual-based navigation
KW  - agricultural robots
KW  - antennas
KW  - computational efficiency
KW  - feature extraction
KW  - robots
KW  - environment perceptions
KW  - memory requirements
KW  - operational frequency
KW  - probabilistic scores
KW  - simultaneous localization and mapping
KW  - state-of-the-art techniques
KW  - temporal coherency
KW  - visual vocabularies
KW  - query processing
N1  - cited By 8
ER  - 

TY  - CPAPER
AU  - Ikeda, K
AU  - Tanaka, K
AD  - Faculty of Engineering, University of Fukui, Japan
TI  - Visual robot localization using compact binary landmarks
SP  - 4397-4403
PY  - 2010
DA  - 2010
AB  - This paper is concerned with the problem of mobile robot localization
      using a novel compact representation of visual landmarks. With recent
      progress in lifelong map-learning as well as in information sharing
      networks, compact representation of a large-size landmark database has
      become crucial. In this paper, we propose a compact binary code (e.g.
      32bit code) landmark representation by employing the semantic hashing
      technique from web-scale image retrieval. We show how well such a binary
      representation achieves compactness of a landmark database while
      maintaining efficiency of the localization system. In our contribution, we
      investigate the cost-performance, the semantic gap, the saliency
      evaluation using the presented techniques as well as challenge to further
      reduce the resources (#bits) per landmark. Experiments using a high-speed
      car-like robot show promising results. ©2010 IEEE.
DO  - 10.1109/ROBOT.2010.5509579
C1  - Anchorage, AK
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955794780&doi=10.1109%2fROBOT.2010.5509579&partnerID=40&md5=5cfdb097472df1942b1b542961349728
UR  - http://dx.doi.org/10.1109/ROBOT.2010.5509579
KW  - binary representations
KW  - bit codes
KW  - car-like robot
KW  - compact representation
KW  - cost performance
KW  - hashing techniques
KW  - high-speed
KW  - information sharing network
KW  - landmark database
KW  - large sizes
KW  - localization system
KW  - mobile robot localization
KW  - recent progress
KW  - robot localization
KW  - semantic gap
KW  - visual landmarks
KW  - automobiles
KW  - information retrieval
KW  - robot applications
KW  - robots
KW  - semantic web
KW  - semantics
KW  - robotics
N1  - cited By 14; Conference of 2010 IEEE International Conference on Robotics
      and Automation, ICRA 2010 ; Conference Date: 3 May 2010 Through 7 May
      2010; Conference Code:81416
ER  - 

TY  - CPAPER
AU  - Konolige, K
AU  - Bowman, J
AD  - Willow Garage, Menlo Park, CA, United States
TI  - Towards lifelong visual maps
SP  - 1156-1163
PY  - 2009
DA  - 2009
AB  - The typical SLAM mapping system assumes a static environment and
      constructs a map that is then used without regard for ongoing changes.
      Most SLAM systems, such as FastSLAM, also require a single connected run
      to create a map. In this paper we present a system of visual mapping,
      using only input from a stereo camera, that continually updates an
      optimized metric map in large indoor spaces with movable objects: people,
      furniture, partitions, etc. The system can be stopped and restarted at
      arbitrary disconnected points, is robust to occlusion and localization
      failures, and efficiently maintains alternative views of a dynamic
      environment. It operates completely online at a 30 Hz frame rate. © 2009
      IEEE.
DO  - 10.1109/IROS.2009.5354121
C1  - St. Louis, MO
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249093571&doi=10.1109%2fIROS.2009.5354121&partnerID=40&md5=1c84f8beebd53935fa55e577ea93daaa
UR  - http://dx.doi.org/10.1109/IROS.2009.5354121
KW  - dynamic environments
KW  - fast-slam
KW  - frame rate
KW  - indoor space
KW  - mapping systems
KW  - ongoing changes
KW  - static environment
KW  - stereo cameras
KW  - visual map
KW  - visual mapping
KW  - intelligent robots
N1  - cited By 109; Conference of 2009 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2009 ; Conference Date: 11 October
      2009 Through 15 October 2009; Conference Code:79280
ER  - 

TY  - JOUR
AU  - MacTavish, K
AU  - Paton, M
AU  - Barfoot, T D
AD  - Institute for Aerospace Studies, Faculty of Applied Science & Engineering,
      University of Toronto, Toronto, ON, Canada
TI  - Selective memory: Recalling relevant experience for long-term visual
      localization
T2  - Journal of Field Robotics
VL  - 35
IS  - 8
SP  - 1265-1292
PY  - 2018
DA  - 2018
PB  - John Wiley and Sons Inc.
AB  - Visual navigation is a key enabling technology for autonomous mobile
      vehicles. The ability to provide large-scale, long-term navigation using
      low-cost, low-power vision sensors is appealing for industrial
      applications. A crucial requirement for long-term navigation systems is
      the ability to localize in environments whose appearance is constantly
      changing over time—due to lighting, weather, seasons, and physical
      changes. This paper presents a multiexperience localization (MEL) system
      that uses a powerful map representation—storing every visual experience in
      layers—that does not make assumptions about underlying appearance
      modalities and generators. Our localization system provides real-time
      performance by selecting online, a subset of experiences against which to
      localize. We achieve this task through a novel experience-triage algorithm
      based on collaborative filtering, which selects experiences relevant to
      the live view, outperforming competing techniques. Based on classical
      memory-based recommender systems, this technique also enables
      landmark-level recommendations, is entirely online, and requires no
      training data. We demonstrate the capabilities of the MEL system in the
      context of long-term autonomous path following in unstructured outdoor
      environments with a challenging 100-day field experiment through day,
      night, snow, spring, and summer. We furthermore provide offline analysis
      comparing our system to several state-of-the-art alternatives. We show
      that the combination of the novel methods presented in this paper enable
      full use of incredibly rich multiexperience maps, opening the door to
      robust long-term visual localization. © 2018 Wiley Periodicals, Inc.
SN  - 1556-4959
DO  - 10.1002/rob.21838
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055930460&doi=10.1002%2frob.21838&partnerID=40&md5=2beb6b5d9c8eb7fb7d832bcbebbd288d
UR  - http://dx.doi.org/10.1002/rob.21838
KW  - mapping
KW  - position estimation
KW  - terrestrial robotics
KW  - collaborative filtering
KW  - costs
KW  - navigation systems
KW  - robots
KW  - autonomous mobile vehicles
KW  - enabling technologies
KW  - localization system
KW  - map representations
KW  - outdoor environment
KW  - real time performance
KW  - visual localization
KW  - online systems
N1  - cited By 7
ER  - 

TY  - CPAPER
AU  - Pirker, K
AU  - Rüther, M
AU  - Bischof, H
AD  - Institute for Computer Graphics and Vision, University of Technology,
      Graz, Austria
TI  - CD SLAM - Continuous localization and mapping in a dynamic world
SP  - 3990-3997
PY  - 2011
DA  - 2011
AB  - When performing large-scale perpetual localization and mapping one faces
      problems like memory consumption or repetitive and dynamic scene elements
      requiring robust data association. We propose a visual SLAM method which
      handles short- and long-term scene dynamics in large environments using a
      single camera only. Through visibility-dependent map filtering and
      efficient keyframe organization we reach a considerable performance gain
      only through incorporation of a slightly more complex map representation.
      Experiments on a large, mixed indoor/outdoor dataset over a time period of
      two weeks demonstrate the scalability and robustness of our approach. ©
      2011 IEEE.
DO  - 10.1109/IROS.2011.6048253
C1  - San Francisco, CA
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455179037&doi=10.1109%2fIROS.2011.6048253&partnerID=40&md5=60745ea0c111857b93343287fc874a6e
UR  - http://dx.doi.org/10.1109/IROS.2011.6048253
KW  - data sets
KW  - dynamic scenes
KW  - dynamic world
KW  - indoor/outdoor
KW  - key frames
KW  - memory consumption
KW  - performance gain
KW  - robust datum
KW  - single cameras
KW  - time-periods
KW  - visual slam
KW  - intelligent robots
KW  - robotics
N1  - cited By 30; Conference of 2011 IEEE/RSJ International Conference on
      Intelligent Robots and Systems: Celebrating 50 Years of Robotics, IROS'11
      ; Conference Date: 25 September 2011 Through 30 September 2011; Conference
      Code:87712
ER  - 

TY  - CPAPER
AU  - Wang, K
AU  - Lin, Y
AU  - Wang, L
AU  - Han, L
AU  - Hua, M
AU  - Wang, X
AU  - Lian, S
AU  - Huang, B
AD  - CloudMinds Technologies Inc, Beijing, 100102, China
TI  - A unified framework for mutual improvement of SLAM and semantic
      segmentation
VL  - 2019-May
SP  - 5224-5230
PY  - 2019
DA  - 2019
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - This paper presents a novel framework for simultaneously implementing
      localization and segmentation, which are two of the most important
      vision-based tasks for robotics. While the goals and techniques used for
      them were considered to be different previously, we show that by making
      use of the intermediate results of the two modules, their performance can
      be enhanced at the same time. Our framework is able to handle both the
      instantaneous motion and long-term changes of instances in localization
      with the help of the segmentation result, which also benefits from the
      refined 3D pose information. We conduct experiments on various datasets,
      and prove that our framework works effectively on improving the precision
      and robustness of the two tasks and outperforms existing localization and
      segmentation algorithms. © 2019 IEEE.
DO  - 10.1109/ICRA.2019.8793499
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071467185&doi=10.1109%2fICRA.2019.8793499&partnerID=40&md5=05fc4d5a3dc8587bf249a1eed15b955a
UR  - http://dx.doi.org/10.1109/ICRA.2019.8793499
N1  - cited By 11; Conference of 2019 International Conference on Robotics and
      Automation, ICRA 2019 ; Conference Date: 20 May 2019 Through 24 May 2019;
      Conference Code:150804
ER  - 

TY  - JOUR
AU  - Zhang, K
AU  - Jiang, X
AU  - Ma, J
AD  - Electronic Information School, Wuhan University, Wuhan, 430072, China
TI  - Appearance-Based Loop Closure Detection via Locality-Driven Accurate
      Motion Field Learning
T2  - IEEE Transactions on Intelligent Transportation Systems
VL  - 23
IS  - 3
SP  - 2350-2365
PY  - 2022
DA  - 2022
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Loop closure detection (LCD) is of significant importance in simultaneous
      localization and mapping. It represents the robot's ability to recognize
      whether the current surrounding corresponds to a previously observed one.
      In this paper, we conduct this task in a two-step strategy: candidate
      frame selection and loop closure verification. The first step aims to
      search semantically similar images for the query one using features
      obtained by Key.Net with HardNet. Instead of adopting the traditional
      Bag-of-Words strategy, we utilize the aggregated selective match kernel to
      calculate the similarity between images. Subsequently, based on the
      potential property of motion field in the LCD scene, we propose a novel
      feature matching method, i.e., exploiting the smoothness prior and
      learning the motion field for an image pair in a reproducing kernel
      Hilbert space (RKHS), to implement loop closure verification. Concretely,
      we formulate the learning problem into a Bayesian framework with latent
      variables indicating the true/false correspondences and a mixture model
      accounting for the distribution of data. Furthermore, we propose a
      locality-driven mechanism to enhance the local relevance of motion vectors
      and term the algorithm as locality-driven accurate motion field learning
      (LAL). To satisfy the requirement of efficiency in the LCD task, we use a
      sparse approximation and search a suboptimal solution for the motion field
      in the RKHS, termed as LAL∗. Extensive experiments are conducted on public
      datasets for feature matching and LCD tasks. The quantitative results
      demonstrate the effectiveness of our method over the current
      state-of-the-art, meanwhile showing its potential for long-term visual
      localization. The codes of LAL and LAL∗ are publicly available at
      https://github.com/KN-Zhang/LAL. © 2000-2011 IEEE.
SN  - 1524-9050
DO  - 10.1109/TITS.2021.3086822
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112137846&doi=10.1109%2fTITS.2021.3086822&partnerID=40&md5=f812b052eeeebb7e670b6067e3d100a8
UR  - http://dx.doi.org/10.1109/TITS.2021.3086822
KW  - autonomous vehicle
KW  - feature matching
KW  - loop closure detection
KW  - place recognition
KW  - slam
KW  - computer applications
KW  - intelligent systems
KW  - bayesian frameworks
KW  - feature matching methods
KW  - quantitative result
KW  - reproducing kernel hilbert spaces
KW  - simultaneous localization and mapping
KW  - sparse approximations
KW  - suboptimal solution
KW  - visual localization
KW  - learning systems
N1  - cited By 1
ER  - 

TY  - JOUR
AU  - Clement, L
AU  - Gridseth, M
AU  - Tomasi, J
AU  - Kelly, J
AD  - Space and Terrestrial Autonomous Robotic Systems (STARS) Lab, University
      of Toronto Institute for Aerospace Studies (UTIAS), Toronto, ON M3H 5T6,
      Canada; Autonomous Space Robotics Lab (ASRL), UTIAS, North York, ON M3H
      5T6, Canada
TI  - Learning Matchable Image Transformations for Long-Term Metric Visual
      Localization
T2  - IEEE Robotics and Automation Letters
VL  - 5
IS  - 2
SP  - 1492-1499
PY  - 2020
DA  - 2020
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Long-term metric self-localization is an essential capability of
      autonomous mobile robots, but remains challenging for vision-based systems
      due to appearance changes caused by lighting, weather, or seasonal
      variations. While experience-based mapping has proven to be an effective
      technique for bridging the 'appearance gap,' the number of experiences
      required for reliable metric localization over days or months can be very
      large, and methods for reducing the necessary number of experiences are
      needed for this approach to scale. Taking inspiration from color constancy
      theory, we learn a nonlinear RGB-to-grayscale mapping that explicitly
      maximizes the number of inlier feature matches for images captured under
      different lighting and weather conditions, and use it as a pre-processing
      step in a conventional single-experience localization pipeline to improve
      its robustness to appearance change. We train this mapping by
      approximating the target non-differentiable localization pipeline with a
      deep neural network, and find that incorporating a learned low-dimensional
      context feature can further improve cross-appearance feature matching.
      Using synthetic and real-world datasets, we demonstrate substantial
      improvements in localization performance across day-night cycles, enabling
      continuous metric localization over a 30-hour period using a single
      mapping experience, and allowing experience-based localization to scale to
      long deployments with dramatically reduced data requirements. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2020.2967659
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079679538&doi=10.1109%2fLRA.2020.2967659&partnerID=40&md5=86c18ca6085961f1d756911157a03dcb
UR  - http://dx.doi.org/10.1109/LRA.2020.2967659
KW  - deep learning in robotics and automation
KW  - localization
KW  - visual learning
KW  - visual-based navigation
KW  - deep neural networks
KW  - image enhancement
KW  - lighting
KW  - mapping
KW  - pipelines
KW  - robots
KW  - user experience
KW  - autonomous mobile robot
KW  - image transformations
KW  - localization performance
KW  - pre-processing step
KW  - vision based system
KW  - visual localization
KW  - deep learning
N1  - cited By 6
ER  - 

TY  - CPAPER
AU  - Camara, L G
AU  - Gabert, C
AU  - Preucil, L
AD  - Czech Technical University in Prague, Czech Institute of Informatics,
      Prague, 160 00, Czech Republic
TI  - Highly Robust Visual Place Recognition Through Spatial Matching of CNN
      Features
SP  - 3748-3755
PY  - 2020
DA  - 2020
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - We revise, improve and extend the system previously introduced by us and
      named SSM-VPR (Semantic and Spatial Matching Visual Place Recognition),
      largely boosting its performance above the current state of the art. The
      system encodes images of places by employing the activations of different
      layers of a pre-trained, off-the-shelf, VGG16 Convolutional Neural Network
      (CNN) architecture. It consists of two stages: given a query image of a
      place, (1) a list of candidates is selected from a database of places and
      (2) the candidates are geometrically compared with the query. The
      comparison is made by matching CNN features and, equally important, their
      spatial locations, selecting the best candidate as the recognized place.
      The performance of the system is maximized by finding optimal image
      resolutions during the second stage and by exploiting temporal correlation
      between consecutive frames in the employed datasets. © 2020 IEEE.
DO  - 10.1109/ICRA40945.2020.9196967
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092722679&doi=10.1109%2fICRA40945.2020.9196967&partnerID=40&md5=21e5a38bec75fc8b093d83d3c6acc0f1
UR  - http://dx.doi.org/10.1109/ICRA40945.2020.9196967
KW  - convolutional neural networks
KW  - life-long navigation
KW  - loop closure
KW  - slam
KW  - visual place recognition
KW  - agricultural robots
KW  - image resolution
KW  - multilayer neural networks
KW  - query processing
KW  - robotics
KW  - semantics
KW  - different layers
KW  - place recognition
KW  - query images
KW  - spatial location
KW  - spatial matching
KW  - state of the art
KW  - temporal correlations
N1  - cited By 10; Conference of 2020 IEEE International Conference on Robotics
      and Automation, ICRA 2020 ; Conference Date: 31 May 2020 Through 31 August
      2020; Conference Code:163172
ER  - 

TY  - CPAPER
AU  - Murphy, L
AU  - Sibley, G
AD  - Department of Computer Science, George Washington University, Washington,
      DC 20052, United States
TI  - Incremental unsupervised topological place discovery
SP  - 1312-1318
PY  - 2014
DA  - 2014
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - This paper describes an online place discovery and recognition engine that
      fuses information over time to create topologically distinct places. A key
      motivation is the recognition that a single image may be a poor exemplar
      of what constitutes a place. Images are not 'places' nor are they
      'documents'. Instead, by treating image-sequences as a multimodal
      distribution over topics - and by discovering topics incrementally and
      online - it is possible to both reduce the memory footprint of place
      recognition systems, and to improve precision and recall. Distinctive
      key-places are represented by a cluster topics found from the covisibility
      graph of a relative simultaneous localization and mapping engine -
      key-places inherently span many images. A dynamic vocabulary of visual
      words and density based clustering is used to continually estimate a set
      of visual topics, changes in which drive the place-recognition process.
      The system is evaluated using an indoor robot sequence, a standard outdoor
      robot sequence and a long-term sequence from a static camera. Experiments
      demonstrate qualitatively distinct themes associated with discovered
      places - from common place types such as 'hallway', or 'desk-area', to
      temporal concepts such as 'dusk', 'dawn' or 'mid-day'. Compared to
      traditional image-based place-recognition, this reduces the information
      that must be stored without reducing place-recognition performance. © 2014
      IEEE.
DO  - 10.1109/ICRA.2014.6907022
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926205234&doi=10.1109%2fICRA.2014.6907022&partnerID=40&md5=6426e21c070c0eb17d5ac8f29a27d4eb
UR  - http://dx.doi.org/10.1109/ICRA.2014.6907022
KW  - engines
KW  - online systems
KW  - optical character recognition
KW  - robotics
KW  - topology
KW  - density-based clustering
KW  - memory footprint
KW  - multimodal distributions
KW  - place recognition
KW  - precision and recall
KW  - recognition engines
KW  - simultaneous localization and mapping
KW  - temporal concepts
KW  - image enhancement
N1  - cited By 22; Conference of 2014 IEEE International Conference on Robotics
      and Automation, ICRA 2014 ; Conference Date: 31 May 2014 Through 7 June
      2014; Conference Code:107395
ER  - 

TY  - CPAPER
AU  - Sun, L
AU  - Taher, M
AU  - Wild, C
AU  - Zhao, C
AU  - Zhang, Y
AU  - Majer, F
AU  - Yan, Z
AU  - Krajnik, T
AU  - Prescott, T
AU  - Duckett, T
AD  - University of Sheffield, Sheffield Robotics, United Kingdom; Department of
      Engineering Science, University of Oxford, United Kingdom; Harbin
      Institute of Technology, China; Czech Technical University in Prague,
      Czech Republic; CIAD UMR7533, Univ. Bourgogne Franche-Comte, UTBM, France;
      L-CAS, University of Lincoln, United Kingdom
TI  - Robust and Long-term Monocular Teach and Repeat Navigation using a
      Single-experience Map
SP  - 2635-2642
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - This paper presents a robust monocular visual teach-and-repeat (VTR)
      navigation system for long-term operation in outdoor environments. The
      approach leverages deep-learned descriptors to deal with the high
      illumination variance of the real world. In particular, a tailored
      self-supervised descriptor, DarkPoint, is proposed for autonomous
      navigation in outdoor environments. We seamlessly integrate the
      localisation with control, in which proportional-integral control is used
      to eliminate the visual error with the pitfall of the unknown depth.
      Consequently, our approach achieves day-to-night navigation using a
      single-experience map and is able to repeat complex and fast manoeuvres.
      To verify our approach, we performed a vast array of navigation
      experiments in various outdoor environments, where both navigation
      accuracy and robustness of the proposed system are investigated. The
      experimental results show that our approach is superior to the baseline
      method with regards to accuracy and robustness. © 2021 IEEE.
DO  - 10.1109/IROS51168.2021.9635886
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124182975&doi=10.1109%2fIROS51168.2021.9635886&partnerID=40&md5=de01da53c446e01abaa8fbcac9e76fa3
UR  - http://dx.doi.org/10.1109/IROS51168.2021.9635886
KW  - air navigation
KW  - computer vision
KW  - intelligent robots
KW  - robustness (control systems)
KW  - two term control systems
KW  - autonomous navigation
KW  - baseline methods
KW  - descriptors
KW  - experience maps
KW  - localisation
KW  - navigation accuracy
KW  - outdoor environment
KW  - proportional-integral control
KW  - real-world
KW  - navigation systems
N1  - cited By 1; Conference of 2021 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2021 ; Conference Date: 27 September
      2021 Through 1 October 2021; Conference Code:175617
ER  - 

TY  - JOUR
AU  - Sun, L
AU  - Yan, Z
AU  - Zaganidis, A
AU  - Zhao, C
AU  - Duckett, T
AD  - Lincoln Centre for Autonomous Systems (L-CAS), University of Lincoln,
      Lincoln, LN6 7TS, United Kingdom; Laboratoire Electronique, Informatique
      et Image, CNRS, University of Technology of Belfort-Montbéliard (UTBM),
      Belfort, 90010, France
TI  - Recurrent-OctoMap: Learning State-Based Map Refinement for Long-Term
      Semantic Mapping with 3-D-Lidar Data
T2  - IEEE Robotics and Automation Letters
VL  - 3
IS  - 4
SP  - 3749-3756
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - This letter presents a novel semantic mapping approach, Recurrent-OctoMap,
      learned from long-term three-dimensional (3-D) Lidar data. Most existing
      semantic mapping approaches focus on improving semantic understanding of
      single frames, rather than 3-D refinement of semantic maps (i.e. fusing
      semantic observations). The most widely used approach for the 3-D semantic
      map refinement is 'Bayes update,' which fuses the consecutive predictive
      probabilities following a Markov-chain model. Instead, we propose a
      learning approach to fuse the semantic features, rather than simply fusing
      predictions from a classifier. In our approach, we represent and maintain
      our 3-D map as an OctoMap, and model each cell as a recurrent neural
      network, to obtain a Recurrent-OctoMap. In this case, the semantic mapping
      process can be formulated as a sequence-to-sequence encoding-decoding
      problem. Moreover, in order to extend the duration of observations in our
      Recurrent-OctoMap, we developed a robust 3-D localization and mapping
      system for successively mapping a dynamic environment using more than two
      weeks of data, and the system can be trained and deployed with arbitrary
      memory length. We validate our approach on the ETH long-term 3-D Lidar
      dataset. The experimental results show that our proposed approach
      outperforms the conventional 'Bayes update' approach. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2018.2856268
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053706009&doi=10.1109%2fLRA.2018.2856268&partnerID=40&md5=594f4eb4a1b1a7839a2c5df96628102e
UR  - http://dx.doi.org/10.1109/LRA.2018.2856268
KW  - deep learning in robotics and automation
KW  - mapping
KW  - object detection
KW  - segmentation and categorization
KW  - simultaneous localization and mapping (slam)
KW  - deep learning
KW  - feature extraction
KW  - markov processes
KW  - object recognition
KW  - optical radar
KW  - recurrent neural networks
KW  - robotics
KW  - semantics
KW  - dynamic environments
KW  - markov chain models
KW  - recurrent neural network (rnn)
KW  - semantic understanding
KW  - short term memory
KW  - simultaneous localization and mapping
KW  - slam
KW  - two-dimensional displays
KW  - three dimensional displays
N1  - cited By 33
ER  - 

TY  - JOUR
AU  - Tang, L
AU  - Wang, Y
AU  - Tan, Q
AU  - Xiong, R
AD  - Department of Control Science and Engineering, Zhejiang University,
      Hangzhou, China; Beijing Key Laboratory of Intelligent Space Robotic
      System Technology and Applications, Beijing Institute of Spacecraft System
      Engineering, Beijing, China
TI  - Explicit feature disentanglement for visual place recognition across
      appearance changes
T2  - International Journal of Advanced Robotic Systems
VL  - 18
IS  - 6
PY  - 2021
DA  - 2021
PB  - SAGE Publications Inc.
AB  - In the long-term deployment of mobile robots, changing appearance brings
      challenges for localization. When a robot travels to the same place or
      restarts from an existing map, global localization is needed, where place
      recognition provides coarse position information. For visual sensors,
      changing appearances such as the transition from day to night and seasonal
      variation can reduce the performance of a visual place recognition system.
      To address this problem, we propose to learn domain-unrelated features
      across extreme changing appearance, where a domain denotes a specific
      appearance condition, such as a season or a kind of weather. We use an
      adversarial network with two discriminators to disentangle domain-related
      features and domain-unrelated features from images, and the
      domain-unrelated features are used as descriptors in place recognition.
      Provided images from different domains, our network is trained in a
      self-supervised manner which does not require correspondences between
      these domains. Besides, our feature extractors are shared among all
      domains, making it possible to contain more appearance without increasing
      model complexity. Qualitative and quantitative results on two toy cases
      are presented to show that our network can disentangle domain-related and
      domain-unrelated features from given data. Experiments on three public
      datasets and one proposed dataset for visual place recognition are
      conducted to illustrate the performance of our method compared with
      several typical algorithms. Besides, an ablation study is designed to
      validate the effectiveness of the introduced discriminators in our
      network. Additionally, we use a four-domain dataset to verify that the
      network can extend to multiple domains with one model while achieving
      similar performance. © The Author(s) 2021.
SN  - 1729-8806
DO  - 10.1177/17298814211037497
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120968635&doi=10.1177%2f17298814211037497&partnerID=40&md5=2631bf76da69fa4a7cff8a70dea9878f
UR  - http://dx.doi.org/10.1177/17298814211037497
KW  - adversarial
KW  - changing appearance
KW  - feature disentanglement
KW  - place recognition
KW  - self-supervised
KW  - discriminators
KW  - toys
KW  - global localization
KW  - localisation
KW  - performance
KW  - position information
KW  - visual sensor
KW  - machine learning
N1  - cited By 0
ER  - 

TY  - JOUR
AU  - Tang, L
AU  - Wang, Y
AU  - Ding, X
AU  - Yin, H
AU  - Xiong, R
AU  - Huang, S
AD  - State Key Laboratory of Industrial Control and Technology, and Institute
      of Cyber-Systems and Control, Zhejiang University, Hangzhou, China;
      iPlusBot, Hangzhou, China; Center for Autonomous Systems (CAS), University
      of Technology Sydney, Sydney, Australia
TI  - Topological local-metric framework for mobile robots navigation: a long
      term perspective
T2  - Autonomous Robots
VL  - 43
IS  - 1
SP  - 197-211
PY  - 2019
DA  - 2019
PB  - Springer New York LLC
AB  - Long term mapping and localization are the primary components for mobile
      robots in real world application deployment, of which the crucial
      challenge is the robustness and stability. In this paper, we introduce a
      topological local-metric framework (TLF), aiming at dealing with
      environmental changes, erroneous measurements and achieving constant
      complexity. TLF organizes the sensor data collected by the robot in a
      topological graph, of which the geometry is only encoded in the edge, i.e.
      the relative poses between adjacent nodes, relaxing the global consistency
      to local consistency. Therefore the TLF is more robust to unavoidable
      erroneous measurements from sensor information matching since the error is
      constrained in the local. Based on TLF, as there is no global coordinate,
      we further propose the localization and navigation algorithms by switching
      across multiple local metric coordinates. Besides, a lifelong memorizing
      mechanism is presented to memorize the environmental changes in the TLF
      with constant complexity, as no global optimization is required. In
      experiments, the framework and algorithms are evaluated on 21-session data
      collected by stereo cameras, which are sensitive to illumination, and
      compared with the state-of-art global consistent framework. The results
      demonstrate that TLF can achieve similar localization accuracy with that
      from global consistent framework, but brings higher robustness with lower
      cost. The localization performance can also be improved from sessions
      because of the memorizing mechanism. Finally, equipped with TLF, the robot
      navigates itself in a 1 km session autonomously. © 2018, Springer
      Science+Business Media, LLC, part of Springer Nature.
SN  - 0929-5593
DO  - 10.1007/s10514-018-9724-7
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044547008&doi=10.1007%2fs10514-018-9724-7&partnerID=40&md5=03f451b4d5483027fc8f6194bda4949a
UR  - http://dx.doi.org/10.1007/s10514-018-9724-7
KW  - lifelong learning
KW  - localization
KW  - mobile robot
KW  - navigation
KW  - global optimization
KW  - large scale systems
KW  - stereo image processing
KW  - topology
KW  - application deployment
KW  - framework and algorithms
KW  - life long learning
KW  - localization and navigation
KW  - localization performance
KW  - long-term perspective
KW  - mapping and localization
KW  - mobile robots
N1  - cited By 20
ER  - 

TY  - CPAPER
AU  - Wang, L
AU  - Chen, W
AU  - Wang, J
AD  - Institute of Medical Robotics, Shanghai Jiao Tong University, Key
      Laboratory of System Control and Information Processing, Ministry of
      Education, Department of Automation, Shanghai, 200240, China
TI  - Long-term localization with time series map prediction for mobile robots
      in dynamic environments
VL  - 2020-January
SP  - 8587-8593
PY  - 2020
DA  - 2020
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - In many applications of mobile robot, the environment is constantly
      changing. How to use historical information to analysis environmental
      changes and generate a map corresponding with current environment is
      important to achieve high-precision localization. Inspired by predictive
      mechanism of brain, this paper presents a long-term localization approach
      named ArmMPU (ARMA-based Map Prediction and Update) based on time series
      modeling and prediction. Autoregressive moving average model (ARMA), a
      kind of time series modeling method, is employed for environmental map
      modeling and prediction, then predicted map and filtered observation are
      fused to fix the prediction error. The simulation and experiment results
      show that the proposed method improves long-term localization performance
      in dynamic environments. © 2020 IEEE.
DO  - 10.1109/IROS45743.2020.9468884
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112463942&doi=10.1109%2fIROS45743.2020.9468884&partnerID=40&md5=b0389a34803e04922792176550055984
UR  - http://dx.doi.org/10.1109/IROS45743.2020.9468884
KW  - agricultural robots
KW  - autoregressive moving average model
KW  - forecasting
KW  - mobile robots
KW  - time series
KW  - auto-regressive moving average model (arma)
KW  - dynamic environments
KW  - environmental change
KW  - high-precision localization
KW  - historical information
KW  - localization performance
KW  - modeling and predictions
KW  - predictive mechanisms
KW  - intelligent robots
N1  - cited By 0; Conference of 2020 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2020 ; Conference Date: 24 October
      2020 Through 24 January 2021; Conference Code:167055
ER  - 

TY  - CPAPER
AU  - Wu, L
AU  - Wu, Y
AD  - School of Software Microelectronics of Peking University, Institute of
      Automation, Chinese Academy of Sciences, National Laboratory of Pattern
      Recognition, China; Institute of Automation, Chinese Academy of Sciences;
      University of Chinese Academy of Sciences, National Laboratory of Pattern
      Recognition, China
TI  - Deep Supervised Hashing with Similar Hierarchy for Place Recognition
SP  - 3781-3786
PY  - 2019
DA  - 2019
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Place recognition as one of the most significant requirements for
      long-term simultaneous localization and mapping (SLAM) has been developed
      rapidly in recent years. Also, deep learning is proved to be more capable
      than traditional methods to extract features under some complex
      environments. However, in real-world environments, there are many
      challenging problems such as viewpoint changes and illumination changes.
      The existing deep learning-based place recognition in extracting feature
      phases and matching process is both time-consuming. Moreover, features
      extracted from convolution neural network (CNN) are floating-point type
      with high dimension. In this paper, we propose deep supervised hashing for
      place recognition, where we design a similar hierarchy loss function to
      learn a model. The model can distinguish the similar images more
      accurately which is well suitable to place recognition. Besides the model
      can learn high quality hash codes by maximizing the likelihood of triplet
      labels. Experiments on several benchmark datasets for place recognition
      show that our approach is robust to viewpoints, illuminations and season
      changes with high accuracy. Furthermore, the trained model can extract
      features and match in real time on CPU with less memory consumption. ©
      2019 IEEE.
DO  - 10.1109/IROS40897.2019.8968599
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081154808&doi=10.1109%2fIROS40897.2019.8968599&partnerID=40&md5=6ed7066fbafaf54d127df7d764ba456d
UR  - http://dx.doi.org/10.1109/IROS40897.2019.8968599
KW  - digital arithmetic
KW  - hash functions
KW  - intelligent robots
KW  - robotics
KW  - benchmark datasets
KW  - complex environments
KW  - convolution neural network
KW  - extracting features
KW  - illumination changes
KW  - memory consumption
KW  - real world environments
KW  - simultaneous localization and mapping
KW  - deep learning
N1  - cited By 2; Conference of 2019 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2019 ; Conference Date: 3 November
      2019 Through 8 November 2019; Conference Code:157163
ER  - 

TY  - JOUR
AU  - Bosse, M
AU  - Zlot, R
AD  - Autonomous Systems Laboratory, CSIRO ICT Centre, Australia
TI  - Keypoint design and evaluation for place recognition in 2D lidar maps
T2  - Robotics and Autonomous Systems
VL  - 57
IS  - 12
SP  - 1211-1224
PY  - 2009
DA  - 2009
AB  - We address the place recognition problem, which we define as the problem
      of establishing whether an observed location has been previously seen, and
      if so, determining the transformation aligning the current observations to
      an existing map. In the contexts of robot navigation and mapping, place
      recognition amounts to globally localizing a robot or map segment without
      being given any prior estimate. An efficient method of solving this
      problem involves first selecting a set of keypoints in the scene which
      store an encoding of their local region, and then utilizing a
      sublinear-time search into a database of keypoints previously generated
      from the global map to identify places with common features. We present an
      algorithm to embed arbitrary keypoint descriptors in a reduced-dimension
      metric space, in order to frame the problem as an efficient nearest
      neighbor search. Given that there are a multitude of possibilities for
      keypoint design, we propose a general methodology for comparing keypoint
      location selection heuristics and descriptor models that describe the
      region around the keypoint. With respect to selecting keypoint locations,
      we introduce a metric that encodes how likely it is that the keypoint will
      be found in the presence of noise and occlusions during mapping passes.
      Metrics for keypoint descriptors are used to assess the distinguishability
      between the distributions of matches and non-matches and the probability
      the correct match will be found in an approximate k-nearest neighbors
      search. Verification of the test outcomes is done by comparing the various
      keypoint designs on a kilometers-scale place recognition problem. We apply
      our design evaluation methodology to three keypoint selection heuristics
      and six keypoint descriptor models. A full place recognition system is
      presented, including a series of match verification algorithms which
      effectively filter out false positives. Results from city-scale and
      long-term mapping problems illustrate our approach for both offline and
      online SLAM, map merging, and global localization and demonstrate that our
      algorithm is able to produce accurate maps over trajectories of hundreds
      of kilometers. Crown Copyright © 2009.
SN  - 0921-8890
DO  - 10.1016/j.robot.2009.07.009
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350728823&doi=10.1016%2fj.robot.2009.07.009&partnerID=40&md5=e83c3997ee99c0ccf0722acb23a94718
UR  - http://dx.doi.org/10.1016/j.robot.2009.07.009
KW  - data association
KW  - dimension reduction
KW  - localization
KW  - mapping
KW  - place recognition
KW  - regional point descriptor
KW  - slam
KW  - design
KW  - heuristic methods
KW  - location
KW  - optical radar
KW  - probability distributions
KW  - set theory
KW  - topology
KW  - problem solving
N1  - cited By 81
ER  - 

TY  - JOUR
AU  - Bürki, M
AU  - Cadena, C
AU  - Gilitschenski, I
AU  - Siegwart, R
AU  - Nieto, J
AD  - Autonomous Systems Lab, ETH Zürich, Zürich, Switzerland; Computer Science
      and Artificial Intelligence Lab, MIT, Cambridge, MA, United States
TI  - Appearance-based landmark selection for visual localization
T2  - Journal of Field Robotics
VL  - 36
IS  - 6
SP  - 1041-1073
PY  - 2019
DA  - 2019
PB  - John Wiley and Sons Inc
AB  - Visual localization in outdoor environments is subject to varying
      appearance conditions rendering it difficult to match current camera
      images against a previously recorded map. Although it is possible to
      extend the respective maps to allow precise localization across a wide
      range of differing appearance conditions, these maps quickly grow in size
      and become impractical to handle on a mobile robotic platform. To address
      this problem, we present a landmark selection algorithm that exploits
      appearance co-observability for efficient visual localization in outdoor
      environments. Based on the appearance condition inferred from recently
      observed landmarks, a small fraction of landmarks useful under the current
      appearance condition is selected and used for localization. This allows to
      greatly reduce the bandwidth consumption between the mobile platform and a
      map backend in a shared-map scenario, and significantly lowers the demands
      on the computational resources on said mobile platform. We derive a
      landmark ranking function that exhibits high performance under vastly
      changing appearance conditions and is agnostic to the distribution of
      landmarks across the different map sessions. Furthermore, we relate and
      compare our proposed appearance-based landmark ranking function to popular
      ranking schemes from information retrieval, and validate our results on
      the challenging University of Michigan North Campus long-term vision and
      LIDAR data sets (NCLT), including an evaluation of the localization
      accuracy using ground-truth poses. In addition to that, we investigate the
      computational and bandwidth resource demands. Our results show that by
      selecting 20–30% of landmarks using our proposed approach, a similar
      localization performance as the baseline strategy using all landmarks is
      achieved. © 2019 Wiley Periodicals, Inc.
SN  - 1556-4959
DO  - 10.1002/rob.21870
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067640191&doi=10.1002%2frob.21870&partnerID=40&md5=852a36bfcbca474822776f4dc134d84c
UR  - http://dx.doi.org/10.1002/rob.21870
KW  - landmark selection
KW  - long-term localization
KW  - multisession mapping
KW  - visual localization
KW  - wheeled robots
KW  - agricultural robots
KW  - drilling platforms
KW  - information retrieval
KW  - bandwidth consumption
KW  - computational resources
KW  - localization performance
KW  - university of michigan
KW  - wheeled robot
KW  - bandwidth
N1  - cited By 8
ER  - 

TY  - CPAPER
AU  - Dymczyk, M
AU  - Stumm, E
AU  - Nieto, J
AU  - Siegwart, R
AU  - Gilitschenski, I
AD  - Autonomous Systems Lab, ETH Zurich, Switzerland
TI  - Will it last? Learning stable features for long-term visual localization
SP  - 572-581
PY  - 2016
DA  - 2016
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - An increasing number of simultaneous localization and mapping (SLAM)
      systems are using appearance-based localization to improve the quality of
      pose estimates. However, with the growing time-spans and size of the areas
      we want to cover, appearance-based maps are often becoming too large to
      handle and are consisting of features that are not always reliable for
      localization purposes. This paper presents a method for selecting map
      features that are persistent over time and thus suited for long-term
      localization. Our methodology relies on a CNN classifier based on image
      patches and depth maps for recognizing which features are suitable for
      life-long matchability. Thus, the classifier not only considers the
      appearance of a feature but also takes into account its expected lifetime.
      As a result, our feature selection approach produces more compact maps
      with a high fraction of temporally-stable features compared to the current
      state-of-the-art, while rejecting unstable features that typically harm
      localization. Our approach is validated on indoor and outdoor datasets,
      that span over a period of several months. © 2016 IEEE.
DO  - 10.1109/3DV.2016.66
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011318002&doi=10.1109%2f3DV.2016.66&partnerID=40&md5=6e17d2ff56ae3b9f45f149fc69900f26
UR  - http://dx.doi.org/10.1109/3DV.2016.66
KW  - cnn
KW  - feature selection
KW  - localization
KW  - machine learning
KW  - mapping
KW  - place recognition
KW  - slam
KW  - indoor positioning systems
KW  - learning systems
KW  - robotics
KW  - appearance based
KW  - expected lifetime
KW  - simultaneous localization and mapping
KW  - state of the art
KW  - visual localization
KW  - feature extraction
N1  - cited By 11; Conference of 4th International Conference on 3D Vision, 3DV
      2016 ; Conference Date: 25 October 2016 Through 28 October 2016;
      Conference Code:125615
ER  - 

TY  - CPAPER
AU  - Dymczyk, M
AU  - Lynen, S
AU  - Cieslewski, T
AU  - Bosse, M
AU  - Siegwart, R
AU  - Furgale, P
AD  - Autonomous Systems Lab, ETH Zurich, Switzerland
TI  - The gist of maps - Summarizing experience for lifelong localization
VL  - 2015-June
SP  - 2767-2773
PY  - 2015
DA  - 2015
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Robust, scalable place recognition is a core competency for many robotic
      applications. However, when revisiting places over and over, many
      state-of-the-art approaches exhibit reduced performance in terms of
      computation and memory complexity and in terms of accuracy. For successful
      deployment of robots over long time scales, we must develop algorithms
      that get better with repeated visits to the same environment, while still
      working within a fixed computational budget. This paper presents and
      evaluates an algorithm that alternates between online place recognition
      and offline map maintenance with the goal of producing the best
      performance with a fixed map size. At the core of the algorithm is the
      concept of a Summary Map, a reduced map representation that includes only
      the landmarks that are deemed most useful for place recognition. To assign
      landmarks to the map, we use a scoring function that ranks the utility of
      each landmark and a sampling policy that selects the landmarks for each
      place. The Summary Map can then be used by any descriptor-based inference
      method for constant-complexity online place recognition. We evaluate a
      number of scoring functions and sampling policies and show that it is
      possible to build and maintain maps of a constant size and that
      place-recognition performance improves over multiple visits. © 2015 IEEE.
DO  - 10.1109/ICRA.2015.7139575
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938252809&doi=10.1109%2fICRA.2015.7139575&partnerID=40&md5=ae0e3d100d94b16754f0e9449013429f
UR  - http://dx.doi.org/10.1109/ICRA.2015.7139575
KW  - agricultural robots
KW  - budget control
KW  - computational budget
KW  - core competencies
KW  - inference methods
KW  - map representations
KW  - place recognition
KW  - robotic applications
KW  - scoring functions
KW  - state-of-the-art approach
KW  - robotics
N1  - cited By 46; Conference of 2015 IEEE International Conference on Robotics
      and Automation, ICRA 2015 ; Conference Date: 26 May 2015 Through 30 May
      2015; Conference Code:113196
ER  - 

TY  - CPAPER
AU  - Dymczyk, M
AU  - Schneider, T
AU  - Gilitschenski, I
AU  - Siegwart, R
AU  - Stumm, E
AD  - Autonomous Systems Lab, ETH Zurich, Switzerland
TI  - Erasing bad memories: Agent-side summarization for long-term mapping
VL  - 2016-Noember
SP  - 4572-4579
PY  - 2016
DA  - 2016
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Precisely estimating the pose of an agent in a global reference frame is a
      crucial goal that unlocks a multitude of robotic applications, including
      autonomous navigation and collaboration. In order to achieve this, current
      state-oftheart localization approaches collect data provided by one or
      more agents and create a single, consistent localization map, maintained
      over time. However, with the introduction of lengthier sorties and the
      growing size of the environments, data transfers between the backend
      server where the global map is stored and the agents are becoming
      prohibitively large. While some existing methods partially address this
      issue by building compact summary maps, the data transfer from the agents
      to the backend can still easily become unmanageable. In this paper, we
      propose a method that is designed to reduce the amount of data that needs
      to be transferred from the agent to the backend, functioning in
      large-scale, multisession mapping scenarios. Our approach is based upon a
      landmark selection method that exploits information coming from multiple,
      possibly weak and correlated, landmark utility predictors; fused using
      learned feature coefficients. Such a selection yields a drastic reduction
      in data transfer while maintaining localization performance and the
      ability to efficiently summarize environments over time. We evaluate our
      approach on a data set that was autonomously collected in a dynamic indoor
      environment over a period of several months. © 2016 IEEE.
DO  - 10.1109/IROS.2016.7759673
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006470261&doi=10.1109%2fIROS.2016.7759673&partnerID=40&md5=3d025ca7098dfd25d280867572eee4e5
UR  - http://dx.doi.org/10.1109/IROS.2016.7759673
KW  - data transfer
KW  - intelligent robots
KW  - mapping
KW  - robots
KW  - autonomous navigation
KW  - back-end servers
KW  - indoor environment
KW  - landmark selection
KW  - localization performance
KW  - mapping scenarios
KW  - reference frame
KW  - robotic applications
KW  - autonomous agents
N1  - cited By 12; Conference of 2016 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2016 ; Conference Date: 9 October
      2016 Through 14 October 2016; Conference Code:125056
ER  - 

TY  - CPAPER
AU  - Gadd, M
AU  - Newman, P
AD  - Mobile Robotics Group, University of Oxford, Oxford, United Kingdom
TI  - Checkout my map: Version control for fleetwide visual localisation
VL  - 2016-Noember
SP  - 5729-5736
PY  - 2016
DA  - 2016
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - This paper is about underpinning long-term operations of fleets of
      vehicles using visual localisation. In particular it examines ways in
      which vehicles, considered as independent agents, can share, update and
      leverage each others' visual experiences in a mutually beneficial way. We
      draw on our previous work in Experience-based Navigation (EBN) [1], in
      which a visual map supporting multiple representations of the same place
      is built, yielding real-time localisation capability for a solitary
      vehicle. We now consider how any number of such agents might operate in
      concert via data sharing policies that are germane to the shared task of
      lifelong localisation. We rapidly construct considerable maps by the
      conjoining of work distributed to asynchronous processes, and share
      expertise amongst the team by the selective dispensing of mission-specific
      map contents. We demonstrate and evaluate our system against 100km of data
      collected in North Oxford over a period of a month featuring diverse
      deviation in appearance due to atmospheric, lighting, and structural
      dynamics. We show that our framework is capable of creating maps in a
      fraction of the time required by single-agent EBN, with no significant
      loss in localisation robustness, and is able to furnish robots on
      realworld forays with maps which require much less storage. © 2016 IEEE.
DO  - 10.1109/IROS.2016.7759843
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006365185&doi=10.1109%2fIROS.2016.7759843&partnerID=40&md5=cba5c4d812c08ce91767c008220cf366
UR  - http://dx.doi.org/10.1109/IROS.2016.7759843
KW  - digital storage
KW  - intelligent robots
KW  - robots
KW  - structural dynamics
KW  - vehicles
KW  - data sharing
KW  - independent agents
KW  - localisation
KW  - multiple representation
KW  - real-world
KW  - single-agent
KW  - version control
KW  - visual experiences
KW  - visual servoing
N1  - cited By 13; Conference of 2016 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2016 ; Conference Date: 9 October
      2016 Through 14 October 2016; Conference Code:125056
ER  - 

TY  - JOUR
AU  - Labbé, M
AU  - Michaud, F
AD  - Department of Electrical Engineering and Computer Engineering,
      Interdisciplinary Institute of Technological Innovation (3IT), Université
      de Sherbrooke, Sherbrooke, QC, Canada
TI  - RTAB-Map as an open-source lidar and visual simultaneous localization and
      mapping library for large-scale and long-term online operation
T2  - Journal of Field Robotics
VL  - 36
IS  - 2
SP  - 416-446
PY  - 2019
DA  - 2019
PB  - John Wiley and Sons Inc.
AB  - Distributed as an open-source library since 2013, real-time
      appearance-based mapping (RTAB-Map) started as an appearance-based loop
      closure detection approach with memory management to deal with large-scale
      and long-term online operation. It then grew to implement simultaneous
      localization and mapping (SLAM) on various robots and mobile platforms. As
      each application brings its own set of constraints on sensors, processing
      capabilities, and locomotion, it raises the question of which SLAM
      approach is the most appropriate to use in terms of cost, accuracy,
      computation power, and ease of integration. Since most of SLAM approaches
      are either visual- or lidar-based, comparison is difficult. Therefore, we
      decided to extend RTAB-Map to support both visual and lidar SLAM,
      providing in one package a tool allowing users to implement and compare a
      variety of 3D and 2D solutions for a wide range of applications with
      different robots and sensors. This paper presents this extended version of
      RTAB-Map and its use in comparing, both quantitatively and qualitatively,
      a large selection of popular real-world datasets (e.g., KITTI, EuRoC, TUM
      RGB-D, MIT Stata Center on PR2 robot), outlining strengths, and
      limitations of visual and lidar SLAM configurations from a practical
      perspective for autonomous navigation applications. © 2018 Wiley
      Periodicals, Inc.
SN  - 1556-4959
DO  - 10.1002/rob.21831
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055707168&doi=10.1002%2frob.21831&partnerID=40&md5=7375b4295b7a885075c4b065b7997993
UR  - http://dx.doi.org/10.1002/rob.21831
KW  - perception
KW  - position estimation
KW  - slam
KW  - mapping
KW  - optical radar
KW  - robots
KW  - sensory perception
KW  - autonomous navigation
KW  - open-source libraries
KW  - processing capability
KW  - real-world datasets
KW  - simultaneous localization and mapping
KW  - visual simultaneous localization and mappings
KW  - robotics
N1  - cited By 228
ER  - 

TY  - JOUR
AU  - Mazuran, M
AU  - Burgard, W
AU  - Tipaldi, G D
AD  - Department of Computer Science, University of Freiburg,
      Georges-Koehler-Allee 79, Freiburg, 79110, Germany
TI  - Nonlinear factor recovery for long-term SLAM
T2  - International Journal of Robotics Research
VL  - 35
IS  - 1-3
SP  - 50-72
PY  - 2016
DA  - 2016
PB  - SAGE Publications Inc.
AB  - For long-term operations, graph-based simultaneous localization and
      mapping (SLAM) approaches require nodes to be marginalized in order to
      control the computational cost. In this paper, we present a method to
      recover a set of nonlinear factors that best represents the marginal
      distribution in terms of Kullback-Leibler divergence. The proposed method,
      which we call nonlinear factor recovery (NFR), estimates both the mean and
      the information matrix of the set of nonlinear factors, where the recovery
      of the latter is equivalent to solving a convex optimization problem. NFR
      is able to provide either the dense distribution or a sparse approximation
      of it. In contrast to previous algorithms, our method does not necessarily
      require a global linearization point and can be used with any nonlinear
      measurement function. Moreover, we are not restricted to only using
      tree-based sparse approximations and binary factors, but we can include
      any topology and correlations between measurements. Experiments performed
      on several publicly available datasets demonstrate that our method
      outperforms the state of the art with respect to the Kullback-Leibler
      divergence and the sparsity of the solution. © The Author(s) 2015.
SN  - 0278-3649
DO  - 10.1177/0278364915581629
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953331595&doi=10.1177%2f0278364915581629&partnerID=40&md5=fe8ccca02738b5c5b3603d3096498578
UR  - http://dx.doi.org/10.1177/0278364915581629
KW  - graphical models
KW  - localization
KW  - mapping
KW  - mobile robotics
KW  - nonlinear optimization
KW  - slam
KW  - binary trees
KW  - convex optimization
KW  - graphic methods
KW  - nonlinear programming
KW  - optimization
KW  - recovery
KW  - graphical model
KW  - mobile robotic
KW  - non-linear optimization
KW  - robotics
N1  - cited By 35
ER  - 

TY  - CPAPER
AU  - Mohan, M
AU  - Galvez-Lopez, D
AU  - Monteleoni, C
AU  - Sibley, G
AD  - Department of Computer Science, George Washington University, United
      States; Department of Computer Science, University of Colorado Boulder,
      United States
TI  - Environment selection and hierarchical place recognition
VL  - 2015-June
SP  - 5487-5494
PY  - 2015
DA  - 2015
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - As robots continue to create long-term maps, the amount of information
      that they need to handle increases over time. In terms of place
      recognition, this implies that the number of images being considered may
      increase until exceeding the computational resources of the robot. In this
      paper we consider a scenario where, given multiple independent large maps,
      possibly from different cities or locations, a robot must effectively and
      in real time decide whether it can localize itself in one of those known
      maps. Since the number of images to be handled by such a system is likely
      to be extremely large, we find that it is beneficial to decompose the set
      of images into independent groups or environments. This raises a new
      question: Given a query image, how do we select the best environment? This
      paper proposes a similarity criterion that can be used to solve this
      problem. It is based on the observation that, if each environment is
      described in terms of its co-occurrent features, similarity between
      environments can be established by comparing their co-occurrence matrices.
      We show that this leads to a novel place recognition algorithm that
      divides the collection of images into environments and arranges them in a
      hierarchy of inverted indices. By selecting first the relevant environment
      for the operating robot, we can reduce the number of images to perform the
      actual loop detection, reducing the execution time while preserving the
      accuracy. The practicality of this approach is shown through experimental
      results on several large datasets covering a combined distance of more
      than 750Km. © 2015 IEEE.
DO  - 10.1109/ICRA.2015.7139966
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938263668&doi=10.1109%2fICRA.2015.7139966&partnerID=40&md5=59b92fe86c41ff1a95eff3fee8a498ff
UR  - http://dx.doi.org/10.1109/ICRA.2015.7139966
KW  - agricultural robots
KW  - robotics
KW  - robots
KW  - amount of information
KW  - co-occurrence-matrix
KW  - computational resources
KW  - environment selection
KW  - inverted indices
KW  - loop detection
KW  - place recognition
KW  - similarity criteria
KW  - large dataset
N1  - cited By 15; Conference of 2015 IEEE International Conference on Robotics
      and Automation, ICRA 2015 ; Conference Date: 26 May 2015 Through 30 May
      2015; Conference Code:113196
ER  - 

TY  - CPAPER
AU  - Rapp, M
AU  - Hahn, M
AU  - Thom, M
AU  - Dickmann, K J A D
AD  - Rapp, M (Corresponding Author), Univ Ulm, Inst Measurement Control &
      Microtechnol, D-89069 Ulm, Germany. Rapp, Matthias; Thom, Markus;
      Dietmayer, Klaus, Univ Ulm, Inst Measurement Control & Microtechnol,
      D-89069 Ulm, Germany. Hahn, Markus; Dickmann, Juergen, Daimler AG, Ulm,
      Germany.
TI  - Semi-Markov Process Based Localization using Radar in Dynamic Environments
T2  - 2015 IEEE 18TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION
      SYSTEMS
J2  - IEEE International Conference on Intelligent Transportation Systems-ITSC
SP  - 423-429
PY  - 2015
DA  - 2015
PB  - IEEE
AB  - Automotive localization in urban environment faces natural long-term
      changes of the surroundings. In this work, a robust Monte-Carlo based
      localization is presented. Robustness is achieved through a stochastic
      analysis of previous observations of the area of interest. The model uses
      a grid-based Markov chain to instantly model changes. An extension of this
      model by a Levy process allows statements about reliability and prediction
      for each cell of the grid. Experiments with a vehicle equipped with four
      short range radars show the localization accuracy performance improvement
      in a dynamic environment.
DO  - 10.1109/ITSC.2015.77
C1  - 345 E 47TH ST, NEW YORK, NY 10017 USA
UR  - http://dx.doi.org/10.1109/ITSC.2015.77
KW  - maps
N1  - 18th IEEE International Conference on Intelligent Transportation Systems,
      SPAIN, SEP 15-18, 2015
ER  - 

TY  - CPAPER
AU  - Lázaro, M T
AU  - Capobianco, R
AU  - Grisetti, G
AD  - Dipartimento di Ingegneria Informatica Automatica e Gestionale 'An-tonio
      Ruberti', Sapienza University of Rome, Italy
TI  - Efficient Long-term Mapping in Dynamic Environments
SP  - 153-160
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - As autonomous robots are increasingly being introduced in real-world
      environments operating for long periods of time, the difficulties of
      long-term mapping are attracting the attention of the robotics research
      community. This paper proposes a full SLAM system capable of handling the
      dynamics of the environment across a single or multiple mapping sessions.
      Using the pose graph SLAM paradigm, the system works on local maps in the
      form of 2D point cloud data which are updated over time to store the most
      up-to-date state of the environment. The core of our system is an
      efficient ICP-based alignment and merging procedure working on the clouds
      that copes with non-static entities of the environment. Furthermore, the
      system retains the graph complexity by removing out-dated nodes upon
      robust inter- and intra-session loop closure detections while graph
      coherency is preserved by using condensed measurements. Experiments
      conducted with real data from longterm SLAM datasets demonstrate the
      efficiency, accuracy and effectiveness of our system in the management of
      the mapping problem during long-term robot operation. © 2018 IEEE.
DO  - 10.1109/IROS.2018.8594310
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063005233&doi=10.1109%2fIROS.2018.8594310&partnerID=40&md5=84f88619fbe474b6f45e06effa5e5f36
UR  - http://dx.doi.org/10.1109/IROS.2018.8594310
KW  - intelligent robots
KW  - mapping
KW  - robotics
KW  - dynamic environments
KW  - graph complexity
KW  - mapping problem
KW  - merging procedure
KW  - point cloud data
KW  - real world environments
KW  - robot operations
KW  - robotics research
KW  - information management
N1  - cited By 16; Conference of 2018 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2018 ; Conference Date: 1 October
      2018 Through 5 October 2018; Conference Code:144267
ER  - 

TY  - CPAPER
AU  - Zhang, M
AU  - Chen, Y
AU  - Li, M
AD  - Alibaba DAMO Academy AI Labs, Hangzhou, China
TI  - SDF-Loc: Signed distance Field based 2D relocalization and map update in
      dynamic environments
VL  - 2019-July
SP  - 1997-2004
PY  - 2019
DA  - 2019
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - To empower an autonomous robot to perform long-term navigation in a given
      area, a concurrent localization and map update algorithm is required. In
      this paper, we tackle this problem by providing both theoretical analysis
      and algorithm design for robotic systems equipped with 2D laser range
      finders. The first key contribution of this paper is that we propose a
      hybrid signed distance field (SDF) framework for laser based localization.
      The proposed hybrid SDF integrates two methods with complementary
      characteristics, namely Euclidean SDF (ESDF) and Truncated SDF (TSDF).
      With our framework, accurate pose estimation and fast map update can be
      performed simultaneously. Moreover, we introduce a novel sliding window
      estimator which attains better accuracy by consistently utilizing sensor
      and map information with both scan-to-scan and scan-to-map data
      association. Real-world experimental results demonstrate that the proposed
      algorithm can be used for commercial robots in various environments with
      long-term usage. Experiments also show that our approach outperforms
      competing approaches by a wide margin. © 2019 American Automatic Control
      Council.
DO  - 10.23919/acc.2019.8814347
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072285847&doi=10.23919%2facc.2019.8814347&partnerID=40&md5=691a0c6bddc2e2f24a7b491b68e5bb87
UR  - http://dx.doi.org/10.23919/acc.2019.8814347
N1  - cited By 6; Conference of 2019 American Control Conference, ACC 2019 ;
      Conference Date: 10 July 2019 Through 12 July 2019; Conference Code:151322
ER  - 

TY  - JOUR
AU  - Carlevaris-Bianco, N
AU  - Kaess, M
AU  - Eustice, R M
AD  - Department of Electrical Engineering and Computer Science, University of
      Michigan, Ann Arbor, MI 48109-2145, United States; Robotics Institute,
      Carnegie Mellon University, Pittsburgh, PA 15213, United States;
      Department of Naval Architecture and Marine Engineering, University of
      Michigan, Ann Arbor, MI 48109-2145, United States
TI  - Generic node removal for factor-graph SLAM
T2  - IEEE Transactions on Robotics
VL  - 30
IS  - 6
SP  - 1371-1385
PY  - 2014
DA  - 2014
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - This paper reports on a generic factor-based method for node removal in
      factor-graph simultaneous localization and mapping (SLAM),whichwe call
      generic linear constraints (GLCs). The need for a generic node removal
      tool is motivated by long-term SLAMapplications,whereby nodes are removed
      in order to control the computational cost of graph optimization. GLC is
      able to produce a new set of linearized factors over the elimination
      clique that can represent either the true marginalization (i.e., dense
      GLC) or a sparse approximation of the true marginalization using a
      Chow-Liu tree (i.e., parse GLC). The proposed algorithm improves upon
      commonly used methods in two key ways: First, it is not limited to graphs
      with strictly full-state relative-pose factors and works equally well with
      other low-rank factors, such as those produced by monocular vision.
      Second, the new factors are produced in such a way that accounts for
      measurement correlation, which is a problem encountered in other methods
      that rely strictly upon pairwise measurement composition. We evaluate the
      proposed method over multiple real-world SLAM graphs and show that it
      outperforms other recently proposed methods in terms of Kullback-Leibler
      divergence. Additionally, we experimentally demonstrate that the proposed
      GLC method provides a principled and flexible tool to control the
      computational complexity of long-term graph SLAM, with results shown for
      34.9 h of real-world indoor-outdoor data covering 147.4 km collected over
      27 mapping sessions spanning a period of 15 months. © 2014 IEEE. Personal.
SN  - 1552-3098
DO  - 10.1109/TRO.2014.2347571
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916882069&doi=10.1109%2fTRO.2014.2347571&partnerID=40&md5=c0b614f7ca2c8a707375141c52c952ae
UR  - http://dx.doi.org/10.1109/TRO.2014.2347571
KW  - factor-graphs
KW  - long-term autonomy
KW  - marginalization
KW  - mobile robotics
KW  - simultaneous localization and mapping (slam)
KW  - computational complexity
KW  - mapping
KW  - robotics
KW  - mobile robotic
KW  - simultaneous localization and mapping
KW  - trees (mathematics)
N1  - cited By 59
ER  - 

TY  - JOUR
AU  - Chebrolu, N
AU  - Labe, T
AU  - Stachniss, C
AD  - Institute of Geodesy and Geoinformation, University of Bonn, Bonn, 53113,
      Germany
TI  - Robust long-term registration of UAV images of crop fields for precision
      agriculture
T2  - IEEE Robotics and Automation Letters
VL  - 3
IS  - 4
SP  - 3097-3104
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Continuous crop monitoring is an important aspect of precision agriculture
      and requires the registration of sensor data over longer periods of time.
      Often, fields are monitored using cameras mounted on unmanned aerial
      vehicles (UAVs) but strong changes in the visual appearance of the growing
      crops and the field itself poses serious challenges to conventional image
      registration methods. In this letter, we present a method for registering
      images of agricultural fields taken by an UAV over the crop season and
      present a complete pipeline for computing temporally aligned
      three-dimensional (3-D) point clouds of the field. Our approach exploits
      the inherent geometry of the crop arrangement in the field, which remains
      mostly static over time. This allows us to register the images even in the
      presence of strong visual changes. To this end, we propose a scale
      invariant, geometric feature descriptor that encodes the local plant
      arrangement geometry. The experiments suggest that we are able to register
      images taken over the crop season, including situations where matching
      with an off-the-shelf visual descriptor fails. We evaluate the accuracy of
      our matching system with respect to manually labeled ground truth. We
      furthermore illustrate that the reconstructed 3-D models are qualitatively
      correct and the registration results allow for monitoring growth
      parameters at a per plant level. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2018.2849603
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063304907&doi=10.1109%2fLRA.2018.2849603&partnerID=40&md5=4d18064900d98631d26ee6b4e4f6f539
UR  - http://dx.doi.org/10.1109/LRA.2018.2849603
KW  - robotics in agriculture and forestry
KW  - slam
KW  - antennas
KW  - forestry
KW  - geometry
KW  - precision agriculture
KW  - robotics
KW  - three dimensional computer graphics
KW  - unmanned aerial vehicles (uav)
KW  - agricultural fields
KW  - crop monitoring
KW  - geometric feature
KW  - growth parameters
KW  - matching system
KW  - registration methods
KW  - visual appearance
KW  - crops
N1  - cited By 32
ER  - 

TY  - JOUR
AU  - Piasco, N
AU  - Sidibé, D
AU  - Gouet-Brunet, V
AU  - Demonceaux, C
AD  - ImViA, VIBOT ERL CNRS 6000, Université Bourgogne Franche-Comté, Dijon,
      France; LaSTIG, IGN, ENSG, Université Paris-Est, Saint-Mandé, 94160,
      France; IBISC, Université Paris-Saclay - Univ Evry, Evry, 91020, France
TI  - Improving Image Description with Auxiliary Modality for Visual
      Localization in Challenging Conditions
T2  - International Journal of Computer Vision
VL  - 129
IS  - 1
SP  - 185-202
PY  - 2021
DA  - 2021
PB  - Springer
AB  - Image indexing for lifelong localization is a key component for a large
      panel of applications, including robot navigation, autonomous driving or
      cultural heritage valorization. The principal difficulty in long-term
      localization arises from the dynamic changes that affect outdoor
      environments. In this work, we propose a new approach for outdoor large
      scale image-based localization that can deal with challenging scenarios
      like cross-season, cross-weather and day/night localization. The key
      component of our method is a new learned global image descriptor, that can
      effectively benefit from scene geometry information during training. At
      test time, our system is capable of inferring the depth map related to the
      query image and use it to increase localization accuracy. We show through
      extensive evaluation that our method can improve localization
      performances, especially in challenging scenarios when the visual
      appearance of the scene has changed. Our method is able to leverage both
      visual and geometric clues from monocular images to create discriminative
      descriptors for cross-season localization and effective matching of images
      acquired at different time periods. Our method can also use weakly
      annotated data to localize night images across a reference dataset of
      daytime images. Finally we extended our method to reflectance modality and
      we compare multi-modal descriptors respectively based on geometry,
      material reflectance and a combination of both. © 2020, Springer
      Science+Business Media, LLC, part of Springer Nature.
SN  - 0920-5691
DO  - 10.1007/s11263-020-01363-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089885445&doi=10.1007%2fs11263-020-01363-6&partnerID=40&md5=dfe9bac192f78822b583fc9cf5506a51
UR  - http://dx.doi.org/10.1007/s11263-020-01363-6
KW  - depth from monocular
KW  - global image descriptor
KW  - image retrieval
KW  - localization
KW  - side modality learning
KW  - geometry
KW  - reflection
KW  - robots
KW  - search engines
KW  - geometry information
KW  - image descriptions
KW  - image-based localizations
KW  - localization accuracy
KW  - localization performance
KW  - outdoor environment
KW  - visual localization
KW  - weakly annotated data
KW  - image enhancement
N1  - cited By 4
ER  - 

TY  - CPAPER
AU  - Zhang, N
AU  - Warren, M
AU  - Barfoot, T D
AD  - University of Toronto, University of Toronto Institute for Aerospace
      Studies (UTIAS), Dufferin St, ON 4925, Canada
TI  - Learning Place-and-Time-Dependent Binary Descriptors for Long-Term Visual
      Localization
SP  - 828-835
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Vision-based navigation is extremely susceptible to natural scene changes.
      This can result in localization failures in less than a few hours after
      map creation. To combat short-term illumination changes as well as
      long-term seasonal variations, we propose using a place-and-time-dependent
      binary descriptor that adapts to different scenarios in an online fashion.
      This is achieved by extending the GRIEF [6] evolution algorithm in two
      ways: correspondence generation using a known pose change and the
      inclusion of LATCH triplets in addition to BRIEF comparisons for
      descriptor generation. We show the adaptive descriptor outperforms a
      single descriptor scheme for localization within a single-experience
      Visual Teach and Repeat (VTR) system while maintaining the efficiency of
      binary descriptors. By adapting the description function to different
      environmental conditions, it allows the system to operate for a longer
      period before a new experience is required. In the presence of extreme
      illumination changes from day to night, we obtain 40% more inlier matches
      compared to SURF. In the case of seasonal variations, a 70% increase is
      demonstrated. The increased correspondences result in more localizable
      sections along the paths, amounting to a 25% and 150% increase in the
      lighting and seasonal cases, respectively. © 2018 IEEE.
DO  - 10.1109/ICRA.2018.8460674
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063134571&doi=10.1109%2fICRA.2018.8460674&partnerID=40&md5=40c1dce45bcbd2c341fd66c6fcf6d880
UR  - http://dx.doi.org/10.1109/ICRA.2018.8460674
KW  - evolutionary algorithms
KW  - robotics
KW  - environmental conditions
KW  - evolution algorithms
KW  - extreme illuminations
KW  - illumination changes
KW  - on-line fashion
KW  - seasonal variation
KW  - vision based navigation
KW  - visual localization
KW  - robots
N1  - cited By 5; Conference of 2018 IEEE International Conference on Robotics
      and Automation, ICRA 2018 ; Conference Date: 21 May 2018 Through 25 May
      2018; Conference Code:139796
ER  - 

TY  - CPAPER
AU  - Vysotska, O
AU  - Naseer, T
AU  - Spinello, L
AU  - Burgard, W
AU  - Stachniss, C
TI  - Efficient and effective matching of image sequences under substantial
      appearance changes exploiting GPS priors
SP  - 2774-2779
PY  - 2015
DA  - 2015
AB  - The ability to localize a robot is an important capability and matching of
      observations under substantial changes is a prerequisite for robust
      long-term operation. This paper investigates the problem of efficiently
      coping with seasonal changes in image data. We present an extension of a
      recent approach [15] to visual image matching using sequence information.
      Our extension allows for exploiting GPS priors in the matching process to
      overcome the main computational bottleneck of the previous method and to
      handle loops within the image sequences. We present an experimental
      evaluation using real world data containing substantial seasonal changes
      and show that our approach outperforms the previous method in case a noisy
      GPS pose prior is available.
DO  - 10.1109/ICRA.2015.7139576
C1  - Piscataway, NJ, USA
UR  - http://dx.doi.org/10.1109/ICRA.2015.7139576
KW  - global positioning system
KW  - image matching
KW  - image sequences
KW  - slam (robots)
N1  - image sequences matching;GPS priors;image data;visual image
      matching;sequence information;seasonal changes;
ER  - 

TY  - JOUR
AU  - Biber, P
AU  - Duckett, T
AD  - Deptartment of Computer Science, WSI-GRIS, University of Tübingen,
      Tübingen, Germany; Deptartment of Computing and Informatics, University of
      Lincoln, Lincoln LN6 7TS, United Kingdom
TI  - Experimental analysis of sample-based maps for long-term SLAM
T2  - International Journal of Robotics Research
VL  - 28
IS  - 1
SP  - 20-33
PY  - 2009
DA  - 2009
AB  - This paper presents a system for long-term SLAM (simultaneous localization
      and mapping) by mobile service robots and its experimental evaluation in a
      real dynamic environment. To deal with the stability-plasticity dilemma
      (the trade-off between adaptation to new patterns and preservation of old
      patterns), the environment is represented by multiple timescales
      simultaneously (five in our experiments). A sample-based representation is
      proposed, where older memories fade at different rates depending on the
      timescale and robust statistics are used to interpret the samples. The
      dynamics of this representation are analyzed in a five-week experiment,
      measuring the relative influence of short- and long-term memories over
      time and further demonstrating the robustness of the approach. © 2009 SAGE
      Publications.
SN  - 0278-3649
DO  - 10.1177/0278364908096286
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149352781&doi=10.1177%2f0278364908096286&partnerID=40&md5=78d5804d47c15f3159e4dadb9c5c1bf0
UR  - http://dx.doi.org/10.1177/0278364908096286
KW  - dynamic environments
KW  - lifelong learning
KW  - mobile robot navigation
KW  - multi-timescale representations
KW  - simultaneous localization and mapping
KW  - conformal mapping
KW  - experiments
KW  - mobile robots
KW  - navigation
KW  - navigation systems
KW  - robotics
KW  - robots
N1  - cited By 53
ER  - 

TY  - CPAPER
AU  - Egger, P
AU  - Borges, P V K
AU  - Catt, G
AU  - Pfrunder, A
AU  - Siegwart, R
AU  - Dube, R
AD  - Data61, Robotics and Autonomous Systems Group, CSIRO, Australia; ETH
      Zurich, Autonomous Systems Lab, Switzerland
TI  - PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization
SP  - 3430-3437
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Reliable long-term localization is key for robotic systems in dynamic
      environments. In this paper, we propose a novel approach for long-term
      localization using 3D LiDARs, coined PoseMap. In essence, we extract
      distinctive features from range measurements and bundle these into local
      views along with observation poses. The sensor's trajectory is then
      estimated in a sliding window fashion by matching current and old features
      and minimizing the distances in-between. The map representation
      facilitates finding a suitable set of old features, by selecting the
      closest local map(s) for matching. Similarly to a visibility analysis,
      this procedure provides a suitable set of features for localization but at
      a fraction of the computational cost. PoseMap also allows for updates and
      extensions of the map at any time by replacing and adding local maps when
      necessary. We evaluate our approach using two platforms both equipped with
      a 3D LiDAR and an IMU, demonstrating localization at 8 Hz and robustness
      to changes in the environment such as moving vehicles and changing
      vegetation. PoseMap was implemented on an autonomous vehicle allowing it
      to drive autonomously over a period of 18 months through a mix of
      industrial and unstructured off-road environments, covering more than 100
      kms without a single localization failure. © 2018 IEEE.
DO  - 10.1109/IROS.2018.8593854
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062964443&doi=10.1109%2fIROS.2018.8593854&partnerID=40&md5=a4ffa03c55cdcd426ae0323d4f43dd8a
UR  - http://dx.doi.org/10.1109/IROS.2018.8593854
KW  - autonomous vehicles
KW  - off road vehicles
KW  - optical radar
KW  - computational costs
KW  - dynamic environments
KW  - map representations
KW  - moving vehicles
KW  - multi-environments
KW  - range measurements
KW  - road environment
KW  - visibility analysis
KW  - intelligent robots
N1  - cited By 24; Conference of 2018 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2018 ; Conference Date: 1 October
      2018 Through 5 October 2018; Conference Code:144267
ER  - 

TY  - CPAPER
AU  - Ganti, P
AU  - Waslander, S
AD  - Department of Mechanical and Mechatronics Engineering, University of
      Waterloo, Waterloo, Canada; Institute for Aerospace Studies, University of
      Toronto, Toronto, Canada
TI  - Network uncertainty informed semantic feature selection for visual SLAM
SP  - 121-128
PY  - 2019
DA  - 2019
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - In order to facilitate long-term localization using a visual simultaneous
      localization and mapping (SLAM) algorithm, careful feature selection can
      help ensure that reference points persist over long durations and the
      runtime and storage complexity of the algorithm remain consistent. We
      present SIVO (Semantically Informed Visual Odometry and Mapping), a novel
      information-theoretic feature selection method for visual SLAM which
      incorporates semantic segmentation and neural network uncertainty into the
      feature selection pipeline. Our algorithm selects points which provide the
      highest reduction in Shannon entropy between the entropy of the current
      state and the joint entropy of the state, given the addition of the new
      feature with the classification entropy of the feature from a Bayesian
      neural network. Each selected feature significantly reduces the
      uncertainty of the vehicle state and has been detected to be a static
      object (building, traffic sign, etc.) repeatedly with a high confidence.
      This selection strategy generates a sparse map which can facilitate
      long-term localization. The KITTI odometry dataset is used to evaluate our
      method, and we also compare our results against ORB-SLAM2. Overall, SIVO
      performs comparably to the baseline method while reducing the map size by
      almost 70%. © 2019 IEEE.
DO  - 10.1109/CRV.2019.00024
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071049485&doi=10.1109%2fCRV.2019.00024&partnerID=40&md5=d432634030828d03acd9dfa6a43c0970
UR  - http://dx.doi.org/10.1109/CRV.2019.00024
KW  - deep learning
KW  - information theory
KW  - localization
KW  - mapping
KW  - semantic segmentation
KW  - slam
KW  - agricultural robots
KW  - classification (of information)
KW  - computational complexity
KW  - computer vision
KW  - neural networks
KW  - robots
KW  - semantic web
KW  - semantics
KW  - bayesian neural networks
KW  - feature selection methods
KW  - network uncertainties
KW  - storage complexity
KW  - visual simultaneous localization and mappings
KW  - feature extraction
N1  - cited By 6; Conference of 16th Conference on Computer and Robot Vision,
      CRV 2019 ; Conference Date: 29 May 2019 Through 31 May 2019; Conference
      Code:150294
ER  - 

TY  - CPAPER
AU  - Gao, P
AU  - Zhang, H
AD  - Colorado School of Mines, Human-Centered Robotics Lab, Golden, CO 80401,
      United States
TI  - Long-term Place Recognition through Worst-case Graph Matching to Integrate
      Landmark Appearances and Spatial Relationships
SP  - 1070-1076
PY  - 2020
DA  - 2020
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Place recognition is an important component for simultaneously
      localization and mapping in a variety of robotics applications. Recently,
      several approaches using landmark information to represent a place showed
      promising performance to address long-term environment changes. However,
      previous approaches do not explicitly consider changes of the landmarks,
      i,e., old landmarks may disappear and new ones often appear over time. In
      addition, representations used in these approaches to represent landmarks
      are limited, based upon visual or spatial cues only. In this paper, we
      introduce a novel worst-case graph matching approach that integrates
      spatial relationships of landmarks with their appearances for long-term
      place recognition. Our method designs a graph representation to encode
      distance and angular spatial relationships as well as visual appearances
      of landmarks in order to represent a place. Then, we formulate place
      recognition as a graph matching problem under the worst-case scenario. Our
      approach matches places by computing the similarities of distance and
      angular spatial relationships of the landmarks that have the least similar
      appearances (i.e., worst-case). If the worst appearance similarity of
      landmarks is small, two places are identified to be not the same, even
      though their graph representations have high spatial relationship
      similarities. We evaluate our approach over two public benchmark datasets
      for long-term place recognition, including St. Lucia and CMU-VL. The
      experimental results have validated that our approach obtains the
      state-of-the-art place recognition performance, with a changing number of
      landmarks. © 2020 IEEE.
DO  - 10.1109/ICRA40945.2020.9196906
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092746607&doi=10.1109%2fICRA40945.2020.9196906&partnerID=40&md5=4837eae23583d73b511e84dbba4bf3c9
UR  - http://dx.doi.org/10.1109/ICRA40945.2020.9196906
KW  - agricultural robots
KW  - graph structures
KW  - knowledge representation
KW  - appearance similarities
KW  - environment change
KW  - graph matching problems
KW  - graph representation
KW  - localization and mappings
KW  - robotics applications
KW  - spatial relationships
KW  - worst case scenario
KW  - robotics
N1  - cited By 3; Conference of 2020 IEEE International Conference on Robotics
      and Automation, ICRA 2020 ; Conference Date: 31 May 2020 Through 31 August
      2020; Conference Code:163172
ER  - 

TY  - JOUR
AU  - Mühlfellner, P
AU  - Bürki, M
AU  - Bosse, M
AU  - Derendarz, W
AU  - Philippsen, R
AU  - Furgale, P
AD  - Department for Driver Assistance and Integrated Safety, Volkswagen AG,
      Halmstad University, Letter Box 011/1777, Wolfsburg, Germany; Autonomous
      Systems Lab, ETH Zürich, Leonhardstrasse 21, Zürich, Switzerland;
      Department for Driver Assistance and Integrated Safety, Volkswagen AG,
      Letter Box 011/1777, Wolfsburg, Germany; Intelligent Systems Lab, Halmstad
      University, Kristian IV's väg 3, Halmstad, Sweden
TI  - Summary Maps for Lifelong Visual Localization
T2  - Journal of Field Robotics
VL  - 33
IS  - 5
SP  - 561-590
PY  - 2016
DA  - 2016
PB  - John Wiley and Sons Inc.
AB  - Robots that use vision for localization need to handle environments that
      are subject to seasonal and structural change, and operate under changing
      lighting and weather conditions. We present a framework for lifelong
      localization and mapping designed to provide robust and metrically
      accurate online localization in these kinds of changing environments. Our
      system iterates between offline map building, map summary, and online
      localization. The offline mapping fuses data from multiple visually varied
      datasets, thus dealing with changing environments by incorporating new
      information. Before passing these data to the online localization system,
      the map is summarized, selecting only the landmarks that are deemed useful
      for localization. This Summary Map enables online localization that is
      accurate and robust to the variation of visual information in natural
      environments while still being computationally efficient. We present a
      number of summary policies for selecting useful features for localization
      from the multisession map, and we explore the tradeoff between
      localization performance and computational complexity. The system is
      evaluated on 77 recordings, with a total length of 30 kilometers,
      collected outdoors over 16 months. These datasets cover all seasons,
      various times of day, and changing weather such as sunshine, rain, fog,
      and snow. We show that it is possible to build consistent maps that span
      data collected over an entire year, and cover day-to-night transitions.
      Simple statistics computed on landmark observations are enough to produce
      a Summary Map that enables robust and accurate localization over a wide
      range of seasonal, lighting, and weather conditions. © 2015 Wiley
      Periodicals, Inc.
SN  - 1556-4959
DO  - 10.1002/rob.21595
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931069651&doi=10.1002%2frob.21595&partnerID=40&md5=42e1dd551bdb31f08dbc661d2e3a7100
UR  - http://dx.doi.org/10.1002/rob.21595
KW  - computer vision
KW  - data visualization
KW  - lighting
KW  - mapping
KW  - meteorology
KW  - changing environment
KW  - computationally efficient
KW  - localization and mappings
KW  - localization performance
KW  - natural environments
KW  - on-line localization
KW  - visual information
KW  - visual localization
KW  - social networking (online)
N1  - cited By 52
ER  - 

TY  - JOUR
AU  - Neubert, P
AU  - Sunderhauf, N
AU  - Protzel, P
TI  - Superpixel-based appearance change prediction for long-term navigation
      across seasons
T2  - Robotics and Autonomous Systems
VL  - 69
SP  - 15-27
PY  - 2015
DA  - 2015
CY  - Netherlands
AB  - Changing environments pose a serious problem to current robotic systems
      aiming at long term operation under varying seasons or local weather
      conditions. This paper is built on our previous work where we propose to
      learn to predict the changes in an environment. Our key insight is that
      the occurring scene changes are in part systematic, repeatable and
      therefore predictable. The goal of our work is to support existing
      approaches to place recognition by learning how the visual appearance of
      an environment changes over time and by using this learned knowledge to
      predict its appearance under different environmental conditions. We
      describe the general idea of appearance change prediction (ACP) and
      investigate properties of our novel implementation based on vocabularies
      of superpixels (SP-ACP). Our previous work showed that the proposed
      approach significantly improves the performance of SeqSLAM and BRIEF-Gist
      for place recognition on a subset of the Nordland dataset under extremely
      different environmental conditions in summer and winter. This paper
      deepens the understanding of the proposed SP-ACP system and evaluates the
      influence of its parameters. We present the results of a large-scale
      experiment on the complete 10 h Nordland dataset and appearance change
      predictions between different combinations of seasons. [All rights
      reserved Elsevier].
SN  - 0921-8890
DO  - 10.1016/j.robot.2014.08.005
UR  - http://dx.doi.org/10.1016/j.robot.2014.08.005
KW  - image recognition
KW  - knowledge based systems
KW  - mobile robots
KW  - navigation
KW  - robot vision
KW  - slam (robots)
N1  - superpixel-based appearance change prediction;long-term navigation;robotic
      systems;weather conditions;place recognition;visual appearance;learned
      knowledge;SP-ACP;SeqSLAM;BRIEF-Gist;Nordland dataset;
ER  - 

TY  - JOUR
AU  - Ozog, P
AU  - Carlevaris-Bianco, N
AU  - Kim, A
AU  - Eustice, R M
AD  - Department of Electrical Engineering and Computer Science, University of
      Michigan, Ann Arbor, MI, United States; Department of Civil and
      Environmental Engineering, Korea Advanced Institute of Science and
      Technology, Daejeon, S., South Korea; Department of Naval Architecture and
      Marine Engineering, University of Michigan, Ann Arbor, MI, United States
TI  - Long-term Mapping Techniques for Ship Hull Inspection and Surveillance
      using an Autonomous Underwater Vehicle
T2  - Journal of Field Robotics
VL  - 33
IS  - 3
SP  - 265-289
PY  - 2016
DA  - 2016
PB  - John Wiley and Sons Inc.
AB  - This paper reports on a system for an autonomous underwater vehicle to
      perform in situ, multiple session hull inspection using long-term
      simultaneous localization and mapping (SLAM). Our method assumes very
      little a priori knowledge, and it does not require the aid of acoustic
      beacons for navigation, which is a typical mode of navigation in this type
      of application. Our system combines recent techniques in underwater
      saliency-informed visual SLAM and a method for representing the ship hull
      surface as a collection of many locally planar surface features. This
      methodology produces accurate maps that can be constructed in real-time on
      consumer-grade computing hardware. A single-session SLAM result is
      initially used as a prior map for later sessions, where the robot
      automatically merges the multiple surveys into a common hull-relative
      reference frame. To perform the relocalization step, we use a particle
      filter that leverages the locally planar representation of the ship hull
      surface, and a fast visual descriptor matching algorithm. Finally, we
      apply the recently developed graph sparsification tool, generic linear
      constraints, as a way to manage the computational complexity of the SLAM
      system as the robot accumulates information across multiple sessions. We
      show results for 20 SLAM sessions for two large vessels over the course of
      days, months, and even up to three years, with a total path length of
      approximately 10.2 km. © 2015 Wiley Periodicals, Inc.
SN  - 1556-4959
DO  - 10.1002/rob.21582
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928197207&doi=10.1002%2frob.21582&partnerID=40&md5=6a622d7d83d9cb22408e58d13ed80ff6
UR  - http://dx.doi.org/10.1002/rob.21582
KW  - algorithms
KW  - autonomous underwater vehicles
KW  - distributed computer systems
KW  - image matching
KW  - information management
KW  - mapping
KW  - robotics
KW  - robots
KW  - ships
KW  - computing hardware
KW  - descriptor matching
KW  - graph sparsification
KW  - linear constraints
KW  - mapping techniques
KW  - ship-hull inspections
KW  - simultaneous localization and mapping
KW  - total path length
KW  - hulls (ship)
N1  - cited By 41
ER  - 

TY  - CPAPER
AU  - Schmuck, P
AU  - Chli, M
AD  - Vision for Robotics Lab, ETH Zurich, Zurich, 8092, Switzerland
TI  - On the Redundancy Detection in Keyframe-Based SLAM
SP  - 594-603
PY  - 2019
DA  - 2019
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Egomotion and scene estimation is a key component in automating robot
      navigation, as well as in virtual reality applications for mobile phones
      or head-mounted displays. It is well known, however, that with long
      exploratory trajectories and multi-session mapping for long-term autonomy
      or collaborative applications, the maintenance of the ever-increasing size
      of these maps quickly becomes a bottleneck. With the explosion of data
      resulting in increasing runtime of the optimization algorithms ensuring
      the accuracy of the Simultaneous Localization And Mapping (SLAM)
      estimates, the large quantity of collected experiences is imposing hard
      limits on the scalability of such techniques. Considering the
      keyframe-based paradigm of SLAM techniques, this paper investigates the
      redundancy inherent in SLAM maps, by quantifying the information of
      different experiences of the scene as encoded in keyframes. Here we
      propose and evaluate different information-theoretic and heuristic metrics
      to remove dispensable scene measurements with minimal impact on the
      accuracy of the SLAM estimates. Evaluating the proposed metrics in two
      state-of-the-art centralized collaborative SLAM systems, we provide our
      key insights into how to identify redundancy in keyframe-based SLAM. ©
      2019 IEEE.
DO  - 10.1109/3DV.2019.00071
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074975753&doi=10.1109%2f3DV.2019.00071&partnerID=40&md5=1e08202b3a5556a5948113d200ecdddb
UR  - http://dx.doi.org/10.1109/3DV.2019.00071
KW  - collaborative slam
KW  - graph compression
KW  - keyframe selection
KW  - multi robot systems
KW  - redundancy detection
KW  - slam
KW  - global system for mobile communications
KW  - helmet mounted displays
KW  - information theory
KW  - mapping
KW  - multipurpose robots
KW  - redundancy
KW  - virtual reality
KW  - graph compressions
KW  - key frame selection
KW  - multi-robot systems
KW  - robotics
N1  - cited By 3; Conference of 7th International Conference on 3D Vision, 3DV
      2019 ; Conference Date: 15 September 2019 Through 18 September 2019;
      Conference Code:153712
ER  - 

TY  - JOUR
AU  - Yin, P
AU  - Xu, L
AU  - Zhang, J
AU  - Choset, H
AD  - Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United
      States
TI  - FusionVLAD: A Multi-View Deep Fusion Networks for Viewpoint-Free 3D Place
      Recognition
T2  - IEEE Robotics and Automation Letters
VL  - 6
IS  - 2
SP  - 2304-2310
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Real-time 3D place recognition is a crucial technology to recover from
      localization failure in applications like autonomous driving, last-mile
      delivery, and service robots. However, it is challenging for 3D place
      retrieval methods to be accurate, efficient, and robust to the variant
      viewpoints differences. In this letter, we propose FusionVLAD, a
      fusion-based network that encodes a multi-view representation of sparse 3D
      point clouds into viewpoint-free global descriptors. The system consists
      of two parallel branches: a spherical-view branch for
      orientation-invariant feature extraction, and the top-down view branch for
      translation-insensitive feature extraction. Furthermore, we design a
      parallel fusion module to enhance the combination of region-wise feature
      connection between the two branches. Experiments on two public datasets
      and two generated datasets show that our method outperforms
      state-of-the-art with robust place recognition accuracy and efficient
      inference time. Besides, FusionVLAD requires limited computation resources
      and makes it extremely suitable for low-cost robots' long-term place
      recognition task. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2021.3061375
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101756700&doi=10.1109%2fLRA.2021.3061375&partnerID=40&md5=83956754921b8e7dea2e5676e9ba60f4
UR  - http://dx.doi.org/10.1109/LRA.2021.3061375
KW  - recognition
KW  - slam
KW  - visual learning
KW  - agricultural robots
KW  - extraction
KW  - robots
KW  - autonomous driving
KW  - computation resources
KW  - crucial technology
KW  - global descriptors
KW  - insensitive features
KW  - invariant feature extraction
KW  - place recognition
KW  - retrieval methods
KW  - feature extraction
N1  - cited By 4
ER  - 

TY  - CPAPER
AU  - Yin, P
AU  - Xu, L
AU  - Liu, Z
AU  - Li, L
AU  - Salman, H
AU  - He, Y
AU  - Xu, W
AU  - Wang, H
AU  - Choset, H
AD  - State Key Laboratory of Robotics, Shenyang Institute of Automation,
      Chinese Academy of Sciences, Shenyang, China; Department of Mechanical and
      Automation Engineering, Chinese University of Hong Kong, Hong Kong, Hong
      Kong; Robotics Institute at Carnegie Mellon University, Pittsburgh, United
      States; Department of Mechanical Engineering, University of Auckland, New
      Zealand
TI  - Stabilize an Unsupervised Feature Learning for LiDAR-based Place
      Recognition
SP  - 1162-1167
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Place recognition is one of the major challenges for the LiDAR-based
      effective localization and mapping task. Traditional methods are usually
      relying on geometry matching to achieve place recognition, where a global
      geometry map need to be restored. In this paper, we accomplish the place
      recognition task based on an end-to-end feature learning framework with
      the LiDAR inputs. This method consists of two core modules, a dynamic
      octree mapping module that generates local 2D maps with the consideration
      of the robot's motion; and an unsupervised place feature learning module
      which is an improved adversarial feature learning network with additional
      assistance for the long-term place recognition requirement. More
      specially, in place feature learning, we present an additional Generative
      Adversarial Network with a designed Conditional Entropy Reduction module
      to stabilize the feature learning process in an unsupervised manner. We
      evaluate the proposed method on the Kitti dataset and North Campus
      Long-Term LiDAR dataset. Experimental results show that the proposed
      method outperforms state-of-the-art in place recognition tasks under
      long-term applications. What's more, the feature size and inference
      efficiency in the proposed method are applicable in real-time performance
      on practical robotic platforms. © 2018 IEEE.
DO  - 10.1109/IROS.2018.8593562
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062952780&doi=10.1109%2fIROS.2018.8593562&partnerID=40&md5=1514be492fd378c184ff696d0250624f
UR  - http://dx.doi.org/10.1109/IROS.2018.8593562
KW  - intelligent robots
KW  - mapping
KW  - optical radar
KW  - adversarial networks
KW  - conditional entropy
KW  - geometry matching
KW  - inference efficiency
KW  - localization and mappings
KW  - real time performance
KW  - robotic platforms
KW  - unsupervised feature learning
KW  - machine learning
N1  - cited By 10; Conference of 2018 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2018 ; Conference Date: 1 October
      2018 Through 5 October 2018; Conference Code:144267
ER  - 

TY  - JOUR
AU  - Meng, Q
AU  - Guo, H
AU  - Zhao, X
AU  - Cao, D
AU  - Chen, H
AD  - State Key Laboratory of Automotive Simulation, And Control Jilin
      University Campus Nanling, Changchun, China
TI  - Loop-Closure Detection with a Multiresolution Point Cloud Histogram Mode
      in Lidar Odometry and Mapping for Intelligent Vehicles
T2  - IEEE/ASME Transactions on Mechatronics
VL  - 26
IS  - 3
SP  - 1307-1317
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Precise positioning is the basic condition for intelligent vehicles to
      complete perception, decision making and control tasks. In response to
      this challenge, in this article, lidar simultaneous localization and
      mapping (SLAM) is taken as the research object, and a SLAM system is
      designed that integrates motion compensation and ground information
      removal functions, and can construct a real-time environment map and
      determine its own position on the map while the vehicle is driving. A
      loop-closure detection method with a multiresolution point cloud histogram
      mode is proposed, which can effectively detect whether the vehicle passes
      through the same position and perform optimization to obtain globally
      consistent pose and map information in the urban conditions with more
      driving loops. We conduct experiments on the well-known KITTI dataset and
      compare the results with those of state-of-the-art systems. The
      experiments confirm that the lidar SLAM system designed in this article
      can provide accurate and effective positioning information for intelligent
      vehicles. The proposed loop-closure detection algorithm has an excellent
      real-time performance and accuracy, which can guarantee the long-term
      driving operation of these vehicles. © 1996-2012 IEEE.
SN  - 1083-4435
DO  - 10.1109/TMECH.2021.3062647
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102292282&doi=10.1109%2fTMECH.2021.3062647&partnerID=40&md5=b228c0eaa7aa4b6916bb4c620ab72874
UR  - http://dx.doi.org/10.1109/TMECH.2021.3062647
KW  - kitti dataset
KW  - lidar simultaneous localization and mapping (slam)
KW  - loop-closure detection
KW  - multiresolution histogram
KW  - pose estimation
KW  - decision making
KW  - intelligent vehicle highway systems
KW  - mapping
KW  - motion compensation
KW  - optical radar
KW  - slam robotics
KW  - detection algorithm
KW  - driving operations
KW  - positioning information
KW  - precise positioning
KW  - real time performance
KW  - real-time environment
KW  - simultaneous localisation and mappings
KW  - state-of-the-art system
KW  - vehicles
N1  - cited By 1
ER  - 

TY  - JOUR
AU  - Arroyo, R
AU  - Alcantarilla, P F
AU  - Bergasa, L M
AU  - Romera, E
TI  - Are you ABLE to perform a life-long visual topological localization?
T2  - Autonomous Robots
VL  - 42
IS  - 3
SP  - 665-685
PY  - 2018
DA  - 2018
CY  - Germany
AB  - Visual topological localization is a process typically required by varied
      mobile autonomous robots, but it is a complex task if long operating
      periods are considered. This is because of the appearance variations
      suffered in a place: dynamic elements, illumination or weather. Due to
      these problems, long-term visual place recognition across seasons has
      become a challenge for the robotics community. For this reason, we propose
      an innovative method for a robust and efficient life-long localization
      using cameras. In this paper, we describe our approach (ABLE), which
      includes three different versions depending on the type of images:
      monocular, stereo and panoramic. This distinction makes our proposal more
      adaptable and effective, because it allows to exploit the extra
      information that can be provided by each type of camera. Besides, we
      contribute a novel methodology for identifying places, which is based on a
      fast matching of global binary descriptors extracted from sequences of
      images. The presented results demonstrate the benefits of using ABLE,
      which is compared to the most representative state-of-the-art algorithms
      in long-term conditions.
SN  - 0929-5593
DO  - 10.1007/s10514-017-9664-7
UR  - http://dx.doi.org/10.1007/s10514-017-9664-7
KW  - image matching
KW  - image sequences
KW  - mobile robots
KW  - robot dynamics
KW  - robot vision
KW  - slam (robots)
N1  - life-long visual topological localization;dynamic elements;long-term
      visual place recognition;robotics community;mobile autonomous
      robots;robust life-long localization;cameras;ABLE;image sequence;image
      matching;
ER  - 

TY  - JOUR
AU  - Mur-Artal, R
AU  - Montiel, J M M
AU  - Tardos, J D
AD  - Instituto de Investigacion en Ingenieria de Aragon, Universidad de
      Zaragoza, Zaragoza, 50018, Spain
TI  - ORB-SLAM: A Versatile and Accurate Monocular SLAM System
T2  - IEEE Transactions on Robotics
VL  - 31
IS  - 5
SP  - 1147-1163
PY  - 2015
DA  - 2015
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - This paper presents ORB-SLAM, a feature-based monocular simultaneous
      localization and mapping (SLAM) system that operates in real time, in
      small and large indoor and outdoor environments. The system is robust to
      severe motion clutter, allows wide baseline loop closing and
      relocalization, and includes full automatic initialization. Building on
      excellent algorithms of recent years, we designed from scratch a novel
      system that uses the same features for all SLAM tasks: tracking, mapping,
      relocalization, and loop closing. A survival of the fittest strategy that
      selects the points and keyframes of the reconstruction leads to excellent
      robustness and generates a compact and trackable map that only grows if
      the scene content changes, allowing lifelong operation. We present an
      exhaustive evaluation in 27 sequences from the most popular datasets.
      ORB-SLAM achieves unprecedented performance with respect to other
      state-of-the-art monocular SLAM approaches. For the benefit of the
      community, we make the source code public. © 2004-2012 IEEE.
SN  - 1552-3098
DO  - 10.1109/TRO.2015.2463671
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988339174&doi=10.1109%2fTRO.2015.2463671&partnerID=40&md5=4b4977ad9316c26565f7ec3db1348c31
UR  - http://dx.doi.org/10.1109/TRO.2015.2463671
KW  - lifelong mapping
KW  - localization
KW  - monocular vision
KW  - recognition
KW  - simultaneous localization and mapping (slam)
KW  - mapping
KW  - robotics
KW  - vision
KW  - automatic initialization
KW  - outdoor environment
KW  - simultaneous localization and mapping
KW  - state of the art
KW  - survival-of-the-fittest
KW  - indoor positioning systems
N1  - cited By 3399
ER  - 

TY  - JOUR
AU  - Paul, R
AU  - Newman, P
AD  - Oxford University Mobile Robotics Research Group, United Kingdom
TI  - Self-help: Seeking out perplexing images for ever improving topological
      mapping
T2  - International Journal of Robotics Research
VL  - 32
IS  - 14
SP  - 1742-1766
PY  - 2013
DA  - 2013
AB  - In this work, we present a novel approach that allows a robot to improve
      its own navigation performance through introspection and then targeted
      data retrieval. It is a step in the direction of life-long learning and
      adaptation and is motivated by the desire to build robots that have
      plastic competencies which are not baked in. They should react to and
      benefit from use. We consider a particular instantiation of this problem
      in the context of place recognition. Based on a topic-based probabilistic
      representation for images, we use a measure of perplexity to evaluate how
      well a working set of background images explain the robot's online view of
      the world. Offline, the robot then searches an external resource to seek
      out additional background images that bolster its ability to localize in
      its environment when used next. In this way the robot adapts and improves
      performance through use. We demonstrate this approach using data collected
      from a mobile robot operating in outdoor workspaces. © The Author(s) 2013.
SN  - 0278-3649
DO  - 10.1177/0278364913509859
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892570749&doi=10.1177%2f0278364913509859&partnerID=40&md5=ae84e124e6511bb39915fcda8cfb006d
UR  - http://dx.doi.org/10.1177/0278364913509859
KW  - life-long learning
KW  - perplexity
KW  - topic models
KW  - topological mapping
KW  - external resources
KW  - life long learning
KW  - navigation performance
KW  - place recognition
KW  - probabilistic representation
KW  - topic model
KW  - topology
KW  - robots
N1  - cited By 3
ER  - 

TY  - JOUR
AU  - Griffith, S
AU  - Pradalier, C
AD  - GeorgiaTech Lorraine-UMI 2958 GT-CNRS, France
TI  - Survey Registration for Long-Term Natural Environment Monitoring
T2  - Journal of Field Robotics
VL  - 34
IS  - 1
SP  - 188-208
PY  - 2017
DA  - 2017
PB  - John Wiley and Sons Inc.
AB  - This paper presents a survey registration framework to assist in the
      recurrent inspection of a natural environment. Our framework coarsely
      aligns surveys at the image-level using visual simultaneous localization
      and mapping (SLAM), and it registers images at the pixel-level using SIFT
      Flow, which enables rapid manual inspection. The variation in appearance
      of natural environments makes data association a primary challenge of this
      work. We discuss this and other challenges, including 1) alternative
      approaches for coarsely aligning surveys of a natural environment, 2) how
      to select which images to compare between two surveys, and 3) strategies
      to boost image registration accuracy. We evaluate each stage of our
      approach, emphasizing alignment accuracy and stability with respect to
      large seasonal variations. Our domain is lakeshore monitoring, in which an
      autonomous surface vessel surveyed a 1-km lakeshore 33 times in 14 months.
      Our results show that our framework precisely aligns a significant number
      of images between surveys captured up to roughly three months apart, often
      across marked variation in appearance. Using these results, a human was
      able to spot several changes between surveys that would have otherwise
      gone unnoticed. © 2016 Wiley Periodicals, Inc.
SN  - 1556-4959
DO  - 10.1002/rob.21664
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978300256&doi=10.1002%2frob.21664&partnerID=40&md5=8f24475aa12c1a445ba7ae2aa797e1a7
UR  - http://dx.doi.org/10.1002/rob.21664
KW  - robotics
KW  - alignment accuracy
KW  - autonomous surface vessels
KW  - data association
KW  - manual inspection
KW  - natural environments
KW  - registration accuracy
KW  - seasonal variation
KW  - visual simultaneous localization and mappings
KW  - surveys
N1  - cited By 6
ER  - 

TY  - CPAPER
AU  - Hochdorfer, S
AU  - Schlegel, C
AD  - Department of Computer Science, University of Applied Sciences Ulm, 89075
      Ulm, Germany
TI  - Towards a robust visual SLAM approach: Addressing the challenge of
      life-long operation
SP  - 1-6
PY  - 2009
DA  - 2009
AB  - Localization and mapping are fundamental problems in service robotics.
      Knowledge about the own pose and representations of the environment are
      needed for a series of high level applications. Service robots should be
      designed for life-long and robust operation in dynamic environments. The
      contribution of this paper is twofold. First, an approach to address the
      ever growing number of landmarks in life-long operation is presented.
      Typically, SLAM approaches just accumulate features over time and do not
      discard them anymore. Therefore, the required resources in terms of memory
      and processing power are growing over time. In our approach, the absolute
      number of landmarks can be restricted by an upper bound since we introduce
      a method to specifically select and replace landmarks once the upper bound
      has been reached. The second contribution is related to improving the
      robustness of the landmark assignment problem in case of image based
      features as needed with natural landmarks. The approach has been
      successfully evaluated in a real world experiment on a Pioneer-3DX
      platform within a complex unmodified indoor environment.
C1  - Munich
UR  - https://ieeexplore.ieee.org/document/5174794
KW  - absolute number
KW  - assignment problems
KW  - dynamic environments
KW  - fundamental problem
KW  - high level applications
KW  - image-based features
KW  - indoor environment
KW  - natural landmark
KW  - processing power
KW  - real world experiment
KW  - robust operation
KW  - service robotics
KW  - service robots
KW  - slam approach
KW  - upper bound
KW  - visual slam
KW  - robotics
KW  - robots
N1  - cited By 12; Conference of 2009 International Conference on Advanced
      Robotics, ICAR 2009 ; Conference Date: 22 June 2009 Through 26 June 2009;
      Conference Code:78376
ER  - 

TY  - CPAPER
AU  - Hochdorfer, S
AU  - Lutz, M
AU  - Schlegel, C
AD  - University of Applied Sciences Ulm, Computer Science Department,
      Prittwitzstr. 10, D-89075 Ulm, Germany
TI  - Lifelong localization of a mobile service-robot in everyday indoor
      environments using omnidirectional vision
SP  - 161-166
PY  - 2009
DA  - 2009
AB  - SLAM (Simultaneous Localization and Mapping) mechanisms are a key
      component towards advanced service robotics applications. Currently, a
      major hurdle on the way to lifelong localization is the handling of the
      ever growing amount of landmarks over time. Therefore, the required
      resources in terms of memory and processing power are also growing over
      time. An approach to restrict the absolute number of landmarks by an upper
      bound was presented in [1]. The key was a method to specifically select
      and replace landmarks once an upper bound has been reached. In this paper,
      we extend that landmark rating and selection approach. The here presented
      extension improves the landmark rating and selection process. Landmarks
      are kept such that their visibility regions better approximate the robot's
      operational area. A landmark with a low information content in a sparsely
      known region is often more useful than a landmark with a higher
      information content in a well-known region. Clustering algorithms are used
      to identify regions in the environment with a high landmark density.
      Removing a landmark from a cluster with high localization support will
      have the smallest degradation of robot localization quality. Real-world
      experiments are used to demonstrate the performance of our approach. These
      experiments are performed on a P3DX-platform with a bearing-only SLAM
      approach. All three approaches of handling landmarks (the standard
      approach without upper bound on the number of landmarks, the improved and
      the previous landmark rating and selection process) are compared against
      each other. ©2009 IEEE.
DO  - 10.1109/TEPRA.2009.5339626
C1  - Woburn, MA
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-71849120407&doi=10.1109%2fTEPRA.2009.5339626&partnerID=40&md5=d4275ef0f8c53e56efb4e8b783fd1447
UR  - http://dx.doi.org/10.1109/TEPRA.2009.5339626
KW  - absolute number
KW  - bearing-only
KW  - indoor environment
KW  - information contents
KW  - key component
KW  - mobile service
KW  - omni-directional vision
KW  - operational area
KW  - processing power
KW  - real world experiment
KW  - robot localization
KW  - selection process
KW  - service robotics
KW  - slam (simultaneous localization and mapping)
KW  - slam approach
KW  - upper bound
KW  - clustering algorithms
KW  - robot applications
KW  - robots
N1  - cited By 5; Conference of 2009 IEEE International Conference on
      Technologies for Practical Robot Applications, TePRA 2009 ; Conference
      Date: 9 November 2009 Through 10 November 2009; Conference Code:78905
ER  - 

TY  - CPAPER
AU  - Luthardt, S
AU  - Willert, V
AU  - Adamy, J
AD  - Control Methods and Robotics, TU Darmstadt, Germany
TI  - LLama-SLAM: Learning High-Quality Visual Landmarks for Long-Term Mapping
      and Localization
VL  - 2018-Noember
SP  - 2645-2652
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - The precise localization of vehicles is an important requirement for
      autonomous driving or advanced driver assistance systems. Using common
      GNSS the ego position can be measured but not with the reliability and
      precision necessary. An alternative approach to achieve precise
      localization is the usage of visual landmarks observed by a camera mounted
      in the vehicle. However, this raises the necessity of reliable visual
      landmarks that are easily recognizable and persistent. We propose a novel
      SLAM algorithm that focuses on learning and mapping such visual long-term
      landmarks (LLamas). The algorithm therefore processes stereo image streams
      from several recording sessions in the same spatial area. The key part
      within LLama-SLAM is the assessment of the landmarks with quality values
      that are inferred as viewpoint dependent probabilities from observation
      statistics. By adding solely landmarks of high quality to the final LLama
      Map, it can be kept compact while still allowing reliable localization.
      Due to the long-term evaluation of the GNSS measurement during the
      sessions, the landmarks can be positioned precisely in a global referenced
      coordinate system. For a first assessment of the algorithm's capabilities,
      we present some experimental results from the mapping process combining
      three sessions recorded over two months on the same route. © 2018 IEEE.
DO  - 10.1109/ITSC.2018.8569323
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060435416&doi=10.1109%2fITSC.2018.8569323&partnerID=40&md5=c3c0d03fa57e999e0de3ad4ebd75fa83
UR  - http://dx.doi.org/10.1109/ITSC.2018.8569323
KW  - automobile drivers
KW  - global positioning system
KW  - image recording
KW  - intelligent systems
KW  - mapping
KW  - stereo image processing
KW  - autonomous driving
KW  - co-ordinate system
KW  - long-term evaluation
KW  - mapping and localization
KW  - mapping process
KW  - quality value
KW  - slam algorithm
KW  - visual landmarks
KW  - advanced driver assistance systems
N1  - cited By 5; Conference of 21st IEEE International Conference on
      Intelligent Transportation Systems, ITSC 2018 ; Conference Date: 4
      November 2018 Through 7 November 2018; Conference Code:143575
ER  - 

TY  - JOUR
AU  - Nuske, S
AU  - Roberts, J
AU  - Wyeth, G
AD  - School of Information Technology and Electrical Engineering, University of
      Queensland, St Lucia, QLD 4072, Australia; Autonomous Systems Lab.,
      CSIROICT Centre, P.O. Box 883, Kenmore, QLD 4069, Australia; Robotics
      Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States
TI  - Robust outdoor visual localization using a three-dimensional-edge map
T2  - Journal of Field Robotics
VL  - 26
IS  - 9
SP  - 728-756
PY  - 2009
DA  - 2009
AB  - Visual localization systems that are practical for autonomous vehicles in
      outdoor industrial applications must perform reliably in a wide range of
      conditions. Changing outdoor conditions cause difficulty by drastically
      altering the information available in the camera images. To confront the
      problem, we have developed a visual localization system that uses a
      surveyed three-dimensional (3D)-edge map of permanent structures in the
      environment. The map has the invariant properties necessary to achieve
      long-term robust operation. Previous 3D-edge map localization systems
      usually maintain a single pose hypothesis, making it difficult to
      initialize without an accurate prior pose estimate and also making them
      susceptible to misalignment with unmapped edges detected in the camera
      image. A multihypothesis particle filter is employed here to perform the
      initialization procedure with significant uncertainty in the vehicle's
      initial pose. A novel observation function for the particle filter is
      developed and evaluated against two existing functions. The new function
      is shown to further improve the abilities of the particle filter to
      converge given a very coarse estimate of the vehicle's initial pose. An
      intelligent exposure control algorithm is also developed that improves the
      quality of the pertinent information in the image. Results gathered over
      an entire sunny day and also during rainy weather illustrate that the
      localization system can operate in a wide range of outdoor conditions. The
      conclusion is that an invariant map, a robust multihypothesis localization
      algorithm, and an intelligent exposure control algorithm all combine to
      enable reliable visual localization through challenging outdoor
      conditions. © 2009 Wiley Periodicals, Inc.
SN  - 1556-4959
DO  - 10.1002/rob.20306
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449347269&doi=10.1002%2frob.20306&partnerID=40&md5=69c315cf2d49f400e52e72af3182bb6e
UR  - http://dx.doi.org/10.1002/rob.20306
KW  - autonomous vehicles
KW  - camera images
KW  - edge map
KW  - invariant properties
KW  - localization algorithm
KW  - localization system
KW  - particle filter
KW  - robust operation
KW  - three-dimensional (3d)
KW  - visual localization
KW  - air filters
KW  - algorithms
KW  - cameras
KW  - exposure controls
KW  - industrial applications
KW  - nonlinear filtering
KW  - three dimensional
KW  - edge detection
N1  - cited By 26
ER  - 

TY  - JOUR
AU  - Ouerghi, S
AU  - Boutteau, R
AU  - Savatier, X
AU  - Thai, F
AD  - Ouerghi, S (Corresponding Author), Carthage Univ, SUPCOM, GRESCOM, El
      Ghazela 2083, Tunisia. Ouerghi, Safa; Thai, Fethi, Carthage Univ, SUPCOM,
      GRESCOM, El Ghazela 2083, Tunisia. Boutteau, Remi; Savatier, Xavier,
      Normandie Univ, UNIROUEN, ESIGELEC, IRSEEM, F-76000 Rouen, France.
TI  - Visual Odometry and Place Recognition Fusion for Vehicle Position Tracking
      in Urban Environments
T2  - SENSORS
VL  - 18
IS  - 4
SP  - 939
PY  - 2018
DA  - 2018
PB  - MDPI
CY  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
AB  - In this paper, we address the problem of vehicle localization in urban
      environments. We rely on visual odometry, calculating the incremental
      motion, to track the position of the vehicle and on place recognition to
      correct the accumulated drift of visual odometry, whenever a location is
      recognized. The algorithm used as a place recognition module is SeqSLAM,
      addressing challenging environments and achieving quite remarkable
      results. Specifically, we perform the long-term navigation of a vehicle
      based on the fusion of visual odometry and SeqSLAM. The template library
      for this latter is created online using navigation information from the
      visual odometry module. That is, when a location is recognized, the
      corresponding information is used as an observation of the filter. The
      fusion is done using the EKF and the UKF, the well-known nonlinear state
      estimation methods, to assess the superior alternative. The algorithm is
      evaluated using the KITTI dataset and the results show the reduction of
      the navigation errors by loop-closure detection. The overall position
      error of visual odometery with SeqSLAM is 0.22% of the trajectory, which
      is much smaller than the navigation errors of visual odometery alone
      0.45%. In addition, despite the superiority of the UKF in a variety of
      estimation problems, our results indicate that the UKF performs as
      efficiently as the EKF at the expense of an additional computational
      overhead. This leads to the conclusion that the EKF is a better choice for
      fusing visual odometry and SeqSlam in a long-term navigation context.
DO  - 10.3390/s18040939
UR  - http://dx.doi.org/10.3390/s18040939
KW  - real-time navigation
KW  - visual-odometry
KW  - seqslam
KW  - loop-closure
KW  - ekf
KW  - ukf
KW  - fab-map
KW  - localization
ER  - 

TY  - CPAPER
AU  - Siva, S
AU  - Zhang, H
AD  - Department of Computer Science, Human-Centered Robotics Lab Colorado
      School of Mines, Golden, CO 80401, United States
TI  - Omnidirectional multisensory perception fusion for long-term place
      recognition
SP  - 5175-5181
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Over the recent years, long-term place recognition has attracted an
      increasing attention to detect loops for largescale Simultaneous
      Localization and Mapping (SLAM) in loopy environments during long-term
      autonomy. Almost all existing methods are designed to work with
      traditional cameras with a limited field of view. Recent advances in
      omnidirectional sensors offer a robot an opportunity to perceive the
      entire surrounding environment. However, no work has existed thus far to
      research how omnidirectional sensors can help long-term place recognition,
      especially when multiple types of omnidirectional sensory data are
      available. In this paper, we propose a novel approach to integrate
      observations obtained from multiple sensors from different viewing angles
      in the omnidirectional observation in order to perform multi-directional
      place recognition in longterm autonomy. Our approach also answers two new
      questions when omnidirectional multisensory data is available for place
      recognition, including whether it is possible to recognize a place with
      long-term appearance variations when robots approach it from various
      directions, and whether observations from various viewing angles are the
      same informative. To evaluate our approach and hypothesis, we have
      collected the first large-scale dataset that consists of omnidirectional
      multisensory (intensity and depth) data collected in urban and suburban
      environments across a year. Experimental results have shown that our
      approach is able to achieve multi-directional long-term place recognition,
      and identifies the most discriminative viewing angles from the
      omnidirectional observation. © 2018 IEEE.
DO  - 10.1109/ICRA.2018.8461042
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063155590&doi=10.1109%2fICRA.2018.8461042&partnerID=40&md5=8bbd4edf62e44e00f08c8ea0f729f192
UR  - http://dx.doi.org/10.1109/ICRA.2018.8461042
KW  - large dataset
KW  - large-scale dataset
KW  - multiple sensors
KW  - multisensory data
KW  - multisensory perceptions
KW  - omni-directional sensors
KW  - place recognition
KW  - simultaneous localization and mapping
KW  - surrounding environment
KW  - robotics
N1  - cited By 13; Conference of 2018 IEEE International Conference on Robotics
      and Automation, ICRA 2018 ; Conference Date: 21 May 2018 Through 25 May
      2018; Conference Code:139796
ER  - 

TY  - CPAPER
AU  - Siva, S
AU  - Nahman, Z
AU  - Zhang, H
AD  - Human-Centered Robotics Lab at Colorado School of Mines, Golden, CO 80401,
      United States
TI  - Voxel-based representation learning for place recognition based on 3D
      point clouds
SP  - 8351-8357
PY  - 2020
DA  - 2020
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Place recognition is a critical component towards addressing the key
      problem of Simultaneous Localization and Mapping (SLAM). Most existing
      methods use visual images; whereas, place recognition using 3D point
      clouds, especially based on the voxel representations, has not been well
      addressed yet. In this paper, we introduce the novel approach of
      voxel-based representation learning (VBRL) that uses 3D point clouds to
      recognize places with long-term environment variations. VBRL splits a 3D
      point cloud input into voxels and uses multi-modal features extracted from
      these voxels to perform place recognition. Additionally, VBRL uses
      structured sparsity-inducing norms to learn representative voxels and
      feature modalities that are important to match places under long-term
      changes. Both place recognition, and voxel and feature learning are
      integrated into a unified regularized optimization formulation. As the
      sparsity-inducing norms are non-smooth, it is hard to solve the formulated
      optimization problem. Thus, we design a new iterative optimization
      algorithm, which has a theoretical convergence guarantee. Experimental
      results have shown that VBRL performs place recognition well using 3D
      point cloud data and is capable of learning the importance of voxels and
      feature modalities. © 2020 IEEE.
DO  - 10.1109/IROS45743.2020.9340992
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102409922&doi=10.1109%2fIROS45743.2020.9340992&partnerID=40&md5=901ba2eba9cb42bbb3aa0d8657670586
UR  - http://dx.doi.org/10.1109/IROS45743.2020.9340992
KW  - agricultural robots
KW  - iterative methods
KW  - optimization
KW  - critical component
KW  - iterative optimization algorithms
KW  - optimization problems
KW  - place recognition
KW  - regularized optimizations
KW  - simultaneous localization and mapping
KW  - structured sparsities
KW  - voxel representation
KW  - intelligent robots
N1  - cited By 2; Conference of 2020 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2020 ; Conference Date: 24 October
      2020 Through 24 January 2021; Conference Code:167055
ER  - 

TY  - JOUR
AU  - Williams, S
AU  - Indelman, V
AU  - Kaess, M
AU  - Roberts, R
AU  - Leonard, J J
AU  - Dellaert, F
AD  - Institute for Robotics and Intelligent Machines, Georgia Institute of
      Technology, 801 Atlantic Drive, Atlanta, GA 30309, United States; Field
      Robotics Center, Robotics Institute, Carnegie Mellon University,
      Pittsburgh, PA, United States; Computer Science and Artificial
      Intelligence Laboratory, MIT, Cambridge, MA, United States
TI  - Concurrent filtering and smoothing: A parallel architecture for real-time
      navigation and full smoothing
T2  - International Journal of Robotics Research
VL  - 33
IS  - 12
SP  - 1544-1568
PY  - 2014
DA  - 2014
PB  - SAGE Publications Inc.
AB  - We present a parallelized navigation architecture that is capable of
      running in real-time and incorporating long-term loop closure constraints
      while producing the optimal Bayesian solution. This architecture splits
      the inference problem into a low-latency update that incorporates new
      measurements using just the most recent states (filter), and a
      high-latency update that is capable of closing long loops and smooths
      using all past states (smoother). This architecture employs the
      probabilistic graphical models of factor graphs, which allows the
      low-latency inference and high-latency inference to be viewed as
      sub-operations of a single optimization performed within a single
      graphical model. A specific factorization of the full joint density is
      employed that allows the different inference operations to be performed
      asynchronously while still recovering the optimal solution produced by a
      full batch optimization. Due to the real-time, asynchronous nature of this
      algorithm, updates to the state estimates from the high-latency smoother
      will naturally be delayed until the smoother calculations have completed.
      This architecture has been tested within a simulated aerial environment
      and on real data collected from an autonomous ground vehicle. In all
      cases, the concurrent architecture is shown to recover the full batch
      solution, even while updated state estimates are produced in real-time. ©
      2014 The Author(s).
SN  - 0278-3649
DO  - 10.1177/0278364914531056
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910065546&doi=10.1177%2f0278364914531056&partnerID=40&md5=5510798a1a83009b95aa1a26029fbbed
UR  - http://dx.doi.org/10.1177/0278364914531056
KW  - filtering
KW  - information fusion
KW  - probabilistic graphical models
KW  - real time navigation
KW  - slam
KW  - smoothing
KW  - agricultural robots
KW  - antennas
KW  - filtration
KW  - graphic methods
KW  - information filtering
KW  - navigation
KW  - state estimation
KW  - autonomous ground vehicles
KW  - concurrent architecture
KW  - loop closure constraints
KW  - navigation architectures
KW  - real-time navigation
KW  - parallel architectures
N1  - cited By 18
ER  - 

TY  - JOUR
AU  - Yang, S
AU  - Fan, G
AU  - Bai, L
AU  - Zhao, C
AU  - Li, D
AD  - School of Mechanical and Precision Instrument Engineering, Xi’an
      University of Technology, Xi’an, 710048, China
TI  - SGC-VSLAM: A semantic and geometric constraints VSLAM for dynamic indoor
      environments
T2  - Sensors (Switzerland)
VL  - 20
IS  - 8
SP  - 2432
PY  - 2020
DA  - 2020
PB  - MDPI AG
AB  - As one of the core technologies for autonomous mobile robots, Visual
      Simultaneous Localization and Mapping (VSLAM) has been widely researched
      in recent years. However, most state-of-the-art VSLAM adopts a strong
      scene rigidity assumption for analytical convenience, which limits the
      utility of these algorithms for real-world environments with independent
      dynamic objects. Hence, this paper presents a semantic and geometric
      constraints VSLAM (SGC-VSLAM), which is built on the RGB-D mode of
      ORB-SLAM2 with the addition of dynamic detection and static point cloud
      map construction modules. In detail, a novel improved quadtree-based
      method was adopted for SGC-VSLAM to enhance the performance of the feature
      extractor in ORB-SLAM (Oriented FAST and Rotated BRIEF-SLAM). Moreover, a
      new dynamic feature detection method called semantic and geometric
      constraints was proposed, which provided a robust and fast way to filter
      dynamic features. The semantic bounding box generated by YOLO v3 (You Only
      Look Once, v3) was used to calculate a more accurate fundamental matrix
      between adjacent frames, which was then used to filter all of the truly
      dynamic features. Finally, a static point cloud was estimated by using a
      new drawing key frame selection strategy. Experiments on the public TUM
      RGB-D (Red-Green-Blue Depth) dataset were conducted to evaluate the
      proposed approach. This evaluation revealed that the proposed SGC-VSLAM
      can effectively improve the positioning accuracy of the ORB-SLAM2 system
      in high-dynamic scenarios and was also able to build a map with the static
      parts of the real environment, which has long-term application value for
      autonomous mobile robots. © 2020 by the authors. Licensee MDPI, Basel,
      Switzerland.
SN  - 1424-8220
DO  - 10.3390/s20082432
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084169421&doi=10.3390%2fs20082432&partnerID=40&md5=b537da1df8b9754e0e37ba9435b039bf
UR  - http://dx.doi.org/10.3390/s20082432
KW  - dynamic feature filtering
KW  - dynamic indoor environment
KW  - orb-slam2
KW  - point cloud map
KW  - visual slam
KW  - geometry
KW  - indoor positioning systems
KW  - mobile robots
KW  - navigation
KW  - autonomous mobile robot
KW  - fundamental matrix
KW  - geometric constraint
KW  - indoor environment
KW  - key frame selection
KW  - positioning accuracy
KW  - real world environments
KW  - visual simultaneous localization and mappings
KW  - semantics
N1  - cited By 2
ER  - 

TY  - CPAPER
AU  - Zhu, S
AU  - Zhang, X
AU  - Guo, S
AU  - Li, J
AU  - Liu, H
AD  - The State Key Laboratory of Automotive Safety and Energy, The School of
      Vehicle and Mobility, Tsinghua University, Beijing, 100084, China; The
      Department of Computer Science and Technology, Tsinghua University,
      Beijing, 100084, China
TI  - Lifelong Localization in Semi-Dynamic Environment
VL  - 2021-May
SP  - 14389-14395
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Mapping and localization in non-static environments are fundamental
      problems in robotics. Most of previous methods mainly focus on static and
      highly dynamic objects in the environment, which may suffer from
      localization failure in semi-dynamic scenarios without considering objects
      with lower dynamics, such as parked cars and stopped pedestrians. In this
      paper, we introduce semantic mapping and lifelong localization approaches
      to recognize semi-dynamic objects in non-static environments. We also
      propose a generic framework that can integrate mainstream object detection
      algorithms with mapping and localization algorithms. The mapping method
      combines an object detection algorithm and a SLAM algorithm to detect
      semi-dynamic objects and constructs a semantic map that only contains
      semi-dynamic objects in the environment. During navigation, the
      localization method can classify observation corresponding to static and
      non-static objects respectively and evaluate whether those semi-dynamic
      objects have moved, to reduce the weight of invalid observation and
      localization fluctuation. Real-world experiments show that the proposed
      method can improve the localization accuracy of mobile robots in
      non-static scenarios. © 2021 IEEE
DO  - 10.1109/ICRA48506.2021.9561584
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125438883&doi=10.1109%2fICRA48506.2021.9561584&partnerID=40&md5=cd3e37612d5d11bb6fe9b4fa8507f254
UR  - http://dx.doi.org/10.1109/ICRA48506.2021.9561584
N1  - cited By 0; Conference of 2021 IEEE International Conference on Robotics
      and Automation, ICRA 2021 ; Conference Date: 30 May 2021 Through 5 June
      2021; Conference Code:177050
ER  - 

TY  - JOUR
AU  - An, S-Y
AU  - Lee, L-K
AU  - Oh, S-Y
AD  - Electronics and Telecommunications Research Institute (ETRI), Daegu,
      711-883, South Korea; Department of Electrical Engineering, Pohang
      University of Science and Technology (POSTECH), Pohang, Gyungbuk 790-784,
      South Korea
TI  - Ceiling vision-based active SLAM framework for dynamic and wide-open
      environments
T2  - Autonomous Robots
VL  - 40
IS  - 2
SP  - 291-324
PY  - 2016
DA  - 2016
PB  - Springer New York LLC
AB  - A typical indoor environment can be divided into three categories; office
      (or room), hallway, and wide-open space such as lobby and hall. There have
      been numerous approaches for solving simultaneous localization and mapping
      (SLAM) problem in office (or room) and hallway. However, direct
      application of the existing approaches to wide-open space may be failed,
      because it has some distinguished features compared to other indoor
      places. To solve this problem, this paper proposes a new ceiling
      vision-based active SLAM framework, with an emphasis on practical
      deployment of service robot for commercial use in dynamically changing and
      wide-open environments by adopting the ceiling vision. First, for defining
      ceiling feature which can be extracted regardless of complexity of ceiling
      pattern we introduce a model-free landmark, i.e., visual node descriptor,
      which consists of edge points and their orientations in image space.
      Second, a recursive ‘explore and exploit’ is proposed for autonomous
      mapping. It is recursively performed by spreading out mapped area
      gradually while the robot is actively localized in the map. It can improve
      map accuracy due to frequent small loop closing. Third, a dynamic edge
      link (DEL) is proposed to cope with environmental changes in the map.
      Owing to DEL, we do not need to filter out corrupted sensor data and to
      distinguish moving object from static one. Also, a self-repairing map
      mechanism is introduced to deal with unexpected installation or removal of
      inner structures. We therefore achieve long-term navigation. Several
      simulations and real experiments in various places show that the proposed
      active SLAM framework could build a topologically consistent map, and
      demonstrated that it can be applied well to real environments such as
      wide-open space in a city hall and railway station. © 2015, Springer
      Science+Business Media New York.
SN  - 0929-5593
DO  - 10.1007/s10514-015-9453-0
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955714361&doi=10.1007%2fs10514-015-9453-0&partnerID=40&md5=e80767e732bc3e6c550a488949184c68
UR  - http://dx.doi.org/10.1007/s10514-015-9453-0
KW  - ceiling vision
KW  - dynamic environment
KW  - mobile robot
KW  - slam
KW  - wide-open area
KW  - ceilings
KW  - mapping
KW  - mobile robots
KW  - robotics
KW  - robots
KW  - dynamic environments
KW  - environmental change
KW  - indoor environment
KW  - railway stations
KW  - real environments
KW  - simultaneous localization and mapping problems
KW  - indoor positioning systems
N1  - cited By 5
ER  - 

TY  - JOUR
AU  - Krajnik, T
AU  - Fentanes, J P
AU  - Santos, J M
AU  - Duckett, T
AD  - Lincoln Centre for Autonomous Systems, University of Lincoln, Lincoln, LN6
      7TS, United Kingdom; Faculty of Electrical Engineering, Czech Technical
      University, Prague, 16636, Czech Republic
TI  - FreMEn: Frequency map enhancement for long-term mobile robot autonomy in
      changing environments
T2  - IEEE Transactions on Robotics
VL  - 33
IS  - 4
SP  - 964-977
PY  - 2017
DA  - 2017
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - We present a new approach to long-term mobile robot mapping in dynamic
      indoor environments. Unlike traditional world models that are tailored to
      represent static scenes, our approach explicitly models environmental
      dynamics. We assume that some of the hidden processes that influence the
      dynamic environment states are periodic and model the uncertainty of the
      estimated state variables by their frequency spectra. The spectral model
      can represent arbitrary timescales of environment dynamics with low memory
      requirements. Transformation of the spectral model to the time domain
      allows for the prediction of the future environment states, which improves
      the robot's long-term performance in changing environments. Experiments
      performed over time periods of months to years demonstrate that the
      approach can efficiently represent large numbers of observations and
      reliably predict future environment states. The experiments indicate that
      the model's predictive capabilities improve mobile robot localization and
      navigation in changing environments. © 2004-2012 IEEE.
SN  - 1552-3098
DO  - 10.1109/TRO.2017.2665664
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015635452&doi=10.1109%2fTRO.2017.2665664&partnerID=40&md5=4d33d405535b3dd578db6a208b94485d
UR  - http://dx.doi.org/10.1109/TRO.2017.2665664
KW  - localization
KW  - long-term autonomy
KW  - mapping
KW  - mobile robots
KW  - robot applications
KW  - uncertainty analysis
KW  - changing environment
KW  - dynamic environments
KW  - environment dynamics
KW  - environmental dynamics
KW  - long term performance
KW  - mobile robot localization
KW  - mobile robot mappings
KW  - predictive capabilities
KW  - robots
N1  - cited By 64
ER  - 

TY  - CPAPER
AU  - Naseer, T
AU  - Suger, B
AU  - Ruhnke, M
AU  - Burgard, W
AD  - Autonomous Intelligent Systems Group, University of Freiburg, Germany
TI  - Vision-based Markov localization across large perceptual changes
SP  - 1-6
PY  - 2015
DA  - 2015
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Recently, there has been significant progress towards lifelong, autonomous
      operation of mobile robots, especially in the field of localization and
      mapping. One important challenge in this context is visual localization
      under substantial perceptual changes, for example, coming from different
      seasons. In this paper, we present an approach to localize a mobile robot
      with a low frequency camera with respect to an image sequence, recorded
      previously within a different season. Our approach uses a discrete Bayes
      filter and a sensor model based on whole image descriptors. Thereby it
      exploits sequential information to model the dynamics of the system. Since
      we compute a probability distribution over the whole state space, our
      approach can handle more complex trajectories that may include same season
      loop-closures as well as fragmented sub-sequences. Throughout an extensive
      experimental evaluation on challenging datasets, we demonstrate that our
      approach outperforms state-of-the-art techniques. © 2015 IEEE.
DO  - 10.1109/ECMR.2015.7324181
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962272490&doi=10.1109%2fECMR.2015.7324181&partnerID=40&md5=0450cc0fa1e6c4c9196d2d181a5f0d30
UR  - http://dx.doi.org/10.1109/ECMR.2015.7324181
KW  - cameras
KW  - context
KW  - databases
KW  - lead
KW  - matched filters
KW  - robot sensing systems
KW  - database systems
KW  - image processing
KW  - mobile robots
KW  - probability distributions
KW  - autonomous operations
KW  - experimental evaluation
KW  - localization and mappings
KW  - markov localizations
KW  - robot sensing system
KW  - sequential information
KW  - state-of-the-art techniques
KW  - robots
N1  - cited By 8; Conference of European Conference on Mobile Robots, ECMR 2015
      ; Conference Date: 2 September 2015 Through 4 September 2015; Conference
      Code:118603
ER  - 

TY  - CPAPER
AU  - Naseer, T
AU  - Oliveira, G L
AU  - Brox, T
AU  - Burgard, W
TI  - Semantics-aware visual localization under challenging perceptual
      conditions
SP  - 2614-2620
PY  - 2017
DA  - 2017
AB  - Visual place recognition under difficult perceptual conditions remains a
      challenging problem due to changing weather conditions, illumination and
      seasons. Long-term visual navigation approaches for robot localization
      should be robust to these dynamics of the environment. Existing methods
      typically leverage feature descriptions of whole images or image regions
      from Deep Convolutional Neural Networks. Some approaches also exploit
      sequential information to alleviate the problem of spatially inconsistent
      and non-perfect image matches. In this paper, we propose a novel approach
      for learning a discriminative holistic image representation which exploits
      the image content to create a dense and salient scene description. These
      salient descriptions are learnt over a variety of datasets under large
      perceptual changes. Such an approach enables us to precisely segment the
      regions of an image which are geometrically stable over large time lags.
      We combine features from these salient regions and an off-the-shelf
      holistic representation to form a more robust scene descriptor. We also
      introduce a semantically labeled dataset which captures extreme perceptual
      and structural scene dynamics over the course of 3 years. We evaluated our
      approach with extensive experiments on data collected over several
      kilometers in Freiburg and show that our learnt image representation
      outperforms off-the-shelf features from the deep networks and hand-crafted
      features.
DO  - 10.1109/ICRA.2017.7989305
C1  - Piscataway, NJ, USA
UR  - http://dx.doi.org/10.1109/ICRA.2017.7989305
KW  - convolution
KW  - image matching
KW  - image representation
KW  - image segmentation
KW  - neural nets
KW  - robot vision
KW  - slam (robots)
N1  - semantics-aware visual localization;visual place recognition;long-term
      visual navigation;robot localization;image feature descriptions;deep
      convolutional neural networks;spatially inconsistent image
      matches;nonperfect image matches;discriminative holistic image
      representation;dense scene description;salient scene description;image
      segmentation;perceptual scene dynamics;structural scene dynamics;Freiburg;
ER  - 

TY  - CPAPER
AU  - Qin, T
AU  - Chen, T
AU  - Chen, Y
AU  - Su, Q
AD  - IAS BU, Huawei Technologies, Shanghai, China
TI  - AVP-SLAM: Semantic visual mapping and localization for autonomous vehicles
      in the parking lot
SP  - 5939-5945
PY  - 2020
DA  - 2020
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Autonomous valet parking is a specific application for autonomous
      vehicles. In this task, vehicles need to navigate in narrow, crowded and
      GPS-denied parking lots. Accurate localization ability is of great
      importance. Traditional visual-based methods suffer from tracking lost due
      to texture-less regions, repeated structures, and appearance changes. In
      this paper, we exploit robust semantic features to build the map and
      localize vehicles in parking lots. Semantic features contain guide signs,
      parking lines, speed bumps, etc, which typically appear in parking lots.
      Compared with traditional features, these semantic features are long-term
      stable and robust to the perspective and illumination change. We adopt
      four surround-view cameras to increase the perception range. Assisting by
      an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system
      generates a global visual semantic map. This map is further used to
      localize vehicles at the centimeter level. We analyze the accuracy and
      recall of our system and compare it against other methods in real
      experiments. Furthermore, we demonstrate the practicability of the
      proposed system by the autonomous parking application. © 2020 IEEE.
DO  - 10.1109/IROS45743.2020.9340939
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098088026&doi=10.1109%2fIROS45743.2020.9340939&partnerID=40&md5=33fc7efe5b80a1c30d2caac0f3be48bf
UR  - http://dx.doi.org/10.1109/IROS45743.2020.9340939
KW  - agricultural robots
KW  - cameras
KW  - intelligent robots
KW  - semantics
KW  - textures
KW  - traffic signs
KW  - autonomous parking
KW  - centimeter levels
KW  - illumination changes
KW  - inertial measurement unit
KW  - semantic features
KW  - visual mapping
KW  - visual semantics
KW  - wheel encoders
KW  - autonomous vehicles
N1  - cited By 12; Conference of 2020 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2020 ; Conference Date: 24 October
      2020 Through 24 January 2021; Conference Code:167055
ER  - 

TY  - CPAPER
AU  - Taisho, T
AU  - Kanji, T
AD  - Graduate School of Engineering, University of Fukui, Japan
TI  - Mining DCNN landmarks for long-term visual SLAM
SP  - 570-576
PY  - 2016
DA  - 2016
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Long-term visual SLAM, in familiar, semi-dynamic, and partially changing
      environments is an important area of research in robotics. The main
      problem we faced is the question of how to describe a scene
      discriminatively and compactly-both of which are necessary in order to
      cope with changes in appearance and a large amount of visual information.
      In this study, we address the above issues by mining visual experience.
      Our strategy is to mine a library of raw visual images, termed visual
      experience, to find the relevant visual patterns to effectively explain
      the input scene. From a practical point of view, our work offers three
      main contributions over the previous work. First, it is the first
      application of discriminative visual features from deep convolutional
      neural networks (DCNN) to the task of visual landmark mining. Second, we
      show how to interpret a high-dimensional DCNN feature to a compact
      semantic representation of visual word. Third, we show that our approach
      can turn the scene description task with any feature (including the DCNN
      feature) into the task of mining visual experience. Experiments on a
      challenging cross-domain visual place recognition validate efficacy of the
      proposed approach. © 2016 IEEE.
DO  - 10.1109/ROBIO.2016.7866383
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016791083&doi=10.1109%2fROBIO.2016.7866383&partnerID=40&md5=e36421b6149f38e2c8739cd54eaa2a80
UR  - http://dx.doi.org/10.1109/ROBIO.2016.7866383
KW  - biomimetics
KW  - neural networks
KW  - semantics
KW  - changing environment
KW  - convolutional neural network
KW  - high-dimensional
KW  - place recognition
KW  - scene description
KW  - semantic representation
KW  - visual experiences
KW  - visual information
KW  - robotics
N1  - cited By 2; Conference of 2016 IEEE International Conference on Robotics
      and Biomimetics, ROBIO 2016 ; Conference Date: 3 December 2016 Through 7
      December 2016; Conference Code:126712
ER  - 

TY  - JOUR
AU  - Zeng, T
AU  - Si, B
AD  - Institute of Science and Technology for Brain-Inspired Intelligence, Fudan
      University, Shanghai, China; Key Laboratory of Computational Neuroscience
      and Brain-Inspired Intelligence (Fudan University), Ministry of Education,
      Shanghai, China; School of Systems Science, Beijing Normal University,
      Beijing, 100875, China
TI  - A brain-inspired compact cognitive mapping system
T2  - Cognitive Neurodynamics
VL  - 15
IS  - 1
SP  - 91-101
PY  - 2021
DA  - 2021
PB  - Springer Science and Business Media B.V.
AB  - In many simultaneous localization and mapping (SLAM) systems, the map of
      the environment grows over time as the robot explores the environment. The
      ever-growing map prevents long-term mapping, especially in large-scale
      environments. In this paper, we develop a compact cognitive mapping
      approach inspired by neurobiological experiments. Mimicking the firing
      activities of neighborhood cells, neighborhood fields determined by
      movement information, i.e. translation and rotation, are modeled to
      describe one of the distinct segments of the explored environment. The
      vertices with low neighborhood field activities are avoided to be added
      into the cognitive map. The optimization of the cognitive map is
      formulated as a robust non-linear least squares problem constrained by the
      transitions between vertices, and is numerically solved efficiently.
      According to the cognitive decision-making of place familiarity, loop
      closure edges are clustered depending on time intervals, and then batch
      global optimization of the cognitive map is performed to satisfy the
      combined constraint of the whole cluster. After the loop closure process,
      scene integration is performed, in which revisited vertices are removed
      subsequently to further reduce the size of the cognitive map. The compact
      cognitive mapping approach is tested on a monocular visual SLAM system in
      a naturalistic maze for a biomimetic animated robot. Our results
      demonstrate that the proposed method largely restricts the growth of the
      size of the cognitive map over time, and meanwhile, the compact cognitive
      map correctly represents the overall layout of the environment. The
      compact cognitive mapping method is well suitable for the representation
      of large-scale environments to achieve long-term robot navigation. © 2020,
      Springer Nature B.V.
SN  - 1871-4080
DO  - 10.1007/s11571-020-09621-6
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088837751&doi=10.1007%2fs11571-020-09621-6&partnerID=40&md5=6700e97fd085d68511c5eedc83964960
UR  - http://dx.doi.org/10.1007/s11571-020-09621-6
KW  - compact cognitive map
KW  - long-term mapping
KW  - neighborhood cells
KW  - neighborhood fields
KW  - slam
KW  - article
KW  - brain
KW  - cognitive map
KW  - decision making
KW  - human
KW  - human experiment
KW  - least square analysis
KW  - neighborhood
KW  - robotics
KW  - rotation
N1  - cited By 5
ER  - 

TY  - JOUR
AU  - Nguyen, T-M
AU  - Cao, M
AU  - Yuan, S
AU  - Lyu, T H Y A N
AU  - Xie, L
AD  - Nguyen, TM (Corresponding Author), Nanyang Technol Univ, Sch Elect & Elect
      Engn, Singapore 639798, Singapore. Nguyen, Thien-Minh; Cao, Muqing; Yuan,
      Shenghai; Lyu, Yang; Nguyen, Thien Hoang; Xie, Lihua, Nanyang Technol
      Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
TI  - VIRAL-Fusion: A Visual-Inertial-Ranging-Lidar Sensor Fusion Approach
T2  - IEEE TRANSACTIONS ON ROBOTICS
VL  - 38
IS  - 2
SP  - 958-977
PY  - 2022
DA  - 2022
PB  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
CY  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
AB  - In recent years, onboard self-localization (OSL) methods based on cameras
      or lidar have achieved many significant progresses. However, some issues
      such as estimation drift and robustness in low-texture environment still
      remain inherent challenges for OSL methods. On the other hand,
      infrastructure-based methods can generally overcome these issues, but at
      the expense of some installation cost. This poses an interesting problem
      of how to effectively combine these methods, so as to achieve localization
      with long-term consistency as well as flexibility compared to any single
      method. To this end, we propose a comprehensive optimization-based
      estimator for the 15-D state of an unmanned aerial vehicle (UAV), fusing
      data from an extensive set of sensors: inertial measurement unit (IMU),
      ultrawideband (UWB) ranging sensors, and multiple onboard visual-inertial
      and lidar odometry subsystems. In essence, a sliding window is used to
      formulate a sequence of robot poses, where relative rotational and
      translational constraints between these poses are observed in the IMU
      preintegration and OSL observations, while orientation and position are
      coupled in the body-offset UWB range observations. An optimization-based
      approach is developed to estimate the trajectory of the robot in this
      sliding window. We evaluate the performance of the proposed scheme in
      multiple scenarios, including experiments on public datasets,
      high-fidelity graphical-physical simulation, and field-collected data from
      UAV flight tests. The result demonstrates that our integrated localization
      method can effectively resolve the drift issue, while incurring minimal
      installation requirements.
SN  - 1552-3098
DO  - 10.1109/TRO.2021.3094157
UR  - http://dx.doi.org/10.1109/TRO.2021.3094157
KW  - quaternions
KW  - location awareness
KW  - matrix converters
KW  - visualization
KW  - simultaneous localization and mapping
KW  - laser radar
KW  - distance measurement
KW  - aerial robots
KW  - localization
KW  - optimization
KW  - versatile
KW  - odometry
KW  - slam
ER  - 

TY  - JOUR
AU  - Nguyen, V A
AU  - Starzyk, J A
AU  - Goh, W-B
AD  - CeMNet, N4-B2c-06, School of Computer Engineering, Nanyang Technological
      University, Singapore 639798, Singapore; School of Electrical Engineering
      and Computer Science, Russ College of Engineering and Technology, Ohio
      University, United States; Department of Applied Information Systems,
      University of Information Technology and Management, Rzeszow, Poland
TI  - A spatio-temporal Long-term Memory approach for visual place recognition
      in mobile robotic navigation
T2  - Robotics and Autonomous Systems
VL  - 61
IS  - 12
SP  - 1744-1758
PY  - 2013
DA  - 2013
AB  - This paper proposes a solution to the problem of mobile robotic
      localization using visual indoor image sequences with a biologically
      inspired spatio-temporal neural network approach. The system contains
      three major subsystems: a feature extraction module, a scene quantization
      module and a spatio-temporal long-term memory (LTM) module. During
      learning, the scene quantization module clusters the visual images set
      into scene tokens. A K-Iteration Fast Learning Artificial Neural Network
      (KFLANN) is employed as the core unit of the quantization module. The
      KFLANN network is driven by intrinsic statistics of the data stream and
      therefore does not require the number of clusters to be predefined. In
      addition, the KFLANN performance is less sensitive to data presentation
      ordering compared to popular clustering methods such as k-means, and can
      therefore produce a consistent number of stable centroids. Using scene
      tokens, the topological structure of the environment can be composed into
      sequences of tokens. These sequences are then learnt and stored in memory
      units in an LTM architecture, which is able to continuously and robustly
      recognize the visual input stream. The design of memory units addresses
      two critical problems in spatio-temporal learning, namely error tolerance
      and memory forgetting. The primary objective of this work is to explore
      the synergy between the strength of KFLANN and LTM models to address the
      visual topological localization problem. We demonstrate the efficiency and
      efficacy of the proposed framework on the challenging COsy Localization
      Dataset. © 2013 Elsevier B.V. All rights reserved.
SN  - 0921-8890
DO  - 10.1016/j.robot.2012.12.004
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887220871&doi=10.1016%2fj.robot.2012.12.004&partnerID=40&md5=d003d8a8ff3d0e1d3d41a67aa9350880
UR  - http://dx.doi.org/10.1016/j.robot.2012.12.004
KW  - long-term memory
KW  - spatio-temporal neural networks
KW  - topological robotic mapping
KW  - visual place recognition
KW  - fast learning artificial neural networks
KW  - long term memory
KW  - place recognition
KW  - robotic mapping
KW  - spatio-temporal
KW  - spatio-temporal learning
KW  - topological localization
KW  - topological structure
KW  - feature extraction
KW  - iterative methods
KW  - neural networks
KW  - topology
KW  - robotics
N1  - cited By 6
ER  - 

TY  - JOUR
AU  - Ila, V
AU  - Polok, L
AU  - Solony, M
AU  - Svoboda, P
AD  - Australian National University, Canberra, Australia; Faculty of
      Information Technology, Brno University of Technology, Brno, Czech
      Republic
TI  - SLAM++1-A highly efficient and temporally scalable incremental SLAM
      framework
T2  - International Journal of Robotics Research
VL  - 36
IS  - 2
SP  - 210-230
PY  - 2017
DA  - 2017
PB  - SAGE Publications Inc.
AB  - The most common way to deal with the uncertainty present in noisy
      sensorial perception and action is to model the problem with a
      probabilistic framework. Maximum likelihood estimation is a well-known
      estimation method used in many robotic and computer vision applications.
      Under Gaussian assumption, the maximum likelihood estimation converts to a
      nonlinear least squares problem. Efficient solutions to nonlinear least
      squares exist and they are based on iteratively solving sparse linear
      systems until convergence. In general, the existing solutions provide only
      an estimation of the mean state vector, the resulting covariance being
      computationally too expensive to recover. Nevertheless, in many
      simultaneous localization and mapping (SLAM) applications, knowing only
      the mean vector is not enough. Data association, obtaining reduced state
      representations, active decisions and next best view are only a few of the
      applications that require fast state covariance recovery. Furthermore,
      computer vision and robotic applications are in general performed online.
      In this case, the state is updated and recomputed every step and its size
      is continuously growing, therefore, the estimation process may become
      highly computationally demanding. This paper introduces a general
      framework for incremental maximum likelihood estimation called SLAM++,
      which fully benefits from the incremental nature of the online
      applications, and provides efficient estimation of both the mean and the
      covariance of the estimate. Based on that, we propose a strategy for
      maintaining a sparse and scalable state representation for large scale
      mapping, which uses information theory measures to integrate only
      informative and non-redundant contributions to the state representation.
      SLAM++ differs from existing implementations by performing all the matrix
      operations by blocks. This led to extremely fast matrix manipulation and
      arithmetic operations used in nonlinear least squares. Even though this
      paper tests SLAM++ efficiency on SLAM problems, its applicability remains
      general. © The Author(s) 2017.
SN  - 0278-3649
DO  - 10.1177/0278364917691110
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018786448&doi=10.1177%2f0278364917691110&partnerID=40&md5=96366e7817914c3554d762e11fe5740f
UR  - http://dx.doi.org/10.1177/0278364917691110
KW  - compact state representation
KW  - incremental covariance recovery
KW  - long-term slam
KW  - loop closure
KW  - nonlinear least squares
KW  - computer vision
KW  - information theory
KW  - iterative methods
KW  - linear systems
KW  - mapping
KW  - matrix algebra
KW  - maximum likelihood
KW  - nonlinear analysis
KW  - recovery
KW  - robotics
KW  - covariance recoveries
KW  - non-linear least squares
KW  - state representation
KW  - maximum likelihood estimation
N1  - cited By 51
ER  - 

TY  - JOUR
AU  - Ali, W
AU  - Liu, P
AU  - Ying, R
AU  - Gong, Z
AD  - School of Electronic Information and Electrical Engineering, Shanghai
      Jiaotong University, Shanghai, China
TI  - A Life-Long SLAM Approach Using Adaptable Local Maps Based on Rasterized
      LIDAR Images
T2  - IEEE Sensors Journal
VL  - 21
IS  - 19
SP  - 21740-21749
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Most real-Time autonomous robot applications require a robot to traverse
      through a dynamic space for a long time. In some cases, a robot needs to
      work in the same environment. Such applications give rise to the problem
      of a life-long SLAM system. Life-long SLAM presents two main challenges
      i.e.The tracking should not fail in a dynamic environment and the need for
      a robust and efficient mapping strategy. The system should update maps
      with new information; while also keeping track of older observations. But,
      mapping for a long time can require higher computational requirements. In
      this paper, we propose a solution to the problem of life-long SLAM. We
      represent the global map as a set of rasterized images of local maps along
      with a map management system responsible for updating local maps and
      keeping track of older values. We also present an efficient approach of
      using the bag of visual words method for loop closure detection and
      relocalization. We evaluate the performance of our system on the KITTI
      dataset and an indoor dataset. Our loop closure system reported recall and
      precision of above 90 percent. The computational cost of our system is
      much lower as compared to state-of-The-Art methods. Our method reports
      lower computational requirements even for long-Term operation. © 2001-2012
      IEEE.
SN  - 1530-437X
DO  - 10.1109/JSEN.2021.3100882
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112591954&doi=10.1109%2fJSEN.2021.3100882&partnerID=40&md5=aabb8ee3623483cf83881b2657334f5b
UR  - http://dx.doi.org/10.1109/JSEN.2021.3100882
KW  - bag of words
KW  - laser scanning
KW  - mapping
KW  - place recognition
KW  - rasterization
KW  - simultaneous localization
KW  - artificial life
KW  - robot applications
KW  - robots
KW  - bag-of-visual-words
KW  - computational costs
KW  - computational requirements
KW  - dynamic environments
KW  - mapping strategy
KW  - re-localization
KW  - recall and precision
KW  - state-of-the-art methods
KW  - indoor positioning systems
N1  - cited By 1
ER  - 

TY  - JOUR
AU  - Churchill, W
AU  - Newman, P
AD  - Oxford University Mobile Robotics Group, Oxford, UK, United Kingdom
TI  - Experience-based navigation for long-term localisation
T2  - International Journal of Robotics Research
VL  - 32
IS  - 14
SP  - 1645-1661
PY  - 2013
DA  - 2013
AB  - This paper is about long-term navigation in environments whose appearance
      changes over time, suddenly or gradually. We describe, implement and
      validate an approach which allows us to incrementally learn a model whose
      complexity varies naturally in accordance with variation of scene
      appearance. It allows us to leverage the state of the art in pose
      estimation to build over many runs, a world model of sufficient richness
      to allow simple localisation despite a large variation in conditions. As
      our robot repeatedly traverses its workspace, it accumulates distinct
      visual experiences that in concert, implicitly represent the scene
      variation: each experience captures a visual mode. When operating in a
      previously visited area, we continually try to localise in these previous
      experiences while simultaneously running an independent vision-based pose
      estimation system. Failure to localise in a sufficient number of prior
      experiences indicates an insufficient model of the workspace and
      instigates the laying down of the live image sequence as a new distinct
      experience. In this way, over time we can capture the typical time-varying
      appearance of an environment and the number of experiences required tends
      to a constant. Although we focus on vision as a primary sensor throughout,
      the ideas we present here are equally applicable to other sensor
      modalities. We demonstrate our approach working on a road vehicle
      operating over a 3-month period at different times of day, in different
      weather and lighting conditions. We present extensive results analysing
      different aspects of the system and approach, in total processing over
      136,000 frames captured from 37 km of driving. © The Author(s) 2013.
SN  - 0278-3649
DO  - 10.1177/0278364913499193
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892567711&doi=10.1177%2f0278364913499193&partnerID=40&md5=07183a43950661361caa62650a4485a9
UR  - http://dx.doi.org/10.1177/0278364913499193
KW  - field and service robotics
KW  - field robots
KW  - localisation
KW  - mapping
KW  - mobile and distributed robotics
KW  - slam
KW  - distributed robotics
KW  - field robot
KW  - robotics
KW  - sensors
KW  - robots
N1  - cited By 149
ER  - 

TY  - CPAPER
AU  - Maddern, W
AU  - Milford, M
AU  - Wyeth, G
AD  - Maddern, W (Corresponding Author), Queensland Univ Technol, Fac Sci &
      Engn, Sch Elect Engn & Comp Sci, Brisbane, Qld 4001, Australia. Maddern,
      Will; Milford, Michael; Wyeth, Gordon, Queensland Univ Technol, Fac Sci &
      Engn, Sch Elect Engn & Comp Sci, Brisbane, Qld 4001, Australia.
TI  - Capping Computation Time and Storage Requirements for Appearance-based
      Localization with CAT-SLAM
T2  - 2012 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)
J2  - IEEE International Conference on Robotics and Automation ICRA
SP  - 822-827
PY  - 2012
DA  - 2012
PB  - IEEE
AB  - Appearance-based localization is increasingly used for loop closure
      detection in metric SLAM systems. Since it relies only upon the
      appearance-based similarity between images from two locations, it can
      perform loop closure regardless of accumulated metric error. However, the
      computation time and memory requirements of current appearance-based
      methods scale linearly not only with the size of the environment but also
      with the operation time of the platform. These properties impose severe
      restrictions on long-term autonomy for mobile robots, as loop closure
      performance will inevitably degrade with increased operation time. We
      present a set of improvements to the appearance-based SLAM algorithm
      CAT-SLAM to constrain computation scaling and memory usage with minimal
      degradation in performance over time. The appearance-based comparison
      stage is accelerated by exploiting properties of the particle observation
      update, and nodes in the continuous trajectory map are removed according
      to minimal information loss criteria. We demonstrate constant time and
      space loop closure detection in a large urban environment with recall
      performance exceeding FAB-MAP by a factor of 3 at 100% precision, and
      investigate the minimum computational and memory requirements for
      maintaining mapping performance.
DO  - 10.1109/ICRA.2012.6224622
C1  - 345 E 47TH ST, NEW YORK, NY 10017 USA
UR  - http://dx.doi.org/10.1109/ICRA.2012.6224622
N1  - IEEE International Conference on Robotics and Automation (ICRA), St Paul,
      MN, MAY 14-18, 2012
ER  - 

TY  - CPAPER
AU  - Maddern, W
AU  - Milford, M
AU  - Wyeth, G
A2  - Newman P., Srinivasa S, Roy N
AD  - School of Electrical Engineering and Computer Science, Queensland
      University of Technology, Brisbane, QLD, Australia
TI  - Towards persistent localization and mapping with a continuous
      appearance-based topology
VL  - 8
SP  - 281-288
PY  - 2013
DA  - 2013
PB  - MIT Press Journals
AB  - Appearance-based localization can provide loop closure detection at vast
      scales regardless of accumulated metric error. However, the computation
      time and memory requirements of current appearance-based methods scale not
      only with the size of the environment but also with the operation time of
      the platform. Additionally, repeated visits to locations will develop
      multiple competing representations, which will reduce recall performance
      over time. These properties impose severe restrictions on long-Term
      autonomy for mobile robots, as loop closure performance will inevitably
      degrade with increased operation time. In this paper we present a
      graphical extension to CAT-SLAM, a particle filter-based algorithm for
      appearancebased localization and mapping, to provide constant computation
      and memory requirements over time and minimal degradation of recall
      performance during repeated visits to locations. We demonstrate loop
      closure detection in a large urban environment with capped computation
      time and memory requirements and performance exceeding previous
      appearance-based methods by a factor of 2. We discuss the limitations of
      the algorithm with respect to environment size, appearance change over
      time and applications in topological planning and navigation for long-Term
      robot operation. © 2013 Massachusetts Institute of Technology.
DO  - 10.15607/rss.2012.viii.036
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959269406&doi=10.15607%2frss.2012.viii.036&partnerID=40&md5=a8db41048130a303fe5d7645f10e623d
UR  - http://dx.doi.org/10.15607/rss.2012.viii.036
KW  - localization
KW  - mapping
KW  - slam
KW  - vision
KW  - agricultural robots
KW  - robot programming
KW  - robotics
KW  - topology
KW  - appearance based
KW  - appearance-based methods
KW  - computation time
KW  - large urban environments
KW  - localization and mappings
KW  - memory requirements
KW  - robots
N1  - cited By 4; Conference of International Conference on Robotics Science and
      Systems, RSS 2012 ; Conference Date: 9 July 2012 Through 13 July 2012;
      Conference Code:166169
ER  - 

TY  - CPAPER
AU  - DIng, X
AU  - Wang, Y
AU  - Tang, L
AU  - Yin, H
AU  - Xiong, R
AD  - Zhejiang University, State Key Laboratory of Industrial Control and
      Technology, Hangzhou, China
TI  - Communication constrained cloud-based long-term visual localization in
      real time
SP  - 2159-2166
PY  - 2019
DA  - 2019
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Visual localization is one of the primary capabilities for mobile robots.
      Long-term visual localization in real time is particularly challenging, in
      which the robot is required to efficiently localize itself using visual
      data where appearance may change significantly over time. In this paper,
      we propose a cloud-based visual localization system targeting at long-term
      localization in real time. On the robot, we employ two estimators to
      achieve accurate and real-time performance. One is a sliding-window based
      visual inertial odometry, which integrates constraints from consecutive
      observations and self-motion measurements, as well as the constraints
      induced by localization results from the cloud. This estimator builds a
      local visual submap as the virtual observation which is then sent to the
      cloud as new localization constraints. The other one is a delayed state
      Extended Kalman Filter to fuse the pose of the robot localized from the
      cloud, the local odometry and the high-frequency inertial measurements. On
      the cloud, we propose a longer sliding-window based localization method to
      aggregate the virtual observations for larger field of view, leading to
      more robust alignment between virtual observations and the map. Under this
      architecture, the robot can achieve drift-free and real-time localization
      using onboard resources even in a network with limited bandwidth, high
      latency and existence of package loss, which enables the autonomous
      navigation in real-world environment. We evaluate the effectiveness of our
      system on a dataset with challenging seasonal and illuminative variations.
      We further validate the robustness of the system under challenging network
      conditions. © 2019 IEEE.
DO  - 10.1109/IROS40897.2019.8968550
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081157270&doi=10.1109%2fIROS40897.2019.8968550&partnerID=40&md5=86e32650397f0d38cecbd132b2b583d1
UR  - http://dx.doi.org/10.1109/IROS40897.2019.8968550
KW  - kalman filters
KW  - autonomous navigation
KW  - inertial measurements
KW  - localization method
KW  - real time performance
KW  - real-time localization
KW  - sliding window-based
KW  - virtual observations
KW  - visual localization
KW  - intelligent robots
N1  - cited By 0; Conference of 2019 IEEE/RSJ International Conference on
      Intelligent Robots and Systems, IROS 2019 ; Conference Date: 3 November
      2019 Through 8 November 2019; Conference Code:157163
ER  - 

TY  - JOUR
AU  - Ding, X
AU  - Wang, Y
AU  - Xiong, R
AU  - Li, D
AU  - Tang, L
AU  - Yin, H
AU  - Zhao, L
AD  - State Key Laboratory of Industrial Control and Technology, Zhejiang
      University, Hangzhou, 310007, China; Center for Autonomous Systems (CAS),
      University of Technology Sydney, Sydney, NSW 2007, Australia
TI  - Persistent Stereo Visual Localization on Cross-Modal Invariant Map
T2  - IEEE Transactions on Intelligent Transportation Systems
VL  - 21
IS  - 11
SP  - 4646-4658
PY  - 2020
DA  - 2020
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Autonomous mobile vehicles are expected to perform persistent and accurate
      localization with low-cost equipment. To achieve this goal, we propose a
      stereo camera based visual localization method using a modified laser map,
      which takes the advantage of both the low cost of camera, and high
      geometric precision of laser data to achieve long-term performance.
      Considering that LiDAR and camera give measurements of the same
      environment in different modalities, the cross-modal invariance is
      investigated to modify the laser map for visual localization.
      Specifically, a map learning algorithm is introduced to sample the robust
      subsets in laser maps that are useful for visual localization using
      multi-session visual and laser data. Further, a generative map model is
      derived to describe this cross-modal invariance, based on which two types
      of measurements are defined to model the laser map points as appropriate
      visual observations. Tightly coupling these measurements within the local
      bundle adjustment during online sliding-window based visual odometry, the
      vehicle can achieve robust localization even one year after the map was
      built. The effectiveness of the proposed method is evaluated on both the
      public KITTI datasets and self-collected datasets in our campus, which
      include seasonal, illumination and object variations. On all experimental
      localization sessions, our method provides satisfactory results, even when
      the direction is opposite to that in the mapping session, verifying the
      superior performance of the laser map based visual localization method. ©
      2000-2011 IEEE.
SN  - 1524-9050
DO  - 10.1109/TITS.2019.2942760
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096218582&doi=10.1109%2fTITS.2019.2942760&partnerID=40&md5=ddbd709c48aa1d0a30a37cfece3d07b6
UR  - http://dx.doi.org/10.1109/TITS.2019.2942760
KW  - map incorporated bundle adjustment
KW  - map maintenance
KW  - persistent autonomy
KW  - visual localization
KW  - cameras
KW  - costs
KW  - stereo image processing
KW  - autonomous mobile vehicles
KW  - geometric precision
KW  - local bundle adjustments
KW  - long term performance
KW  - low-cost equipment
KW  - sliding window-based
KW  - visual observations
KW  - learning algorithms
N1  - cited By 4
ER  - 

TY  - JOUR
AU  - Xu, X
AU  - Yin, H
AU  - Chen, Z
AU  - Li, Y
AU  - Wang, Y
AU  - Xiong, R
AD  - State Key Laboratory of Industrial Control Technology, Institute of
      Cyber-Systems and Control, Zhejiang University, Hangzhou, Zhejiang,
      310027, China; Zhejiang Lab, Hangzhou Zhejiang, 310014, China
TI  - DiSCO: Differentiable Scan Context with Orientation
T2  - IEEE Robotics and Automation Letters
VL  - 6
IS  - 2
SP  - 2791-2798
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Global localization is essential for robot navigation, of which the first
      step is to retrieve a query from the map database. This problem is called
      place recognition. In recent years, LiDAR scan based place recognition has
      drawn attention as it is robust against the appearance change. In this
      letter, we propose a LiDAR-based place recognition method, named
      Differentiable Scan Context with Orientation (DiSCO), which simultaneously
      finds the scan at a similar place and estimates their relative
      orientation. The orientation can further be used as the initial value for
      the down-stream local optimal metric pose estimation, improving the pose
      estimation especially when a large orientation between the current scan
      and retrieved scan exists. Our key idea is to transform the feature into
      the frequency domain. We utilize the magnitude of the spectrum as the
      place descriptor, which is theoretically rotation-invariant. In addition,
      based on the differentiable phase correlation, we can efficiently estimate
      the global optimal relative orientation using the spectrum. With such
      structural constraints, the network can be learned in an end-to-end
      manner, and the backbone is fully shared by the two tasks, achieving
      better interpretability and lightweight. Finally, DiSCO is validated on
      three datasets with long-term outdoor conditions, showing better
      performance than the compared methods. Codes are released at
      https://github.com/MaverickPeter/DiSCO-pytorch. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2021.3060741
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101737460&doi=10.1109%2fLRA.2021.3060741&partnerID=40&md5=023af0c57f7fca9526f233dc73fba59d
UR  - http://dx.doi.org/10.1109/LRA.2021.3060741
KW  - localization
KW  - range sensing
KW  - slam
KW  - agricultural robots
KW  - maps
KW  - optical radar
KW  - query processing
KW  - robots
KW  - frequency domains
KW  - global localization
KW  - interpretability
KW  - phase correlation
KW  - place recognition
KW  - relative orientation
KW  - rotation invariant
KW  - structural constraints
KW  - frequency domain analysis
N1  - cited By 6
ER  - 

TY  - JOUR
AU  - Bouaziz, Y
AU  - Royer, E
AU  - Bresson, G
AU  - Dhome, M
AD  - Institut Pascal, CNRS, Clermont-Ferrand, Institut VEDECOM, Versailles,
      France; Institut Pascal, CNRS, SIGMA Clermont, Clermont-Ferrand, France;
      Institut VEDECOM, Versailles, France
TI  - Map management for robust long-term visual localization of an autonomous
      shuttle in changing conditions
T2  - Multimedia Tools and Applications
PY  - 2022
DA  - 2022
PB  - Springer
AB  - Changes in appearance present a tremendous problem for the visual
      localization of an autonomous vehicle in outdoor environments. Data
      association between the current image and the landmarks in the map can be
      challenging in cases where the map was built with different environmental
      conditions. This paper introduces a solution to build and use
      multi-session maps incorporating sequences recorded in different
      conditions (day, night, fog, snow, rain, change of season, etc.). During
      visual localization, we exploit a ranking function to extract the most
      relevant keyframes from the map. This ranking function is designed to take
      into account the pose of the vehicle as well as the current environmental
      condition. In the mapping phase, covering all conditions by constantly
      adding data to the map leads to a continuous growth in the map size which
      in turn deteriorates the localization speed and performance. Our map
      management strategy is an incremental approach that aims to limit the size
      of the map while keeping it as diverse as possible. Our experiments were
      performed on real data collected with our autonomous shuttle as well as on
      a widely used public dataset. The results demonstrate that our
      keyframe-based ranking function is suitable for long-term scenarios. Our
      map management algorithm aims to build a map with as much diversity as
      possible whereas some state of the art approaches tend to filter out the
      less observed landmarks. This strategy shows a reduction of localization
      failures while maintaining real-time performance. © 2022, The Author(s),
      under exclusive licence to Springer Science+Business Media, LLC, part of
      Springer Nature.
SN  - 1380-7501
DO  - 10.1007/s11042-021-11870-4
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128007063&doi=10.1007%2fs11042-021-11870-4&partnerID=40&md5=5fd1fe540df0e3cc196b34c691ad9280
UR  - http://dx.doi.org/10.1007/s11042-021-11870-4
KW  - computer vision for transportation
KW  - slam
KW  - visual-based navigation
KW  - information retrieval
KW  - robotics
KW  - condition
KW  - environmental conditions
KW  - key-frames
KW  - localisation
KW  - map managements
KW  - ranking functions
KW  - visual localization
KW  - computer vision
N1  - cited By 0
ER  - 

TY  - CPAPER
AU  - Latif, Y
AU  - Cadena, C
AU  - Neira, J
AD  - Instituto de Investigación en Ingeniería de Aragón (I3A), Universidad de
      Zaragoza, Zaragoza 50018, Spain
TI  - Realizing, reversing, recovering: Incremental robust loop closing over
      time using the iRRR algorithm
SP  - 4211-4217
PY  - 2012
DA  - 2012
AB  - The ability to reconsider information over time allows to detect failures
      and is crucial for long term robust autonomous robot applications. This
      applies to loop closure decisions in localization and mapping systems.
      This paper describes a method to analyze all available information up to
      date in order to robustly remove past incorrect loop closures from the
      optimization process. The main novelties of our algorithm are: 1.
      incrementally reconsidering loop closures and 2. handling multi-session,
      spatially related or unrelated experiments. We validate our proposal in
      real multi-session experiments showing better results than those obtained
      by state of the art methods. © 2012 IEEE.
DO  - 10.1109/IROS.2012.6385879
C1  - Vilamoura, Algarve
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872329568&doi=10.1109%2fIROS.2012.6385879&partnerID=40&md5=ed60f89bc7db0bd163164b2a28f50d90
UR  - http://dx.doi.org/10.1109/IROS.2012.6385879
KW  - loop closing
KW  - loop closure
KW  - mapping systems
KW  - optimization process
KW  - state-of-the-art methods
KW  - experiments
KW  - intelligent systems
KW  - robot applications
KW  - algorithms
N1  - cited By 20; Conference of 25th IEEE/RSJ International Conference on
      Robotics and Intelligent Systems, IROS 2012 ; Conference Date: 7 October
      2012 Through 12 October 2012; Conference Code:94955
ER  - 

TY  - JOUR
AU  - Latif, Y
AU  - Huang, G
AU  - Leonard, J
AU  - Neira, J
AD  - ARC Center for Robotic Vision, University of Adelaide, Adelaide, SA 5005,
      Australia; Department of Mechanical Engineering, University of Delaware,
      Newark, DE 19716, United States; Computer Science and Artificial
      Intelligence Laboratory Massachusetts Institute of Technology, Cambridge,
      MA 02139, United States; Instituto de Investigación en Ingeniería de
      Aragón (I3A) Universidad de Zaragoza, Zaragoza, Spain
TI  - Sparse optimization for robust and efficient loop closing
T2  - Robotics and Autonomous Systems
VL  - 93
SP  - 13-26
PY  - 2017
DA  - 2017
PB  - Elsevier B.V.
AB  - It is essential for a robot to be able to detect revisits or loop closures
      for long-term visual navigation. A key insight explored in this work is
      that the loop-closing event inherently occurs sparsely, i.e., the image
      currently being taken matches with only a small subset (if any) of
      previous images. Based on this observation, we formulate the problem of
      loop-closure detection as a sparse, convexℓ1-minimization problem. By
      leveraging fast convex optimization techniques, we are able to efficiently
      find loop closures, thus enabling real-time robot navigation. This novel
      formulation requires no offline dictionary learning, as required by most
      existing approaches, and thus allows online incremental operation. Our
      approach ensures a unique hypothesis by choosing only a single globally
      optimal match when making a loop-closure decision. Furthermore, the
      proposed formulation enjoys a flexible representation with no restriction
      imposed on how images should be represented, while requiring only that the
      representations are “close” to each other when the corresponding images
      are visually similar. The proposed algorithm is validated extensively
      using real-world datasets. © 2017 Elsevier B.V.
SN  - 0921-8890
DO  - 10.1016/j.robot.2017.03.016
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019598639&doi=10.1016%2fj.robot.2017.03.016&partnerID=40&md5=42a608af6dad7479faaba36e0f74e344
UR  - http://dx.doi.org/10.1016/j.robot.2017.03.016
KW  - place recognition
KW  - relocalization
KW  - slam
KW  - sparse optimization
KW  - convex optimization
KW  - convex optimization techniques
KW  - minimization problems
KW  - off-line dictionaries
KW  - re-localization
KW  - real-world datasets
KW  - sparse optimizations
KW  - robots
N1  - cited By 10
ER  - 

TY  - CPAPER
AU  - Song, Y
AU  - Zhu, D
AU  - Li, J
AU  - Tian, Y
AU  - Li, M
TI  - Learning Local Feature Descriptor with Motion Attribute For Vision-based
      Localization
SP  - 3794-3801
PY  - 2019
DA  - 2019
AB  - In recent years, camera-based localization has been widely used for
      robotic applications, and most proposed algorithms rely on local features
      extracted from recorded images. For better performance, the features used
      for open-loop localization are required to be short-term globally static,
      and the ones used for re-localization or loop closure detection need to be
      long-term static. Therefore, the motion attribute of a local feature point
      could be exploited to improve localization performance, e.g., the feature
      points extracted from moving persons or vehicles can be excluded from
      these systems due to their unsteadiness. In this paper, we design a fully
      convolutional network (FCN), named MD-Net, to perform motion attribute
      estimation and feature description simultaneously. MD-Net has a shared
      backbone network to extract features from the input image and two network
      branches to complete each sub-task. With MD-Net, we can obtain the motion
      attribute while avoiding increasing much more computation. Experimental
      results demonstrate that the proposed method can learn distinct local
      feature descriptor along with motion attribute only using an FCN, by
      outperforming competing methods by a wide margin. We also show that the
      proposed algorithm can be integrated into a vision-based localization
      algorithm to improve estimation accuracy significantly.
DO  - 10.1109/IROS40897.2019.8967749
C1  - Piscataway, NJ, USA
UR  - http://dx.doi.org/10.1109/IROS40897.2019.8967749
KW  - cameras
KW  - convolutional neural nets
KW  - feature extraction
KW  - learning (artificial intelligence)
KW  - robot vision
KW  - slam (robots)
N1  - local feature point;fully convolutional network;MD-Net;motion attribute
      estimation;feature description;local feature descriptor;vision-based
      localization algorithm;camera-based localization;open-loop
      localization;re-localization;loop closure detection;
ER  - 

TY  - CPAPER
AU  - Yue, Y
AU  - Yang, C
AU  - Zhang, J
AU  - Wen, M
AU  - Wu, Z
AU  - Zhang, H
AU  - Wang, D
TI  - Day and Night Collaborative Dynamic Mapping in Unstructured Environment
      Based on Multimodal Sensors
SP  - 2981-2987
PY  - 2020
DA  - 2020
AB  - Enabling long-term operation during day and night for collaborative robots
      requires a comprehensive understanding of the unstructured environment.
      Besides, in the dynamic environment, robots must be able to recognize
      dynamic objects and collaboratively build a global map. This paper
      proposes a novel approach for dynamic collaborative mapping based on
      multimodal environmental perception. For each mission, robots first apply
      heterogeneous sensor fusion model to detect humans and separate them to
      acquire static observations. Then, the collaborative mapping is performed
      to estimate the relative position between robots and local 3D maps are
      integrated into a globally consistent 3D map. The experiment is conducted
      in the day and night rainforest with moving people. The results show the
      accuracy, robustness, and versatility in 3D map fusion missions.
DO  - 10.1109/ICRA40945.2020.9197072
C1  - Piscataway, NJ, USA
UR  - http://dx.doi.org/10.1109/ICRA40945.2020.9197072
KW  - groupware
KW  - image fusion
KW  - mobile robots
KW  - multi-robot systems
KW  - path planning
KW  - sensor fusion
KW  - slam (robots)
N1  - dynamic collaborative mapping;multimodal environmental
      perception;heterogeneous sensor fusion model;local 3D maps;night
      rainforest;3D map fusion missions;multimodal sensors;long-term
      operation;collaborative robots;dynamic environment;dynamic objects;
ER  - 

TY  - JOUR
AU  - Chen, Z
AU  - Liu, L
AU  - Sa, I
AU  - Ge, Z
AU  - Chli, M
AD  - Vision for Robotics Lab, ETH Zurich, Zurich, 8092, Switzerland; School of
      Computer Science, University of Adelaide, Adelaide, SA 5005, Australia;
      Autonomous Systems Lab, ETH Zurich, Zurich, 8092, Switzerland; EResearch
      Centre, University of Monash, Melbourne, VIC 3800, Australia
TI  - Learning Context Flexible Attention Model for Long-Term Visual Place
      Recognition
T2  - IEEE Robotics and Automation Letters
VL  - 3
IS  - 4
SP  - 4015-4022
PY  - 2018
DA  - 2018
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Identifying regions of interest in an image has long been of great
      importance in a wide range of tasks, including place recognition. In this
      letter, we propose a novel attention mechanism with flexible context,
      which can be incorporated into existing feedforward network architecture
      to learn image representations for long-term place recognition. In
      particular, in order to focus on regions that contribute positively to
      place recognition, we introduce a multiscale context-flexible network to
      estimate the importance of each spatial region in the feature map. Our
      model is trained end-to-end for place recognition and can detect regions
      of interest of arbitrary shape. Extensive experiments have been conducted
      to verify the effectiveness of our approach and the results demonstrate
      that our model can achieve consistently better performance than the state
      of the art on standard benchmark datasets. Finally, we visualize the
      learned attention maps to generate insights into what attention the
      network has learned. © 2016 IEEE.
SN  - 2377-3766
DO  - 10.1109/LRA.2018.2859916
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060562544&doi=10.1109%2fLRA.2018.2859916&partnerID=40&md5=a6b78ef6a8cc54753d01a85b58c07b54
UR  - http://dx.doi.org/10.1109/LRA.2018.2859916
KW  - deep learning in robotics and automation
KW  - localization
KW  - visual-based navigation
KW  - benchmarking
KW  - network architecture
KW  - robots
KW  - attention mechanisms
KW  - benchmark datasets
KW  - feed-forward network
KW  - flexible networks
KW  - image representations
KW  - place recognition
KW  - regions of interest
KW  - deep learning
N1  - cited By 44
ER  - 

TY  - JOUR
AU  - Hong, Z
AU  - Petillot, Y
AU  - Wallace, A
AU  - Wang, S
AD  - Wang, S (Corresponding Author), Heriot Watt Univ, Sch Engn & Phys Sci,
      Inst Sensors Signals & Syst, Edinburgh EH14 4AS, Midlothian, Scotland.
      Hong, Ziyang; Petillot, Yvan; Wallace, Andrew; Wang, Sen, Heriot Watt
      Univ, Edinburgh Ctr Robot, Edinburgh, Midlothian, Scotland.
TI  - RadarSLAM: A robust simultaneous localization and mapping system for all
      weather conditions
T2  - INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH
PY  - 2022
DA  - 2022
PB  - SAGE PUBLICATIONS LTD
CY  - 1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND
AB  - A Simultaneous Localization and Mapping (SLAM) system must be robust to
      support long-term mobile vehicle and robot applications. However, camera
      and LiDAR based SLAM systems can be fragile when facing challenging
      illumination or weather conditions which degrade the utility of imagery
      and point cloud data. Radar, whose operating electromagnetic spectrum is
      less affected by environmental changes, is promising although its distinct
      sensor model and noise characteristics bring open challenges when being
      exploited for SLAM. This paper studies the use of a Frequency Modulated
      Continuous Wave radar for SLAM in large-scale outdoor environments. We
      propose a full radar SLAM system, including a novel radar motion
      estimation algorithm that leverages radar geometry for reliable feature
      tracking. It also optimally compensates motion distortion and estimates
      pose by joint optimization. Its loop closure component is designed to be
      simple yet efficient for radar imagery by capturing and exploiting
      structural information of the surrounding environment. Extensive
      experiments on three public radar datasets, ranging from city streets and
      residential areas to countryside and highways, show competitive accuracy
      and reliability performance of the proposed radar SLAM system compared to
      the state-of-the-art LiDAR, vision and radar methods. The results show
      that our system is technically viable in achieving reliable SLAM in
      extreme weather conditions on the RADIATE Dataset, for example, heavy snow
      and dense fog, demonstrating the promising potential of using radar for
      all-weather localization and mapping.
SN  - 0278-3649
DO  - 10.1177/02783649221080483
UR  - http://dx.doi.org/10.1177/02783649221080483
KW  - radar sensing
KW  - simultaneous localization and mapping
KW  - all-weather perception
KW  - slam
KW  - versatile
ER  - 

TY  - JOUR
AU  - Pan, Z
AU  - Chen, H
AU  - Li, S
AU  - Liu, Y
AD  - School of Mechanical Engineering and Automation, Harbin Institute of
      Technology Shenzhen, Shenzhen, 518055, China; Department of Mechanical and
      Automation Engineering, Chinese University of Hong Kong, Hong Kong;
      Shenzhen University Town, Building G1011, Nanshan, Shenzhen, 518055,
      China; Chinese University of Hong Kong, Room 208, William M.W. Mong
      Engineering Building, Shatin, Hong Kong
TI  - Clustermap building and relocalization in urban environments for unmanned
      vehicles
T2  - Sensors (Switzerland)
VL  - 19
IS  - 19
SP  - 4252
PY  - 2019
DA  - 2019
PB  - MDPI AG
AB  - Map building and map-based relocalization techniques are important for
      unmanned vehicles operating in urban environments. The existing approaches
      require expensive high-density laser range finders and suffer from
      relocalization problems in long-term applications. This study proposes a
      novel map format called the ClusterMap, on the basis of which an approach
      to achieving relocalization is developed. The ClusterMap is generated by
      segmenting the perceived point clouds into different point clusters and
      filtering out clusters belonging to dynamic objects. A location descriptor
      associated with each cluster is designed for differentiation. The
      relocalization in the global map is achieved by matching cluster
      descriptors between local and global maps. The solution does not require
      high-density point clouds and high-precision segmentation algorithms. In
      addition, it prevents the effects of environmental changes on illumination
      intensity, object appearance, and observation direction. A consistent
      ClusterMap without any scale problem is built by utilizing a 3D
      visual–LIDAR simultaneous localization and mapping solution by fusing
      LIDAR and visual information. Experiments on the KITTI dataset and our
      mobile vehicle illustrates the effectiveness of the proposed approach. ©
      2019 by the authors. Licensee MDPI, Basel, Switzerland.
SN  - 1424-8220
DO  - 10.3390/s19194252
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072847843&doi=10.3390%2fs19194252&partnerID=40&md5=3fed38ddd93130619cc093b9e7f332b8
UR  - http://dx.doi.org/10.3390/s19194252
KW  - clustermap
KW  - lidar-based map building
KW  - localization
KW  - map descriptor
KW  - relocalization
KW  - slam
KW  - image segmentation
KW  - optical radar
KW  - range finders
KW  - robotics
KW  - urban planning
KW  - descriptors
KW  - map building
KW  - re-localization
KW  - unmanned vehicles
N1  - cited By 3
ER  - 

TY  - CPAPER
AU  - Wang, Z
AU  - Li, S
AU  - Cao, M
AU  - Chen, H
AU  - Liu, Y
AD  - Harbin Institute of Technology, School of Mechanical Engineering and
      Automation, Shenzhen, China; Department of Mechanical and Automation
      Engineering, Chinese University of Hong Kong, Hong Kong
TI  - Pole-like Objects Mapping and Long-Term Robot Localization in Dynamic
      Urban Scenarios
SP  - 998-1003
PY  - 2021
DA  - 2021
PB  - Institute of Electrical and Electronics Engineers Inc.
AB  - Localization on 3D data is a challenging task for unmanned vehicles,
      especially in long-term dynamic urban scenarios. Due to the generality and
      long-term stability, the pole-like objects are very suitable as landmarks
      for unmanned vehicle localization in time-varying scenarios. In this
      paper, a long-term LiDAR-only localization algorithm based on semantic
      cluster map is proposed. At first, the Convolutional Neural Network(CNN)
      is used to infer the semantics of LiDAR point clouds. Combined with the
      point cloud segmentation, the static objects pole/trunk are extracted and
      registered into global semantic cluster map. When the unmanned vehicle
      re-enters the environment again, the relocalization is completed by
      matching the clusters of current scan with the clusters of the global map.
      Furthermore, the matching between the local and global maps stably outputs
      the global pose at 2Hz to correct the drift of the 3D LiDAR odometry. The
      experimental results on our campus dataset demonstrate that the proposed
      approach performs better in localization accuracy compared with the
      current state-of-the-art methods. The source of this paper is available
      at: http://www.github.com/HITSZ-NRSL/long-term-localization. © 2021 IEEE.
DO  - 10.1109/ROBIO54168.2021.9739599
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128207203&doi=10.1109%2fROBIO54168.2021.9739599&partnerID=40&md5=a1f4a3dc07c3e38e9fc08e9a53382167
UR  - http://dx.doi.org/10.1109/ROBIO54168.2021.9739599
KW  - autonomous vehicles
KW  - intelligent vehicle highway systems
KW  - neural networks
KW  - poles
KW  - robot applications
KW  - semantics
KW  - unmanned vehicles
KW  - 'current
KW  - 3d data
KW  - cluster maps
KW  - global map
KW  - localisation
KW  - long term dynamics
KW  - matchings
KW  - robot localization
KW  - semantic clusters
KW  - urban scenarios
KW  - optical radar
N1  - cited By 0; Conference of 2021 IEEE International Conference on Robotics
      and Biomimetics, ROBIO 2021 ; Conference Date: 27 December 2021 Through 31
      December 2021; Conference Code:178223
ER  - 

TY  - CPAPER
AU  - Xin, Z
AU  - Cui, X
AU  - Zhang, J
AU  - Yang, Y
AU  - Wang, Y
TI  - Visual place recognition with CNNs: from global to partial
SP  - 6pp.-
PY  - 2017
DA  - 2017
AB  - Visual place recognition is one of the most challenging problems in
      computer vision, due to the large diversities that real-world places can
      represent. Recently, visual place recognition has become a key part of
      loop closure detection and topological localization in long-term mobile
      robot autonomy. In this work, we build up a novel visual place recognition
      pipeline composed of a first filtering stage followed by a partial
      reranking process. In the filtering stage, image-wise features are
      utilized to find a small set of potential places. Afterwards, stable
      region-wise landmarks are extracted for more accurate matching in the
      partial reranking process. All global and partial image representations
      are derived from pre-trained Convolutional Neural Networks (CNNs), and the
      landmarks are extracted by object proposal techniques. Moreover, a new
      similarity measurement is provided by considering both spatial and scale
      distribution of landmarks. Compared with current methods only considering
      scale distribution, the presented similarity measurement can benefit
      recognition precision and robustness effectively. Experiments with varied
      viewpoints and environmental conditions demonstrate that the proposed
      method achieves superior performance against state-of-the-art methods.
DO  - 10.1109/IPTA.2017.8310121
C1  - Piscataway, NJ, USA
UR  - http://dx.doi.org/10.1109/IPTA.2017.8310121
KW  - computer vision
KW  - feature extraction
KW  - feedforward neural nets
KW  - image matching
KW  - image recognition
KW  - image representation
KW  - learning (artificial intelligence)
KW  - mobile robots
KW  - object detection
KW  - object recognition
KW  - robot vision
KW  - slam (robots)
N1  - long-term mobile robot autonomy;partial reranking process;potential
      places;global image representations;partial image
      representations;recognition precision;pre-trained convolutional neural
      networks;visual place recognition pipeline;CNN;
ER  - 

TY  - JOUR
AU  - Xing, Z
AU  - Zhu, X
AU  - Dong, D
AD  - School of Mechanical Engineering and Automation, Harbin Institute of
      Technology, Shenzhen, Shenzhen, China
TI  - DE-SLAM: SLAM for highly dynamic environment
T2  - Journal of Field Robotics
PY  - 2022
DA  - 2022
PB  - John Wiley and Sons Inc
AB  - Simultaneous localization and mapping (SLAM) is crucial for autonomous
      mobile robots. Most of the current SLAM systems are based on an
      assumption: the environment is static. However, the real environment is
      full of dynamic elements, such as pedestrians or vehicles, as well as
      changes in illumination and appearance over time. In this paper, DE-SLAM,
      a visual SLAM system that can deal with short-term and long-term dynamic
      elements at the same time is proposed. A novel dynamic detection and
      tracking module that utilizes both semantic and metric information is
      proposed, and the localization accuracy is highly improved by eliminating
      features falling on the dynamic objects. A unified loop detection, loop
      check and global optimization module is used to perform loop closure.
      Experimental results on datasets and real environments show that DE-SLAM
      outperforms other state-of-the-art SLAM systems in dynamic environments. ©
      2022 Wiley Periodicals LLC.
SN  - 1556-4959
DO  - 10.1002/rob.22062
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124594693&doi=10.1002%2frob.22062&partnerID=40&md5=822d8ca8ca362491b59dc80ae35d924d
UR  - http://dx.doi.org/10.1002/rob.22062
KW  - localization
KW  - slam
KW  - global optimization
KW  - robotics
KW  - 'current
KW  - autonomous mobile robot
KW  - dynamic elements
KW  - dynamic environments
KW  - localisation
KW  - localisation systems
KW  - mapping systems
KW  - real environments
KW  - simultaneous localization and mapping
KW  - visual simultaneous localization and mappings
KW  - semantics
N1  - cited By 0
ER  - 

TY  - JOUR
AU  - Yang, Z
AU  - Pan, Y
AU  - Deng, L
AU  - Xie, Y
AU  - Huan, R
AD  - College of Information Science and Electronic Engineering, Zhejiang
      University, Hangzhou, China; Department of Electrical and Computer
      Engineering, University of California, Santa Barbara, CA, United States;
      College of Computer Science and Technology, Zhejiang University of
      Technology, Hangzhou, China
TI  - PLSAV: Parallel loop searching and verifying for loop closure detection
T2  - IET Intelligent Transport Systems
VL  - 15
IS  - 5
SP  - 683-698
PY  - 2021
DA  - 2021
PB  - Institution of Engineering and Technology
AB  - Visual simultaneous localization and mapping (vSLAM), one of the most
      important applications in autonomous vehicles and robots to estimate the
      position and pose using inexpensive visual sensors, suffers from error
      accumulation for long-term navigation without loop closure detection.
      Recently, deep neural networks (DNNs) are leveraged to achieve high
      accuracy for loop closure detection, however the execution time is much
      slower than those employing handcrafted visual features. In this paper, a
      parallel loop searching and verifying method for loop closure detection
      with both high accuracy and high speed, which combines two parallel tasks
      using handcrafted and DNN features, respectively, is proposed. A fast loop
      searching is proposed to link the bag-of-words features and histogram for
      higher accuracy, and it splits the images into multiple grids for high
      parallelism; meanwhile, a DNN feature extractor is utilized for further
      verification. A loop state control method based on a finite state machine
      to control these tasks is designed, wherein the loop closure detection is
      described as a context-related procedure. The framework is implemented on
      a real machine, and the top-2 best accuracy and fastest execution time of
      80-543 frames per second (min: 1.84ms, and max: 12.45ms) are achieved on
      several public benchmarks compared with some existing algorithms. © 2021
      The Authors. IET Intelligent Transport Systems published by John Wiley &
      Sons Ltd on behalf of The Institution of Engineering and Technology
SN  - 1751-956X
DO  - 10.1049/itr2.12054
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102304010&doi=10.1049%2fitr2.12054&partnerID=40&md5=e8ada28dd07dc1e251d2e75f2a31bd18
UR  - http://dx.doi.org/10.1049/itr2.12054
KW  - deep neural networks
KW  - robots
KW  - visual servoing
KW  - error accumulation
KW  - feature extractor
KW  - frames per seconds
KW  - parallel loops
KW  - parallel task
KW  - visual feature
KW  - visual sensor
KW  - visual simultaneous localization and mappings
KW  - feature extraction
N1  - cited By 0
ER  - 

TY  - JOUR
AU  - Du, Z-J
AU  - Huang, S-S
AU  - Mu, T-J
AU  - Zhao, Q
AU  - Martin, R R
AU  - Xu, K
TI  - Accurate Dynamic SLAM Using CRF-Based Long-Term Consistency
T2  - IEEE Transactions on Visualization and Computer Graphics
VL  - 28
IS  - 4
SP  - 1745-1757
PY  - 2022
DA  - 2022
CY  - USA
AB  - Accurate camera pose estimation is essential and challenging for real
      world dynamic 3D reconstruction and augmented reality applications. In
      this article, we present a novel RGB-D SLAM approach for accurate camera
      pose tracking in dynamic environments. Previous methods detect dynamic
      components only across a short time-span of consecutive frames. Instead,
      we provide a more accurate dynamic 3D landmark detection method, followed
      by the use of long-term consistency via conditional random fields, which
      leverages long-term observations from multiple frames. Specifically, we
      first introduce an efficient initial camera pose estimation method based
      on distinguishing dynamic from static points using graph-cut RANSAC. These
      static/dynamic labels are used as priors for the unary potential in the
      conditional random fields, which further improves the accuracy of dynamic
      3D landmark detection. Evaluation using the TUM and Bonn RGB-D dynamic
      datasets shows that our approach significantly outperforms
      state-of-the-art methods, providing much more accurate camera trajectory
      estimation in a variety of highly dynamic environments. We also show that
      dynamic 3D reconstruction can benefit from the camera poses estimated by
      our RGB-D SLAM approach.
SN  - 1941-0506
DO  - 10.1109/TVCG.2020.3028218
UR  - http://dx.doi.org/10.1109/TVCG.2020.3028218
KW  - augmented reality
KW  - cameras
KW  - feature extraction
KW  - graph theory
KW  - image reconstruction
KW  - image sequences
KW  - motion estimation
KW  - pose estimation
KW  - slam (robots)
N1  - efficient initial camera;estimation method;distinguishing
      dynamic;conditional random fields;accurate camera trajectory
      estimation;highly dynamic environments;accurate dynamic SLAM;CRF-based
      long-term consistency;world dynamic 3D reconstruction;reality
      applications;novel RGB-D SLAM approach;dynamic components;short
      time-span;consecutive frames;accurate dynamic 3D landmark detection
      method;long-term observations;
ER  - 
