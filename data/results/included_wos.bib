@article{davison-murray:2002:1017615,
  author = {A. Davison and D. Murray},
  journal = {IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE},
  title = {Simultaneous localization and map-building using active vision},
  volume = {24},
  number = {7},
  pages = {865--880},
  doi = {10.1109/TPAMI.2002.1017615},
  publisher = {IEEE COMPUTER SOC},
  address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
  year = {2002},
  month = {7},
  abstract = {An active approach to sensing can provide the focused measurement
capability over a wide field of view which allows correctly formulated
Simultaneous Localization and Map-Building (SLAM) to be implemented with
vision, permitting repeatable long-term localization using only
naturally occurring, automatically-detected features. In this paper, we
present the first example of a general system for autonomous
localization using active vision, enabled here by a high-performance
stereo head, addressing such issues as uncertainty-based measurement
selection, automatic map-maintenance, and goal-directed steering. We
present varied real-time experiments in a complex environment.},
  affiliation = {Davison, AJ (Corresponding Author), Univ Oxford, Dept Engn Sci, Robot Res Grp, Oxford OX1 3PJ, England.
Univ Oxford, Dept Engn Sci, Robot Res Grp, Oxford OX1 3PJ, England.},
  affiliations = {League of European Research Universities - LERU; University of Oxford},
  author-email = {ajd@robots.ox.ac.uk
dwm@robots.ox.ac.uk},
  cited-references = {Ayache N., 1991, ARTIFICIAL VISION MO.
BEARDSLEY PA, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P58, DOI 10.1109/ICCV.1995.466806.
BOUGT JY, 1995, ICCV5, P645.
CASTELLANOS JA, 1998, THESIS U ZARAGOZA SP.
CHIUSO A, 2000, P 6 EUR C COMP VIS.
Chong KS, 1999, INT J ROBOT RES, V18, P3.
Davison AJ, 2001, ROBOT AUTON SYST, V36, P171, DOI 10.1016/S0921-8890(01)00141-5.
DAVISON AJ, 1998, P 5 EUR C COMP VIS F, P809.
DAVISON AJ, 2000, P IEEE RSJ C INT ROB.
DAVISON AJ, 1998, THESIS U OXFORD.
DURRANTWHYTE H, 1994, IND ROBOT, V21, P11, DOI 10.1108/EUM0000000004145.
DURRANTWHYTE HF, 1999, P 9 INT S ROB RES SN, P121.
Grossman CRS, 1999, BEHAV SOC SCI LIBR, V17, P11, DOI 10.1300/J103v17n02\_02.
Harris C., 1992, ACTIVE VISION.
Harris C. G., 1988, ALVEY VISION C, P1, DOI DOI 10.5244/C.2.23.
HARRIS CG, 1987, P 3 ALV VIS C, P233.
KNIGHT JGH, 2001, P IEEE RSJ C INT ROB.
LAND MF, 1994, NATURE, V369, P742, DOI 10.1038/369742a0.
LEONARD JJ, 2000, ROBOTICS RES.
MACCORMICK J, 2000, P 6 EUR C COMP VIS.
Murray DW, 1997, PERCEPTION, V26, P1519, DOI 10.1068/p261519.
NAYAR S, 1997, P IEEE C COMP VIS PA.
Pollefeys M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P90, DOI 10.1109/ICCV.1998.710705.
SANDINI G, 1990, P IEEE INT WORKSH RO.
SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794.
SMITH R, 1987, P 4 INT S ROB RES.
Thrun S., 2000, P IEEE INT C ROB AUT.
Thrun S., 1998, MACHINE LEARNING, V31.
TISTARELLI M, 1992, CVGIP-IMAG UNDERSTAN, V56, P108, DOI 10.1016/1049-9660(92)90089-L.
Torr P, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P485, DOI 10.1109/ICCV.1998.710762.
Whaite P, 1997, IEEE T PATTERN ANAL, V19, P193, DOI 10.1109/34.584097.},
  da = {2022-05-17},
  doc-delivery-number = {566UF},
  eissn = {1939-3539},
  issn = {0162-8828},
  journal-iso = {IEEE Trans. Pattern Anal. Mach. Intell.},
  keywords = {active vision; simultaneous localization and map-building; mobile robots},
  language = {English},
  number-of-cited-references = {31},
  oa = {Green Submitted},
  research-areas = {Computer Science; Engineering},
  times-cited = {298},
  type = {Article},
  unique-id = {WOS:000176446100001},
  usage-count-last-180-days = {6},
  usage-count-since-2013 = {71},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
Electronic},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{kawewong-et-al:2013:826410,
  author = {A. Kawewong and N. Tongprasit and O. Hasegawa},
  journal = {ADVANCED ROBOTICS},
  title = {A speeded-up online incremental vision-based loop-closure detection for
long-term SLAM},
  volume = {27},
  number = {17},
  pages = {1325--1336},
  doi = {10.1080/01691864.2013.826410},
  publisher = {TAYLOR \& FRANCIS LTD},
  address = {2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND},
  year = {2013},
  month = {12},
  abstract = {An online incremental method of vision-only loop-closure detection for
long-term robot navigation is proposed. The method is based on the
scheme of direct feature matching which has recently become more
efficient than the Bag-of-Words scheme in many challenging environments.
The contributions of the paper are the application of hierarchical
k-means to speed-up feature matching time and the improvement of the
score calculation technique used to determine the loop-closing location.
As a result, the presented method runs quickly in long term while
retaining high accuracy. The searching cost has been markedly reduced.
The proposed method requires no any off-line dictionary generation
processes. It can start anywhere at any times. The evaluation has been
done on standard well-known datasets being challenging in perceptual
aliasing and dynamic changes. The results show that the proposed method
offers high precision-recall in large-scale different environments with
real-time computation comparing to other vision-only loop-closure
detection methods.},
  affiliation = {Tongprasit, N (Corresponding Author), Tokyo Inst Technol, Dept Computat Intelligence \& Syst Sci, Imaging Sci \& Engn Lab, Midori Ku, 4259-J3 Nagatsuta, Yokohama, Kanagawa 2265803, Japan.
Kawewong, Aram, Chiang Mai Univ, Fac Engn, Dept Comp Engn, Chiang Mai 50200, Thailand.
Kawewong, Aram, Chiang Mai Univ, Mat Sci Res Ctr, Fac Sci, Chiang Mai 50200, Thailand.
Tongprasit, Noppharit; Hasegawa, Osamu, Tokyo Inst Technol, Dept Computat Intelligence \& Syst Sci, Imaging Sci \& Engn Lab, Midori Ku, Yokohama, Kanagawa 2265803, Japan.},
  affiliations = {Chiang Mai University; Chiang Mai University; Tokyo Institute of
Technology},
  author-email = {noppharit@gmail.com},
  cited-references = {Angeli A, 2009, P IEEE INTT C ROB AU.
Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514.
Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Cummins M, 2010, IEEE T ROBOT, V26, P1042, DOI 10.1109/TRO.2010.2080390.
Eade E., 2008, BRIT MACH VIS C BMVC.
Filliat D, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P248, DOI 10.1109/IROS.2008.4650681.
Kawewong A, 2011, ROBOT AUTON SYST, V59, P727, DOI 10.1016/j.robot.2011.05.007.
Kawewong A, 2011, INT J ROBOT RES, V30, P33, DOI 10.1177/0278364910371855.
Kawewong A, 2010, IEICE T INF SYST, VE93D, P2587, DOI 10.1587/transinf.E93.D.2587.
KNUTH DE, 1992, AM MATH MON, V99, P403, DOI 10.2307/2325085.
Krainin M, 2011, INT J ROBOT RES, V30, P1311, DOI 10.1177/0278364911403178.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Morioka H, 2011, P IEEE RSJ INT C INT.
Nister D, 2006, P IEEE INT C COMP VI.
Paul R., 2010, P IEEE INT C ROB AUT.
Sivic J., 2003, P IEEE INT C COMP VI.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.},
  da = {2022-05-17},
  doc-delivery-number = {229EM},
  eissn = {1568-5535},
  funding-acknowledgement = {JST CREST Project of Japan; Thailand Research Fund; Thai Network
Information Center Foundation {[}TRG5680062]; National Research
University Project under Thailand's Office of Higher Education
Commission},
  funding-text = {This study was supported by a JST CREST Project of Japan, by The
Thailand Research Fund and Thai Network Information Center Foundation
(Grant No. TRG5680062), and by the research grant from the National
Research University Project under Thailand's Office of Higher Education
Commission.},
  issn = {0169-1864},
  journal-iso = {Adv. Robot.},
  keywords = {vision-based loop-closure detection; simultaneous localization and
mapping; robotics navigation; place localization},
  keywords-plus = {FAB-MAP},
  language = {English},
  number-of-cited-references = {19},
  research-areas = {Robotics},
  times-cited = {2},
  type = {Article},
  unique-id = {WOS:000325244600003},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {17},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{schaefer-et-al:2021:103709,
  author = {A. Schaefer and D. Buescher and J. Vertens and L. Lukas and W. Burgard},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {Long-term vehicle localization in urban environments based on pole
landmarks extracted from 3-D lidar scans},
  volume = {136},
  pages = {103709},
  doi = {10.1016/j.robot.2020.103709},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2021},
  month = {2},
  abstract = {Due to their ubiquity and long-term stability, pole-like objects are
well suited to serve as landmarks for vehicle localization in urban
environments. In this work, we present a complete mapping and long-term
localization system based on pole landmarks extracted from 3-D lidar
data. Our approach features a novel pole detector, a mapping module, and
an online localization module, each of which are described in detail,
and for which we provide an open-source implementation (Schaefer and
Buscher, 0000). In extensive experiments, we demonstrate that our method
improves on the state of the art with respect to long-term reliability
and accuracy: First, we prove reliability by tasking the system with
localizing a mobile robot over the course of 15 months in an urban area
based on an initial map, confronting it with constantly varying routes,
differing weather conditions, seasonal changes, and construction sites.
Second, we show that the proposed approach clearly outperforms a
recently published method in terms of accuracy. (C) 2020 Elsevier B.V.
All rights reserved.},
  affiliation = {Buscher, D (Corresponding Author), Univ Freiburg, Dept Comp Sci, Freiburg, Germany.
Schaefer, Alexander; Buescher, Daniel; Vertens, Johan; Luft, Lukas; Burgard, Wolfram, Univ Freiburg, Dept Comp Sci, Freiburg, Germany.},
  affiliations = {League of European Research Universities - LERU; University of Freiburg},
  article-number = {103709},
  author-email = {aschaef@cs.uni-freiburg.de
buescher@cs.uni-freiburg.de
vertensj@cs.uni-freiburg.de
luft@cs.uni-freiburg.de
burgard@cs.uni-freiburg.de},
  cited-references = {Brenner C, 2009, LECT NOTES COMPUT SC, V5748, P61.
Buscher, LONG TERM URBAN VEHI.
Cabo C, 2014, ISPRS J PHOTOGRAMM, V87, P47, DOI 10.1016/j.isprsjprs.2013.10.008.
Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638.
Deusch H., 2014, P IEEE INT VEH S JUN, P555, DOI {[}10.1109/IVS.2014.6856413, DOI 10.1109/IVS.2014.6856413].
FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/tit.1975.1055330.
Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297.
Hata AY, 2016, IEEE T INTELL TRANSP, V17, P420, DOI 10.1109/TITS.2015.2477817.
Huang J, 2015, IEEE INT CONF ROBOT, P3032, DOI 10.1109/ICRA.2015.7139615.
Im JH, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16081268.
Kummerle J, 2019, IEEE INT CONF ROBOT, P5965, DOI 10.1109/ICRA.2019.8793497.
Lehtomaki M, 2010, REMOTE SENS-BASEL, V2, P641, DOI 10.3390/rs2030641.
Levinson J, 2010, IEEE INT CONF ROBOT, P4372, DOI 10.1109/ROBOT.2010.5509700.
Li FS, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10040531.
Luft L, 2017, IEEE INT C INT ROBOT, P6678, DOI 10.1109/IROS.2017.8206583.
Blanco-Claraco JL, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19143155.
Modsching M., 2006, P 3 WORKSH POS NAV C.
Ordonez C, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17071465.
Qin B, 2012, IEEE INT CONF ROBOT, P2640, DOI 10.1109/ICRA.2012.6224913.
Rodriguez-Cuenca B, 2015, REMOTE SENS-BASEL, V7, P12680, DOI 10.3390/rs71012680.
Schindler A, 2013, 2013 IEEE INTELLIGENT VEHICLES SYMPOSIUM WORKSHOPS (IV WORKSHOPS), P134, DOI 10.1109/IVWorkshops.2013.6615239.
Schreiber M, 2013, IEEE INT VEH SYM, P449, DOI 10.1109/IVS.2013.6629509.
Sefati M, 2017, IEEE INT VEH SYM, P13, DOI 10.1109/IVS.2017.7995692.
Spangenberg R, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P2161, DOI 10.1109/IROS.2016.7759339.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Tombari F, 2014, IEEE INT C INT ROBOT, P4922, DOI 10.1109/IROS.2014.6943262.
Tombari F, 2009, ICINCO 2009: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON INFORMATICS IN CONTROL, AUTOMATION AND ROBOTICS, VOL 2, P5.
Wang R., 2016, INT ARCH PHOTOGRAMM, V41.
Welzel A, 2015, IEEE INT C INTELL TR, P2728, DOI 10.1109/ITSC.2015.438.
Weng LH, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING AND ROBOTICS (IEEE RCAR), P96, DOI 10.1109/RCAR.2018.8621688.
Wu F, 2017, IEEE T INTELL TRANSP, V18, P292, DOI 10.1109/TITS.2016.2565698.
Yan WY, 2016, OPT LASER TECHNOL, V77, P162, DOI 10.1016/j.optlastec.2015.09.017.
Yokoyama H., 2013, INT J CADCAM, V13, P31.
Yu YT, 2015, IEEE T GEOSCI REMOTE, V53, P1374, DOI 10.1109/TGRS.2014.2338915.},
  da = {2022-05-17},
  doc-delivery-number = {PU7BC},
  eissn = {1872-793X},
  funding-acknowledgement = {Samsung Electronics Co. Ltd. under the GRO program},
  funding-text = {This work has been partially supported by Samsung Electronics Co. Ltd.
under the GRO program.},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Mapping; Localization; Lidar; Pole; Landmark; Feature extraction;
Autonomous driving},
  keywords-plus = {AUTOMATIC DETECTION; LIGHT POLES; OBJECTS; CLASSIFICATION},
  language = {English},
  number-of-cited-references = {35},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  times-cited = {3},
  type = {Article},
  unique-id = {WOS:000609454000004},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {9},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{walcott-bryant-et-al:2012:6385561,
  author = {A. Walcott-Bryant and M. Kaess and J. J. H. A. L. Johannsson},
  booktitle = {2012 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS)},
  title = {Dynamic Pose Graph SLAM: Long-term Mapping in Low Dynamic Environments},
  pages = {1871--1878},
  doi = {10.1109/IROS.2012.6385561},
  note = {25th IEEE\textbackslash{}RSJ International Conference on Intelligent
Robots and Systems (IROS), Algarve, PORTUGAL, OCT 07-12, 2012},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2012},
  abstract = {Maintaining a map of an environment that changes over time is a critical
challenge in the development of persistently autonomous mobile robots.
Many previous approaches to mapping assume a static world. In this work
we incorporate the time dimension into the mapping process to enable a
robot to maintain an accurate map while operating in dynamical
environments. This paper presents Dynamic Pose Graph SLAM (DPG-SLAM), an
algorithm designed to enable a robot to remain localized in an
environment that changes substantially over time. Using incremental
smoothing and mapping (iSAM) as the underlying SLAM state estimation
engine, the Dynamic Pose Graph evolves over time as the robot explores
new places and revisits previously mapped areas. The approach has been
implemented for planar indoor environments, using laser scan matching to
derive constraints for SLAM state estimation. Laser scans for the same
portion of the environment at different times are compared to perform
change detection; when sufficient change has occurred in a location, the
dynamic pose graph is edited to remove old poses and scans that no
longer match the current state of the world. Experimental results are
shown for two real-world dynamic indoor laser data sets, demonstrating
the ability to maintain an up-to-date map despite long-term
environmental changes.},
  affiliation = {Walcott-Bryant, A (Corresponding Author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
Walcott-Bryant, Aisha; Kaess, Michael; Johannsson, Hordur; Leonard, John J., MIT, Cambridge, MA 02139 USA.},
  affiliations = {Massachusetts Institute of Technology (MIT)},
  author-email = {aisha.w.bryant@gmail.com
kaess@mit.edu
hordurj@mit.edu
jleonard@mit.edu},
  book-group-author = {IEEE
Robotics Society of Japan},
  cited-references = {Andrade-Cetto J, 2002, INT J PATTERN RECOGN, V16, P361, DOI 10.1142/S0218001402001745.
Biber P., 2005, P ROB SCI SYST RSS.
Biber P, 2009, INT J ROBOT RES, V28, P20, DOI 10.1177/0278364908096286.
Biswas R, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P1014.
Bosse M, 2003, IEEE INT CONF ROBOT, P1899, DOI 10.1109/ROBOT.2003.1241872.
Fox D., 1998, P NAT C ART INT AAAI.
Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925.
GUTMANN JS, 1999, INT S COMP INT ROB A.
Hahnel D, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P496, DOI 10.1109/IRDS.2002.1041439.
Ji XC, 2009, LECT NOTES COMPUT SC, V5399, P507.
Johannsson H., 2012, MITCSAILTR2012013.
Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Meyer-Delius D., 2011, THESIS U FREIBURG.
Mitsou N.C., 2007, CONTR AUT 2007 MED 0, P1.
Olson EB, 2009, IEEE INT CONF ROBOT, P1233.
Wang CC, 2003, IEEE INT CONF ROBOT, P842.
Wolf DF, 2003, PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS 2003, VOL 1-3, P594.
Wyeth G., 2009, P 14 INT S ROB RES I.},
  da = {2022-05-17},
  doc-delivery-number = {BEK21},
  isbn = {978-1-4673-1736-8},
  issn = {2153-0858},
  keywords-plus = {LOCALIZATION},
  language = {English},
  number-of-cited-references = {19},
  oa = {Green Submitted},
  orcid-numbers = {Kaess, Michael/0000-0002-7590-3357},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  series = {IEEE International Conference on Intelligent Robots and Systems},
  times-cited = {75},
  type = {Proceedings Paper},
  unique-id = {WOS:000317042702066},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {10},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{bescos-et-al:2018:2860039,
  author = {B. Bescos and J. M. Facil and J. Civera and J. Neira},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes},
  volume = {3},
  number = {4},
  pages = {4076--4083},
  doi = {10.1109/LRA.2018.2860039},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2018},
  month = {10},
  abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a
strong assumption limits the use of most visual SLAM systems in
populated real-world environments, which are the target of several
relevant applications like service robotics or autonomous vehicles. In
this letter we present DynaSLAM, a visual SLAM system that, building on
ORB-SLAM2, adds the capabilities of dynamic object detection and
background inpainting. DynaSLAM is robust in dynamic scenarios for
monocular, stereo, and RGB-D configurations. We are capable of detecting
the moving objects either by multiview geometry, deep learning, or both.
Having a static map of the scene allows inpainting the frame background
that has been occluded by such dynamic objects. We evaluate our system
in public monocular, stereo, and RGB-D datasets. We study the impact of
several accuracy/speed trade-offs to assess the limits of the proposed
methodology. DynaSLAM outperforms the accuracy of standard visual SLAM
baselines in highly dynamic scenarios. And it also estimates a map of
the static parts of the scene, which is a must for long-term
applications in real-world environments.},
  affiliation = {Bescos, B (Corresponding Author), Univ Zaragoza, I3A, Zaragoza 50018, Spain.
Bescos, Berta; Facil, Jose M.; Civera, Javier; Neira, Jose, Univ Zaragoza, I3A, Zaragoza 50018, Spain.},
  affiliations = {University of Zaragoza},
  author-email = {bbescos@unizar.es
jmfacil@unizar.es
jcivera@unizar.es
jneira@unizar.es},
  cited-references = {Alcantarilla PF, 2012, IEEE INT CONF ROBOT, P1290, DOI 10.1109/ICRA.2012.6224690.
Ambrus R., 2016, P IEEE RAS 16 INT C.
Concha A, 2015, IEEE INT C INT ROBOT, P5686, DOI 10.1109/IROS.2015.7354184.
Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297.
Gerlach NL, 2014, BRIT J ORAL MAX SURG, V52, P838, DOI 10.1016/j.bjoms.2014.07.253.
Graber G., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P708, DOI 10.1109/ICCVW.2011.6130318.
He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322.
Kim DH, 2016, IEEE T ROBOT, V32, P1565, DOI 10.1109/TRO.2016.2609395.
Klein George, 2007, P1.
Li SL, 2017, IEEE ROBOT AUTOM LET, V2, P2263, DOI 10.1109/LRA.2017.2724759.
Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
Montiel J.M.M., 2017, 2017 EUR C MOB ROB E, P1.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513.
Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278.
Stuhmer J, 2010, LECT NOTES COMPUT SC, V6376, P11.
Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773.
Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012.
Tan W, 2013, INT SYM MIX AUGMENT, P209, DOI 10.1109/ISMAR.2013.6671781.
Wang YB, 2014, 2014 11TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P3122, DOI 10.1109/WCICA.2014.7053228.
Wangsiripitak S, 2009, IEEE INT CONF ROBOT, P705.},
  da = {2022-05-17},
  doc-delivery-number = {GR1UO},
  esi-highly-cited-paper = {Y},
  esi-hot-paper = {N},
  funding-acknowledgement = {NVIDIA; Spanish Ministry of Economy and Competitiveness
{[}DPI2015-68905-P, DPI2015-67275-P]; FPI {[}BES-2016-077836]; Aragon
regional government (Grupo DGA T04-FSE)},
  funding-text = {This work was supported in part by NVIDIA through the donation of a
Titan X GRU, in part by the Spanish Ministry of Economy and
Competitiveness under Projects DPI2015-68905-P and DPI2015-67275-P, FPI
Grant BES-2016-077836, and in part by the Aragon regional government
(Grupo DGA T04-FSE).},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {SLAM; visual-based navigation; localization},
  keywords-plus = {SLAM},
  language = {English},
  number-of-cited-references = {24},
  oa = {Green Published, Green Submitted},
  orcid-numbers = {Civera, Javier/0000-0003-1368-1151
Bescos Torcal, Berta/0000-0003-1188-8415
Facil, Jose M./0000-0002-8757-4749
Neira Parra, Jose/0000-0003-0668-977X},
  research-areas = {Robotics},
  researcherid-numbers = {Neira, Jose/AAM-6571-2020
Civera, Javier/I-3651-2015
Neira Parra, Jose/F-8887-2013},
  times-cited = {179},
  type = {Article},
  unique-id = {WOS:000442338200002},
  usage-count-last-180-days = {34},
  usage-count-since-2013 = {127},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{kim-et-al:2021:3047421,
  author = {C. Kim and S. Cho and M. Sunwoo and B. P. A. B. Resende and K. Jo},
  journal = {IEEE ACCESS},
  title = {A Geodetic Normal Distribution Map for Long-Term LiDAR Localization on
Earth},
  volume = {9},
  pages = {470--484},
  doi = {10.1109/ACCESS.2020.3047421},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2021},
  abstract = {Light detection and ranging (LiDAR) sensors enable a vehicle to estimate
a pose by matching their measurements with a point cloud (PCD) map.
However, the PCD map structure, widely used in robot fields, has some
problems to be applied for mass production in automotive fields. First,
the PCD map is too big to store all map data at in-vehicle units or
download the map data from a wireless network according to the vehicle
location. Second, the PCD map, represented by a single origin in the
Cartesian coordinates, causes coordinate conversion errors due to an
inaccurate plane-orb projection, when the vehicle estimate the geodetic
pose on Earth. To solve two problems, this paper presents a geodetic
normal distribution (GND) map structure. The GND map structure supports
a geodetic quad-tree tiling system with multiple origins to minimize the
coordinate conversion errors. The map data managed by the GND map
structure are compressed by using Cartesian probabilistic distributions
of points as map features. The truncation errors by heterogeneous
coordinates between the geodetic tiling system and Cartesian
distributions are compensated by the Cartesian voxelization rule. In
order to match the LiDAR measurements with the GND map structure, the
paper proposes map-matching approaches based on Monte-Carlo and
optimization. The paper performed some experiments to evaluate the map
size compression and the long-term localization on Earth: comparison
with the PCD map structure, localization in various continents, and
long-term localization.},
  affiliation = {Jo, K (Corresponding Author), Konkuk Univ, Dept Smart Vehicle Engn, Seoul 05029, South Korea.
Kim, Chansoo; Cho, Sungjin; Sunwoo, Myoungho, Hanyang Univ, Dept Automot Engn, Seoul 04763, South Korea.
Resende, Paulo; Bradai, Benazouz, Valeo Driving Assistance Res Ctr, F-93012 Bobigny, France.
Jo, Kichun, Konkuk Univ, Dept Smart Vehicle Engn, Seoul 05029, South Korea.},
  affiliations = {Hanyang University; Valeo SA; Konkuk University},
  author-email = {kichun.jo@gmail.com},
  cited-references = {Akai N., 2020, P IEEE INT VEH S 4 O, P1588.
Akai N, 2020, IEEE ROBOT AUTOM LET, V5, P4384, DOI 10.1109/LRA.2020.2998403.
Al Hage J, 2019, IEEE INT VEH SYM, P1232, DOI 10.1109/IVS.2019.8813988.
Biber P, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2743, DOI 10.1109/iros.2003.1249285.
Cho S., 2020, IEEE T INTELL TRANSP, DOI {[}10.1109/TITS.2020.3035801, DOI 10.1109/TITS.2020.3035801].
Choi J, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P3082, DOI 10.1109/ITSC.2014.6958185.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297.
Google, GOOGLE PROTOCOL BUFF.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Hao Fu, 2020, 2020 12th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC), P217, DOI 10.1109/IHMSC49165.2020.10127.
Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0.
Javanmardi E, 2018, IEEE INT C INTELL TR, P2237, DOI 10.1109/ITSC.2018.8569236.
Jeong J, 2017, IEEE INT VEH SYM, P1736, DOI 10.1109/IVS.2017.7995958.
Jo H, 2020, IEEE ACCESS, V8, P74485, DOI 10.1109/ACCESS.2020.2988464.
Jo K., 2019, 2019010491 SAE.
Jo KC, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124119.
Jo K, 2016, IEEE T INTELL TRANSP, V17, P2008, DOI 10.1109/TITS.2015.2475620.
Kim C., 2017, P 14 WORKSH POS NAV, P1.
Kim C, 2019, IEEE ACCESS, V7, P92791, DOI 10.1109/ACCESS.2019.2927736.
Kim C, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124172.
Kim KW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041220.
Kuutti S, 2018, IEEE INTERNET THINGS, V5, P829, DOI 10.1109/JIOT.2018.2812300.
Levinson J., 2007, ROBOTICS SCI SYSTEMS, V4.
Li L, 2016, IEEE INT VEH SYM, P883, DOI 10.1109/IVS.2016.7535492.
Liu R, 2020, J NAVIGATION, V73, P324, DOI 10.1017/S0373463319000638.
Liu ZZ, 2018, IEEE INT VEH SYM, P662, DOI 10.1109/IVS.2018.8500641.
Maddern W, 2015, IEEE INT CONF ROBOT, P1684, DOI 10.1109/ICRA.2015.7139414.
Magnusson M, 2007, J FIELD ROBOT, V24, P803, DOI 10.1002/rob.20204.
Mobileye, ROAD EXPERIENCE MANA.
Montemerlo M, 2008, J FIELD ROBOT, V25, P569, DOI 10.1002/rob.20258.
Morton G. M., 1966, IBM RES.
Mousavian A., 2015, P CVPR WORKSH.
Qu XZ, 2015, IEEE INT VEH SYM, P605, DOI 10.1109/IVS.2015.7225751.
Reid T. G. R., 2019, SAE INT J CONNECTED, P1.
Schreiber M, 2013, IEEE INT VEH SYM, P449, DOI 10.1109/IVS.2013.6629509.
Stewart AD, 2012, IEEE INT CONF ROBOT, P2625, DOI 10.1109/ICRA.2012.6224750.
Stoyanov T, 2012, INT J ROBOT RES, V31, P1377, DOI 10.1177/0278364912460895.
Suhr JK, 2017, IEEE T INTELL TRANSP, V18, P1078, DOI 10.1109/TITS.2016.2595618.
Thrun S., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P321, DOI 10.1109/ROBOT.2000.844077.
Thrun S, 2006, J FIELD ROBOT, V23, P661, DOI 10.1002/rob.20147.
TomTom, TOMTOM HD MAP ROADDN.
Trehard G, 2015, IEEE INT VEH SYM, P814, DOI 10.1109/IVS.2015.7225785.
Trehard G, 2014, IEEE INT C INT ROBOT, P2699, DOI 10.1109/IROS.2014.6942931.
Urmson C, 2008, J FIELD ROBOT, V25, P425, DOI 10.1002/rob.20255.
Wang ZP, 2020, IEEE T INTELL VEHICL, V5, P616, DOI 10.1109/TIV.2020.3003699.
Wiggers K., 2020, BOSCH DEBUTS LONG RA.
Wolcott RW, 2017, INT J ROBOT RES, V36, P292, DOI 10.1177/0278364917696568.
Wolcott RW, 2015, IEEE INT CONF ROBOT, P2814, DOI 10.1109/ICRA.2015.7139582.
Wurm K. M., 2010, P ICRA WORKSH BEST P, V2.
Xu YQ, 2017, IEEE INT VEH SYM, P487, DOI 10.1109/IVS.2017.7995765.
Yue YF, 2020, IEEE INT C INT ROBOT, P6188, DOI 10.1109/IROS45743.2020.9340970.
Ziegler J, 2014, IEEE INT VEH SYM, P1231, DOI 10.1109/IVS.2014.6856560.},
  da = {2022-05-17},
  doc-delivery-number = {PQ5BP},
  funding-acknowledgement = {BK21 Plus Program through the Ministry of Education, South Korea
{[}22A20130000045]; Industrial Strategy Technology Development Program
{[}10039673, 10060068, 10079961]; International Collaborative Research
and Development Program through the Ministry of Trade, Industry, and
Energy (MOTIE Korea) {[}N0001992]; National Research Foundation of Korea
(NRF) - Korean Government (MEST) {[}2011-0017495]},
  funding-text = {This work was supported in part by the BK21 Plus Program through the
Ministry of Education, South Korea, under Grant 22A20130000045; in part
by the Industrial Strategy Technology Development Program under Grant
10039673, Grant 10060068, and Grant 10079961; in part by the
International Collaborative Research and Development Program through the
Ministry of Trade, Industry, and Energy (MOTIE Korea) under Grant
N0001992; and in part by the National Research Foundation of Korea (NRF)
Grant funded by the Korean Government (MEST) under Grant 2011-0017495.},
  issn = {2169-3536},
  journal-iso = {IEEE Access},
  keywords = {World-scale map management; map compression; normal distribution map;
registration},
  language = {English},
  number-of-cited-references = {53},
  oa = {gold},
  orcid-numbers = {Myoungho, Sunwoo/0000-0002-3505-6675
Cho, Sungjin/0000-0002-5819-4288
Resende, Paulo/0000-0002-5170-7837
Kim, Chansoo/0000-0002-3382-6250},
  research-areas = {Computer Science; Engineering; Telecommunications},
  times-cited = {1},
  type = {Article},
  unique-id = {WOS:000606561000001},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Computer Science, Information Systems; Engineering, Electrical \&
Electronic; Telecommunications},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ball-et-al:2013:9,
  author = {D. Ball and S. Heath and J. Wiles and P. G. A. C. Wyeth and M. Milford},
  journal = {AUTONOMOUS ROBOTS},
  title = {OpenRatSLAM: an open source brain-based SLAM system},
  volume = {34},
  number = {3},
  pages = {149--176},
  doi = {10.1007/s10514-012-9317-9},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2013},
  month = {4},
  abstract = {RatSLAM is a navigation system based on the neural processes underlying
navigation in the rodent brain, capable of operating with low resolution
monocular image data. Seminal experiments using RatSLAM include mapping
an entire suburb with a web camera and a long term robot delivery trial.
This paper describes OpenRatSLAM, an open-source version of RatSLAM with
bindings to the Robot Operating System framework to leverage advantages
such as robot and sensor abstraction, networking, data playback, and
visualization. OpenRatSLAM comprises connected ROS nodes to represent
RatSLAM's pose cells, experience map, and local view cells, as well as a
fourth node that provides visual odometry estimates. The nodes are
described with reference to the RatSLAM model and salient details of the
ROS implementation such as topics, messages, parameters, class diagrams,
sequence diagrams, and parameter tuning strategies. The performance of
the system is demonstrated on three publicly available open-source
datasets.},
  affiliation = {Ball, D (Corresponding Author), Queensland Univ Technol, Sch Elect Engn \& Comp Sci, Brisbane, Qld 4001, Australia.
Ball, David; Wyeth, Gordon; Corke, Peter; Milford, Michael, Queensland Univ Technol, Sch Elect Engn \& Comp Sci, Brisbane, Qld 4001, Australia.
Heath, Scott; Wiles, Janet, Queensland Univ Technol, Sch Informat Technol \& Elect Engn, Brisbane, Qld 4001, Australia.},
  affiliations = {Queensland University of Technology (QUT); Queensland University of
Technology (QUT)},
  author-email = {david.ball@qut.edu.au
michael.milford@qut.edu.au},
  cited-references = {Andreasson H, 2008, IEEE T ROBOT, V24, P991, DOI 10.1109/TRO.2008.2004642.
Ball D., 2009, RATSLAM.
Ball D., 2010, AUSTR C ROB AUT BRIS.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Cummins M., 2009, ROBOTICS SCI SYSTEMS.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049.
Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721.
Heath S., 2011, AUSTR C ROB AUT MELB.
Jacobson A., 2012, BRAIN BASED SENSOR F.
Knuth D., 1977, INFORM PROCESSING LE, P6.
Konolige K, 2008, IEEE T ROBOT, V24, P1066, DOI 10.1109/TRO.2008.2004832.
Konolige K, 2008, SPRINGER TRAC ADV RO, V39, P179.
Kyprou S., 2009, SIMPLE EFFECTIVE PER.
Labbe M., 2011, IEEE RSJ INT C INT R.
Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410.
Maddern W, 2012, INT J ROBOT RES, V31, P429, DOI 10.1177/0278364912438273.
Milford M., 2008, INT C ROB AUT PAS US.
Milford M., 2013, IEEE INT C ROB AUT.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
Milford MJ, 2008, IEEE T ROBOT, V24, P1038, DOI 10.1109/TRO.2008.2004520.
Milford MJ, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000995.
Milford MJ, 2008, SPRINGER TRAC ADV RO, V41, P1.
Newman P, 2009, INT J ROBOT RES, V28, P1406, DOI 10.1177/0278364909341483.
Quigley M, 2009, IEEE INT C ROB AUT K.
Samsonovich A, 1997, J NEUROSCI, V17, P5900.
Sibley G, 2010, INT J ROBOT RES, V29, P958, DOI 10.1177/0278364910369268.
Smith Devin, 2009, J COMPUTING SCI COLL, V24, P168.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.
Strasdat H., 2010, SCALE DRIFT AWARE LA.
Sunderhauf N, 2010, IEEE INT C EMERG.
Sunderhauf N, 2012, IEEE INT CONF ROBOT, P1254, DOI 10.1109/ICRA.2012.6224709.
Zhang Alan M, 2009, International Journal of Robotics Research, V28, P331, DOI 10.1177/0278364908098412.
Zoccolan D, 2009, P NATL ACAD SCI USA, V106, P8748, DOI 10.1073/pnas.0811583106.},
  da = {2022-05-17},
  doc-delivery-number = {115NX},
  eissn = {1573-7527},
  funding-acknowledgement = {Australian Research Council under Discovery Project {[}DP0987078,
DP1212775]; Australian Research Council under Special Research
Initiative on Thinking Systems {[}TS0669699]},
  funding-text = {This work was supported in part by the Australian Research Council under
a Discovery Project Grant DP0987078 to GW and JW, a Special Research
Initiative on Thinking Systems TS0669699 to GW and JW and a Discovery
Project Grant DP1212775 to MM. We would like to thank Samuel Brian for
coding an iRat ground truth tracking system.},
  issn = {0929-5593},
  journal-iso = {Auton. Robot.},
  keywords = {RatSLAM; OpenRatSLAM; SLAM; Navigation; Mapping; Brain-based;
Appearance-based; ROS; Open-source; Hippocampus},
  keywords-plus = {PROBABILISTIC LOCALIZATION; NAVIGATION; VISION; MAP},
  language = {English},
  number-of-cited-references = {35},
  orcid-numbers = {Corke, Peter/0000-0001-6650-367X
Milford, Michael/0000-0002-5162-1793
Wyeth, Gordon/0000-0002-4996-3612},
  research-areas = {Computer Science; Robotics},
  researcherid-numbers = {Corke, Peter/C-6770-2009
Milford, Michael/J-1304-2012},
  times-cited = {53},
  type = {Article},
  unique-id = {WOS:000316820200003},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {60},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{martini-et-al:2020:s20216002,
  author = {D. D. Martini and M. Gadd and P. Newman},
  journal = {SENSORS},
  title = {kRadar plus plus : Coarse-to-Fine FMCW Scanning Radar Localisation},
  volume = {20},
  number = {21},
  pages = {6002},
  doi = {10.3390/s20216002},
  publisher = {MDPI},
  address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  year = {2020},
  month = {11},
  abstract = {Simple Summary
This paper presents a hierarchical approach to place recognition and
pose refinement for Frequency-Modulated Continuous-Wave (FMCW) scanning
radar localisation.
This paper presents a novel two-stage system which integrates
topological localisation candidates from a radar-only place recognition
system with precise pose estimation using spectral landmark-based
techniques. We prove that the-recently available-seminal radar place
recognition (RPR) and scan matching sub-systems are complementary in a
style reminiscent of the mapping and localisation systems underpinning
visual teach-and-repeat (VTR) systems which have been exhibited robustly
in the last decade. Offline experiments are conducted on the most
extensive radar-focused urban autonomy dataset available to the
community with performance comparing favourably with and even rivalling
alternative state-of-the-art radar localisation systems. Specifically,
we show the long-term durability of the approach and of the sensing
technology itself to autonomous navigation. We suggest a range of
sensible methods of tuning the system, all of which are suitable for
online operation. For both tuning regimes, we achieve, over the course
of a month of localisation trials against a single static map, high
recalls at high precision, and much reduced variance in erroneous metric
pose estimation. As such, this work is a necessary first step towards a
radar teach-and-repeat (RTR) system and the enablement of autonomy
across extreme changes in appearance or inclement conditions.},
  affiliation = {De Martini, D; Gadd, M (Corresponding Author), Univ Oxford, Oxford Robot Inst, Dept Engn Sci, Oxford OX1 3PJ, England.
De Martini, Daniele; Gadd, Matthew; Newman, Paul, Univ Oxford, Oxford Robot Inst, Dept Engn Sci, Oxford OX1 3PJ, England.},
  affiliations = {League of European Research Universities - LERU; University of Oxford},
  article-number = {6002},
  author-email = {Correspondencedaniele@robots.ox.ac.uk
mattgadd@robots.ox.ac.uk
pnewman@robots.ox.ac.uk},
  cited-references = {Aldera R., 2019, P IEEE INT TRANSP SY.
Aldera R, 2019, IEEE INT CONF ROBOT, P1190, DOI 10.1109/ICRA.2019.8794014.
Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Barnes D., 2019, ARXIV190903752.
Barnes D, 2020, IEEE INT CONF ROBOT, P9484, DOI 10.1109/ICRA40945.2020.9196835.
Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8\_26.
Cen SH, 2019, IEEE INT CONF ROBOT, P298, DOI 10.1109/ICRA.2019.8793990.
Cen SH, 2018, IEEE INT CONF ROBOT, P6045, DOI 10.1109/ICRA.2018.8460687.
Chen X., 2020, P ROB SCI ROB SCI SY.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Churchill W, 2012, IEEE INT CONF ROBOT, P4525, DOI 10.1109/ICRA.2012.6224596.
Cieslewski T, 2018, IEEE INT CONF ROBOT, P2466.
Dequaire J., 2016, P IEEE INT C ROB AUT.
Farina B, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082287.
Furgale P, 2010, J FIELD ROBOT, V27, P534, DOI 10.1002/rob.20342.
Gadd M., 2020, P IEEE INT VEH S 4 W.
Gadd M., 2020, P IEEE ION POS LOC N.
Garcia-Fidalgo E, 2017, IEEE T ROBOT, V33, P1061, DOI 10.1109/TRO.2017.2704598.
Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1.
Hong ZY, 2020, IEEE INT C INT ROBOT, P5164, DOI 10.1109/IROS45743.2020.9341287.
Horn RA., 1990, MATRIX ANAL.
Kaul P., 2020, P IEEE INT VEH S IV.
Kim G, 2020, IEEE INT CONF ROBOT, P6246.
Kim G, 2018, IEEE INT C INT ROBOT, P4802, DOI 10.1109/IROS.2018.8593953.
Kim T, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20154126.
Krajnik T, 2018, IEEE INT C INT ROBOT, P1657, DOI 10.1109/IROS.2018.8593803.
Kyberd S., 2019, P INT C FIELD SERV R.
Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482.
MacTavish Kirk, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2065, DOI 10.1109/ICRA.2017.7989238.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
Maddern W, 2015, IEEE INT CONF ROBOT, P1684, DOI 10.1109/ICRA.2015.7139414.
Middelberg S, 2014, LECT NOTES COMPUT SC, V8690, P268, DOI 10.1007/978-3-319-10605-2\_18.
Mielle M, 2019, 2019 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR).
Reina G, 2015, SENSORS-BASEL, V15, P14661, DOI 10.3390/s150614661.
Sarlin P.E., 2018, P 2 ANN C ROB LEARN.
Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300.
Sattler T, 2015, IEEE I CONF COMP VIS, P2102, DOI 10.1109/ICCV.2015.243.
Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682.
Sftescu S., 2020, P IEEE INT C ROB AUT.
Sibley G, 2010, IEEE INT CONF ROBOT, P285, DOI 10.1109/ROBOT.2010.5509527.
Sibley G, 2010, INT J ROBOT RES, V29, P958, DOI 10.1177/0278364910369268.
Simonyan K, 2014, C TRACK P.
Sprunk C, 2013, IEEE INT C INT ROBOT, P3144, DOI 10.1109/IROS.2013.6696803.
Strasdat H, 2011, IEEE I CONF COMP VIS, P2352, DOI 10.1109/ICCV.2011.6126517.
Tang TY, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI.
Tang TYQ, 2020, IEEE ROBOT AUTOM LET, V5, P1087, DOI 10.1109/LRA.2020.2965907.
Mau TN, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10072539.
Vivet D, 2013, SENSORS-BASEL, V13, P4527, DOI 10.3390/s130404527.
Wan Y, 2020, J COASTAL RES, P9, DOI 10.2112/SI99-002.1.
Wang TH, 2018, IEEE INT CONF ROBOT, P2341.
Weston R, 2019, IEEE INT CONF ROBOT, P5446, DOI 10.1109/ICRA.2019.8793263.
Williams WD, 2005, LAKES HANDBOOK, VOL 2: LAKE RESTORATION AND REHABILITATION, P200.},
  da = {2022-05-17},
  doc-delivery-number = {OR2OX},
  eissn = {1424-8220},
  funding-acknowledgement = {Lloyd's Register Foundation; University of York; UK EPSRC
{[}EP/M019918/1]; EPSRC {[}EP/M019918/1] Funding Source: UKRI},
  funding-text = {This project is supported by the Assuring Autonomy International
Programme, a partnership between Lloyd's Register Foundation and the
University of York and UK EPSRC Programme Grant EP/M019918/1.},
  journal-iso = {Sensors},
  keywords = {radar; mapping; localisation; place recognition; autonomous vehicles;
deep learning},
  keywords-plus = {NAVIGATION},
  language = {English},
  number-of-cited-references = {52},
  oa = {Green Submitted, gold, Green Published},
  orcid-numbers = {Gadd, Matthew/0000-0001-9447-8619},
  research-areas = {Chemistry; Engineering; Instruments \& Instrumentation},
  researcherid-numbers = {Gadd, Matthew/AAS-4274-2020},
  times-cited = {6},
  type = {Article},
  unique-id = {WOS:000589315500001},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Chemistry, Analytical; Engineering, Electrical \& Electronic;
Instruments \& Instrumentation},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{falliat:2007:364080,
  author = {D. Falliat},
  booktitle = {PROCEEDINGS OF THE 2007 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND
AUTOMATION, VOLS 1-10},
  title = {A visual bag of words method for interactive qualitative localization
and mapping},
  pages = {3921--3926},
  doi = {10.1109/ROBOT.2007.364080},
  note = {IEEE International Conference on Robotics and Automation, Rome, ITALY,
APR 10-14, 2007},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2007},
  abstract = {Localization for low cost humanoid or animal-like personal robots has to
rely on cheap sensors and has to be robust to user manipulations of the
robot. We present a visual localization and map-learning system that
relies on vision only and that is able to incrementally learn to
recognize the different rooms of an apartment from any robot position.
This system is inspired by visual categorization algorithms called bag
of words methods that we modified to make fully incremental and to allow
a user-interactive training. Our system is able to reliably recognize
the room in which the robot is after a short training time and is stable
for long term use. Empirical validation on a real robot and on an image
database acquired in real environments are presented.},
  affiliation = {Falliat, D (Corresponding Author), ENSTA, 32 Blvd Victor, F-75015 Paris, France.
ENSTA, F-75015 Paris, France.},
  affiliations = {Institut Polytechnique de Paris},
  author-email = {david.filliat@ensta.fr},
  book-group-author = {IEEE},
  cited-references = {BAILLIE JC, 2004, INT J HUMANOID ROBOT.
Beis JS, 1999, IEEE T PATTERN ANAL, V21, P1000, DOI 10.1109/34.799907.
Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5.
Cauwenberghs G, 2001, ADV NEUR IN, V13, P409.
Csurka G, 2004, P WORKSH STAT LEARN.
DAVISON A, 2004, P IFAC S INT AUT VEH.
Filliat D., 2003, COGN SYST RES, V4, P243, DOI DOI 10.1016/S1389-0417(03)00008-1.
Fox D, 1998, ROBOT AUTON SYST, V25, P195, DOI 10.1016/S0921-8890(98)00049-9.
Fox D., 1999, J ARTIFICIAL INTELLI, V11.
Gonzalez-Barbosa JJ, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P1365, DOI 10.1109/ROBOT.2002.1014733.
HAHNEL D, 2003, P IEEE RSJ INT C INT.
Jogan M, 2003, ROBOT AUTON SYST, V45, P51, DOI 10.1016/S0921-8890(03)00064-2.
Jude Frederic, 2005, INT C COMP VIS.
Kosecka J, 2004, INT C PATT RECOG, P319, DOI 10.1109/ICPR.2004.1333767.
Kuipers B., 1991, Robotics and Autonomous Systems, V8, P47, DOI 10.1016/0921-8890(91)90014-C.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Mikolajczyk K, 2003, PROC CVPR IEEE, P257.
MOZOS OM, 2005, P INT C ROB AUT ICRA.
Porta JM, 2003, IEEE INT CONF ROBOT, P2842, DOI 10.1109/ROBOT.2003.1242023.
Rybski PE, 2003, IEEE INT CONF ROBOT, P850.
Smith R., 1988, UNCERTAINTY ARTIFICI, V5, P435, DOI DOI 10.1016/B978-0-444-70396-5.50042-X.
Tapus A., 2005, P IEEE RSJ INT C INT.
Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243.
ULRICH I, 2000, P IEEE INT C ROB AUT, V2, P1023.
WOLF J, 2002, P INT C IM VID RETR.},
  da = {2022-05-17},
  doc-delivery-number = {BGW23},
  eissn = {2577-087X},
  isbn = {978-1-4244-0601-2},
  issn = {1050-4729},
  language = {English},
  number-of-cited-references = {25},
  research-areas = {Automation \& Control Systems; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {122},
  type = {Proceedings Paper},
  unique-id = {WOS:000250915303152},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {7},
  web-of-science-categories = {Automation \& Control Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{opdenbosch-et-al:2018:00114,
  author = {D. V. Opdenbosch and T. Aykut and M. Oelsch and A. Nicolas and E. Steinbach},
  booktitle = {2018 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV
2018)},
  title = {Efficient Map Compression for Collaborative Visual SLAM},
  pages = {992--1000},
  doi = {10.1109/WACV.2018.00114},
  note = {18th IEEE Winter Conference on Applications of Computer Vision (WACV),
NV, MAR 12-15, 2018},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2018},
  abstract = {Swarm robotics is receiving increasing interest, because the
collaborative completion of tasks, such as the exploration of unknown
environments, leads to improved performance and reduced effort. The
ability to exchange map information is an essential requirement for
collaborative exploration. When moving to large-scale environments,
where the communication data rate between the swarm participants is
typically limited, efficient compression algorithms and an approach for
discarding less informative parts of the map are key for a successful
long-term operation. In this paper, we present a novel compression
approach for environment maps obtained from a visual SLAM system. We
apply feature coding to the visual information to compress the map
efficiently. We make use of a minimum spanning tree to connect all
features that serve as observations of a single map point. Thereby, we
can exploit inter-feature dependencies and obtain an optimal coding
order. Additionally, we add a map sparsification step to keep only
useful map points by solving a linear integer programming problem, which
preserves the map points that exhibit both good compression properties
and high observability. We evaluate the proposed method on a standard
dataset and show that our approach outperforms state-of-the-art
techniques.},
  affiliation = {Van Opdenbosch, D (Corresponding Author), Tech Univ Munich, Chair Media Technol, Munich, Germany.
Van Opdenbosch, Dominik; Aykut, Tamay; Oelsch, Martin; Alt, Nicolas; Steinbach, Eckehard, Tech Univ Munich, Chair Media Technol, Munich, Germany.},
  affiliations = {Technical University of Munich},
  author-email = {dominik.van-opdenbosch@tum.de
eckehard.steinbach@tum.de},
  book-group-author = {IEEE},
  cited-references = {Baroffio L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2445294.
Baroffio L, 2014, IEEE T IMAGE PROCESS, V23, P2262, DOI 10.1109/TIP.2014.2312617.
Burri M., 2016, INT J ROBOT RES, V35, P1.
Cheng WT, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2623488.
Cieslewski T, 2015, IEEE INT CONF ROBOT, P6241, DOI 10.1109/ICRA.2015.7140075.
Contreras Luis, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4509, DOI 10.1109/ICRA.2017.7989523.
Contreras L, 2015, IEEE INT C INT ROBOT, P133, DOI 10.1109/IROS.2015.7353365.
Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034.
Dymczyk M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4572, DOI 10.1109/IROS.2016.7759673.
Dymczyk M, 2015, IEEE INT C INT ROBOT, P2536, DOI 10.1109/IROS.2015.7353722.
Dymczyk M, 2015, IEEE INT CONF ROBOT, P2767, DOI 10.1109/ICRA.2015.7139575.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
G. O. Inc, 2017, GUROBI OPTIMIZER REF.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Huiskes M. J., 2008, P 1 INT ACM C MULT I, P39, DOI DOI 10.1145/1460096.1460104.
Karp R.M., 1972, COMPLEXITY COMPUTER, P85, DOI DOI 10.1007/978-1-4684-2001-2\_9.
Klein George, 2007, P1.
Kruskal JB., 1956, AM MATH SOC, V7, P48, DOI {[}10.1090/s0002-9939-1956-0078686-7, 10.1090/S0002-9939-1956-0078686-7].
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Lynen Simon, 2015, ROBOTICS SCI SYSTEMS, VXI.
Merzic Hamza, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3200, DOI 10.1109/ICRA.2017.7989363.
MURARTAL R, 2017, IEEE T ROBOTICS, P1, DOI DOI 10.1109/TR0.2017.2705103.
Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513.
Opdenbosch D. V., 2017, IEEE C VIS COMM IM P.
Park HS, 2013, IEEE COMPUT SOC CONF, P229, DOI 10.1109/CVPRW.2013.41.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Strasdat H, 2011, IEEE I CONF COMP VIS, P2352, DOI 10.1109/ICCV.2011.6126517.
Williams B, 2009, ROBOT AUTON SYST, V57, P1188, DOI 10.1016/j.robot.2009.06.010.},
  da = {2022-05-17},
  doc-delivery-number = {BK2ZI},
  funding-acknowledgement = {space agency of the German Aerospace Center; Federal Ministry of
Economics and Technology of the German Bundestag {[}50NA1515]},
  funding-text = {This work is supported by the space agency of the German Aerospace
Center with funds from the Federal Ministry of Economics and Technology
on the basis of a resolution of the German Bundestag under the reference
50NA1515.},
  isbn = {978-1-5386-4886-5},
  issn = {2472-6737},
  language = {English},
  number-of-cited-references = {28},
  orcid-numbers = {Aykut, Tamay/0000-0002-8497-9521},
  research-areas = {Computer Science; Engineering},
  researcherid-numbers = {Aykut, Tamay/E-8936-2017},
  series = {IEEE Winter Conference on Applications of Computer Vision},
  times-cited = {6},
  type = {Proceedings Paper},
  unique-id = {WOS:000434349200108},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
Electronic},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{derner-et-al:2021:103676,
  author = {E. Derner and C. Gomez and A. C. Hernandez and B. Ramon and R. Babuska},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {Change detection using weighted features for image-based localization},
  volume = {135},
  pages = {103676},
  doi = {10.1016/j.robot.2020.103676},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2021},
  month = {1},
  abstract = {Autonomous mobile robots are becoming increasingly important in many
industrial and domestic environments. Dealing with unforeseen situations
is a difficult problem that must be tackled to achieve long-term robot
autonomy. In vision-based localization and navigation methods, one of
the major issues is the scene dynamics. The autonomous operation of the
robot may become unreliable if the changes occurring in dynamic
environments are not detected and managed. Moving chairs, opening and
closing doors or windows, replacing objects and other changes make many
conventional methods fail. To deal with these challenges, we present a
novel method for change detection based on weighted local visual
features. The core idea of the algorithm is to distinguish the valuable
information in stable regions of the scene from the potentially
misleading information in the regions that are changing. We evaluate the
change detection algorithm in a visual localization framework based on
feature matching by performing a series of long-term localization
experiments in various real-world environments. The results show that
the change detection method yields an improvement in the localization
accuracy, compared to the baseline method without change detection. In
addition, an experimental evaluation on a public long-term localization
data set with more than 10000 images reveals that the proposed method
outperforms two alternative localization methods on images recorded
several months after the initial mapping. (c) 2020 Elsevier B.V. All
rights reserved.},
  affiliation = {Derner, E (Corresponding Author), Czech Tech Univ, Czech Inst Informat Robot \& Cybernet, Prague, Czech Republic.
Derner, Erik; Babuska, Robert, Czech Tech Univ, Czech Inst Informat Robot \& Cybernet, Prague, Czech Republic.
Derner, Erik, Czech Tech Univ, Fac Elect Engn, Dept Control Engn, Prague, Czech Republic.
Gomez, Clara; Hernandez, Alejandra C.; Barber, Ramon, Carlos III Univ Madrid, Dept Syst Engn \& Automat, Robot Lab, Madrid, Spain.
Babuska, Robert, Delft Univ Technol, Cognit Robot, Delft, Netherlands.},
  affiliations = {Czech Technical University Prague; Czech Technical University Prague;
Delft University of Technology},
  article-number = {103676},
  author-email = {erik.derner@cvut.cz},
  cited-references = {Alcantarilla PF, 2018, AUTON ROBOT, V42, P1301, DOI 10.1007/s10514-018-9734-5.
Alterovitz R, 2016, AI MAG, V37, P76, DOI 10.1609/aimag.v37i2.2651.
Andrade-Cetto J, 2002, INT J PATTERN RECOGN, V16, P361, DOI 10.1142/S0218001402001745.
Arroyo R, 2015, IEEE INT CONF ROBOT, P6328, DOI 10.1109/ICRA.2015.7140088.
Atkinson RC., 1968, PSYCHOL LEARN MOTIV, P89, DOI {[}10.1016/S0079-7421(08)60422-3, DOI 10.1016/S0079-7421(08)60422-3].
Bacca B, 2011, ROBOT AUTON SYST, V59, P840, DOI 10.1016/j.robot.2011.06.008.
Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
Bezdek JC., 2013, PATTERN RECOGN.
Biswas J, 2014, IEEE INT CONF ROBOT, P3969, DOI 10.1109/ICRA.2014.6907435.
Boniardi F, 2019, ROBOT AUTON SYST, V112, P84, DOI 10.1016/j.robot.2018.11.003.
Carlevaris-Bianco N, 2014, IEEE INT C INT ROBOT, P2769, DOI 10.1109/IROS.2014.6942941.
Chen ZT, 2018, IEEE ROBOT AUTOM LET, V3, P4015, DOI 10.1109/LRA.2018.2859916.
CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dayoub F, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3364, DOI 10.1109/IROS.2008.4650701.
Drews P, 2013, 2013 16TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR).
Fehr Marius, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5237, DOI 10.1109/ICRA.2017.7989614.
Finman R, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P178, DOI 10.1109/ECMR.2013.6698839.
FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
Folkesson, 2018, ARXIV PREPRINT ARXIV.
Garcia-Fidalgo E, 2015, ROBOT AUTON SYST, V64, P1, DOI 10.1016/j.robot.2014.11.009.
Hanheide, 2014, IEEE RSJ INT C INT R, DOI {[}10.1109/ IROS.2014.6943205, DOI 10.1109/IROS.2014.6943205].
Hawes N, 2017, IEEE ROBOT AUTOM MAG, V24, P146, DOI 10.1109/MRA.2016.2636359.
Johns E, 2014, INT J COMPUT VISION, V106, P297, DOI 10.1007/s11263-013-0648-6.
Johns E, 2013, IEEE INT CONF ROBOT, P3212, DOI 10.1109/ICRA.2013.6631024.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Kunze L., 2018, AAAI.
Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Naseer T, 2018, IEEE T ROBOT, V34, P289, DOI 10.1109/TRO.2017.2788045.
Neira, 2018, ARXIV180605620.
Neuman B, 2011, IEEE INT CONF ROBOT.
Nistr D., 2006, P IEEE COMP VIS PAT, V2, P2161.
Nobre F, 2018, IEEE INT CONF ROBOT, P3661.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663.
Sun L, 2018, IEEE ROBOT AUTOM LET, V3, P3749, DOI 10.1109/LRA.2018.2856268.
Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832.
Valgren C., 2007, EUR C MOB ROB.
Wellhausen L, 2017, 2017 IEEE INTERNATIONAL SYMPOSIUM ON SAFETY, SECURITY AND RESCUE ROBOTICS (SSRR), P81, DOI 10.1109/SSRR.2017.8088144.},
  da = {2022-05-17},
  doc-delivery-number = {OZ9QO},
  eissn = {1872-793X},
  funding-acknowledgement = {European Regional Development Fund under the project Robotics for
Industry 4.0 {[}CZ.02.1.01/0.0/0.0/15\_003/0000470]; Grant Agency of the
Czech Technical University in Prague {[}SGS19/174/OHK3/3T/13]; HEROITEA:
Heterogeneous Intelligent Multi-Robot Team for Assistance of Elderly
People - Spanish Ministerio de Economia y Competitividad
{[}RTI2018-095599-B-C21]; RoboCity2030 -DIH-CM project
{[}S2018/NMT-4331]},
  funding-text = {This work was supported by the European Regional Development Fund under
the project Robotics for Industry 4.0 (reg. no.
CZ.02.1.01/0.0/0.0/15\_003/0000470) and by the Grant Agency of the Czech
Technical University in Prague, grant no. SGS19/174/OHK3/3T/13. This
research has also received funding from HEROITEA: Heterogeneous
Intelligent Multi-Robot Team for Assistance of Elderly People
(RTI2018-095599-B-C21), funded by Spanish Ministerio de Economia y
Competitividad, and the RoboCity2030 -DIH-CM project (S2018/NMT-4331,
RoboCity2030 -Madrid Robotics Digital Innovation Hub, Spain).},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Mobile robotics; Image-based localization; Change detection; Long-term
autonomy},
  language = {English},
  number-of-cited-references = {42},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  times-cited = {2},
  type = {Article},
  unique-id = {WOS:000595253700003},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {11},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{einhorn-gross:2015:008,
  author = {E. Einhorn and H.-M. Gross},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {Generic NDT mapping in dynamic environments and its application for
lifelong SLAM},
  volume = {69},
  number = {SI},
  pages = {28--39},
  doi = {10.1016/j.robot.2014.08.008},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2015},
  month = {7},
  abstract = {In this paper, we present a new, generic approach for Simultaneous
Localization and Mapping (SLAM). First of all, we propose an abstraction
of the underlying sensor data using Normal Distribution Transform (NDT)
maps that are suitable for making our approach independent from the used
sensor and the dimension of the generated maps. We present several
modifications for the original NDT mapping to handle free-space
measurements explicitly. We additionally describe a method to detect and
handle dynamic objects such as moving persons. This enables the usage of
the proposed approach in highly dynamic environments. In the second part
of this paper we describe our graph-based SLAM approach that is designed
for lifelong usage. Therefore, the memory and computational complexity
is limited by pruning the pose graph in an appropriate way. (C) 2014
Elsevier B.V. All rights reserved.},
  affiliation = {Einhorn, E (Corresponding Author), Ilmenau Univ Technol, Ilmenau, Germany.
Einhorn, Erik; Gross, Horst-Michael, Ilmenau Univ Technol, Ilmenau, Germany.},
  affiliations = {Technische Universitat Ilmenau},
  author-email = {Erik.Einhorn@tu-ilmenau.de},
  cited-references = {Biber P, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2743, DOI 10.1109/iros.2003.1249285.
Brechtel Sebastian, 2010, 2010 IEEE International Conference on Robotics and Automation (ICRA 2010), P3932, DOI 10.1109/ROBOT.2010.5509931.
Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049.
Einhorn E, 2010, IEEE INT C INT ROBOT, P816, DOI 10.1109/IROS.2010.5651741.
Einhorn E, 2011, IEEE INT CONF ROBOT, P1843, DOI 10.1109/ICRA.2011.5980084.
Fairfield N., 2007, J FIELD ROBOTICS.
Fox D, 2001, ADV NEURAL INFORM PR, V14.
Frese U, 2010, KUNSTL INTELL, V24, P191, DOI 10.1007/s13218-010-0040-4.
Frisken S., 2003, J GRAPHICS TOOLS, V7.
Gindele T, 2009, IEEE INT VEH SYM, P669, DOI 10.1109/IVS.2009.5164357.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Gross H-M, 2011, 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011), P2430, DOI 10.1109/IROS.2011.6048377.
Gross HM, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2005, DOI 10.1109/IROS.2009.5354497.
Kaess Michael, 2011, 2011 IEEE International Conference on Robotics and Automation, P3281.
Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706.
Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968.
Kretzschmar H, 2010, KUNSTL INTELL, V24, P199, DOI 10.1007/s13218-010-0034-2.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Magnusson M, 2009, THESIS OREBRO U.
Moravec H, 1996, TECHNICAL REPORT.
Olesen SM, 2015, J REAL-TIME IMAGE PR, V10, P105, DOI 10.1007/s11554-012-0261-x.
Park JH, 2012, SENSORS-BASEL, V12, P8640, DOI 10.3390/s120708640.
Payeur P., 1997, P IEEE INT C ROB AUT.
Rusinkiewicz S., 2001, INT C 3 D DIG IM MOD.
Saarinen J, 2013, IEEE INT CONF ROBOT, P2233, DOI 10.1109/ICRA.2013.6630878.
Saarinen JP, 2013, INT J ROBOT RES, V32, P1627, DOI 10.1177/0278364913499415.
Schroeter C, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P2078, DOI 10.1109/IROS.2008.4651137.
Stoyanov Todor, 2011, 2011 IEEE International Conference on Robotics and Automation, P4080.
Stoyanov T, 2012, IEEE INT CONF ROBOT, P5196, DOI 10.1109/ICRA.2012.6224717.
Stricker R., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P695, DOI 10.1109/ROMAN.2012.6343832.
Sunderhauf N, 2012, IEEE INT C INT ROBOT, P1879, DOI 10.1109/IROS.2012.6385590.
Thrun S., 2005, PROBABILISTIC ROBOTI.
WANG CC, 2003, P IEEE INT C ROB AUT.
Wurm K. M., 2010, P IEEE INT C ROB AUT.},
  da = {2022-05-17},
  doc-delivery-number = {CG2CO},
  eissn = {1872-793X},
  funding-acknowledgement = {German Federal Ministry of Education and Research as part of the ROREAS
project {[}16SV6133]},
  funding-text = {This work has received funding from the German Federal Ministry of
Education and Research as part of the ROREAS project under grant
agreement no. 16SV6133.},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Lifelong SLAM; Detection and tracking of moving objects; 20 and 3D
mapping; Normal Distribution Transform; Occupancy mapping; Map
registration; Mobile robots},
  language = {English},
  number-of-cited-references = {34},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  times-cited = {28},
  type = {Article},
  unique-id = {WOS:000353082700004},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {40},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{boniardi-et-al:2019:003,
  author = {F. Boniardi and T. Caselitz and R. Kuemmerle and B. Wolfram},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {A pose graph-based localization system for long-term navigation in CAD
floor plans},
  volume = {112},
  pages = {84--97},
  doi = {10.1016/j.robot.2018.11.003},
  publisher = {ELSEVIER SCIENCE BV},
  address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
  year = {2019},
  month = {2},
  abstract = {Accurate localization is an essential technology for flexible
automation. Industrial applications require mobile platforms to be
precisely localized in complex environments, often subject to continuous
changes and reconfiguration. Most of the approaches use precomputed maps
both for localization and for interfacing robots with workers and
operators. This results in increased deployment time and costs as
mapping experts are required to setup the robotic systems in factory
facilities. Moreover, such maps need to be updated whenever significant
changes in the environment occur in order to be usable within commanding
tools. To overcome those limitations, in this work we present a robust
and highly accurate method for long-term LiDAR-based indoor localization
that uses CAD-based architectural floor plans. The system leverages a
combination of graph-based mapping techniques and Bayes filtering to
maintain a sparse and up-to-date globally consistent map that represents
the latest configuration of the environment. This map is aligned to the
CAD drawing using prior constraints and is exploited for relative
localization, thus allowing the robot to estimate its current pose with
respect to the global reference frame of the floor plan. Furthermore,
the map helps in limiting the disturbances caused by structures and
clutter not represented in the drawing. Several long-term experiments in
changing real-world environments show that our system outperforms common
state-of-the-art localization methods in terms of accuracy and
robustness while remaining memory and computationally efficient. (C)
2018 Elsevier B.V. All rights reserved.},
  affiliation = {Boniardi, F (Corresponding Author), Univ Freiburg, Georges Kohler Allee 80, D-79110 Freiburg, Germany.
Boniardi, Federico; Caselitz, Tim; Burgard, Wolfram, Univ Freiburg, Georges Kohler Allee 80, D-79110 Freiburg, Germany.
Kuemmerle, Rainer, KUKA, Zugspitzstr 140, D-86165 Augsburg, Germany.},
  affiliations = {League of European Research Universities - LERU; University of Freiburg},
  author-email = {boniardi@informatik.uni-freiburg.de
caselitz@informatik.uni-freiburg.de
rainer.kuemmerle@kuka.com
burgard@informatik.uni-freiburg.de},
  cited-references = {Agarwal P, 2013, P IEEE INT C ROB AUT.
Biber P., 2005, P ROB SCI SYST.
Boniardi F., 2017, P IEEE RSJ INT C INT.
Bowen-Biggs L, 2016, P INT C SOC ROB ICSR.
Burgard W., 1996, P NAT C ART INT.
Carlevaris-Bianco N., 2013, P IEEE INT C ROB AUT.
Censi A., 2007, P IEEE INT C ROB AUT.
Censi A., 2008, P IEEE INT C ROB AUT.
Donnelly William, 2005, GPU GEMS, V2, P3.
ELFES A, 1987, IEEE T ROBOTIC AUTOM, V3, P249, DOI 10.1109/JRA.1987.1087096.
Fox D, 1999, J ARTIF INTELL RES, V11, P391, DOI 10.1613/jair.616.
Fox D, 2001, STAT ENG IN, P401.
Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925.
Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084.
Hile H., 2008, IEEE COMPUT GRAPHICS, V28.
HOPCROFT J, 1973, COMMUN ACM, V16, P372, DOI 10.1145/362248.362272.
Huang G., 2013, P IEEE EUR C MOB ROB.
International Organization for Standardization (ISO), 2015, 1189812015EN ISO, P1.
Ito S, 2014, INT CONF ACOUST SPEE.
Krajnik T, 2017, IEEE T ROBOT, V33, P964, DOI 10.1109/TRO.2017.2665664.
Kretzschmar H, 2012, INT J ROBOT RES, V31, P1219, DOI 10.1177/0278364912455072.
Kummerle R, 2015, J FIELD ROBOT, V32, P565, DOI 10.1002/rob.21534.
Kummerle R, 2011, AUTON ROBOT, V30, P25, DOI 10.1007/s10514-010-9204-1.
Kummerle R., 2011, P IEEE INT C ROB AUT.
LEONARD JJ, 1991, IEEE T ROBOTIC AUTOM, V7, P376, DOI 10.1109/70.88147.
Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733.
Lu F, 1997, J INTELL ROBOT SYST, V18, P249, DOI 10.1023/A:1007957421070.
Luo R. C., 2010, P IEEE C MULT FUS IN.
Mazuran M, 2018, SPR PROC ADV ROBOT, V3, P435, DOI 10.1007/978-3-319-60916-4\_25.
Mazuran M, 2016, INT J ROBOT RES, V35, P50, DOI 10.1177/0278364915581629.
Mendez O., ARXIV170901500.
Meyer-Delius D., 2010, P IEEE RSJ INT C INT.
Meyer-Delius D., 2012, P AAAI C ART INT.
Olson E., 2008, THESIS.
Rosen D., 2016, P IEEE INT C ROB AUT.
Rowekamper J, 2012, P IEEE RSJ INT C INT.
Scharr H, 2000, THESIS.
Schiotka A., 2017, P IEEE RSJ INT C INT.
Segal A., 2009, P ROB SCI SYST.
Siddiqi S., 2003, P INT C ADV ROB ICAR.
Smith R., 1990, AUTONOMOUS ROBOT VEH, P167, DOI DOI 10.1007/978-1-4613-8997-2\_14.
Sprunk C., 2013, P IEEE RSJ INT C INT.
Sunderhauf N., 2012, P IEEE RSJ INT C INT.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Tipaldi GD, 2013, INT J ROBOT RES, V32, P1662, DOI 10.1177/0278364913502830.
VANDEVEN J, 2010, P IEEE INT C ROB AUT.
Vysotska O., 2016, P IEEE RSJ INT C INT.
Walcott A. N., 2011, THESIS.
Walcott-Bryant A, 2012, P IEEE RSJ INT C INT.
Wang DZ, 2015, INT J ROBOT RES, V34, P1039, DOI 10.1177/0278364914562237.
Winterhalter W., 2015, P IEEE RSJ INT C INT.
Yilmaz S., 2011, P INT C EL EL ENG EL.},
  da = {2022-05-17},
  doc-delivery-number = {HK1LV},
  eissn = {1872-793X},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Mobile robotics; Localization; Mapping; SLAM; Adaptive systems},
  keywords-plus = {ROBOT; TRACKING},
  language = {English},
  number-of-cited-references = {52},
  orcid-numbers = {Burgard, Wolfram/0000-0002-5680-6500},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  times-cited = {16},
  type = {Article},
  unique-id = {WOS:000457667300008},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {23},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{cao-et-al:2018:2815956,
  author = {F. Cao and Y. Zhuang and H. Zhang and W. Wang},
  journal = {IEEE SENSORS JOURNAL},
  title = {Robust Place Recognition and Loop Closing in Laser-Based SLAM for UGVs
in Urban Environments},
  volume = {18},
  number = {10},
  pages = {4242--4252},
  doi = {10.1109/JSEN.2018.2815956},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2018},
  month = {5},
  abstract = {Robust place recognition plays a key role for the long-term autonomy of
unmanned ground vehicles (UGVs) working in indoor or outdoor
environments. Although most of the state-of-the-art that approaches for
place recognition are vision-based, visual sensors lack adaptability in
environments with poor or dynamically changing illumination. In this
paper, a 3-D-laser-based place recognition algorithm is proposed to
accomplish loop closure detection for simultaneous localization and
mapping. An image model named bearing angle (BA) is adopted to convert
3-D laser points to 2-D images, and then ORB features extracted from BA
images are utilized to perform scene matching. Since the computational
cost for matching a query BA image with all the BA images in a database
is too high to meet the requirement of performing real-time place
recognition, a visual bag of words approach is used to improve search
efficiency. Furthermore, a speed normalization algorithm and a 3-D
geometry-based verification algorithm are proposed to complete the
proposed place recognition algorithm. Experiments were conducted on two
self-developed UGV platforms to verify the performance of the proposed
method.},
  affiliation = {Zhuang, Y (Corresponding Author), Dalian Univ Technol, Sch Control Sci \& Engn, Dalian 116024, Peoples R China.
Cao, Fengkui; Zhuang, Yan, Dalian Univ Technol, Sch Control Sci \& Engn, Dalian 116024, Peoples R China.
Zhang, Hong, Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2R3, Canada.
Wang, Wei, Dalian Univ Technol, Res Ctr Informat \& Control, Dalian 116024, Peoples R China.},
  affiliations = {Dalian University of Technology; University of Alberta; Dalian
University of Technology},
  author-email = {cfkybfq@mail.dlut.edu.cn
zhuang@dlut.edu.cn
hzhang@ualberta.ca
wangwei@dlut.edu.cn},
  cited-references = {Aliakbarpour H, 2017, IEEE SENS J, V17, P2640, DOI 10.1109/JSEN.2017.2679187.
Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514.
Arandjelovic R, 2016, PROC CVPR IEEE, P5297, DOI 10.1109/CVPR.2016.572.
Cadena C, 2012, IEEE T ROBOT, V28, P871, DOI 10.1109/TRO.2012.2189497.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Cummins M, 2007, IEEE INT CONF ROBOT, P2042, DOI 10.1109/ROBOT.2007.363622.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
De Silva O, 2015, IEEE SENS J, V15, P1716, DOI 10.1109/JSEN.2014.2364684.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Granstrom K, 2009, IEEE INT CONF ROBOT, P1990.
He L, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P231, DOI 10.1109/IROS.2016.7759060.
Ho KL, 2006, ROBOT AUTON SYST, V54, P740, DOI 10.1016/j.robot.2006.04.016.
Hou Y, 2018, AUTON ROBOT, V42, P1169, DOI 10.1007/s10514-017-9684-3.
Korrapati H, 2017, AUTON ROBOT, V41, P967, DOI 10.1007/s10514-016-9560-6.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Magnusson M, 2009, IEEE INT CONF ROBOT, P3364.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Muhammad N., 2011, 2011 Proceedings of IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR 2011), P333, DOI 10.1109/SSRR.2011.6106765.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Newman P, 2005, IEEE INT CONF ROBOT, P635.
Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Rusu RB, 2010, IEEE INT C INT ROBOT, P2155, DOI 10.1109/IROS.2010.5651280.
Scaramuzza D, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P4170, DOI 10.1109/iros.2007.4399276.
Siam Sayem Mohammad, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5702, DOI 10.1109/ICRA.2017.7989671.
Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663.
Steder B, 2010, IEEE INT CONF ROBOT, P1400, DOI 10.1109/ROBOT.2010.5509401.
Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986.
Zhao L, 2015, IEEE SENS J, V15, P1124, DOI 10.1109/JSEN.2014.2360916.
Zhuang Y, 2013, IEEE T INSTRUM MEAS, V62, P438, DOI 10.1109/TIM.2012.2216475.},
  da = {2022-05-17},
  doc-delivery-number = {GE1TM},
  eissn = {1558-1748},
  funding-acknowledgement = {National Natural Science Foundation of China {[}61375088, U1608253]},
  funding-text = {This work was supported by the National Natural Science Foundation of
China under Grant 61375088 and Grant U1608253. The associate editor
coordinating the review of this paper and approving it for publication
was Dr. Rosario Morello. (Corresponding author: Yan Zhuang.)},
  issn = {1530-437X},
  journal-iso = {IEEE Sens. J.},
  keywords = {Laser scanning; place recognition; simultaneous localization and mapping
(SLAM); unmanned ground vehicles (UGVs)},
  keywords-plus = {CLOSURE DETECTION; NAVIGATION; SCENE; BAGS},
  language = {English},
  number-of-cited-references = {31},
  orcid-numbers = {Cao, Fengkui/0000-0002-5869-7352},
  research-areas = {Engineering; Instruments \& Instrumentation; Physics},
  researcherid-numbers = {Cao, Fengkui/X-3624-2019},
  times-cited = {18},
  type = {Article},
  unique-id = {WOS:000430999700038},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {26},
  web-of-science-categories = {Engineering, Electrical \& Electronic; Instruments \& Instrumentation;
Physics, Applied},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{dayoub-et-al:2011:013,
  author = {F. Dayoub and G. Cielniak and T. Duckett},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {Long-term experiments with an adaptive spherical view representation for
navigation in changing environments},
  volume = {59},
  number = {5,SI},
  pages = {285--295},
  doi = {10.1016/j.robot.2011.02.013},
  note = {4th European Conference on Mobile Robots, Dubrovnik, CROATIA, SEP 23-25,
2009},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2011},
  month = {5},
  abstract = {Real-world environments such as houses and offices change over time,
meaning that a mobile robot's map will become out of date. In this work,
we introduce a method to update the reference views in a hybrid
metric-topological map so that a mobile robot can continue to localize
itself in a changing environment. The updating mechanism, based on the
multi-store model of human memory, incorporates a spherical metric
representation of the observed visual features for each node in the map,
which enables the robot to estimate its heading and navigate using
multi-view geometry, as well as representing the local 3D geometry of
the environment. A series of experiments demonstrate the persistence
performance of the proposed system in real changing environments,
including analysis of the long-term stability. (C) 2011 Elsevier B.V.
All rights reserved.},
  affiliation = {Dayoub, F (Corresponding Author), Lincoln Univ, Sch Comp Sci, Lincoln LN6 7TS, England.
Dayoub, Feras; Cielniak, Grzegorz; Duckett, Tom, Lincoln Univ, Sch Comp Sci, Lincoln LN6 7TS, England.},
  affiliations = {University of Lincoln},
  author-email = {fdayoub@lincoln.ac.uk
gcielniak@lincoln.ac.uk
tduckett@lincoln.ac.uk},
  cited-references = {AKIHIKO T, 2005, IEIC TECHNICAL REPOR, V105, P29.
Andrade-Cetto J, 2002, INT J PATTERN RECOGN, V16, P361, DOI 10.1142/S0218001402001745.
Atkinson RC., 1968, PSYCHOL LEARN MOTIV, P89, DOI {[}10.1016/S0079-7421(08)60422-3, DOI 10.1016/S0079-7421(08)60422-3].
BANDARI E, BMVA S SPAT IM PROC.
Bay H., 2006, P EUR C COMP VIS ECC.
BIBBY C, 2007, P ROB SCI SYST RSS A.
BOOIJ O, P IEEE INT C ROB AUT.
Dayoub F, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3364, DOI 10.1109/IROS.2008.4650701.
Elinas P, 2006, IEEE INT CONF ROBOT, P1564, DOI 10.1109/ROBOT.2006.1641930.
Ess A., 2008, P IEEE C COMP VIS PA.
FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
FRAUNDORFER C, 2007, P IEEE INT C INT ROB.
Goedeme T, 2007, INT J COMPUT VISION, V74, P219, DOI 10.1007/s11263-006-0025-9.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
GROSS H, 2003, P IEEE INT C INT ROB.
Guerrero JJ, 2008, IEEE T ROBOT, V24, P494, DOI 10.1109/TRO.2008.918043.
Hartley R., 2004, MULTIPLE VIEW GEOMET, V2nd.
HORN BKP, 1990, INT J COMPUT VISION, V4, P59, DOI 10.1007/BF00137443.
HOWARD A, LASER STABILIZED ODO.
ILA V, IEEE T ROBOTICS.
Kang SB, 1997, INT J COMPUT VISION, V25, P167, DOI 10.1023/A:1007971901577.
Kosecka J, 2005, ROBOT AUTON SYST, V52, P27, DOI 10.1016/j.robot.2005.03.008.
Kuipers B., 1993, LEARNING ROBOTS, V8, P47.
LONGUETHIGGINS HC, 1981, NATURE, V293, P133, DOI 10.1038/293133a0.
Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410.
Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006.
Menegatti E, 2004, ROBOT AUTON SYST, V48, P17, DOI 10.1016/j.robot.2004.05.003.
Nister D., 2006, P IEEE C COMP VIS PA.
SCARAMUZZA D, P IEEE INT C INT SYS.
Se S, 2002, INT J ROBOT RES, V21, P735, DOI 10.1177/027836402761412467.
Sivic J., 2003, P IEEE INT C COMP VI.
Uhlmann J., 1997, INT S AER DEF SENS S, V3, P26.
UHLMANN JK, THESIS U OXFORD OXFO.
Ulrich I., 2000, P IEEE INT C ROB AUT.
VALGREN C, 2006, P IEEE INT C INT ROB.
VLASSIS N, 2002, P IEEE INT C ROB AUT.
Wang CC, 2003, IEEE INT CONF ROBOT, P842.
Yamauchi B, 1996, IEEE T SYST MAN CY B, V26, P496, DOI 10.1109/3477.499799.
Zivkovic Z, 2007, ROBOT AUTON SYST, V55, P411, DOI 10.1016/j.robot.2006.12.005.},
  da = {2022-05-17},
  doc-delivery-number = {772NU},
  eissn = {1872-793X},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Persistent mapping; Omnidirectional vision; Mobile robot navigation},
  keywords-plus = {LOCALIZATION; SCENE},
  language = {English},
  number-of-cited-references = {39},
  oa = {Green Accepted},
  orcid-numbers = {Cielniak, Grzegorz/0000-0002-6299-8465
Dayoub, Feras/0000-0002-4234-7374},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  researcherid-numbers = {Cielniak, Grzegorz/AAS-5387-2020},
  times-cited = {35},
  type = {Article; Proceedings Paper},
  unique-id = {WOS:000291241400004},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {12},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{han-et-al:2017:2662061,
  author = {F. Han and X. Yang and Y. Deng and M. Rentschler and Y. Dejun and H. Zhang},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {SRAL: Shared Representative Appearance Learning for Long-Term Visual
Place Recognition},
  volume = {2},
  number = {2},
  pages = {1172--1179},
  doi = {10.1109/LRA.2017.2662061},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2017},
  month = {4},
  abstract = {Place recognition, or loop closure detection, is an essential component
to address the problem of visual simultaneous localization and mapping
(SLAM). Long-term navigation of robots in outdoor environments
introduces new challenges to enable life-long SLAM, including the strong
appearance change resulting fromvegetation, weather, and illumination
variations across various times of the day, different days, months, or
even seasons. In this paper, we propose a new shared representative
appearance learning (SRAL) approach to address long-term visual place
recognition. Different from previous methods using a single feature
modality or a concatenation of multiple features, our SRAL method
autonomously learns representative features that are shared in all scene
scenarios, and then fuses the features together to represent the
long-term appearance of environments observed by a robot during
life-long navigation. By formulating SRAL as a regularized optimization
problem, we use structured sparsity-inducing norms to model
interrelationships of feature modalities. In addition, an optimization
algorithm is developed to efficiently solve the formulated optimization
problem, which holds a theoretical convergence guarantee. Extensive
empirical study was performed to evaluate the SRAL method using
large-scale benchmark datasets, including St Lucia, CMU-VL, and Nordland
datasets. Experimental results have shown that our SRAL method obtains
superior performance for life-long place recognition using individual
images, outperforms previous single image-based methods, and is capable
of estimating the importance of feature modalities.},
  affiliation = {Han, F (Corresponding Author), Colorado Sch Mines, Div Comp Sci, Golden, CO 80401 USA.
Han, Fei; Yang, Xue; Yang, Dejun; Zhang, Hao, Colorado Sch Mines, Div Comp Sci, Golden, CO 80401 USA.
Deng, Yiming, Michigan State Univ, Dept Elect \& Comp Engn, E Lansing, MI 48824 USA.
Rentschler, Mark, Univ Colorado, Dept Mech Engn, Boulder, CO 80309 USA.},
  affiliations = {Colorado School of Mines; Michigan State University; University of
Colorado System; University of Colorado Boulder},
  author-email = {fhan@mines.edu
xueyang@mines.edu
dengyimi@msu.edu
mark.rentschler@colorado.edu
djyang@mines.edu
hzhang@mines.edu},
  cited-references = {Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514.
Arroyo R, 2015, IEEE INT CONF ROBOT, P6328, DOI 10.1109/ICRA.2015.7140088.
Badino H, 2012, IEEE INT CONF ROBOT, P1635, DOI 10.1109/ICRA.2012.6224716.
Cadena C, 2012, IEEE T ROBOT, V28, P871, DOI 10.1109/TRO.2012.2189497.
Chen C, 2006, INT J ROBOT RES, V25, P953, DOI 10.1177/0278364906068375.
Chen Z., 2014, P AUSTR C ROB AUT.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
Feldman D., 2016, ADV NEURAL INFORM PR, V29, P2766.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Glover AJ, 2010, IEEE INT CONF ROBOT, P3507, DOI 10.1109/ROBOT.2010.5509547.
Gutmann J.-S., 1999, Proceedings 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation. CIRA'99 (Cat. No.99EX375), P318, DOI 10.1109/CIRA.1999.810068.
Hansen P, 2014, IEEE INT C INT ROBOT, P4549, DOI 10.1109/IROS.2014.6943207.
Henry P, 2012, INT J ROBOT RES, V31, P647, DOI 10.1177/0278364911434148.
Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
Kaess M, 2012, INT J ROBOT RES, V31, P216, DOI 10.1177/0278364911430419.
Klopschitz M., 2008, P 3D DAT PROC VIS TR.
Labbe M, 2014, IEEE INT C INT ROBOT, P2661, DOI 10.1109/IROS.2014.6942926.
Latif Y., 2014, P ROB SCI SYST.
Lee D., 2013, P ROB INT TECHN APPL, P485.
Linegar C, 2016, IEEE INT CONF ROBOT, P787, DOI 10.1109/ICRA.2016.7487208.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Mur-Artal R, 2014, IEEE INT CONF ROBOT, P846, DOI 10.1109/ICRA.2014.6906953.
Naseer T, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564.
Naseer T, 2015, IEEE INT C INT ROBOT, P2529, DOI 10.1109/IROS.2015.7353721.
Neubert P., 2013, INT C ROB AUT ICRA W, DOI 10.1016/j.cell.2007.12.011.
Newman P, 2009, P ROB SCI SYST.
Pepperell E, 2014, IEEE INT CONF ROBOT, P1612, DOI 10.1109/ICRA.2014.6907067.
Qiao YL, 2015, LECT NOTES ARTIF INT, V9414, P393, DOI 10.1007/978-3-319-27101-9\_30.
Song DJ, 2010, IEEE T IMAGE PROCESS, V19, P174, DOI 10.1109/TIP.2009.2032939.
Stumm E, 2016, PROC CVPR IEEE, P4535, DOI 10.1109/CVPR.2016.491.
SUNDERHAUF N, 2015, P IEEE RSJ INT C INT, P4297.
SUNDERHAUF N, 2011, P IEEE RSJ INT C INT, P1234.
Sunderhauf N., 2015, P ROB SCI SYST.
Zhang H., 2016, P ROB SCI SYST.},
  da = {2022-05-17},
  doc-delivery-number = {FK8DB},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {Loop closure detection; long-term place recognition; simultaneous
localization and mapping (SLAM); visual learning},
  keywords-plus = {LOCALIZATION; NAVIGATION; SEQUENCES; WORDS; BAGS},
  language = {English},
  number-of-cited-references = {38},
  orcid-numbers = {Yang, Dejun/0000-0002-1811-4423
Zhang, Hao/0000-0001-8043-9184},
  research-areas = {Robotics},
  researcherid-numbers = {Yang, Dejun/AAO-4817-2020
Deng, Yiming/AAE-6096-2020},
  times-cited = {32},
  type = {Article},
  unique-id = {WOS:000413736600104},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {5},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{pomerleau-et-al:2014:6907397,
  author = {F. Pomerleau and P. Krusi and F. Colas and F. Paul and R. Siegwart},
  booktitle = {2014 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {Long-term 3D map maintenance in dynamic environments},
  pages = {3712--3719},
  doi = {10.1109/ICRA.2014.6907397},
  note = {IEEE International Conference on Robotics and Automation (ICRA), Hong
Kong, PEOPLES R CHINA, MAY 31-JUN 07, 2014},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2014},
  abstract = {New applications of mobile robotics in dynamic urban areas require more
than the single-session geometric maps that have dominated simultaneous
localization and mapping (SLAM) research to date; maps must be updated
as the environment changes and include a semantic layer (such as road
network information) to aid motion planning in dynamic environments. We
present an algorithm for long-term localization and mapping in real time
using a three-dimensional (3D) laser scanner. The system infers the
static or dynamic state of each 3D point in the environment based on
repeated observations. The velocity of each dynamic point is estimated
without requiring object models or explicit clustering of the points. At
any time, the system is able to produce a most-likely representation of
underlying static scene geometry. By storing the time history of
velocities, we can infer the dominant motion patterns within the map.
The result is an online mapping and localization system specifically
designed to enable long-term autonomy within highly dynamic
environments. We validate the approach using data collected around the
campus of ETH Zurich over seven months and several kilometers of
navigation. To the best of our knowledge, this is the first work to
unify long-term map update with tracking of dynamic objects.},
  affiliation = {Pomerleau, F (Corresponding Author), ETH, Autonomous Syst Lab, CH-8092 Zurich, Switzerland.
Pomerleau, Francois; Krusi, Philipp; Colas, Francis; Furgale, Paul; Siegwart, Roland, ETH, Autonomous Syst Lab, CH-8092 Zurich, Switzerland.},
  affiliations = {ETH Zurich},
  author-email = {f.pomerleau@gmail.com},
  book-group-author = {IEEE},
  cited-references = {Aijazi AK, 2013, REMOTE SENS-BASEL, V5, P3701, DOI 10.3390/rs5083701.
Alonso-Mora J, 2012, INT J ROBOT RES, V31, P753, DOI 10.1177/0278364912442095.
Anderson-Sprecher Peter, 2011, IEEE International Conference on Robotics and Automation, P3104.
Biber P, 2009, INT J ROBOT RES, V28, P20, DOI 10.1177/0278364908096286.
Burgard W., 2007, AUTONOMOUS NAVIGATIO, P3, DOI 10.1007/978-3-540-73422- 2\_1.
Elseberg J., 2012, J SOFT ENG ROBOT, V3, P2.
FELDMAR J, 1994, CVPR 1994 SEATTL WA, P496.
Furgale P, 2013, IEEE INT VEH SYM, P809, DOI 10.1109/IVS.2013.6629566.
Kaestner R, 2012, IEEE INT CONF ROBOT, P3075, DOI 10.1109/ICRA.2012.6224585.
Kim K., 2011, P IEEE INT C COMP VI.
Kummerle R., 2013, P IEEE INT C ROB AUT.
Moosmann F, 2010, IEEE INT CONF ROBOT, P142, DOI 10.1109/ROBOT.2010.5509381.
Pomerleau F, 2013, AUTON ROBOT, V34, P133, DOI 10.1007/s10514-013-9327-2.
Rufli M, 2013, IEEE T ROBOT, V29, P899, DOI 10.1109/TRO.2013.2258733.
Ryde J, 2011, IEEE INT CONF ROBOT, P1484.
Shackleton J, 2010, P 7 IEEE INT C ADV V, P420, DOI DOI 10.1109/AVSS.2010.52.
Wang CC, 2007, INT J ROBOT RES, V26, P889, DOI 10.1177/0278364907081229.
Wurm K. M., 2010, WORKSH BEST PRACT 3D.},
  da = {2022-05-17},
  doc-delivery-number = {BE9BP},
  eissn = {2577-087X},
  isbn = {978-1-4799-3685-4},
  issn = {1050-4729},
  keywords = {Long-term mapping; dynamic obstacles; ICP; kd-tree; registration; scan
matching; robot; SLAM},
  language = {English},
  number-of-cited-references = {18},
  oa = {Green Submitted},
  orcid-numbers = {Pomerleau, François/0000-0003-1288-2744
Siegwart, Roland/0000-0002-2760-7983},
  research-areas = {Automation \& Control Systems; Robotics},
  researcherid-numbers = {Pomerleau, François/N-7629-2017
Siegwart, Roland/A-4495-2008},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {57},
  type = {Proceedings Paper},
  unique-id = {WOS:000377221103109},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {17},
  web-of-science-categories = {Automation \& Control Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{tipaldi-et-al:2013:0278364913502830,
  author = {G. D. Tipaldi and D. Meyer-Delius and W. Burgard},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {Lifelong localization in changing environments},
  volume = {32},
  number = {14,SI},
  pages = {1662--1678},
  doi = {10.1177/0278364913502830},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2013},
  month = {12},
  abstract = {Robot localization systems typically assume that the environment is
static, ignoring the dynamics inherent in most real-world settings.
Corresponding scenarios include households, offices, warehouses and
parking lots, where the location of certain objects such as goods,
furniture or cars can change over time. These changes typically lead to
inconsistent observations with respect to previously learned maps and
thus decrease the localization accuracy or even prevent the robot from
globally localizing itself. In this paper we present a sound
probabilistic approach to lifelong localization in changing environments
using a combination of a Rao-Blackwellized particle filter with a hidden
Markov model. By exploiting several properties of this model, we obtain
a highly efficient map management approach for dynamic environments,
which makes it feasible to run our algorithm online. Extensive
experiments with a real robot in a dynamically changing environment
demonstrate that our algorithm reliably adapts to changes in the
environment and also outperforms the popular Monte-Carlo localization
approach.},
  affiliation = {Tipaldi, GD (Corresponding Author), Univ Freiburg, Dept Comp Sci, Hugstetter Str 55, D-79106 Freiburg, Germany.
Tipaldi, Gian Diego; Burgard, Wolfram, Univ Freiburg, Dept Comp Sci, D-79106 Freiburg, Germany.
Meyer-Delius, Daniel, KUKA Labs GmbH, Augsburg, Germany.},
  affiliations = {League of European Research Universities - LERU; University of Freiburg},
  author-email = {tipaldi@informatik.uni-freiburg.de},
  cited-references = {Anguelov D., 2002, P C UNC AI UAI.
Avots D., 2002, P IEEE RSJ INT C INT.
Biber P., 2005, P ROB SCI SYST RSS.
Blake Andrew, 1987, VISUAL RECONSTRUCTIO.
Brechtel S., 2010, P IEEE INT C ROB AUT.
Chen C., 2006, P IEEE INT C CONTR A.
Churchill W, 2012, IEEE INT CONF ROBOT, P4525, DOI 10.1109/ICRA.2012.6224596.
Doucet A., 2000, P 16 C UNC ART INT S, P176.
Eliazar A. I., 2004, P IEEE INT C ROB AUT.
Fox D, 1999, J ARTIF INTELL RES, V11, P391, DOI 10.1613/jair.616.
Fox D, 2003, INT J ROBOT RES, V22, P985, DOI 10.1177/0278364903022012001.
Gallagher G, 2009, P IEEE INT C ROB AUT.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925.
HAHNEL D, 2003, P IEEE INT C ROB AUT.
Konolige K, 2009, P IEEE RSJ INT C INT.
Konolige K, 2010, IEEE INT C INT ROBOT.
Kretzschmar H, 2012, INT J ROBOT RES, V31, P1219, DOI 10.1177/0278364912455072.
Levin D. A., 2008, MARKOV CHAINS MIXING.
Meyer-Delius D., 2012, P AAAI C ART INT AAA.
Meyer-Delius D., 2010, P IEEE RSJ INT C INT.
Montemerlo M., 2002, P IEEE INT C ROB AUT.
MONTESANO L, 2005, P IEEE INT C ROB AUT.
Moravec H.P., 1985, P IEEE INT C ROB AUT.
MURPHY K, 1999, P C NEUR INF PROC SY, P1015.
Olson E, 2005, P ROB SCI SYST.
Olson E., 2008, THESIS MIT CAMBRIDGE.
Petrovskaya A., 2007, P INT C ART INT IJCA.
RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626.
Rowekamper J, 2012, P IEEE RSJ INT C INT.
Saarinen J., 2012, P IEEE RSJ INT C INT.
Schulz D, 2003, P INT C ART INT IJCA.
STACHNISS C, 2005, P NAT C ART INT AAAI.
Thrun S, 2001, INT J ROBOT RES, V20, P335, DOI 10.1177/02783640122067435.
Walcott A, 2011, THESIS MIT CAMBRIDGE.
Walcott-Bryant A, 2012, P IEEE RSJ INT C INT.
Wang CC, 2007, INT J ROBOT RES, V26, P889, DOI 10.1177/0278364907081229.
Wolf DF, 2005, AUTON ROBOT, V19, P53, DOI 10.1007/s10514-005-0606-4.
Yang SW, 2011, IEEE INT C ROB AUT I.},
  da = {2022-05-17},
  doc-delivery-number = {287AL},
  eissn = {1741-3176},
  funding-acknowledgement = {European Commission {[}FP7-248258-FirstMM, FP7-260026-TAPAS,
ERC-267686-LifeNav]},
  funding-text = {This work was supported by the European Commission (grant numbers
FP7-248258-FirstMM, FP7-260026-TAPAS and ERC-267686-LifeNav).},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {Mobile and distributed robotics SLAM; localization; mapping; cognitive
robotics; learning and adaptive systems},
  keywords-plus = {TUTORIAL},
  language = {English},
  number-of-cited-references = {39},
  orcid-numbers = {Burgard, Wolfram/0000-0002-5680-6500},
  research-areas = {Robotics},
  researcherid-numbers = {Burgard, Wolfram/N-2381-2019},
  times-cited = {50},
  type = {Article},
  unique-id = {WOS:000329510300005},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {18},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{kim-et-al:2019:2897340,
  author = {G. Kim and B. Park and A. Kim},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {1-Day Learning, 1-Year Localization: Long-Term LiDAR Localization Using
Scan Context Image},
  volume = {4},
  number = {2},
  pages = {1948--1955},
  doi = {10.1109/LRA.2019.2897340},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2019},
  month = {4},
  abstract = {In this letter, we present a long-term localization method that
effectively exploits the structural information of an environment via an
image format. The proposed method presents a robust year-round
localization performance even when learned in just a single day. The
proposed localizer learns a point cloud descriptor, named Scan Context
Image (SCI), and performs robot localization on a grid map by
formulating the place recognition problem as place classification using
a convolutional neural network. Our method is faster than existing
methods proposed for place recognition because it avoids a pairwise
comparison between a query and scans in a database. In addition, we
provide thorough validations using publicly available long-term
datasets, the NCLT dataset and the Oxford RobotCar dataset, and show
that the Scan Context Image (SCI) localization attains consistent
performance over a year and outperforms existing methods.},
  affiliation = {Kim, A (Corresponding Author), Korea Adv Inst Sci \& Technol, Dept Civil \& Environm Engn, Daejeon 34141, South Korea.
Kim, Giseop; Kim, Ayoung, Korea Adv Inst Sci \& Technol, Dept Civil \& Environm Engn, Daejeon 34141, South Korea.
Park, Byungjae, ETRI, Intelligent Robot Syst Res Grp, Daejeon 34129, South Korea.},
  affiliations = {Korea Advanced Institute of Science \& Technology (KAIST); Electronics
\& Telecommunications Research Institute - Korea (ETRI)},
  author-email = {paulgkim@kaist.ac.kr
bjp@etri.re.kr
ayoungk@kaist.ac.kr},
  cited-references = {Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
BENEDIKT ML, 1979, ENVIRON PLANN B, V6, P47, DOI 10.1068/b060047.
Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Cop KP, 2018, IEEE INT CONF ROBOT, P3653, DOI 10.1109/ICRA.2018.8460940.
Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693.
Dube R, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
He L, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P231, DOI 10.1109/IROS.2016.7759060.
Kendall A., 2017, ARXIV PREPRINT ARXIV, P571, DOI {[}10.5244/c.31.57, DOI 10.5244/C.31.57].
Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336.
Kim G, 2018, IEEE INT C INT ROBOT, P4802, DOI 10.1109/IROS.2018.8593953.
Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
Maddern W, 2015, IEEE INT CONF ROBOT, P1684, DOI 10.1109/ICRA.2015.7139414.
Miller D, 2018, IEEE INT CONF ROBOT, P3243.
Porav H, 2018, IEEE INT CONF ROBOT, P1011, DOI 10.1109/ICRA.2018.8462894.
Schonberger JL, 2018, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR.2018.00721.
Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470.
Weyand T, 2016, LECT NOTES COMPUT SC, V9912, P37, DOI 10.1007/978-3-319-46484-8\_3.
Withers Dan, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P6233, DOI 10.1109/ICRA.2017.7989738.
Wohlkinger W., 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2987, DOI 10.1109/ROBIO.2011.6181760.
Ye Y., 2017, P BRIT MACH VIS C.},
  da = {2022-05-17},
  doc-delivery-number = {HO1OR},
  funding-acknowledgement = {Korea Agency for Infrastructure Technology Advancement (KAIA) through
the Ministry of Land, Infrastructure and Transport of Korea
{[}19CTAP-C142170-02]; {[}High-Definition Map Based Precise Vehicle
Localization Using Cameras and LIDARs] project - Naver Labs Corporation},
  funding-text = {This work was supported in part by the Korea Agency for Infrastructure
Technology Advancement (KAIA) through the Ministry of Land,
Infrastructure and Transport of Korea under Grant 19CTAP-C142170-02, and
in part by the {[}High-Definition Map Based Precise Vehicle Localization
Using Cameras and LIDARs] project funded by Naver Labs Corporation.},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {Localization; range sensing; SLAM},
  keywords-plus = {PLACE RECOGNITION},
  language = {English},
  number-of-cited-references = {23},
  orcid-numbers = {Kim, Giseop/0000-0001-6311-0686},
  research-areas = {Robotics},
  times-cited = {38},
  type = {Article},
  unique-id = {WOS:000460678700014},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {9},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{kurz-et-al:2021:9636530,
  author = {G. Kurz and M. Holoch and P. Biber},
  booktitle = {2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS)},
  title = {Geometry-based Graph Pruning for Lifelong SLAM},
  pages = {3313--3320},
  doi = {10.1109/IROS51168.2021.9636530},
  note = {IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), ELECTR NETWORK, SEP 27-OCT 01, 2021},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2021},
  abstract = {Lifelong SLAM considers long-term operation of a robot where already
mapped locations are revisited many times in changing environments. As a
result, traditional graph-based SLAM approaches eventually become
extremely slow due to the continuous growth of the graph and the loss of
sparsity. Both problems can be addressed by a graph pruning algorithm.
It carefully removes vertices and edges to keep the graph size
reasonable while preserving the information needed to provide good SLAM
results. We propose a novel method that considers geometric criteria for
choosing the vertices to be pruned. It is efficient, easy to implement,
and leads to a graph with evenly spread vertices that remain part of the
robot trajectory. Furthermore, we present a novel approach of
marginalization that is more robust to wrong loop closures than existing
methods. The proposed algorithm is evaluated on two publicly available
real-world long-term datasets and compared to the unpruned case as well
as ground truth. We show that even on a long dataset (25h), our approach
manages to keep the graph sparse and the speed high while still
providing good accuracy (40 times speed up, 6cm map error compared to
unpruned case).},
  affiliation = {Kurz, G (Corresponding Author), Robert Bosch GmbH, Corp Res, Renningen, Germany.
Kurz, Gerhard; Holoch, Matthias; Biber, Peter, Robert Bosch GmbH, Corp Res, Renningen, Germany.},
  affiliations = {Bosch},
  author-email = {gerhard2.kurz@de.bosch.com
matthias.holoch@de.bosch.com
peter.biber@de.bosch.com},
  book-group-author = {IEEE},
  cited-references = {Biber P., 2003, 2003 IEEE RSJ INT C.
Burgard W, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2089, DOI 10.1109/IROS.2009.5354691.
Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754.
Carlevaris-Bianco N., 2013, 2013 IEEE INT C ROB.
Carlevaris-Bianco N, 2014, IEEE T ROBOT, V30, P1371, DOI 10.1109/TRO.2014.2347571.
CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142.
Dellaert F., 2012, RIMCPR2012002.
Ta N, 2018, IEEE INT CONF ROBOT, P2494, DOI 10.1109/ICRA.2018.8460979.
Eade E, 2010, IEEE INT C INT ROBOT, P3017, DOI 10.1109/IROS.2010.5649205.
Grisetti G., 2012, 2012 IEEE RSJ INT C.
Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925.
HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136.
Krajnik T., 2016, 2016 IEEE RSJ INT C.
Kretzschmar H, 2010, KUNSTL INTELL, V24, P199, DOI 10.1007/s13218-010-0034-2.
Kretzschmar H, 2011, IEEE INT C INT ROBOT, P865, DOI 10.1109/IROS.2011.6048060.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Lazaro MT, 2018, IEEE INT C INT ROBOT, P153, DOI 10.1109/IROS.2018.8594310.
Lee G. H., 2013, 2013 IEEE RSJ INT C.
Mazuran M, 2016, INT J ROBOT RES, V35, P50, DOI 10.1177/0278364915581629.
Underwood JP, 2013, IEEE INT CONF ROBOT, P4735, DOI 10.1109/ICRA.2013.6631251.
Wang Y, 2015, ADV ROBOTICS, V29, P683, DOI 10.1080/01691864.2014.998707.},
  da = {2022-05-17},
  doc-delivery-number = {BS6ZK},
  isbn = {978-1-6654-1714-3},
  issn = {2153-0858},
  language = {English},
  number-of-cited-references = {21},
  oa = {Green Submitted},
  research-areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
  series = {IEEE International Conference on Intelligent Robots and Systems},
  times-cited = {0},
  type = {Proceedings Paper},
  unique-id = {WOS:000755125502092},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Engineering, Electrical \& Electronic; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{hu-et-al:2022:1003907,
  author = {H. Hu and H. Wang and Z. Liu and W. Chen},
  journal = {IEEE-CAA JOURNAL OF AUTOMATICA SINICA},
  title = {Domain-Invariant Similarity Activation Map Contrastive Learning for
Retrieval-Based Long-Term Visual Localization},
  volume = {9},
  number = {2},
  pages = {313--328},
  doi = {10.1109/JAS.2021.1003907},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2022},
  month = {2},
  abstract = {Visual localization is a crucial component in the application of mobile
robot and autonomous driving. Image retrieval is an efficient and
effective technique in image-based localization methods. Due to the
drastic variability of environmental conditions, e.g., illumination
changes, retrieval-based visual localization is severely affected and
becomes a challenging problem. In this work, a general architecture is
first formulated probabilistically to extract domain-invariant features
through multi-domain image translation. Then, a novel gradient-weighted
similarity activation mapping loss (Grad-SAM) is incorporated for finer
localization with high accuracy. We also propose a new adaptive triplet
loss to boost the contrastive learning of the embedding in a
self-supervised manner. The final coarse-to-fine image retrieval
pipeline is implemented as the sequential combination of models with and
without Grad-SAM loss. Extensive experiments have been conducted to
validate the effectiveness of the proposed approach on the CMU-Seasons
dataset. The strong generalization ability of our approach is verified
with the RobotCar dataset using models pre-trained on urban parts of the
CMU-Seasons dataset. Our performance is on par with or even outperforms
the state-of-the-art image-based localization baselines in medium or
high precision, especially under challenging environments with
illumination variance, vegetation, and night-time images. Moreover,
real-site experiments have been conducted to validate the efficiency and
effectiveness of the coarse-to-fine strategy for localization.},
  affiliation = {Wang, HS (Corresponding Author), Shanghai Jiao Tong Univ, Inst Med Robot, Dept Automat,Key Lab Marine Intelligent Equipment, Key Lab Syst Control \& Informat Proc,Minist Educ, Shanghai 200240, Peoples R China.
Hu, Hanjiang; Chen, Weidong, Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
Wang, Hesheng, Shanghai Jiao Tong Univ, Inst Med Robot, Dept Automat,Key Lab Marine Intelligent Equipment, Key Lab Syst Control \& Informat Proc,Minist Educ, Shanghai 200240, Peoples R China.
Liu, Zhe, Univ Cambridge, Dept Comp Sci \& Technol, Cambridge CB3 0FD, England.},
  affiliations = {Shanghai Jiao Tong University; Shanghai Jiao Tong University; League of
European Research Universities - LERU; University of Cambridge},
  author-email = {huhanjiang@sjtu.edu.cn
wanghesheng@sjtu.edu.cn
zl457@cam.ac.uk
wdchen@sjtu.edu.cn},
  cited-references = {ACHILLE A, 2018, P INF THEOR APPL WOR, P1.
Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI 10.1109/ICRA.2019.8794387.
Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122.
Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Badino H, 2011, IEEE INT CONF ROBOT.
Balntas V, 2018, LECT NOTES COMPUT SC, V11218, P782, DOI 10.1007/978-3-030-01264-9\_46.
Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039.
Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097.
Chen X, 2016, ADV NEUR IN, V29.
Chen ZT, 2018, IEEE ROBOT AUTOM LET, V3, P4015, DOI 10.1109/LRA.2018.2859916.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Doan AD, 2019, IEEE I CONF COMP VIS, P9318, DOI 10.1109/ICCV.2019.00941.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Garg S, 2018, IEEE INT CONF ROBOT, P3645, DOI 10.1109/ICRA.2018.8461051.
Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258.
Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672.
Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8.
Gu HY, 2021, IEEE T IND ELECTRON, V68, P9778, DOI 10.1109/TIE.2020.3028813.
HAN LJ, 2020, IEEEASME T MECHATRON, DOI DOI 10.1109/TMECH.2020.
Hausler S, 2019, IEEE INT C INT ROBOT, P3268, DOI 10.1109/IROS40897.2019.8967783.
Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3\_7.
Hu HJ, 2019, IEEE INT C INT ROBOT, P3684, DOI 10.1109/IROS40897.2019.8968047.
Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9\_11.
Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632.
Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039.
Jenicek T, 2019, IEEE I CONF COMP VIS, P9695, DOI 10.1109/ICCV.2019.00979.
Kim HJ, 2017, PROC CVPR IEEE, P3251, DOI 10.1109/CVPR.2017.346.
Kim H, 2018, PR MACH LEARN RES, V80.
Kumar BGV, 2016, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2016.581.
Liu MY, 2017, ADV NEUR IN, V30.
Liu ZY, 2014, IEEE T PATTERN ANAL, V36, P1258, DOI 10.1109/TPAMI.2013.223.
Lopez-Antequera M, 2017, PATTERN RECOGN LETT, V92, P89, DOI 10.1016/j.patrec.2017.04.017.
Lowry S, 2016, IEEE T ROBOT, V32, P600, DOI 10.1109/TRO.2016.2545711.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P4269, DOI 10.1109/TIP.2017.2717505.
Luo X, 2021, IEEE T SYST MAN CY-S, V51, P916, DOI 10.1109/TSMC.2018.2884191.
Ma YF, 2020, IEEE-CAA J AUTOMATIC, V7, P315, DOI 10.1109/JAS.2020.1003021.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
Makhzani A., 2015, ARXIV151105644.
Mathieu M. F., 2016, P 30 INT C NEUR INF, P5040.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Naseer Tayyab, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2614, DOI 10.1109/ICRA.2017.7989305.
Piasco N, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01363-6.
Piasco N, 2019, IEEE INT CONF ROBOT, P9094, DOI 10.1109/ICRA.2019.8794221.
Porav H, 2018, IEEE INT CONF ROBOT, P1011, DOI 10.1109/ICRA.2018.8462894.
Puckette Miller, 2018, ARXIV PREPRINT ARXIV.
Qiao H, 2014, IEEE T CYBERNETICS, V44, P1485, DOI 10.1109/TCYB.2013.2287014.
Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566.
Radford A., 2015, ARXIV PREPRINT ARXIV.
Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300.
Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342.
Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897.
Sattler T, 2017, IEEE T PATTERN ANAL, V39, P1744, DOI 10.1109/TPAMI.2016.2611662.
Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74.
Shi W, 2019, IEEE-CAA J AUTOMATIC, V6, P917, DOI 10.1109/JAS.2019.1911561.
Siam Sayem Mohammad, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5702, DOI 10.1109/ICRA.2017.7989671.
Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434.
Stenborg E, 2018, IEEE INT CONF ROBOT, P6484, DOI 10.1109/ICRA.2018.8463150.
Tang L, 2020, IEEE INT CONF ROBOT, P1301, DOI 10.1109/ICRA40945.2020.9196518.
Torii A, 2018, IEEE T PATTERN ANAL, V40, P257, DOI 10.1109/TPAMI.2017.2667665.
Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8\_48.
Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180.
Wang LP, 2020, IEEE-CAA J AUTOMATIC, V7, P1190, DOI 10.1109/JAS.2020.1003117.
Wang XH, 2019, IEEE-CAA J AUTOMATIC, V6, P540, DOI 10.1109/JAS.2017.7510664.
Weinberger K. Q., 2008, P 25 INT C MACH LEAR, P1160, DOI DOI 10.1145/1390156.1390302.
Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930.
Wu D, 2022, IEEE T SERV COMPUT, V15, P793, DOI 10.1109/TSC.2019.2961895.
Xin Z, 2019, IEEE INT CONF ROBOT, P5979, DOI 10.1109/ICRA.2019.8794383.
Xing, 2002, NIPS, V15, P505.
Xing Y, 2018, IEEE-CAA J AUTOMATIC, V5, P645, DOI 10.1109/JAS.2018.7511063.
Yin P, 2019, IEEE INT CONF ROBOT, P319, DOI 10.1109/ICRA.2019.8793752.
Zetao Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3223, DOI 10.1109/ICRA.2017.7989366.
Zhou B., 2016, PROC CVPR IEEE, P2921, DOI {[}DOI 10.1109/CVPR.2016.319, 10.1109/CVPR.2016.319].
Zhou Q, 2017, IEEE T SYST MAN CY-S, V47, P1, DOI 10.1109/TSMC.2016.2557222.
Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244.},
  da = {2022-05-17},
  doc-delivery-number = {WR7ZG},
  eissn = {2329-9274},
  issn = {2329-9266},
  journal-iso = {IEEE-CAA J. Automatica Sin.},
  keywords = {Deep representation learning; place recognition; visual localization},
  keywords-plus = {PLACE RECOGNITION; ATTENTION MODEL},
  language = {English},
  number-of-cited-references = {76},
  oa = {Green Submitted},
  orcid-numbers = {Hu, Hanjiang/0000-0002-5698-5887},
  research-areas = {Automation \& Control Systems},
  researcherid-numbers = {Hu, Hanjiang/AHD-8358-2022},
  times-cited = {1},
  type = {Article},
  unique-id = {WOS:000714714500012},
  usage-count-last-180-days = {8},
  usage-count-since-2013 = {8},
  web-of-science-categories = {Automation \& Control Systems},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{karaoguz-bozma:2016:4,
  author = {H. Karaoguz and H. I. Bozma},
  journal = {AUTONOMOUS ROBOTS},
  title = {An integrated model of autonomous topological spatial cognition},
  volume = {40},
  number = {8},
  pages = {1379--1402},
  doi = {10.1007/s10514-015-9514-4},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2016},
  month = {12},
  abstract = {This paper is focused on endowing a mobile robot with topological
spatial cognition. We propose an integrated model-where the concept of a
`place' is defined as a collection of appearances or locations sharing
common perceptual signatures or physical boundaries. In this model, as
the robot navigates, places are detected in a systematic manner via
monitoring coherency in the incoming visual data while pruning out
uninformative or scanty data. Detected places are then either recognized
or learned along with mapping as necessary. The novelties of the model
are twofold: First, it explicitly incorporates a long-term spatial
memory where the knowledge of learned places and their spatial relations
are retained in place and map memories respectively. Second, the
processing modules operate together so that the robot is able to build
its spatial memory in an organized, incremental and unsupervised manner.
Thus, the robot's long-term spatial memory evolves completely on its own
while learned knowledge is organized based on appearance-related
similarities in a manner that is amenable for higher-level semantic
reasoning, As such, the proposed model constitutes a step forward
towards having robots that are capable of interacting with their
environments in an autonomous manner.},
  affiliation = {Karaoguz, H (Corresponding Author), Bogazici Univ, Intelligent Syst Lab, Elect \& Elect Engn, Istanbul, Turkey.
Karaoguz, Hakan; Bozma, H. Isil, Bogazici Univ, Intelligent Syst Lab, Elect \& Elect Engn, Istanbul, Turkey.},
  affiliations = {Bogazici University},
  author-email = {hakan.karaoguz@boun.edu.tr},
  cited-references = {Beeson P, 2010, INT J ROBOT RES, V29, P428, DOI 10.1177/0278364909100586.
Casati R., 2002, TOPOLOGY COGNITION.
Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
Chella Antonio, 2007, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, P741, DOI 10.1109/IROS.2007.4399614.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Denis M, 2007, PSYCHOL RES-PSYCH FO, V71, P235, DOI 10.1007/s00426-006-0079-x.
Dolins F. L., 2010, LINKING SPATIAL PERC.
Erkent O, 2015, IEEE INT CONF ROBOT, P5462, DOI 10.1109/ICRA.2015.7139962.
Erkent O, 2012, IEEE INT CONF ROBOT, P3497, DOI 10.1109/ICRA.2012.6225367.
Galindo C, 2005, 2005 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2278, DOI 10.1109/IROS.2005.1545511.
Glover A, 2012, IEEE INT CONF ROBOT, P4730, DOI 10.1109/ICRA.2012.6224843.
Ho KL, 2007, INT J COMPUT VISION, V74, P261, DOI 10.1007/s11263-006-0020-1.
Karaoguz H, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P218, DOI 10.1109/ICAR.2015.7251459.
Karaoguz H, 2014, IEEE INT CONF ROBOT, P697, DOI 10.1109/ICRA.2014.6906930.
Konolige K, 2010, INT J ROBOT RES, V29, P941, DOI 10.1177/0278364910370376.
Kuipers B, 2000, ARTIF INTELL, V119, P191, DOI 10.1016/S0004-3702(00)00017-5.
Lim J, 2012, INT J ROBOT RES, V31, P1394, DOI 10.1177/0278364912461455.
Liu M, 2012, IEEE INT CONF ROBOT, P3503, DOI 10.1109/ICRA.2012.6225040.
Mozos OM, 2007, ROBOT AUTON SYST, V55, P391, DOI 10.1016/j.robot.2006.12.003.
Martinez-Gomez J, 2011, IEEE INT CONF ROBOT, P1936, DOI 10.1109/ICRA.2011.5980102.
Mozos O. M., 2007, P IEEE RSJ IROS WORK.
Murphy L, 2014, IEEE INT CONF ROBOT, P1312, DOI 10.1109/ICRA.2014.6907022.
Newman P, 2009, INT J ROBOT RES, V28, P1406, DOI 10.1177/0278364909341483.
Posner I, 2008, SPRINGER TRAC ADV RO, V39, P85.
Pronobis A, 2009, INT J ROBOT RES, V28, P588, DOI 10.1177/0278364909103912.
Pronobis A, 2010, 11 INT C INT AUT SYS.
Pronobis A, 2012, IEEE INT CONF ROBOT, P3515, DOI 10.1109/ICRA.2012.6224637.
Ranganathan A., 2010, P ROB SCI SYST.
Ranganathan A, 2012, AUTON ROBOT, V32, P351, DOI 10.1007/s10514-012-9273-4.
Remolina E, 2004, ARTIF INTELL, V152, P47, DOI 10.1016/S0004-3702(03)00114-0.
Robert L., 1997, SPATIAL COGNITION GE.
Shi L, 2012, IEEE INT C INT ROBOT, P2991, DOI 10.1109/IROS.2012.6385549.
SIBSON R, 1973, COMPUT J, V16, P30, DOI 10.1093/comjnl/16.1.30.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.
Tapus A., 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P2429.
TVERSKY B, 1983, COGNITIVE PSYCHOL, V15, P121, DOI 10.1016/0010-0285(83)90006-3.
Tversky B., 1993, Spatial Information Theory. A Theoretical Basis for GIS. European Conference, COSIT `93 Proceedings, P14.
Tversky B., 2005, CAMBRIDGE HDB VISUOS, P1, DOI DOI 10.1017/CBO9780511610448.002.
Ursic P, 2012, IEEE INT C INT ROBOT, P1371, DOI 10.1109/IROS.2012.6385546.
Vasudevan S, 2008, ROBOT AUTON SYST, V56, P522, DOI 10.1016/j.robot.2008.03.005.
Vasudevan S, 2007, ROBOT AUTON SYST, V55, P359, DOI 10.1016/j.robot.2006.12.008.
Walter MR, 2014, INT J ROBOT RES, V33, P1167, DOI 10.1177/0278364914537359.
Williams B, 2009, ROBOT AUTON SYST, V57, P1188, DOI 10.1016/j.robot.2009.06.010.
Yeh T, 2008, PROC CVPR IEEE, P61.
Zivkovic Z., 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P2480.
Zivkovic Z, 2007, ROBOT AUTON SYST, V55, P411, DOI 10.1016/j.robot.2006.12.005.},
  da = {2022-05-17},
  doc-delivery-number = {EA6AB},
  eissn = {1573-7527},
  funding-acknowledgement = {Bogazici University BAP {[}9164]; Tubitak {[}EEAG 111E285]; Turkish
State Planning Organization (DPT) under the TAM {[}2007K120610]},
  funding-text = {This work has been supported in part by Bogazici University BAP Project
9164 and Tubitak Project EEAG 111E285. The first author is supported by
Turkish State Planning Organization (DPT) under the TAM Project number
2007K120610.},
  issn = {0929-5593},
  journal-iso = {Auton. Robot.},
  keywords = {Spatial cognition; Long-term spatial memory; Place recognition},
  keywords-plus = {SEMANTIC MAPS; LOOP CLOSURE; LARGE-SCALE; FAB-MAP; LOCALIZATION; VISION;
SPACE},
  language = {English},
  number-of-cited-references = {47},
  research-areas = {Computer Science; Robotics},
  researcherid-numbers = {Bozma, Huriye Isil/A-2348-2017},
  times-cited = {5},
  type = {Article},
  unique-id = {WOS:000386705700002},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {15},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{karaoguz-bozma:2020:2,
  author = {H. Karaoguz and H. I. Bozma},
  journal = {AUTONOMOUS ROBOTS},
  title = {Merging of appearance-based place knowledge among multiple robots},
  volume = {44},
  number = {6},
  pages = {1009--1027},
  doi = {10.1007/s10514-020-09911-2},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2020},
  month = {7},
  abstract = {If robots can merge the appearance-based place knowledge of other robots
with their own, they can relate to these places even if they have not
previously visited them. We have investigated this problem using robots
with compatible visual sensing capabilities and with each robot having
its individual long-term place memory. Here, each place refers to a
spatial region as defined by a collection of appearances and in the
place memory, the knowledge is organized in a tree hierarchy. In the
proposed merging approach, the hierarchical organization plays a key
role-as it corresponds to a nested sequence of hyperspheres in the
appearance space. The merging proceeds by considering the extent of
overlap of the respective nested hyperspheres-starting with the largest
covering hypersphere. Thus, differing from related work, knowledge is
merged in as large chunks as possible while the hierarchical structure
is preserved accordingly. As such, the merging scales better as the
extent of knowledge to be merged increases. This is demonstrated in an
extensive set of multirobot experiments where robots share their
knowledge and then use their merged knowledge when visiting these
places.},
  affiliation = {Karaoguz, H (Corresponding Author), Kungliga Tekn Hgsk Stockholm, Stockholm, Sweden.
Karaoguz, Hakan, Kungliga Tekn Hgsk Stockholm, Stockholm, Sweden.
Bozma, H. Isil, Bogazici Univ, Fac Elect \& Elect Engn, Istanbul, Turkey.},
  affiliations = {Royal Institute of Technology; Bogazici University},
  author-email = {hkarao@kth.se},
  cited-references = {Adluru N, 2008, INT C PATT RECOG, P382.
Amigoni F, 2006, P IEEE, V94, P1340, DOI 10.1109/JPROC.2006.876925.
Aragues R, 2012, IEEE T ROBOT, V28, P840, DOI 10.1109/TRO.2012.2192012.
Beeson P, 2010, INT J ROBOT RES, V29, P428, DOI 10.1177/0278364909100586.
Birk A, 2006, P IEEE, V94, P1384, DOI 10.1109/JPROC.2006.876965.
Carpin S, 2005, ROBOT AUTON SYST, V53, P1, DOI 10.1016/j.robot.2005.07.001.
Carpin S, 2008, AUTON ROBOT, V25, P305, DOI 10.1007/s10514-008-9097-4.
Chella A, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P747.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Erinc G, 2014, AUTON ROBOT, V36, P241, DOI 10.1007/s10514-013-9352-1.
Erkent O, 2017, ADV ROBOTICS, V31, P865, DOI 10.1080/01691864.2017.1356746.
Erkent O, 2013, INT J ROBOT RES, V32, P672, DOI 10.1177/0278364913481393.
Estrada C, 2005, IEEE T ROBOT, V21, P588, DOI 10.1109/TRO.2005.844673.
Ferreira F., 2008, ICLAWAR, P1.
Galindo C, 2005, 2005 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2278, DOI 10.1109/IROS.2005.1545511.
Garcia-Fidalgo E, 2017, IEEE T ROBOT, V33, P1061, DOI 10.1109/TRO.2017.2704598.
Gil A, 2010, ROBOT AUTON SYST, V58, P68, DOI 10.1016/j.robot.2009.07.026.
Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925.
Ho K., 2005, IEEE RSJ INT C INT R.
Howard A, 2004, IEEE INT CONF ROBOT, P4198, DOI 10.1109/ROBOT.2004.1308933.
Huang WH, 2005, INT J ROBOT RES, V24, P601, DOI 10.1177/0278364905056348.
Karaoguz H, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P5107, DOI 10.1109/IROS.2016.7759749.
Karaoguz H, 2016, AUTON ROBOT, V40, P1379, DOI 10.1007/s10514-015-9514-4.
Karaoguz H, 2014, IEEE INT CONF ROBOT, P697, DOI 10.1109/ICRA.2014.6906930.
Ko J., 2003, IEEE RSJ INT C INT R, V4.
Konolige K, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P212.
Kostavelis I, 2015, ROBOT AUTON SYST, V66, P86, DOI 10.1016/j.robot.2014.12.006.
Lee HC, 2011, ADV ROBOTICS, V25, P1675, DOI 10.1163/016918611X584631.
Leung KYK, 2011, IEEE INT CONF ROBOT.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Ma X., 2008, WORLD C INT CONTR AU, P5704.
Marjovi A, 2012, ROBOT AUTON SYST, V60, P1191, DOI 10.1016/j.robot.2012.05.007.
Matlin M.W, 2005, COGNITION.
Nieto-Granda C, 2014, INT J ROBOT RES, V33, P519, DOI 10.1177/0278364913515309.
Ozkucur NE, 2009, LECT NOTES COMPUT SC, V5399, P189, DOI 10.1007/978-3-642-02921-9\_17.
Park S, 2016, IEEE T ROBOT, V32, P528, DOI 10.1109/TRO.2016.2544301.
Parker L. E, 2008, J PHYS AGENTS, V2, P5, DOI {[}DOI 10.14198/JOPHA.2008.2.1.02, 10.14198/jopha.2008.2.1.02].
Pronobis A, 2009, INT J ROBOT RES, V28, P588, DOI 10.1177/0278364909103912.
Ranganathan A., 2010, P ROB SCI SYST.
Saeedi S, 2014, ROBOT AUTON SYST, V62, P1408, DOI 10.1016/j.robot.2014.06.002.
Samatova NF, 2002, DISTRIB PARALLEL DAT, V11, P157.
Thrun S., 2000, IEEE INT C ROB AUT, V1.
Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387.
Tomono M, 2013, IEEE INT C INT ROBOT, P5172, DOI 10.1109/IROS.2013.6697104.
Tungadi F, 2010, IEEE INT C INT ROBOT, P7, DOI 10.1109/IROS.2010.5654446.
Williams SB, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P2743, DOI 10.1109/ROBOT.2002.1013647.
Zhou XS, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P1785, DOI 10.1109/IROS.2006.282219.},
  da = {2022-05-17},
  doc-delivery-number = {MI1DC},
  earlyaccessdate = {MAR 2020},
  eissn = {1573-7527},
  funding-acknowledgement = {Royal Institute of Technology; TUBITAK {[}EEEAG-111E285]; Turkish State
Planning Organization (DPT) {[}TAM 2007K120610]; BAP {[}9164]},
  funding-text = {Open access funding provided by Royal Institute of Technology. This work
has been supported in part by TUBITAK EEEAG-111E285, BAP 9164 and by the
Turkish State Planning Organization (DPT) under the TAM 2007K120610.},
  issn = {0929-5593},
  journal-iso = {Auton. Robot.},
  keywords = {Place recognition; Multi-robot; Unsupervised learning},
  keywords-plus = {LARGE-SCALE; SLAM; MAPS},
  language = {English},
  number-of-cited-references = {47},
  oa = {hybrid},
  research-areas = {Computer Science; Robotics},
  times-cited = {0},
  type = {Article},
  unique-id = {WOS:000521864000001},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{kretzschmar-stachniss:2012:0278364912455072,
  author = {H. Kretzschmar and C. Stachniss},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {Information-theoretic compression of pose graphs for laser-based SLAM},
  volume = {31},
  number = {11,SI},
  pages = {1219--1230},
  doi = {10.1177/0278364912455072},
  note = {15th International Symposium on Robotics Research (ISSR), Flagstaff, AZ,
AUG 28-31, 2011},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2012},
  month = {9},
  abstract = {In graph-based simultaneous localization and mapping (SLAM), the pose
graph grows over time as the robot gathers information about the
environment. An ever growing pose graph, however, prevents long-term
mapping with mobile robots. In this paper, we address the problem of
efficient information-theoretic compression of pose graphs. Our approach
estimates the mutual information between the laser measurements and the
map to discard the measurements that are expected to provide only a
small amount of information. Our method subsequently marginalizes out
the nodes from the pose graph that correspond to the discarded laser
measurements. To maintain a sparse pose graph that allows for efficient
map optimization, our approach applies an approximate marginalization
technique that is based on Chow-Liu trees. Our contributions allow the
robot to effectively restrict the size of the pose graph. Alternatively,
the robot is able to maintain a pose graph that does not grow unless the
robot explores previously unobserved parts of the environment.
Real-world experiments demonstrate that our approach to pose graph
compression is well suited for long-term mobile robot mapping.},
  affiliation = {Stachniss, C (Corresponding Author), Univ Freiburg, Dept Comp Sci, Geroges Kohler Allee 79, D-79110 Freiburg, Germany.
Kretzschmar, Henrik; Stachniss, Cyrill, Univ Freiburg, Dept Comp Sci, Geroges Kohler Allee 79, D-79110 Freiburg, Germany.},
  affiliations = {League of European Research Universities - LERU; University of Freiburg},
  author-email = {stachnis@informatik.uni-freiburg.de},
  cited-references = {Bachrach A, 2011, J FIELD ROBOT, V28, P644, DOI 10.1002/rob.20400.
Bosse M, 2003, IEEE INT CONF ROBOT, P1899, DOI 10.1109/ROBOT.2003.1241872.
CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Davison A. J., 2009, P IEEE INT C ROB AUT, P387.
Davison A. J., 2005, P INT C COMP VIS ICC, V1.
Eade E, 2010, IEEE INT C INT ROBOT, P3017, DOI 10.1109/IROS.2010.5649205.
Estrada C, 2005, IEEE T ROBOT, V21, P588, DOI 10.1109/TRO.2005.844673.
Eustice RM, 2006, IEEE T ROBOT, V22, P1100, DOI 10.1109/TRO.2006.886264.
Folkesson J, 2005, IEEE INT CONF ROBOT, P30.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Grisetti G, 2010, IEEE INT CONF ROBOT, P273, DOI 10.1109/ROBOT.2010.5509407.
He R, 2008, IEEE INT CONF ROBOT, P1814, DOI 10.1109/ROBOT.2008.4543471.
Howard A., 2003, ROBOTICS DATA SET RE.
Ila V, 2010, IEEE T ROBOT, V26, P78, DOI 10.1109/TRO.2009.2034435.
Kaess M, 2009, ROBOT AUTON SYST, V57, P1198, DOI 10.1016/j.robot.2009.06.008.
Kim A, 2011, IEEE INT C INT ROBOT, P1647, DOI 10.1109/IROS.2011.6048439.
Kollar T, 2008, P NAT C ART INT AAAI, P1369.
Konolige K, 2008, IEEE T ROBOT, V24, P1066, DOI 10.1109/TRO.2008.2004832.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
KRAUSE A, 2005, P UNC ART INT UAI.
Kretzschmar H, 2011, IEEE INT C INT ROBOT, P865, DOI 10.1109/IROS.2011.6048060.
Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733.
MacKay DJ., 2003, INFORM THEORY INFERE.
Ni K, 2007, IEEE INT CONF ROBOT, P1678, DOI 10.1109/ROBOT.2007.363564.
Olson E., 2008, THESIS.
Olson EB, 2009, IEEE INT CONF ROBOT, P1233.
SMITH RC, 1986, INT J ROBOT RES, V5, P56, DOI 10.1177/027836498600500404.
SNAVELY N, 2008, P IEEE C COMP VIS PA, P1.
Stachniss C, 2011, P INT S ROB RES ISRR.
Stachniss C., 2005, ROBOTICS SCI SYSTEMS, P65.},
  da = {2022-05-17},
  doc-delivery-number = {003ZN},
  eissn = {1741-3176},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {SLAM; long-term; pose graph; compression; mutual information},
  keywords-plus = {SPACE},
  language = {English},
  number-of-cited-references = {31},
  oa = {Green Submitted},
  orcid-numbers = {Stachniss, Cyrill/0000-0003-1173-6972},
  research-areas = {Robotics},
  times-cited = {66},
  type = {Article; Proceedings Paper},
  unique-id = {WOS:000308650000002},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {20},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{thomas-et-al:2021:9561701,
  author = {H. Thomas and B. Agro and M. Gridseth and T. D. J. A. B. Zhang},
  booktitle = {2021 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA
2021)},
  title = {Self-Supervised Learning of Lidar Segmentation for Autonomous Indoor
Navigation},
  pages = {14047--14053},
  doi = {10.1109/ICRA48506.2021.9561701},
  note = {IEEE International Conference on Robotics and Automation (ICRA), Xian,
PEOPLES R CHINA, MAY 30-JUN 05, 2021},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2021},
  abstract = {We present a self-supervised learning approach for the semantic
segmentation of lidar frames. Our method is used to train a deep point
cloud segmentation architecture without any human annotation. The
annotation process is automated with the combination of simultaneous
localization and mapping (SLAM) and ray-tracing algorithms. By
performing multiple navigation sessions in the same environment, we are
able to identify permanent structures, such as walls, and disentangle
short-term and long-term movable objects, such as people and tables,
respectively. New sessions can then be performed using a network trained
to predict these semantic labels. We demonstrate the ability of our
approach to improve itself over time, from one session to the next. With
semantically filtered point clouds, our robot can navigate through more
complex scenarios, which, when added to the training pool, help to
improve our network predictions. We provide insights into our network
predictions and show that our approach can also improve the performances
of common localization techniques.},
  affiliation = {Thomas, H (Corresponding Author), Univ Toronto, Inst Aerosp Studies UTIAS, 4925 Dufferin St, Toronto, ON, Canada.
Thomas, Hugues; Agro, Ben; Gridseth, Mona; Barfoot, Timothy D., Univ Toronto, Inst Aerosp Studies UTIAS, 4925 Dufferin St, Toronto, ON, Canada.
Zhang, Jian, Apple Inc, Cupertino, CA 95014 USA.},
  affiliations = {University of Toronto; Apple Inc},
  author-email = {hugues.thomas@robotics.utias.utoronto.ca
ben.agro@robotics.utias.utoronto.ca
mona.gridseth@robotics.utias.utoronto.ca
jianz@apple.com
tim.barfoot@utoronto.ca},
  book-group-author = {IEEE},
  cited-references = {Biswas J, 2014, IEEE INT CONF ROBOT, P3969, DOI 10.1109/ICRA.2014.6907435.
Brooks CA, 2012, J FIELD ROBOT, V29, P445, DOI 10.1002/rob.21408.
Chen XYL, 2019, IEEE INT C INT ROBOT, P4530, DOI 10.1109/IROS40897.2019.8967704.
Deschaud JE, 2018, IEEE INT CONF ROBOT, P2480.
Dewan A, 2017, IEEE INT C INT ROBOT, P3544, DOI 10.1109/IROS.2017.8206198.
FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
Fox D, 2003, INT J ROBOT RES, V22, P985, DOI 10.1177/0278364903022012001.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Hadsell R, 2009, J FIELD ROBOT, V26, P120, DOI 10.1002/rob.20276.
Hornung A., 2003, AUTON ROBOT, V34, P189.
Lookingbill A, 2007, INT J COMPUT VISION, V74, P287, DOI 10.1007/s11263-006-0024-x.
Mendes E, 2016, IEEE INT SYMP SAFE, P195, DOI 10.1109/SSRR.2016.7784298.
Moravec H., 1985, P 1985 IEEE INT C RO, V2, P116.
Nava M, 2019, IEEE ROBOT AUTOM LET, V4, P1279, DOI 10.1109/LRA.2019.2894849.
Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378.
Pomerleau F, 2014, IEEE INT CONF ROBOT, P3712, DOI 10.1109/ICRA.2014.6907397.
Pomerleau F, 2013, AUTON ROBOT, V34, P133, DOI 10.1007/s10514-013-9327-2.
Ridge B, 2015, INT J ADV ROBOT SYST, V12, DOI 10.5772/59654.
Sofman B, 2006, J FIELD ROBOT, V23, P1059, DOI 10.1002/rob.20169.
Sun L, 2018, IEEE ROBOT AUTOM LET, V3, P3749, DOI 10.1109/LRA.2018.2856268.
Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651.
Wang K, 2019, IEEE INT CONF ROBOT, P5224, DOI 10.1109/ICRA.2019.8793499.
Zhang J., 2014, ROBOT SCI SYST, V2, P9.
Zhang L, 2018, IEEE ACCESS, V6, P75545, DOI {[}10.1109/ACCESS.2018.2873617, 10.1109/TCBB.2018.2848633].},
  da = {2022-05-17},
  doc-delivery-number = {BS8DT},
  eissn = {2577-087X},
  isbn = {978-1-7281-9077-8},
  issn = {1050-4729},
  language = {English},
  number-of-cited-references = {24},
  oa = {Green Submitted},
  research-areas = {Automation \& Control Systems; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {0},
  type = {Proceedings Paper},
  unique-id = {WOS:000771405405079},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Automation \& Control Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{yin-et-al:2021:661199,
  author = {H. Yin and X. Xu and Y. Wang and R. Xiong},
  journal = {FRONTIERS IN ROBOTICS AND AI},
  title = {Radar-to-Lidar: Heterogeneous Place Recognition via Joint Learning},
  volume = {8},
  pages = {661199},
  doi = {10.3389/frobt.2021.661199},
  publisher = {FRONTIERS MEDIA SA},
  address = {AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND},
  year = {2021},
  month = {5},
  abstract = {Place recognition is critical for both offline mapping and online
localization. However, current single-sensor based place recognition
still remains challenging in adverse conditions. In this paper, a
heterogeneous measurement based framework is proposed for long-term
place recognition, which retrieves the query radar scans from the
existing lidar (Light Detection and Ranging) maps. To achieve this, a
deep neural network is built with joint training in the learning stage,
and then in the testing stage, shared embeddings of radar and lidar are
extracted for heterogeneous place recognition. To validate the
effectiveness of the proposed method, we conducted tests and
generalization experiments on the multi-session public datasets and
compared them to other competitive methods. The experimental results
indicate that our model is able to perform multiple place recognitions:
lidar-to-lidar (L2L), radar-to-radar (R2R), and radar-to-lidar (R2L),
while the learned model is trained only once. We also release the source
code publicly:
https://github.com/ZJUYH/radar-to-lidar-place-recognition.},
  affiliation = {Wang, Y (Corresponding Author), Zhejiang Univ, Inst Cyber Syst \& Control, Coll Control Sci \& Engn, Hangzhou, Peoples R China.
Yin, Huan; Xu, Xuecheng; Wang, Yue; Xiong, Rong, Zhejiang Univ, Inst Cyber Syst \& Control, Coll Control Sci \& Engn, Hangzhou, Peoples R China.},
  affiliations = {Zhejiang University},
  article-number = {661199},
  author-email = {ywang24@zju.edu.cn},
  cited-references = {Adolfsson D., 2019, 2019 EUR C MOB ROB E, P1.
Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Barnes D., 2020, C ROB LEARN, P303.
Barnes D, 2020, IEEE INT CONF ROBOT, P6433, DOI 10.1109/ICRA40945.2020.9196884.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Bosse M, 2013, IEEE INT CONF ROBOT, P2677, DOI 10.1109/ICRA.2013.6630945.
Carballo A., 2020, 2020 IEEE INT VEH S.
Cattaneo D, 2020, IEEE INT CONF ROBOT, P4365, DOI 10.1109/ICRA40945.2020.9196859.
Cen SH, 2018, IEEE INT CONF ROBOT, P6045, DOI 10.1109/ICRA.2018.8460687.
Chen XYL, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Ding XQ, 2020, IEEE T INTELL TRANSP, V21, P4646, DOI 10.1109/TITS.2019.2942760.
Dube R, 2020, INT J ROBOT RES, V39, P339, DOI 10.1177/0278364919863090.
Elhousni M, 2020, IEEE INT VEH SYM, P1879, DOI 10.1109/IV47402.2020.9304812.
Falliat D, 2007, IEEE INT CONF ROBOT, P3921.
Feng MD, 2019, IEEE INT CONF ROBOT, P4790, DOI 10.1109/ICRA.2019.8794415.
Gadd Matthew, 2020, 2020 IEEE/ION Position, Location and Navigation Symposium (PLANS), P270, DOI 10.1109/PLANS46316.2020.9109951.
Granstrom K, 2011, INT J ROBOT RES, V30, P1728, DOI 10.1177/0278364911405086.
Groft E., 2012, US Patent, Patent No. {[}8,279,107, 8279107].
He L, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P231, DOI 10.1109/IROS.2016.7759060.
Hong Z., 2020, 2020 IEEE RSJ INT C.
Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039.
Kim G., 2020, IEEE INT C ROB AUT I.
Kim G, 2018, IEEE INT C INT ROBOT, P4802, DOI 10.1109/IROS.2018.8593953.
Kingma D.P., 2015, P 3 INT C LEARN REPR.
Latif Y, 2018, IEEE INT CONF ROBOT, P2349, DOI 10.1109/ICRA.2018.8461081.
Le Gentil C., 2020, 2020 IEEE RSJ INT C.
Li L, 2017, JOINT URB REMOTE SEN.
Liu Z, 2019, IEEE I CONF COMP VIS, P2831, DOI 10.1109/ICCV.2019.00292.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Paszke A, 2019, ADV NEUR IN, V32.
Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4\_28.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Saftescu S, 2020, IEEE INT CONF ROBOT, P4358, DOI 10.1109/ICRA40945.2020.9196682.
Sun L, 2020, IEEE INT CONF ROBOT, P4386, DOI 10.1109/ICRA40945.2020.9196708.
Tang TYQ, 2020, IEEE ROBOT AUTOM LET, V5, P1087, DOI 10.1109/LRA.2020.2965907.
Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470.
Wang Y, 2020, IEEE INT C INT ROBOT, P5769, DOI 10.1109/IROS45743.2020.9341010.
Xie SR, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20102870.
Xu XC, 2021, IEEE ROBOT AUTOM LET, V6, P2791, DOI 10.1109/LRA.2021.3060741.
Yin H., 2020, 2020 IEEE INT C REAL.
Yin H, 2021, IEEE T INTELL TRANSP, DOI 10.1109/TITS.2021.3061165.
Yin H, 2020, IEEE T INTELL TRANSP, V21, P1380, DOI 10.1109/TITS.2019.2905046.
Yin H, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE ROBIO 2017), P481, DOI 10.1109/ROBIO.2017.8324463.
Zhang, 2020, QUANTIFYING HUMAN MO.
Zhang XW, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107760.},
  da = {2022-05-17},
  doc-delivery-number = {SK5EU},
  funding-acknowledgement = {National Key R\&D Program of China {[}2020YFB1313300]},
  funding-text = {This work was supported by the National Key R\&D Program of China under
grant 2020YFB1313300.},
  issn = {2296-9144},
  journal-iso = {Front. Robot. AI},
  keywords = {radar; lidar; heterogeneous measurements; place recognition; deep neural
network; mobile robot},
  keywords-plus = {SCAN CONTEXT; LOCALIZATION},
  language = {English},
  number-of-cited-references = {48},
  oa = {Green Submitted, gold, Green Published},
  orcid-numbers = {Yin, Huan/0000-0002-0872-8202},
  research-areas = {Robotics},
  researcherid-numbers = {Yin, Huan/ABC-9483-2020},
  times-cited = {1},
  type = {Article},
  unique-id = {WOS:000656239300001},
  usage-count-last-180-days = {5},
  usage-count-since-2013 = {14},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Emerging Sources Citation Index (ESCI)},
}

@article{yin-et-al:2020:2905046,
  author = {H. Yin and Y. Wang and X. Ding and L. Tang and H. Shoudong and R. Xiong},
  journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS},
  title = {3D LiDAR-Based Global Localization Using Siamese Neural Network},
  volume = {21},
  number = {4},
  pages = {1380--1392},
  doi = {10.1109/TITS.2019.2905046},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2020},
  month = {4},
  abstract = {Global localization in 3D point clouds is a challenging task for mobile
vehicles in outdoor scenarios, which requires the vehicle to localize
itself correctly in a given map without prior knowledge of its pose.
This is a critical component of autonomous vehicles or robots on the
road for handling localization failures. In this paper, based on reduced
dimension scan representations learned from neural networks, a solution
to global localization is proposed by achieving place recognition first
and then metric pose estimation in the global prior map. Specifically,
we present a semi-handcrafted feature learning method for 3D Light
detection and ranging (LiDAR) point clouds using artificial statistics
and siamese network, which transforms the place recognition problem into
a similarity modeling problem. Additionally, the sensor data using
dimension reduced representations require less storage space and make
the searching easier. With the learned representations by networks and
the global poses, a prior map is built and used in the localization
framework. In the localization step, position only observations obtained
by place recognition are used in a particle filter algorithm to achieve
precise pose estimation. To demonstrate the effectiveness of our place
recognition and localization approach, KITTI benchmark and our
multi-session datasets are employed for comparison with other
geometric-based algorithms. The results show that our system can achieve
both high accuracy and efficiency for long-term autonomy.},
  affiliation = {Wang, Y (Corresponding Author), Zhejiang Univ, State Key Lab Ind Control \& Technol, Hangzhou 310058, Peoples R China.
Wang, Y (Corresponding Author), Zhejiang Univ, Inst Cyber Syst \& Control, Hangzhou 310058, Peoples R China.
Yin, Huan; Wang, Yue; Ding, Xiaqing; Tang, Li; Xiong, Rong, Zhejiang Univ, State Key Lab Ind Control \& Technol, Hangzhou 310058, Peoples R China.
Yin, Huan; Wang, Yue; Ding, Xiaqing; Tang, Li; Xiong, Rong, Zhejiang Univ, Inst Cyber Syst \& Control, Hangzhou 310058, Peoples R China.
Yin, Huan; Wang, Yue; Ding, Xiaqing; Tang, Li; Huang, Shoudong; Xiong, Rong, Zhejiang Univ, Joint Ctr Robot Res, Hangzhou 310058, Peoples R China.
Yin, Huan; Wang, Yue; Ding, Xiaqing; Tang, Li; Xiong, Rong, Univ Technol Sydney, Sydney, NSW 2007, Australia.
Huang, Shoudong, Univ Technol Sydney, CAS, Sydney, NSW 2007, Australia.},
  affiliations = {Zhejiang University; Zhejiang University; Zhejiang University;
University of Technology Sydney; University of Technology Sydney},
  author-email = {wangyue@iipc.zju.edu.cn},
  cited-references = {Anguelova M., 2004, THESIS.
Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
Bosse M, 2013, IEEE INT CONF ROBOT, P2677, DOI 10.1109/ICRA.2013.6630945.
Bromley  J., 1994, ADV NEURAL INFORM PR, P737.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dellaert F, 1999, ICRA `99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1322, DOI 10.1109/ROBOT.1999.772544.
Dube Renaud, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5266, DOI 10.1109/ICRA.2017.7989618.
Falliat D, 2007, IEEE INT CONF ROBOT, P3921.
Fernandez-Moral E, 2013, IEEE INT CONF ROBOT, P2719, DOI 10.1109/ICRA.2013.6630951.
Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Granstrom K, 2011, INT J ROBOT RES, V30, P1728, DOI 10.1177/0278364911405086.
Hadsell R., 2006, 2006 IEEE COMP VIS P, V2, P1735, DOI DOI 10.1109/CVPR.2006.100.
Hata A. Y., 2017, IEEE T INTELL TRANSP, V19, P2839.
He L, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P231, DOI 10.1109/IROS.2016.7759060.
Hess W, 2016, IEEE INT CONF ROBOT, P1271, DOI 10.1109/ICRA.2016.7487258.
Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655.
Kim G, 2018, IEEE INT C INT ROBOT, P4802, DOI 10.1109/IROS.2018.8593953.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Li B, 2017, IEEE INT C INT ROBOT, P1513, DOI 10.1109/IROS.2017.8205955.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Magnusson Martin, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P23, DOI 10.1109/ROBOT.2009.5152712.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Naseer T, 2018, IEEE T ROBOT, V34, P289, DOI 10.1109/TRO.2017.2788045.
Naseer T, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564.
Pomerleau F., 2015, FDN TRENDS ROBOT, V4, P1, DOI DOI 10.1561/2300000035.
Pomerleau F, 2013, AUTON ROBOT, V34, P133, DOI 10.1007/s10514-013-9327-2.
Rohling T, 2015, IEEE INT C INT ROBOT, P736, DOI 10.1109/IROS.2015.7353454.
Rozsa Z, 2018, IEEE T INTELL TRANSP, V19, P2708, DOI 10.1109/TITS.2018.2790264.
Rusu R.B., 2009, IEEE INT C ROB AUT I, P3212, DOI DOI 10.1109/R0B0T.2009.5152473.
Steder B, 2011, IEEE INT C INT ROBOT, P1249, DOI 10.1109/IROS.2011.6048325.
Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986.
Tang L, 2019, AUTON ROBOT, V43, P197, DOI 10.1007/s10514-018-9724-7.
Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679.
Wohlkinger W., 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2987, DOI 10.1109/ROBIO.2011.6181760.
Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405.
Yin HX, 2018, 2018 IEEE 18TH INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT), P721, DOI 10.1109/ICCT.2018.8599957.},
  da = {2022-05-17},
  doc-delivery-number = {KZ7ZL},
  eissn = {1558-0016},
  funding-acknowledgement = {National Key Research and Development Program of China
{[}2017YFB1300400]; National Nature Science Foundation of China
{[}U1609210]},
  funding-text = {This work was supported in part by the National Key Research and
Development Program of China under Grant 2017YFB1300400 and in part by
the National Nature Science Foundation of China Grant U1609210. The
Associate Editor for this paper was L. M. Bergasa. (Corresponding
author: Yue Wang.)},
  issn = {1524-9050},
  journal-iso = {IEEE Trans. Intell. Transp. Syst.},
  keywords = {Three-dimensional displays; Laser radar; Pose estimation; Neural
networks; Task analysis; Robot sensing systems; Measurement; Mobile
vehicles; place recognition; siamese network; global localization},
  keywords-plus = {OBJECT RECOGNITION; ICP},
  language = {English},
  number-of-cited-references = {38},
  orcid-numbers = {Yin, Huan/0000-0002-0872-8202
Huang, Shoudong/0000-0002-6124-4178},
  research-areas = {Engineering; Transportation},
  researcherid-numbers = {Yin, Huan/ABC-9483-2020},
  times-cited = {22},
  type = {Article},
  unique-id = {WOS:000523478400004},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {34},
  web-of-science-categories = {Engineering, Civil; Engineering, Electrical \& Electronic;
Transportation Science \& Technology},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{zhang-et-al:2018:1729881418780178,
  author = {H. Zhang and X. Chen and H. Lu and J. Xiao},
  journal = {INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS},
  title = {Distributed and collaborative monocular simultaneous localization and
mapping for multi-robot systems in large-scale environments},
  volume = {15},
  number = {3},
  doi = {10.1177/1729881418780178},
  publisher = {SAGE PUBLICATIONS INC},
  address = {2455 TELLER RD, THOUSAND OAKS, CA 91320 USA},
  year = {2018},
  month = {6},
  abstract = {In this article, we propose a distributed and collaborative monocular
simultaneous localization and mapping system for the multi-robot system
in large-scale environments, where monocular vision is the only
exteroceptive sensor. Each robot estimates its pose and reconstructs the
environment simultaneously using the same monocular simultaneous
localization and mapping algorithm. Meanwhile, they share the results of
their incremental maps by streaming keyframes through the robot
operating system messages and the wireless network. Subsequently, each
robot in the group can obtain the global map with high efficiency. To
build the collaborative simultaneous localization and mapping
architecture, two novel approaches are proposed. One is a robust
relocalization method based on active loop closure, and the other is a
vision-based multi-robot relative pose estimating and map merging
method. The former is used to solve the problem of tracking failures
when robots carry out long-term monocular simultaneous localization and
mapping in large-scale environments, while the latter uses the
appearance-based place recognition method to determine multi-robot
relative poses and build the large-scale global map by merging each
robot's local map. Both KITTI data set and our own data set acquired by
a handheld camera are used to evaluate the proposed system. Experimental
results show that the proposed distributed multi-robot collaborative
monocular simultaneous localization and mapping system can be used in
both indoor small-scale and outdoor large-scale environments.},
  affiliation = {Chen, XYL (Corresponding Author), Natl Univ Def Technol, Dept Automat, 137 Yanwachi St, Changsha 410073, Hunan, Peoples R China.
Zhang, Hui; Chen, Xieyuanli; Lu, Huimin; Xiao, Junhao, Natl Univ Def Technol, Dept Automat, 137 Yanwachi St, Changsha 410073, Hunan, Peoples R China.},
  affiliations = {National University of Defense Technology - China},
  article-number = {1729881418780178},
  author-email = {chenxieyuanli@hotmail.com},
  cited-references = {Bay H., 2006, EUR C COMP VIS, P404.
Birk A, 2006, P IEEE, V94, P1384, DOI 10.1109/JPROC.2006.876965.
Chebrolu  Nived, COLLABORATIVE VISUAL.
Chen X, 8 ANN IEEE INT C CYB.
Chen X, 2017 SPRING INT C CO, P131.
Chen XYL, 2017, 2017 IEEE INTERNATIONAL SYMPOSIUM ON SAFETY, SECURITY AND RESCUE ROBOTICS (SSRR), P41, DOI 10.1109/SSRR.2017.8088138.
Clemente L. A., 2007, ROBOTICS SCI SYSTEMS, V2.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Davison AJ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1403.
Dellaert F, 1999, ICRA `99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1322, DOI 10.1109/ROBOT.1999.772544.
Doitsidis L., 2011, 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2011), P1661, DOI 10.1109/IROS.2011.6048116.
Dong J, 2015, IEEE INT CONF ROBOT, P5807, DOI 10.1109/ICRA.2015.7140012.
Eade E, 2008, BMVC, V136, P13.
Eade E, 2007, IEEE I CONF COMP VIS, P2112.
Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577.
Engel Jakob, EUR C COMP VIS, P834.
Feder HJS, 1999, INT J ROBOT RES, V18, P650, DOI 10.1177/02783649922066484.
Feng Y, P AS C COMP VIS, P206.
Forster Christian, 2013, 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2013), P3962, DOI 10.1109/IROS.2013.6696923.
Fox D, 2000, AUTON ROBOT, V8, P325, DOI 10.1023/A:1008937911390.
Fox D, 2006, P IEEE, V94, P1325, DOI 10.1109/JPROC.2006.876927.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518.
Howard A, 2006, INT J ROBOT RES, V25, P1243, DOI 10.1177/0278364906072250.
Indelman V, 2014, IEEE INT CONF ROBOT, P593, DOI 10.1109/ICRA.2014.6906915.
Kaess M, 2010, COMPUT VIS IMAGE UND, V114, P286, DOI 10.1016/j.cviu.2009.07.006.
Kim B, 2010, IEEE INT CONF ROBOT, P3185, DOI 10.1109/ROBOT.2010.5509154.
Klein G, EUR C COMP VIS, P802.
Ko J, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P3232.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6.
Leung KYK, 2012, J INTELL ROBOT SYST, V66, P321, DOI 10.1007/s10846-011-9620-2.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Martinelli A, 2005, IEEE INT CONF ROBOT, P2797.
Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Mur-Artal R, 2014, IEEE INT CONF ROBOT, P846, DOI 10.1109/ICRA.2014.6906953.
Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513.
Pupilli M, BMVC, DOI {[}10. 5244/C. 19. 50, DOI 10.5244/C.19.50].
RAHIMI A, 2001, ICCV01, V1, P315.
Reitmayr G, P 5 IEEE ACM INT S M, P109.
Rone W, 2013, ROBOTICA, V31, P1, DOI 10.1017/S0263574712000021.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Saeedi S, 2016, J FIELD ROBOT, V33, P3, DOI 10.1002/rob.21620.
Saeedi S, 2014, ROBOT AUTON SYST, V62, P1408, DOI 10.1016/j.robot.2014.06.002.
Schmuck Patrik, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3863, DOI 10.1109/ICRA.2017.7989445.
Se S, 2005, IEEE T ROBOT, V21, P364, DOI 10.1109/TRO.2004.839228.
Sivic J, NULL, P1470.
Sola J, 2007, IEEE INT CONF ROBOT, P4795, DOI 10.1109/ROBOT.2007.364218.
Stachniss C., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P1505.
Strasdat H, 2012, LOCAL ACCURACY GLOBA.
Strasdat H, 2011, IEEE I CONF COMP VIS, P2352, DOI 10.1109/ICCV.2011.6126517.
Strasdat Hauke, 2010, ROBOT SCI SYST 6, V6, P2.
Straub J, 2013, IEEE IMAGE PROC, P2548, DOI 10.1109/ICIP.2013.6738525.
Vidal-Calleja TA, 2011, ROBOT AUTON SYST, V59, P654, DOI 10.1016/j.robot.2011.05.008.
Williams B, 2011, IEEE T PATTERN ANAL, V33, P1699, DOI 10.1109/TPAMI.2011.41.
Williams B, 2009, ROBOT AUTON SYST, V57, P1188, DOI 10.1016/j.robot.2009.06.010.
Zhou XS, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P1785, DOI 10.1109/IROS.2006.282219.},
  da = {2022-05-17},
  doc-delivery-number = {GK2SM},
  funding-acknowledgement = {National Science Foundation of China {[}61503401, 61773393]},
  funding-text = {The author(s) disclosed receipt of the following financial support for
the research, authorship, and/or publication of this article: This work
was supported by National Science Foundation of China (nos 61503401 and
61773393).},
  issn = {1729-8814},
  journal-iso = {Int. J. Adv. Robot. Syst.},
  keywords = {Multi-robot collaborative SLAM; monocular SLAM; relocalization;
large-scale SLAM},
  keywords-plus = {SLAM},
  language = {English},
  number-of-cited-references = {59},
  oa = {gold},
  orcid-numbers = {Chen, Xieyuanli/0000-0003-0955-6681
Xiao, Junhao/0000-0002-4751-539X},
  research-areas = {Robotics},
  researcherid-numbers = {Chen, Xieyuanli/AAH-6401-2020},
  times-cited = {9},
  type = {Article},
  unique-id = {WOS:000435985000001},
  usage-count-last-180-days = {5},
  usage-count-since-2013 = {34},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{biswas-veloso:2013:0278364913503892,
  author = {J. Biswas and M. M. Veloso},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {Localization and navigation of the CoBots over long-term deployments},
  volume = {32},
  number = {14,SI},
  pages = {1679--1694},
  doi = {10.1177/0278364913503892},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2013},
  month = {12},
  abstract = {For the last three years, we have developed and researched multiple
collaborative robots, CoBots, which have been autonomously traversing
our multi-floor buildings. We pursue the goal of long-term autonomy for
indoor service mobile robots as the ability for them to be deployed
indefinitely while they perform tasks in an evolving environment. The
CoBots include several levels of autonomy, and in this paper we focus on
their localization and navigation algorithms. We present the Corrective
Gradient Refinement (CGR) algorithm, which refines the proposal
distribution of the particle filter used for localization with sensor
observations using analytically computed state space derivatives on a
vector map. We also present the Fast Sampling Plane Filtering algorithm
that extracts planar regions from depth images in real time. These
planar regions are then projected onto the 2D vector map of the
building, and along with the laser rangefinder observations, used with
CGR for localization. For navigation, we present a hierarchical planner,
which computes a topological policy using a graph representation of the
environment, computes motion commands based on the topological policy,
and then modifies the motion commands to side-step perceived obstacles.
We started logging the deployments of the CoBots one and a half years
ago, and have since collected logs of the CoBots traversing more than
130 km over 1082 deployments and a total run time of 182 h, which we
publish as a dataset consisting of more than 10 million laser scans. The
logs show that although there have been continuous changes in the
environment, the robots are robust to most of them, and there exist only
a few locations where changes in the environment cause increased
uncertainty in localization.},
  affiliation = {Biswas, J (Corresponding Author), Carnegie Mellon Univ, Sch Comp Sci, Inst Robot, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
Biswas, Joydeep, Carnegie Mellon Univ, Sch Comp Sci, Inst Robot, Pittsburgh, PA 15213 USA.
Veloso, Manuela M., Carnegie Mellon Univ, Sch Comp Sci, Dept Comp Sci, Pittsburgh, PA 15213 USA.},
  affiliations = {Carnegie Mellon University; Carnegie Mellon University},
  author-email = {joydeepb@cs.cmu.edu},
  cited-references = {Bailey T, 2006, IEEE ROBOT AUTOM MAG, V13, P108, DOI 10.1109/MRA.2006.1678144.
Biber P, 2009, INT J ROBOT RES, V28, P20, DOI 10.1177/0278364908096286.
Biswas J, 2013, ROBOCUP 2013 ROB SOC.
Biswas J, 2012, IEEE INT CONF ROBOT, P1697, DOI 10.1109/icra.2012.6224766.
Biswas J, 2011, IEEE INT C INT ROBOT, P73, DOI 10.1109/IROS.2011.6048263.
Biswas J, 2010, IEEE INT CONF ROBOT, P4379, DOI 10.1109/ROBOT.2010.5509842.
Bruce J, 2007, P 11 INT ROBOCUP S A.
BUHMANN J, 1995, AI MAG, V16, P31.
Churchill W, 2012, IEEE INT CONF ROBOT, P4525, DOI 10.1109/ICRA.2012.6224596.
Dayoub F, 2011, ROBOT AUTON SYST, V59, P285, DOI 10.1016/j.robot.2011.02.013.
Dellaert F, 1999, ICRA `99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1322, DOI 10.1109/ROBOT.1999.772544.
Doucet A., 2000, P 16 C UNC ART INT S, P176.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022.
ELFES A, 1989, COMPUTER, V22, P46, DOI 10.1109/2.30720.
Fox D., 2001, ADV NEURAL INFORM PR, V14, P713.
GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Jetto L, 1999, IEEE T ROBOTIC AUTOM, V15, P219, DOI 10.1109/70.760343.
Kalman R. E., 1960, J FLUIDS ENG, V82, P34, DOI {[}https://doi.org/10.1115/1.3662552, DOI 10.1115/1.3662552].
Koenig S, 1998, ARTIFICIAL INTELLIGENCE AND MOBILE ROBOTS, P91.
Lefebvre T, 2004, INT J CONTROL, V77, P639, DOI 10.1080/00207170410001704998.
Lenser S., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1225, DOI 10.1109/ROBOT.2000.844766.
LEONARD JJ, 1991, IEEE T ROBOTIC AUTOM, V7, P376, DOI 10.1109/70.88147.
Mendoza JP, 2012, IEEE INT C INT ROBOT, P370, DOI 10.1109/IROS.2012.6386189.
Nilsson N, 1984, 323 DTIC.
Nourbakhsh IR, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P3636.
Oyama A, 2009, 27 ANN C ROB SOC JAP, V9, P2009.
Rosenthal S, 2012, P 26 C ART INT AAAI, P886.
Rosenthal S., 2010, P 9 INT C AUT AG MUL, P915.
Roumeliotis SI, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P454, DOI 10.1109/IROS.2000.894646.
Saarinen J, 2012, IEEE INT C INT ROBOT, P3489, DOI 10.1109/IROS.2012.6385629.
Samadi Mehdi, 2012, P IEEE AAAI C ART IN, P2074.
Thrun S, 1999, ICRA `99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1999, DOI 10.1109/ROBOT.1999.770401.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Thrun S. a. o., 2002, EXPLORING ARTIFICIAL, V1, P1.
Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387.
Walcott-Bryant A, 2012, IEEE INT C INT ROBOT, P1871, DOI 10.1109/IROS.2012.6385561.
Zhang L., 2000, P IEEE INT C ROB AUT, P2538, DOI DOI 10.1109/ROBOT.2000.846410.},
  da = {2022-05-17},
  doc-delivery-number = {287AL},
  eissn = {1741-3176},
  funding-acknowledgement = {National Science Foundation {[}NSF IIS-1012733]},
  funding-text = {This work was supported by the National Science Foundation (grant number
NSF IIS-1012733).},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {Localization; navigation; long-term autonomy; indoor mobile robots;
autonomous robots},
  language = {English},
  number-of-cited-references = {39},
  oa = {Green Submitted},
  orcid-numbers = {Biswas, Joydeep/0000-0002-1211-1731},
  research-areas = {Robotics},
  times-cited = {37},
  type = {Article},
  unique-id = {WOS:000329510300006},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {21},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{biswas-veloso:2017:005,
  author = {J. Biswas and M. M. Veloso},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {Episodic non-Markov localization},
  volume = {87},
  pages = {162--176},
  doi = {10.1016/j.robot.2016.09.005},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2017},
  month = {1},
  abstract = {Markov localization and its variants are widely used for mobile robot
localization. These methods assume Markov independence of observations,
implying that the observations can be entirely explained by a map.
However, in real human environments, robots frequently make unexpected
observations due to unmapped static objects like chairs and tables, and
dynamic objects like humans. We therefore introduce Episodic non-Markov
Localization (EnML), which reasons about the world as consisting of
three classes of objects: long-term features corresponding to permanent
mapped objects, short-term features corresponding to unmapped static
objects, and dynamic features corresponding to unmapped moving objects.
Long-term features are represented by a static map, while short-term
features are detected and tracked in real-time. To reason about
unexpected observations and their correlations across poses, we augment
the Dynamic Bayesian Network for Markov localization to include varying
edges and nodes, resulting in a novel Varying Graphical Network
representation. The maximum likelihood estimate of the belief is
incrementally computed by non-linear functional optimization. By
detecting timesteps along the robot's trajectory where unmapped
observations prior to such time steps are unrelated to those afterwards,
EnML limits the history of observations and pose estimates to
``episodes{''} over which the belief is computed. We demonstrate EnML
using different types of sensors including laser rangefinders and depth
cameras, and over multiple datasets, comparing it with alternative
approaches. We further include results of a team of indoor autonomous
service mobile robots traversing hundreds of kilometers using EnML. (C)
2016 Elsevier B.V. All rights reserved.},
  affiliation = {Biswas, J (Corresponding Author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
Biswas, J (Corresponding Author), Univ Massachusetts, Coll Informat \& Comp Sci, Amherst, MA 01003 USA.
Biswas, Joydeep; Veloso, Manuela M., Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
Biswas, Joydeep, Univ Massachusetts, Coll Informat \& Comp Sci, Amherst, MA 01003 USA.},
  affiliations = {Carnegie Mellon University; University of Massachusetts System;
University of Massachusetts Amherst},
  author-email = {joydeepb@cs.cmu.edu
veloso@cs.cmu.edu},
  cited-references = {Agarwal S., 2012, CERES SOLVER TUTORIA.
Bailey T, 2006, IEEE ROBOT AUTOM MAG, V13, P108, DOI 10.1109/MRA.2006.1678144.
BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007.
Biswas J., 2012, 2012 IEEE International Conference on Robotics and Automation (ICRA), P1697, DOI 10.1109/ICRA.2012.6224766.
Biswas J., 2014, VECTOR MAP BASED NON.
Biswas J, 2013, INT J ROBOT RES, V32, P1679, DOI 10.1177/0278364913503892.
Biswas J, 2011, IEEE INT C INT ROBOT, P73, DOI 10.1109/IROS.2011.6048263.
Chen YQ, 2008, ACM T MATH SOFTWARE, V35, DOI 10.1145/1391989.1391995.
Croz J. Du, 1990, FACTORIZATIONS BAND.
Dellaert F, 1999, ICRA `99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1322, DOI 10.1109/ROBOT.1999.772544.
Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022.
Fox D, 2001, STAT ENG IN, P401.
Fox D., 2001, ADV NEURAL INF PROCE.
Fox D., 1998, MARKOV LOCALIZATION.
Griewank A., 2008, EVALUATING DERIVATIV.
Julier SJ, 2004, P IEEE, V92, P401, DOI 10.1109/JPROC.2003.823141.
Koenig S, 1998, ARTIFICIAL INTELLIGENCE AND MOBILE ROBOTS, P91.
Lenser S., 2000, INT C ROB AUT.
LEONARD JJ, 1992, INT J ROBOT RES, V11, P286, DOI 10.1177/027836499201100402.
Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164.
Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733.
Lu F, 1997, J INTELL ROBOT SYST, V18, P249, DOI 10.1023/A:1007957421070.
MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030.
Meyer-Delius D, 2010, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2010.5648920.
Morris T, 2014, IEEE INT CONF ROBOT, P2765, DOI 10.1109/ICRA.2014.6907255.
Nackman, 1994, SCI ENG C INTRO ADV.
Olson E, 2006, IEEE INT CONF ROBOT, P2262, DOI 10.1109/ROBOT.2006.1642040.
Olson EB, 2009, IEEE INT CONF ROBOT, P1233.
Pearl J., 2014, PROBABILISTIC REASON.
Rosenthal S., 2010, P 9 INT C AUT AG MUL, P915.
Saarinen J, 2012, IEEE INT C INT ROBOT, P3489, DOI 10.1109/IROS.2012.6385629.
Stachniss C., 2005, P C ART INT, P1324.
Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387.
Tipaldi G. D., 2012, RSS WORKSH ROB CLUTT.
Walcott-Bryant A, 2012, IEEE INT C INT ROBOT, P1871, DOI 10.1109/IROS.2012.6385561.},
  da = {2022-05-17},
  doc-delivery-number = {EF7KE},
  eissn = {1872-793X},
  funding-acknowledgement = {NSF {[}IIS-1012733]; ONR {[}N00014-09-1-1031]},
  funding-text = {This research was supported by NSF award IIS-1012733 and ONR grant
number N00014-09-1-1031. The views and conclusions contained in this
document are those of the authors only. We thank the members of the
CORAL group, in particular Brian Coltin, Stephanie Rosenthal, and
Richard Wang, for the underlying robot task scheduling, human-robot
interaction planning, and data-collection deployments. We also thank
Mike Licitra for the design and construction of the CoBot robots.},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Localization; Long-term autonomy; Mapping},
  keywords-plus = {ALGORITHM; ALIGNMENT},
  language = {English},
  number-of-cited-references = {37},
  orcid-numbers = {Biswas, Joydeep/0000-0002-1211-1731},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  times-cited = {10},
  type = {Article},
  unique-id = {WOS:000390507700012},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {10},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{li-et-al:2015:7139706,
  author = {J. Li and R. M. Eustice and M. Johnson-Roberson},
  booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {High-Level Visual Features for Underwater Place Recognition},
  pages = {3652--3659},
  doi = {10.1109/ICRA.2015.7139706},
  note = {IEEE International Conference on Robotics and Automation (ICRA),
Seattle, WA, MAY 26-30, 2015},
  publisher = {IEEE COMPUTER SOC},
  address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
  year = {2015},
  abstract = {This paper reports on a method to perform robust visual relocalization
between temporally separated sets of underwater images gathered by a
robot. The place recognition and relocalization problem is more
challenging in the underwater environment mainly due to three factors:
1) changes in illumination; 2) long-term changes in the visual
appearance of features because of phenomena like biofouling on man-made
structures and growth or movement in natural features; and 3) low
density of visually salient features for image matching. To address
these challenges, a patch-based feature matching approach is proposed,
which uses image segmentation and local intensity contrast to locate
salient patches and HOG description to make correspondences between
patches. Compared to traditional point-based features that are sensitive
to dramatic appearance changes underwater, patch-based features are able
to encode higher level information such as shape or structure which
tends to persist across years in underwater environments. The algorithm
is evaluated on real data, from multiple years, collected by a Hovering
Autonomous Underwater Vehicle for ship hull inspection. Results in
relocalization performance across missions from different years are
compared to other traditional methods.},
  affiliation = {Li, J (Corresponding Author), Univ Michigan, Dept Elect Engn \& Comp Sci, Ann Arbor, MI 48109 USA.
Li, Jie, Univ Michigan, Dept Elect Engn \& Comp Sci, Ann Arbor, MI 48109 USA.
Eustice, Ryan M.; Johnson-Roberson, Matthew, Univ Michigan, Dept Naval Architecture \& Marine Engn, Ann Arbor, MI 48109 USA.},
  affiliations = {University of Michigan System; University of Michigan; University of
Michigan System; University of Michigan},
  author-email = {ljlijie@umich.edu
eustice@umich.edu
mattjr@umich.Gdu},
  book-group-author = {IEEE},
  cited-references = {Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66.
Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226.
Bailey T, 2006, IEEE ROBOT AUTOM MAG, V13, P108, DOI 10.1109/MRA.2006.1678144.
Carlevaris-Bianco Nicholas, 2011, IEEE International Conference on Robotics and Automation, P423.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dalal N., 2005, 2005 IEEE COMPUTER S, P886, DOI {[}10.1109/CVPR.2005.177, DOI 10.1109/CVPR.2005.177].
Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049.
Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022.
Eustice RM, 2008, IEEE J OCEANIC ENG, V33, P103, DOI 10.1109/JOE.2008.923547.
Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77.
Harel J., 2007, NIPS C ADV NEUR INF, P545, DOI {[}10.7551/mitpress/7503.0 01.0 0 01, DOI 10.7551/MITPRESS/7503.003.0073].
Hartley R., 2003, MULTIPLE VIEW GEOMET.
Jiang H., 2011, BMVC, V6, P7, DOI DOI 10.5244/C.25.110.(4).
Kim A, 2013, IEEE T ROBOT, V29, P719, DOI 10.1109/TRO.2012.2235699.
Konolige K, 2008, IEEE T ROBOT, V24, P1066, DOI 10.1109/TRO.2008.2004832.
McManus C., 2014, P ROB SCI SYST C BER.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Naseer T, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564.
Ozog P, 2014, IEEE INT CONF ROBOT, P3832, DOI 10.1109/ICRA.2014.6907415.
Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743.},
  da = {2022-05-17},
  doc-delivery-number = {BE3MR},
  eissn = {2577-087X},
  isbn = {978-1-4799-6923-4},
  issn = {1050-4729},
  keywords-plus = {SIMULTANEOUS LOCALIZATION; NAVIGATION; SLAM},
  language = {English},
  number-of-cited-references = {20},
  oa = {Green Submitted},
  research-areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {13},
  type = {Proceedings Paper},
  unique-id = {WOS:000370974903096},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {3},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Engineering, Electrical \& Electronic; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{santos-et-al:2016:2516594,
  author = {J. M. Santos and T. Krajnik and T. J. P. A. D. Fentanes},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {Lifelong Information- Driven Exploration to Complete and Refine 4-D
Spatio-Temporal Maps},
  volume = {1},
  number = {2},
  pages = {684--691},
  doi = {10.1109/LRA.2016.2516594},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2016},
  month = {7},
  abstract = {This letter presents an exploration method that allows mobile robots to
build and maintain spatio-temporal models of changing environments. The
assumption of a perpetually changing world adds a temporal dimension to
the exploration problem, making spatio-temporal exploration a
never-ending, lifelong learning process. We address the problem by
application of information-theoretic exploration methods to
spatio-temporal models that represent the uncertainty of environment
states as probabilistic functions of time. This allows to predict the
potential information gain to be obtained by observing a particular area
at a given time, and consequently, to decide which locations to visit
and the best times to go there. To validate the approach, a mobile robot
was deployed continuously over 5 consecutive business days in a busy
office environment. The results indicate that the robot's ability to
spot environmental changes improved as it refined its knowledge of the
world dynamics.},
  affiliation = {Santos, JM (Corresponding Author), Univ Lincoln, Lincoln Ctr Autonomous Syst, Lincoln LN6 7TS, England.
Santos, Joao Machado; Krajnik, Tomas; Fentanes, Jaime Pulido; Duckett, Tom, Univ Lincoln, Lincoln Ctr Autonomous Syst, Lincoln LN6 7TS, England.},
  affiliations = {University of Lincoln},
  author-email = {jsantos@lincoln.ac.uk},
  cited-references = {Amigoni F, 2010, ROBOT AUTON SYST, V58, P684, DOI 10.1016/j.robot.2009.11.005.
Biswas J, 2014, IEEE INT CONF ROBOT, P3969, DOI 10.1109/ICRA.2014.6907435.
Caglioti V, 2001, IEEE T SYST MAN CY B, V31, P187, DOI 10.1109/3477.915342.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
CROES GA, 1958, OPER RES, V6, P791, DOI 10.1287/opre.6.6.791.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Fentanes JP, 2015, IEEE INT CONF ROBOT, P1112, DOI 10.1109/ICRA.2015.7139315.
Hawes N., 2014, STRANDS SOFTWARE SYS.
Holz D., 2010, ISR 2010 41 INT S RO, P1.
Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0.
Koenig S, 2001, IEEE INT CONF ROBOT, P3594, DOI 10.1109/ROBOT.2001.933175.
Krajnik T., 2015, MOB ROB ECMR 2015 EU, P1.
Krajnik T., 2014, J INTELL ROBOT SYST.
Krajnik T., 2014, ADV AUTONOMOUS ROBOT, P281.
Krajnik T, 2015, IEEE INT CONF ROBOT, P2140, DOI 10.1109/ICRA.2015.7139481.
Krajnik T, 2014, IEEE INT C INT ROBOT, P4537, DOI 10.1109/IROS.2014.6943205.
Krajnik T, 2014, IEEE INT CONF ROBOT, P3706, DOI 10.1109/ICRA.2014.6907396.
LEVOY M, 1990, ACM T GRAPHIC, V9, P245, DOI 10.1145/78964.78965.
Marchant R, 2014, IEEE INT CONF ROBOT, P6136, DOI 10.1109/ICRA.2014.6907763.
Marchant R, 2012, IEEE INT C INT ROBOT, P2242, DOI 10.1109/IROS.2012.6385653.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
MORAVEC HP, 1988, AI MAG, V9, P61.
Muhlfellner P., 2015, J FIELD ROBOT.
Neubert P, 2015, ROBOT AUTON SYST, V69, P15, DOI 10.1016/j.robot.2014.08.005.
Fentanes JAP, 2011, J FIELD ROBOT, V28, P832, DOI 10.1002/rob.20402.
Rahman M, 2011, APPLICATIONS OF FOURIER TRANSFORMS TO GENERALIZED FUNCTIONS, P1.
Saarinen J, 2012, IEEE INT C INT ROBOT, P3489, DOI 10.1109/IROS.2012.6385629.
Singh Amarjeet, 2010, 2010 IEEE International Conference on Robotics and Automation (ICRA 2010), P5490, DOI 10.1109/ROBOT.2010.5509934.
Stachniss C., 2005, P ROB SCI SYST RSS C.
Tipaldi GD, 2013, INT J ROBOT RES, V32, P1662, DOI 10.1177/0278364913502830.
Walcott-Bryant A, 2012, IEEE INT C INT ROBOT, P1871, DOI 10.1109/IROS.2012.6385561.},
  da = {2022-05-17},
  doc-delivery-number = {FK7ZO},
  funding-acknowledgement = {EU ICT project Grant {[}600623]},
  funding-text = {This work was supported by the EU ICT project Grant 600623 `STRANDS.'},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {Mapping; service robots},
  keywords-plus = {MOBILE ROBOTS; ENVIRONMENTS; LOCALIZATION; NAVIGATION},
  language = {English},
  number-of-cited-references = {31},
  oa = {Green Accepted},
  orcid-numbers = {Krajník, Tomáš/0000-0002-4408-7916
Santos, Joao/0000-0002-4797-3542},
  research-areas = {Robotics},
  researcherid-numbers = {Krajník, Tomáš/O-2339-2013},
  times-cited = {17},
  type = {Article},
  unique-id = {WOS:000413726900012},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Emerging Sources Citation Index (ESCI)},
}

@article{oh-eoh:2021:app11198976,
  author = {J. Oh and G. Eoh},
  journal = {APPLIED SCIENCES-BASEL},
  title = {Variational Bayesian Approach to Condition-Invariant Feature Extraction
for Visual Place Recognition},
  volume = {11},
  number = {19},
  pages = {8976},
  doi = {10.3390/app11198976},
  publisher = {MDPI},
  address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  year = {2021},
  month = {10},
  abstract = {As mobile robots perform long-term operations in large-scale
environments, coping with perceptual changes becomes an important issue
recently. This paper introduces a stochastic variational inference and
learning architecture that can extract condition-invariant features for
visual place recognition in a changing environment. Under the assumption
that a latent representation of the variational autoencoder can be
divided into condition-invariant and condition-sensitive features, a new
structure of the variation autoencoder is proposed and a variational
lower bound is derived to train the model. After training the model,
condition-invariant features are extracted from test images to calculate
the similarity matrix, and the places can be recognized even in severe
environmental changes. Experiments were conducted to verify the proposed
method, and the experimental results showed that our assumption was
reasonable and effective in recognizing places in changing environments.},
  affiliation = {Eoh, G (Corresponding Author), Chungbuk Natl Univ, Ind AI Res Ctr, Cheongju 28116, South Korea.
Oh, Junghyun, Kwangwoon Univ, Dept Robot, Seoul 01897, South Korea.
Eoh, Gyuho, Chungbuk Natl Univ, Ind AI Res Ctr, Cheongju 28116, South Korea.},
  affiliations = {Kwangwoon University; Chungbuk National University},
  article-number = {8976},
  author-email = {jhyunoh@kw.ac.kr
gyuho.eoh@cbnu.ac.kr},
  cited-references = {Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Chancan M, 2020, IEEE ROBOT AUTOM LET, V5, P993, DOI 10.1109/LRA.2020.2967324.
Choi Y., 2015, P IEEE INT C COMP VI.
Civera J., 2018, PPNIV WORKSH IROS 20.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
Garg S, 2018, IEEE INT CONF ROBOT, P3645, DOI 10.1109/ICRA.2018.8461051.
Kingma D.P., 2014, AUTOENCODING VARIATI.
Krizhevsky A., P INT C ADV NEUR INF, P1097.
Liu Y, 2012, IEEE INT C INT ROBOT, P1051, DOI 10.1109/IROS.2012.6386145.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Naseer T, 2015, IEEE INT C INT ROBOT, P2529, DOI 10.1109/IROS.2015.7353721.
Neubert P., 2013, INT C ROB AUT ICRA W, DOI 10.1016/j.cell.2007.12.011.
Oh JH, 2017, ELECTRON LETT, V53, DOI 10.1049/el.2017.0037.
Oh J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21124103.
Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724.
Park C, 2020, INT J CONTROL AUTOM, V18, P2699, DOI 10.1007/s12555-019-0891-x.
Pu Y, 2016, ADV NEURAL INFORM PR.
Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897.
Simonyan K, 2015, P CVPR.
Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950.
Sunderhauf N, 2011, IEEE INT C INT ROBOT, P1234, DOI 10.1109/IROS.2011.6048590.
Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.},
  da = {2022-05-17},
  doc-delivery-number = {WG3DI},
  eissn = {2076-3417},
  funding-acknowledgement = {National Research Foundation of Korea (NRF) - Korea government (MSIT)
{[}2020R1F1A1076667]; Korea Institute of Energy Technology Evaluation
and Planning (KETEP); Ministry of Trade, Industry \& Energy (MOTIE) of
the Republic of Korea {[}20174010201620]; Kwangwoon University},
  funding-text = {This work has supported by the National Research Foundation of Korea
(NRF) grant funded by the Korea government(MSIT) (No. 2020R1F1A1076667),
Korea Institute of Energy Technology Evaluation and Planning (KETEP) and
the Ministry of Trade, Industry \& Energy (MOTIE) of the Republic of
Korea (No. 20174010201620). This work was also supported by Research
Resettlement Fund for the new faculty of Kwangwoon University in 2019.},
  journal-iso = {Appl. Sci.-Basel},
  keywords = {place recognition; localization; deep learning; mobile robots;
auto-encoder; SLAM},
  keywords-plus = {SCALE},
  language = {English},
  number-of-cited-references = {26},
  oa = {Green Published, gold},
  orcid-numbers = {Eoh, Gyuho/0000-0003-4931-4396
Oh, Junghyun/0000-0003-0502-7600},
  research-areas = {Chemistry; Engineering; Materials Science; Physics},
  times-cited = {0},
  type = {Article},
  unique-id = {WOS:000706876000001},
  usage-count-last-180-days = {3},
  usage-count-since-2013 = {3},
  web-of-science-categories = {Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
Science, Multidisciplinary; Physics, Applied},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{perez-et-al:2015:y,
  author = {J. Perez and F. Caballero and L. Merino},
  journal = {JOURNAL OF INTELLIGENT \& ROBOTIC SYSTEMS},
  title = {Enhanced Monte Carlo Localization with Visual Place Recognition for
Robust Robot Localization},
  volume = {80},
  number = {3-4,SI},
  pages = {641--656},
  doi = {10.1007/s10846-015-0198-y},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2015},
  month = {12},
  abstract = {This paper proposes extending Monte Carlo Localization methods with
visual place recognition information in order to build a robust robot
localization system. This system is aimed to work in crowded and
non-planar scenarios, where 2D laser rangefinders may not always be
enough to match the robot position within the map. Thus, visual place
recognition will be used in order to obtain robot position clues that
can be used to detect when the robot is lost and also to reset its
positions to the right one. The paper presents experimental results
based on datasets gathered with a real robot in challenging scenarios.},
  affiliation = {Perez, J (Corresponding Author), Univ Pablo de Olavide, Seville, Spain.
Perez, Javier; Merino, Luis, Univ Pablo de Olavide, Seville, Spain.
Caballero, Fernando, Univ Seville, Seville, Spain.},
  affiliations = {Universidad Pablo de Olavide; University of Sevilla},
  author-email = {jiperlar@upo.es
fcaballero@us.es
lmercab@upo.es},
  cited-references = {Alcantarilla PF, 2013, AUTON ROBOT, V34, P47, DOI 10.1007/s10514-012-9312-1.
Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514.
Carloni Luca P, 2009, 2009 3rd ACM/IEEE International Symposium on Networks-on-Chip (NOCS 2009), P1, DOI 10.1109/NOCS.2009.5071456.
Corke P, 2013, IEEE INT C INT ROBOT, P2085, DOI 10.1109/IROS.2013.6696648.
Dayoub F, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3364, DOI 10.1109/IROS.2008.4650701.
Doucet A., 2000, P 16 C UNC ART INT S, P176.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Glover A, 2012, IEEE INT CONF ROBOT, P4730, DOI 10.1109/ICRA.2012.6224843.
Hentschel M, 2011, J ROBOT, V2011, DOI 10.1155/2011/506245.
Himstedt M., 2013, ICRA WORKSH ROB MULT.
Kummerle R, 2013, IEEE INT CONF ROBOT, P3225, DOI 10.1109/ICRA.2013.6631026.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Perez-Lara J., 2014, P INT S ROB ISR.
Perez-Lara J., 2014, IEEE INT C AUT ROB S.
Ruhnke M., 2011, P INT C ROB AUT ICRA.
Thrun S, 2000, INT J ROBOT RES, V19, P972, DOI 10.1177/02783640022067922.
Thrun S, 2001, ARTIF INTELL, V128, P99, DOI 10.1016/S0004-3702(01)00069-8.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Wallach MH, 2006, INT C MACHINE LEARNI, P977, DOI DOI 10.1145/1143844.1143967.},
  da = {2022-05-17},
  doc-delivery-number = {CV0YX},
  eissn = {1573-0409},
  funding-acknowledgement = {FP7 FROG Project - European Commission {[}288235]; PAIS-MultiRobot
Project - Regional Government of Andalucia {[}TIC-7390]},
  funding-text = {This paper is an extension of work presented at ICARSC 2014 {[}14] and
is partially supported by the FP7 FROG Project (Contract 288235) funded
by the European Commission and the PAIS-MultiRobot Project (TIC-7390)
funded by Regional Government of Andalucia},
  issn = {0921-0296},
  journal-iso = {J. Intell. Robot. Syst.},
  keywords = {Monte Carlo localization; Long-term localization; Robust localization;
Crowded environment},
  keywords-plus = {BAGS},
  language = {English},
  number-of-cited-references = {19},
  orcid-numbers = {Merino, Luis/0000-0003-4927-8647
Perez Lara, Javier Ignacio/0000-0002-1167-7557},
  research-areas = {Computer Science; Robotics},
  researcherid-numbers = {Merino, Luis/B-2549-2013
Perez Lara, Javier Ignacio/AAA-7159-2022},
  times-cited = {14},
  type = {Article},
  unique-id = {WOS:000363981000020},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {19},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{berrio-et-al:2019:8814289,
  author = {J. S. Berrio and J. Ward and S. Worrall and N. Eduardo},
  booktitle = {2019 30TH IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV19)},
  title = {Identifying robust landmarks in feature-based maps},
  pages = {1166--1172},
  doi = {10.1109/IVS.2019.8814289},
  note = {30th IEEE Intelligent Vehicles Symposium (IV), Paris, FRANCE, JUN 09-12,
2019},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2019},
  abstract = {To operate in an urban environment, an automated vehicle must be capable
of accurately estimating its position within a global map reference
frame. This is necessary for optimal path planning and safe navigation.
To accomplish this over an extended period of time, the global map
requires long term maintenance. This includes the addition of newly
observable features and the removal of transient features belonging to
dynamic objects. The latter is especially important for the long-term
use of the map as matching against a map with features that no longer
exist can result in incorrect data associations, and consequently
erroneous localisation. This paper addresses the problem of removing
features from the map that correspond to objects that are no longer
observable/present in the environment. This is achieved by assigning a
single score which depends on the geometric distribution and
characteristics when the features are re-detected (or not) on different
occasions. Our approach not only eliminates ephemeral features, but can
also be used as a reduction algorithm for highly dense maps. We tested
our approach using half a year of weekly drives over the same 500 metre
section of road in an urban environment. The results presented
demonstrate the validity of the long term approach to map maintenance.},
  affiliation = {Berrio, JS (Corresponding Author), Univ Sydney, ACFR, Sydney, NSW, Australia.
Berrio, Julie Stephany; Ward, James; Worrall, Stewart; Nebot, Eduardo, Univ Sydney, ACFR, Sydney, NSW, Australia.},
  affiliations = {University of Sydney},
  author-email = {j.berrio@acfr.usyd.edu.au
j.ward@acfr.usyd.edu.au
s.worrall@acfr.usyd.edu.au
e.nebot@acfr.usyd.edu.au},
  book-group-author = {IEEE},
  cited-references = {Burgard W., 2012, 26 AAAI C ART INT, P2024.
Carneiro G, 2009, IMAGE VISION COMPUT, V27, P1143, DOI 10.1016/j.imavis.2008.10.015.
Dissanayake MWMG, 2001, IEEE T ROBOTIC AUTOM, V17, P229, DOI 10.1109/70.938381.
Dymczyk M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4572, DOI 10.1109/IROS.2016.7759673.
Dymczyk M, 2015, IEEE INT CONF ROBOT, P2767, DOI 10.1109/ICRA.2015.7139575.
Guo R, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, VOLS 1-7, CONFERENCE PROCEEDINGS, P1122.
Kaeli J. W., 2014, AUTONOMOUS UNDERWATE, P1.
Kim DI, 2015, INT CONF UBIQ ROBOT, P218, DOI 10.1109/URAI.2015.7358940.
Krajnik T, 2017, IEEE T ROBOT, V33, P964, DOI 10.1109/TRO.2017.2665664.
Krajnik T, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4558, DOI 10.1109/IROS.2016.7759671.
Kuutti S, 2018, IEEE INTERNET THINGS, V5, P829, DOI 10.1109/JIOT.2018.2812300.
Levinson J, 2011, IEEE INT VEH SYM, P163, DOI 10.1109/IVS.2011.5940562.
Lynen S., 2015, ROBOTICS SCI SYSTEMS.
Muhlfellner P, 2016, J FIELD ROBOT, V33, P561, DOI 10.1002/rob.21595.
Nobre F, 2018, IEEE INT CONF ROBOT, P3661.
Pomerleau F, 2014, IEEE INT CONF ROBOT, P3712, DOI 10.1109/ICRA.2014.6907397.
Rosen DM, 2016, IEEE INT CONF ROBOT, P1063, DOI 10.1109/ICRA.2016.7487237.
Siritanawan P, 2017, 2017 18TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P112, DOI 10.1109/ICAR.2017.8023504.
VANI S M., 2017, 2017 SMART CIT S PRA, P1.
Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165.
Yi S., 2019, ARXIV190408585.
Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x.},
  da = {2022-05-17},
  doc-delivery-number = {BO2SH},
  funding-acknowledgement = {ACFR; University of Sydney through the Dean of Engineering and
Information Technologies PhD Scholarship (South America); Australian
Research Council {[}DP160104081]; University of Michigan / Ford Motors
Company Contract ``Next generation Vehicles{''}},
  funding-text = {This work has been funded by the ACFR, the University of Sydney through
the Dean of Engineering and Information Technologies PhD Scholarship
(South America) and the Australian Research Council Discovery Grant
DP160104081 and University of Michigan / Ford Motors Company Contract
``Next generation Vehicles{''}.},
  isbn = {978-1-7281-0560-4},
  issn = {1931-0587},
  keywords-plus = {LOCALIZATION},
  language = {English},
  number-of-cited-references = {22},
  oa = {Green Submitted},
  orcid-numbers = {Worrall, Stewart/0000-0001-7940-4742
Perez, Julie Stephany Berrio/0000-0003-3126-7042},
  research-areas = {Automation \& Control Systems; Robotics; Transportation},
  researcherid-numbers = {Worrall, Stewart/AAB-4633-2020
Perez, Julie Stephany Berrio/AAL-1008-2021},
  series = {IEEE Intelligent Vehicles Symposium},
  times-cited = {5},
  type = {Proceedings Paper},
  unique-id = {WOS:000508184100157},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Automation \& Control Systems; Robotics; Transportation Science \&
Technology},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{berrio-et-al:2021:3094485,
  author = {J. S. Berrio and S. Worrall and M. Shan and N. Eduardo},
  journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS},
  title = {Long-Term Map Maintenance Pipeline for Autonomous Vehicles},
  doi = {10.1109/TITS.2021.3094485},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2021},
  abstract = {For autonomous vehicles to operate persistently in a typical urban
environment, it is essential to have high accuracy position information.
This requires a mapping and localisation system that can adapt to
changes over time. A localisation approach based on a single-survey map
will not be suitable for long-term operation as it does not incorporate
variations in the environment. In this paper, we present new algorithms
to maintain a featured-based map. A map maintenance pipeline is proposed
that can continuously update a map with the most relevant features
taking advantage of the changes in the surroundings. Our pipeline
detects and removes transient features based on their geometrical
relationships with the vehicle's pose. Newly identified features became
part of a new feature map and are assessed by the pipeline as candidates
for the localisation map. By purging out-of-date features and adding
newly detected features, we continually update the prior map to more
accurately represent the most recent environment. We have validated our
approach using the USyd Campus Dataset, which includes more than 18
months of data. The results presented demonstrate that our maintenance
pipeline produces a resilient map which can provide sustained
localisation performance over time.},
  affiliation = {Berrio, JS (Corresponding Author), Univ Sydney, Australian Ctr Field Robot ACFR, Sydney, NSW 2006, Australia.
Berrio, Julie Stephany; Worrall, Stewart; Shan, Mao; Nebot, Eduardo, Univ Sydney, Australian Ctr Field Robot ACFR, Sydney, NSW 2006, Australia.},
  affiliations = {University of Sydney},
  author-email = {j.berrio@acfr.usyd.edu.au
s.worrall@acfr.usyd.edu.au
m.shan@acfr.usyd.edu.au
e.nebot@acfr.usyd.edu.au},
  cited-references = {Berrio J. Stephany, 2020, ARXIV200705490.
Berrio JS, 2019, IEEE INT VEH SYM, P1173, DOI 10.1109/IVS.2019.8814189.
Berrio JS, 2019, IEEE INT VEH SYM, P1166, DOI 10.1109/IVS.2019.8814289.
Berrio JS, 2018, IEEE INT C INT ROBOT, P3174, DOI 10.1109/IROS.2018.8594024.
Biber P, 2009, INT J ROBOT RES, V28, P20, DOI 10.1177/0278364908096286.
Carneiro G, 2009, IMAGE VISION COMPUT, V27, P1143, DOI 10.1016/j.imavis.2008.10.015.
Churchill W, 2012, IEEE INT CONF ROBOT, P4525, DOI 10.1109/ICRA.2012.6224596.
CivilMaps, 2020, FINGERPRINT BAS MAP.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Dymczyk M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4572, DOI 10.1109/IROS.2016.7759673.
Dymczyk M, 2015, IEEE INT CONF ROBOT, P2767, DOI 10.1109/ICRA.2015.7139575.
Egger P, 2018, IEEE INT C INT ROBOT, P3430, DOI 10.1109/IROS.2018.8593854.
Einhorn E, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P240, DOI 10.1109/ECMR.2013.6698849.
Fankhauser P, 2016, STUD COMPUT INTELL, V625, P99, DOI 10.1007/978-3-319-26054-9\_5.
Hilbrandie G., 2018, U.S. Patent, Patent No. {[}10 024 676, 10024676].
Hochdorfer S, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P382, DOI 10.1109/IROS.2009.5354433.
Jo K, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18093145.
Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Krajnik T, 2019, IEEE ROBOT AUTOM LET, V4, P3310, DOI 10.1109/LRA.2019.2926682.
Krajnik T, 2017, IEEE T ROBOT, V33, P964, DOI 10.1109/TRO.2017.2665664.
Kretzschmar H, 2010, KUNSTL INTELL, V24, P199, DOI 10.1007/s13218-010-0034-2.
Lategahn H, 2013, IEEE INT VEH SYM, P285, DOI 10.1109/IVS.2013.6629483.
MacTavish K, 2018, J FIELD ROBOT, V35, P1265, DOI 10.1002/rob.21838.
Mihelich P, 2019, ROS PERCEPTION CAMER.
Muhlfellner P, 2016, J FIELD ROBOT, V33, P561, DOI 10.1002/rob.21595.
Nobre F, 2018, IEEE INT CONF ROBOT, P3661.
Pannen D, 2020, IEEE INT CONF ROBOT, P2288, DOI 10.1109/ICRA40945.2020.9197419.
Pannen D, 2019, IEEE INT CONF ROBOT, P2561, DOI 10.1109/ICRA.2019.8794329.
Pirker K, 2011, IEEE INT C INT ROBOT, P3990, DOI 10.1109/IROS.2011.6048253.
Rosen DM, 2016, IEEE INT CONF ROBOT, P1063, DOI 10.1109/ICRA.2016.7487237.
Sekonix, 2018, SF332X10 FAM DAT.
Ublox, 2020, NEO M8PU BLOX M8 HIG.
Vardhan H, 2018, HD MAPS NEW AGE MAPS.
Vectornav, 2020, VN 100 IMU AHRS.
Velodyne, 2018, VLP 16 US MAN.
Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165.
Verma S, 2019, IEEE INT C INTELL TR, P3906, DOI 10.1109/ITSC.2019.8917108.
Yi SQ, 2019, IEEE INT C INTELL TR, P128, DOI 10.1109/ITSC.2019.8917305.
Yi SQ, 2019, IEEE INT VEH SYM, P1247, DOI 10.1109/IVS.2019.8814106.
Zhou W, 2020, IEEE INTEL TRANSP SY, V12, P23, DOI 10.1109/MITS.2020.2990183.},
  da = {2022-05-17},
  doc-delivery-number = {XT2UA},
  earlyaccessdate = {JUL 2021},
  eissn = {1558-0016},
  funding-acknowledgement = {ACFR; University of Sydney; University of Michigan/Ford Motors Company
Contract Next generation Vehicles},
  funding-text = {This work was supported in part by the ACFR, the University of Sydney
through the Dean of Engineering and Information Technologies PhD
Scholarship (South America) and in part by the University of
Michigan/Ford Motors Company Contract Next generation Vehicles. The
Associate Editor for this article was C. Wen.},
  issn = {1524-9050},
  journal-iso = {IEEE Trans. Intell. Transp. Syst.},
  keywords = {Feature extraction; Pipelines; Maintenance engineering; Transient
analysis; Visualization; Autonomous vehicles; Task analysis; Long-term
localisation; feature-based map; map update},
  language = {English},
  number-of-cited-references = {41},
  oa = {Green Submitted},
  research-areas = {Engineering; Transportation},
  times-cited = {0},
  type = {Article; Early Access},
  unique-id = {WOS:000733448000001},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Engineering, Civil; Engineering, Electrical \& Electronic;
Transportation Science \& Technology},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{tsintotas-et-al:2021:103782,
  author = {K. A. Tsintotas and L. Bampis and A. Gasteratos},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {Modest-vocabulary loop-closure detection with incremental bag of tracked
words},
  volume = {141},
  pages = {103782},
  doi = {10.1016/j.robot.2021.103782},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2021},
  month = {7},
  abstract = {A key feature in the context of simultaneous localization and mapping is
loop-closure detection, a process determining whether the current
robot's environment perception coincides with previous observation.
However, in long-term operations, both computational efficiency and
memory requirements involved in an autonomous robot operation in
uncontrolled environments, are of particular importance. The majority of
approaches scale linearly with the environment's size in terms of
storage and query time. The article at hand presents an efficient
appearance-based loop-closure detection pipeline, which encodes the
traversed trajectory by a low amount of unique visual words generated
on-line through feature tracking. The incrementally constructed visual
vocabulary is referred to as the ``Bag of Tracked Words.{''} A
nearest-neighbor voting scheme is utilized to query the database and
assign probabilistic scores to all visited locations. Exploiting the
inherent temporal coherency in the loop-closure task, the produced
scores are processed through a Bayesian filter to estimate the belief
state about the robot's location on the map. Also, a geometrical
verification step ensures consistency between image matches. Management
is also applied to the resulting vocabulary to reduce its growth rate
and constraint the system's computational complexity while improving its
voting distinctiveness. The proposed approach's performance is
experimentally evaluated on several publicly available and challenging
datasets, including hand-held, car-mounted, aerial, and ground
trajectories. Results demonstrate the method's adaptability, which
retains high operational frequency in environments of up to 13 km and
high recall rates for perfect precision, outperforming other
state-of-the-art techniques. The system's effectiveness is owed to the
reduced vocabulary size, which is at least one order of magnitude
smaller than other contemporary approaches. An open research-oriented
source code has been made publicly available, which is dubbed as
``BoTW-LCD.{''} (C) 2021 Elsevier B.V. All rights reserved.},
  affiliation = {Tsintotas, KA (Corresponding Author), Democritus Univ Thrace, Sch Engn, Dept Prod \& Management Engn, Lab Robot \& Automat, GR-67132 Xanthi, Greece.
Tsintotas, Konstantinos A.; Bampis, Loukas; Gasteratos, Antonios, Democritus Univ Thrace, Sch Engn, Dept Prod \& Management Engn, Lab Robot \& Automat, GR-67132 Xanthi, Greece.},
  affiliations = {Democritus University of Thrace},
  article-number = {103782},
  author-email = {ktsintot@pme.duth.gr
lbampis@pme.duth.gr
agaster@pme.duth.gr},
  cited-references = {Alahakoon D, 2000, IEEE T NEURAL NETWOR, V11, P601, DOI 10.1109/72.846732.
Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3\_16.
An S., 2020, ARXIV PREPRINT ARXIV.
An S, 2019, IEEE INT C INT ROBOT, P378, DOI 10.1109/IROS40897.2019.8968043.
Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514.
Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Arroyo R, 2015, IEEE INT CONF ROBOT, P6328, DOI 10.1109/ICRA.2015.7140088.
Arroyo R, 2014, IEEE INT C INT ROBOT, P3089, DOI 10.1109/IROS.2014.6942989.
Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027.
Babenko A, 2015, IEEE T PATTERN ANAL, V37, P1247, DOI 10.1109/TPAMI.2014.2361319.
BAEZA-YATES R., 1999, MODERN INFORM RETRIE.
Balaska V, 2021, ROBOT AUTON SYST, V139, DOI 10.1016/j.robot.2021.103760.
Balaska V, 2020, ROBOT AUTON SYST, V131, DOI 10.1016/j.robot.2020.103567.
Bampis L, 2018, INT J ROBOT RES, V37, P62, DOI 10.1177/0278364917740639.
Bampis L, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4530, DOI 10.1109/IROS.2016.7759667.
Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007.
Blanco JL, 2009, AUTON ROBOT, V27, P327, DOI 10.1007/s10514-009-9138-7.
Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033.
Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754.
Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1\_56.
CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142.
Company-Corcoles Joan P., 2020, 2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA), P1313, DOI 10.1109/ETFA46521.2020.9212133.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cummins M, 2007, IEEE INT CONF ROBOT, P2042, DOI 10.1109/ROBOT.2007.363622.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Durbin R., 1998, BIOL SEQUENCE ANAL P.
Falliat D, 2007, IEEE INT CONF ROBOT, P3921.
Fritzke B., 1995, Advances in Neural Information Processing Systems 7, P625.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Garcia-Fidalgo Emilio, 2018, IEEE Robotics and Automation Letters, V3, P3051, DOI 10.1109/LRA.2018.2849609.
Garcia-Fidalgo E, 2017, IEEE T ROBOT, V33, P1061, DOI 10.1109/TRO.2017.2704598.
Garcia-Fidalgo E, 2015, ROBOT AUTON SYST, V64, P1, DOI 10.1016/j.robot.2014.11.009.
Gehrig Mathias, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3192, DOI 10.1109/ICRA.2017.7989362.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Hansen P, 2014, IEEE INT C INT ROBOT, P4549, DOI 10.1109/IROS.2014.6943207.
Hiemstra D., 2000, International Journal on Digital Libraries, V3, P131, DOI 10.1007/s007999900025.
Ho KL, 2006, ROBOT AUTON SYST, V54, P740, DOI 10.1016/j.robot.2006.04.016.
Kansizoglou I., 2020, ARXIV PREPRINT ARXIV.
Kawewong A, 2011, INT J ROBOT RES, V30, P33, DOI 10.1177/0278364910371855.
Kazmi SMAM, 2019, IEEE T ROBOT, V35, P1352, DOI 10.1109/TRO.2019.2926475.
Khan S, 2015, IEEE INT CONF ROBOT, P5441, DOI 10.1109/ICRA.2015.7139959.
Kostavelis I, 2015, ROBOT AUTON SYST, V66, P86, DOI 10.1016/j.robot.2014.12.006.
Labbe M, 2013, IEEE T ROBOT, V29, P734, DOI 10.1109/TRO.2013.2242375.
Lan XY, 2014, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2014.156.
Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542.
Liu Y, 2012, IEEE INT CONF ROBOT, P3613, DOI 10.1109/ICRA.2012.6224741.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
LUCAS BD, 1981, P 7 INT JOINT C ART, P674.
Lynen Simon, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P303, DOI 10.1109/3DV.2014.36.
MacQueen J.B., 1967, P 5 BERKELEY S MATH, P281.
Maddern W, 2012, INT J ROBOT RES, V31, P429, DOI 10.1177/0278364912438273.
Maffra F, 2019, IEEE ROBOT AUTOM LET, V4, P1525, DOI 10.1109/LRA.2019.2895826.
Maffra F, 2018, IEEE INT CONF ROBOT, P2542.
Mei C, 2010, IEEE INT C INT ROBOT, P3738, DOI 10.1109/IROS.2010.5652266.
Milford M, 2013, INT J ROBOT RES, V32, P766, DOI 10.1177/0278364913490323.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2014, IEEE INT CONF ROBOT, P846, DOI 10.1109/ICRA.2014.6906953.
Murillo AC, 2013, IEEE T ROBOT, V29, P146, DOI 10.1109/TRO.2012.2220211.
Neubert P, 2019, IEEE ROBOT AUTOM LET, V4, P3200, DOI 10.1109/LRA.2019.2927096.
Nicosevici T, 2012, IEEE T ROBOT, V28, P886, DOI 10.1109/TRO.2012.2192013.
Papapetros IT, 2020, INT CONF UNMAN AIRCR, P1206, DOI 10.1109/ICUAS48674.2020.9213923.
Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0\_1.
Rosten E, 2005, IEEE I CONF COMP VIS, P1508.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Schlegel D, 2018, IEEE ROBOT AUTOM LET, V3, P3741, DOI 10.1109/LRA.2018.2856542.
Senst T, 2012, IEEE T CIRC SYST VID, V22, P1377, DOI 10.1109/TCSVT.2012.2202070.
Shahbazi H, 2011, IEEE INT C INT ROBOT, P1228, DOI 10.1109/IROS.2011.6048862.
Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.
Stumm ES, 2016, INT J ROBOT RES, V35, P334, DOI 10.1177/0278364915570140.
Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986.
Talbot B, 2018, IEEE INT C INT ROBOT, P7758, DOI 10.1109/IROS.2018.8593761.
Tsintotas K.A., 2021, IET COMPUT VIS.
Tsintotas K.A., 2018, P INT C ROB ALP ADR, P580, DOI DOI 10.1007/978-3-030-00232-9\_61.
Tsintotas KA, 2019, LECT NOTES COMPUT SC, V11754, P75, DOI 10.1007/978-3-030-34995-0\_7.
Tsintotas KA, 2019, IEEE ROBOT AUTOM LET, V4, P1737, DOI 10.1109/LRA.2019.2897151.
Tsintotas KA, 2018, IEEE INT CONF ROBOT, P5979, DOI 10.1109/ICRA.2018.8461146.
Yue HS, 2019, IEEE INT C INT ROBOT, P3787, DOI 10.1109/IROS40897.2019.8967726.
Zhang G, 2016, IEEE INT CONF ROBOT, P765, DOI 10.1109/ICRA.2016.7487205.
Zhang GF, 2016, IEEE T IMAGE PROCESS, V25, P5957, DOI 10.1109/TIP.2016.2607425.
Zobel J, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1132956.1132959.},
  da = {2022-05-17},
  doc-delivery-number = {SA8KW},
  earlyaccessdate = {APR 2021},
  eissn = {1872-793X},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Loop-closure detection; Mapping; Recognition; SLAM; Visual-based
navigation},
  keywords-plus = {PLACE RECOGNITION; PROBABILISTIC LOCALIZATION; LARGE-SCALE; FAB-MAP;
SLAM; ONLINE; VISION},
  language = {English},
  number-of-cited-references = {84},
  orcid-numbers = {Tsintotas, Konstantinos A./0000-0002-1808-2601
Gasteratos, Antonios/0000-0002-5421-0332
Tsintotas, Konstantinos A./0000-0002-1808-2601
Bampis, Loukas/0000-0001-7764-4646},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  researcherid-numbers = {Tsintotas, Konstantinos A./ACG-1650-2022
Gasteratos, Antonios/AAI-4740-2021
Tsintotas, Konstantinos A./AAG-2765-2022},
  times-cited = {5},
  type = {Article},
  unique-id = {WOS:000649552900010},
  usage-count-last-180-days = {7},
  usage-count-since-2013 = {16},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{ikeda-tanaka:2010:5509579,
  author = {K. Ikeda and K. Tanaka},
  booktitle = {2010 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {Visual Robot Localization Using Compact Binary Landmarks},
  pages = {4397--4403},
  doi = {10.1109/ROBOT.2010.5509579},
  note = {IEEE International Conference on Robotics and Automation (ICRA),
Anchorage, AK, MAY 03-08, 2010},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2010},
  abstract = {This paper is concerned with the problem of mobile robot localization
using a novel compact representation of visual landmarks. With recent
progress in lifelong map-learning as well as in information sharing
networks, compact representation of a large-size landmark database has
become crucial. In this paper, we propose a compact binary code (e.g.
32bit code) landmark representation by employing the semantic hashing
technique from web-scale image retrieval. We show how well such a binary
representation achieves compactness of a landmark database while
maintaining efficiency of the localization system. In our contribution,
we investigate the cost-performance, the semantic gap, the saliency
evaluation using the presented techniques as well as challenge to
further reduce the resources (\#bits) per landmark. Experiments using a
high-speed car-like robot show promising results.},
  affiliation = {Ikeda, K (Corresponding Author), Univ Fukui, Fac Engn, Fukui, Japan.
Ikeda, Kouichirou; Tanaka, Kanji, Univ Fukui, Fac Engn, Fukui, Japan.},
  affiliations = {University of Fukui},
  author-email = {tnkknj@u-fukui.ac.jp},
  book-group-author = {IEEE},
  cited-references = {Angeli A, 2008, IEEE INT CONF ROBOT, P1842, DOI 10.1109/ROBOT.2008.4543475.
BIBER P, 2005, P ROB SCI SYST 1.
CSURKA G, 2004, ECCV2004 WORKSH STAT.
Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
Dellaert F, 1999, ICRA `99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1322, DOI 10.1109/ROBOT.1999.772544.
Doucet A., 2001, SEQUENTIAL MONTE CAR.
Douze M, 2009, P INT C IM VID RETR.
GIONIS A, 1999, P VER LARG DAT C.
HAYS J, 2008, IEEE COMP VIS PATT R.
Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647.
HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2.
JENSFELT P, 1999, T IEEE ROB AUT, P13.
LENSER S, 2002, P IEEE INT C ROB AUT, P1225.
Lowe D. G., 1999, P 7 IEEE INT C COMP, DOI DOI 10.1109/ICCV.1999.790410.
MONTEMERLO M, 2003, THESIS CARNEGIE MELL.
NEWMAN P, 2008, P ROB SCI SYST 4.
NISTER D, 2006, P IEEE C COMP VIS PA, P2161.
Nister D, 2006, J FIELD ROBOT, V23, P3, DOI 10.1002/rob.20103.
Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724.
ROSENCRANTZ M, 2003, P UAI.
Russell B., LABELME OPEN ANNOTAT.
Saeki Kenichi, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P3523, DOI 10.1109/ROBOT.2009.5152201.
Sala P, 2006, IEEE T ROBOT, V22, P334, DOI 10.1109/TRO.2005.861480.
SALAKHUTDINOV R, 2008, INT J APPROXIMATE RE.
SALTON, 1991, SCIENCE, P253.
Schindler G., 2007, 2007 IEEE C COMP VIS, P1.
Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950.
SUKHATME GS, 2007, RSS07 WORKSH.
Tanaka K, 2008, IEEE INT CONF ROBOT, P2784, DOI 10.1109/ROBOT.2008.4543632.
Vlassis N, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P7, DOI 10.1109/ROBOT.2002.1013331.
Williams BT, 2007, ROUTL STUD LITERACY, V3, P1.},
  da = {2022-05-17},
  doc-delivery-number = {BSD39},
  eissn = {2577-087X},
  isbn = {978-1-4244-5040-4},
  issn = {1050-4729},
  language = {English},
  number-of-cited-references = {31},
  research-areas = {Automation \& Control Systems; Engineering; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {9},
  type = {Proceedings Paper},
  unique-id = {WOS:000284150003008},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {3},
  web-of-science-categories = {Automation \& Control Systems; Engineering, Electrical \& Electronic;
Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{mactavish-et-al:2018:21838,
  author = {K. MacTavish and M. Paton and T. D. Barfoot},
  journal = {JOURNAL OF FIELD ROBOTICS},
  title = {Selective memory: Recalling relevant experience for long-term visual
localization},
  volume = {35},
  number = {8,SI},
  pages = {1265--1292},
  doi = {10.1002/rob.21838},
  publisher = {WILEY},
  address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  year = {2018},
  month = {12},
  abstract = {Visual navigation is a key enabling technology for autonomous mobile
vehicles. The ability to provide large-scale, long-term navigation using
low-cost, low-power vision sensors is appealing for industrial
applications. A crucial requirement for long-term navigation systems is
the ability to localize in environments whose appearance is constantly
changing over time-due to lighting, weather, seasons, and physical
changes. This paper presents a multiexperience localization (MEL) system
that uses a powerful map representation-storing every visual experience
in layers-that does not make assumptions about underlying appearance
modalities and generators. Our localization system provides real-time
performance by selecting online, a subset of experiences against which
to localize. We achieve this task through a novel experience-triage
algorithm based on collaborative filtering, which selects experiences
relevant to the live view, outperforming competing techniques. Based on
classical memory-based recommender systems, this technique also enables
landmark-level recommendations, is entirely online, and requires no
training data. We demonstrate the capabilities of the MEL system in the
context of long-term autonomous path following in unstructured outdoor
environments with a challenging 100-day field experiment through day,
night, snow, spring, and summer. We furthermore provide offline analysis
comparing our system to several state-of-the-art alternatives. We show
that the combination of the novel methods presented in this paper enable
full use of incredibly rich multiexperience maps, opening the door to
robust long-term visual localization.},
  affiliation = {MacTavish, K (Corresponding Author), Univ Toronto, Inst Aerosp Studies, 4925 Dufferin St, N York, ON M3H 5T6, Canada.
MacTavish, Kirk; Paton, Michael; Barfoot, Timothy D., Univ Toronto, Inst Aerosp Studies, Fac Appl Sci \& Engn, Toronto, ON, Canada.},
  affiliations = {University of Toronto},
  author-email = {kirk.mactavish@mail.utoronto.ca},
  cited-references = {Agarwal P, 2013, IEEE INT CONF ROBOT, P62, DOI 10.1109/ICRA.2013.6630557.
Anderson S, 2015, IEEE INT C INT ROBOT, P157, DOI 10.1109/IROS.2015.7353368.
Barfoot, 2014, ICRA WORKSH VIS PLAC, P1.
Barfoot T., 2017, STATE ESTIMATION ROB.
Barfoot TD, 2016, SPRINGER TRAC ADV RO, V114, P487, DOI 10.1007/978-3-319-28872-7\_28.
Barfoot TD, 2014, IEEE T ROBOT, V30, P679, DOI 10.1109/TRO.2014.2298059.
Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
Carlevaris-Bianco N, 2014, IEEE INT C INT ROBOT, P2769, DOI 10.1109/IROS.2014.6942941.
Chum O, 2003, LECT NOTES COMPUT SC, V2781, P236.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Clement L, 2018, IEEE ROBOT AUTOM LET, V3, P2447, DOI 10.1109/LRA.2018.2799741.
Costante G, 2016, IEEE ROBOT AUTOM LET, V1, P18, DOI 10.1109/LRA.2015.2505717.
Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dymczyk M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4572, DOI 10.1109/IROS.2016.7759673.
Furgale P, 2010, J FIELD ROBOT, V27, P534, DOI 10.1002/rob.20342.
GALLER BA, 1964, COMMUN ACM, V7, P301, DOI 10.1145/364099.364331.
Girdhar Y., 2011, 2011 Canadian Conference on Computer and Robot Vision (CRV), P191, DOI 10.1109/CRV.2011.32.
Glover A, 2012, IEEE INT CONF ROBOT, P4730, DOI 10.1109/ICRA.2012.6224843.
Johannsson H, 2013, IEEE INT CONF ROBOT, P54, DOI 10.1109/ICRA.2013.6630556.
Kaess M, 2012, INT J ROBOT RES, V31, P216, DOI 10.1177/0278364911430419.
Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336.
Klein George, 2007, P1.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Liu N, 2010, APPL MECH MATER, V29-32, P95, DOI 10.4028/www.scientific.net/AMM.29-32.95.
Lowry SM, 2014, IEEE INT CONF ROBOT, P3950, DOI 10.1109/ICRA.2014.6907432.
MacTavish Kirk, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2065, DOI 10.1109/ICRA.2017.7989238.
MacTavish K, 2016, SPRINGER TRAC ADV RO, V113, P187, DOI 10.1007/978-3-319-27702-8\_13.
MacTavish K, 2015, 2015 12TH CONFERENCE ON COMPUTER AND ROBOT VISION CRV 2015, P62, DOI 10.1109/CRV.2015.52.
Maddern W, 2015, IEEE INT CONF ROBOT, P1684, DOI 10.1109/ICRA.2015.7139414.
McManus C., 2014, ROBOTICS SCI SYSTEMS.
McManus C, 2013, J FIELD ROBOT, V30, P254, DOI 10.1002/rob.21444.
Muhlfellner P, 2016, J FIELD ROBOT, V33, P561, DOI 10.1002/rob.21595.
Neubert P, 2015, ROBOT AUTON SYST, V69, P15, DOI 10.1016/j.robot.2014.08.005.
Ostafew CJ, 2016, J FIELD ROBOT, V33, P133, DOI 10.1002/rob.21587.
Pascoe G, 2015, IEEE INT CONF ROBOT, P6366, DOI 10.1109/ICRA.2015.7140093.
Paton M., 2018, FIELD SERVICE ROBOTI, P415.
Paton M, 2017, J FIELD ROBOT, V34, P98, DOI 10.1002/rob.21669.
Paton M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1918, DOI 10.1109/IROS.2016.7759303.
POWELL MJD, 1964, COMPUT J, V7, P155, DOI 10.1093/comjnl/7.2.155.
Sunderhauf N, 2012, IEEE INT C INT ROBOT, P1879, DOI 10.1109/IROS.2012.6385590.
Sunderhauf N., 2015, P ROB SCI SYST 12.
Teynor A, 2007, LECT NOTES COMPUT SC, V4841, P610.
TRAHANIAS P, 1989, PATTERN RECOGN, V22, P449, DOI 10.1016/0031-3203(89)90053-8.
Wolcott RW, 2014, IEEE INT C INT ROBOT, P176, DOI 10.1109/IROS.2014.6942558.},
  da = {2022-05-17},
  doc-delivery-number = {HA2OG},
  eissn = {1556-4967},
  funding-acknowledgement = {Clearpath Robotics; NSERC Canadian Field Robotics Network (NCFRN);
Natural Sciences and Engineering Research Council (NSERC)},
  funding-text = {Clearpath Robotics; NSERC Canadian Field Robotics Network (NCFRN);
Natural Sciences and Engineering Research Council (NSERC)},
  issn = {1556-4959},
  journal-iso = {J. Field Robot.},
  keywords = {mapping; position estimation; terrestrial robotics},
  keywords-plus = {NAVIGATION; REPEAT; TEACH},
  language = {English},
  number-of-cited-references = {45},
  research-areas = {Robotics},
  times-cited = {6},
  type = {Article},
  unique-id = {WOS:000450077900005},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {12},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{wang-et-al:2019:8793499,
  author = {K. Wang and Y. Lin and L. Wang and L. Han and H. Minjie and X. Wang and S. Lian and B. Huang},
  booktitle = {2019 INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {A Unified Framework for Mutual Improvement of SLAM and Semantic
Segmentation},
  pages = {5224--5230},
  doi = {10.1109/ICRA.2019.8793499},
  note = {IEEE International Conference on Robotics and Automation (ICRA),
Montreal, CANADA, MAY 20-24, 2019},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2019},
  abstract = {This paper presents a novel framework for simultaneously implementing
localization and segmentation, which are two of the most important
vision-based tasks for robotics. While the goals and techniques used for
them were considered to be different previously, we show that by making
use of the intermediate results of the two modules, their performance
can be enhanced at the same time. Our framework is able to handle both
the instantaneous motion and long-term changes of instances in
localization with the help of the segmentation result, which also
benefits from the refined 3D pose information. We conduct experiments on
various datasets, and prove that our framework works effectively on
improving the precision and robustness of the two tasks and outperforms
existing localization and segmentation algorithms.},
  affiliation = {Wang, K (Corresponding Author), CloudMinds Technol Inc, Beijing 100102, Peoples R China.
Wang, Kai; Lin, Yimin; Wang, Luowei; Han, Liming; Hua, Minjie; Wang, Xiang; Lian, Shiguo; Huang, Bill, CloudMinds Technol Inc, Beijing 100102, Peoples R China.},
  author-email = {kai.wang@cloudminds.com
anson.lin@cloudminds.com
luowei.wang@cloudminds.com
liming.han@cloudminds.com
michael.hua@cloudminds.com
xiang.wang@cloudminds.com
scott.lian@cloudminds.com
bill@cloudminds.com},
  book-group-author = {IEEE},
  cited-references = {Badrinarayanan V., 2015, ARXIV151100561.
Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039.
Chen L.C., 2018, ARXIV180202611.
Chen L.C., 2017, ARXIV170605587.
Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184.
Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261.
Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343.
Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510.
GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81.
Guo YM, 2018, INT J MULTIMED INF R, V7, P87, DOI 10.1007/s13735-017-0141-z.
Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0\_20.
He Kaiming, 2018, IEEE T PATTERN ANAL.
Kaneko M., 2018, P IEEE C COMP VIS PA, P258.
Klappstein J, 2009, LECT NOTES COMPUT SC, V5414, P611.
Klein George, 2007, P1.
Li Y., 2016, ARXIV161107709.
Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1\_48.
Liu Peidong, 2018, P IEEE INT C ROB AUT.
Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504.
Reddy ND, 2015, IEEE INT C INT ROBOT, P1897, DOI 10.1109/IROS.2015.7353626.
Ren SQ, 2015, ADV NEUR IN, V28.
Saputra MRU, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3177853.
Shah S., 2017, ARXIV.
Shi XS, 2015, 2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P802, DOI 10.1109/IAEAC.2015.7428667.
Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773.
Taketomi T., 2017, IPSJ T COMPUTER VISI, V9, P1, DOI {[}10.1186/s41074-016-0012-1, DOI 10.1186/S41074-017-0027-2, 10.1186/s41074-017-0027-2].
Wu T, 2018, IEEE SIGNAL PROC LET, V25, P1196, DOI 10.1109/LSP.2018.2849590.
Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660.
Zhong FW, 2018, IEEE WINT CONF APPL, P1001, DOI 10.1109/WACV.2018.00115.},
  da = {2022-05-17},
  doc-delivery-number = {BO1CN},
  editor = {Howard, A and Althoefer, K and Arai, F and Arrichiello, F and Caputo, B and Castellanos, J and Hauser, K and Isler, V and Kim, J and Liu, H and Oh, P and Santos, V and Scaramuzza, D and Ude, A and Voyles, R and Yamane, K and Okamura, A},
  eissn = {2577-087X},
  isbn = {978-1-5386-6026-3},
  issn = {1050-4729},
  language = {English},
  number-of-cited-references = {31},
  oa = {Green Submitted},
  research-areas = {Automation \& Control Systems; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {7},
  type = {Proceedings Paper},
  unique-id = {WOS:000494942303113},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {5},
  web-of-science-categories = {Automation \& Control Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{zhang-et-al:2022:3086822,
  author = {K. Zhang and X. Jiang and J. Ma},
  journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS},
  title = {Appearance-Based Loop Closure Detection via Locality-Driven Accurate
Motion Field Learning},
  volume = {23},
  number = {3},
  pages = {2350--2365},
  doi = {10.1109/TITS.2021.3086822},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2022},
  month = {3},
  abstract = {Loop closure detection (LCD) is of significant importance in
simultaneous localization and mapping. It represents the robot's ability
to recognize whether the current surrounding corresponds to a previously
observed one. In this paper, we conduct this task in a two-step
strategy: candidate frame selection and loop closure verification. The
first step aims to search semantically similar images for the query one
using features obtained by Key.Net with HardNet. Instead of adopting the
traditional Bag-of-Words strategy, we utilize the aggregated selective
match kernel to calculate the similarity between images. Subsequently,
based on the potential property of motion field in the LCD scene, we
propose a novel feature matching method, i.e., exploiting the smoothness
prior and learning the motion field for an image pair in a reproducing
kernel Hilbert space (RKHS), to implement loop closure verification.
Concretely, we formulate the learning problem into a Bayesian framework
with latent variables indicating the true/false correspondences and a
mixture model accounting for the distribution of data. Furthermore, we
propose a locality-driven mechanism to enhance the local relevance of
motion vectors and term the algorithm as locality-driven accurate motion
field learning (LAL). To satisfy the requirement of efficiency in the
LCD task, we use a sparse approximation and search a suboptimal solution
for the motion field in the RKHS, termed as LAL{*}. Extensive
experiments are conducted on public datasets for feature matching and
LCD tasks. The quantitative results demonstrate the effectiveness of our
method over the current state-of-the-art, meanwhile showing its
potential for long-term visual localization. The codes of LAL and LAL{*}
are publicly available at https://github.com/KN-Zhang/LAL.},
  affiliation = {Ma, JY (Corresponding Author), Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
Zhang, Kaining; Jiang, Xingyu; Ma, Jiayi, Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.},
  affiliations = {Wuhan University},
  author-email = {zkn19961212@whu.edu.cn
jiangx.y@whu.edu.cn
jyma2010@gmail.com},
  cited-references = {An S., 2020, ARXIV201011703.
An S, 2019, IEEE INT C INT ROBOT, P378, DOI 10.1109/IROS40897.2019.8968043.
Andrew AM, 2001, KYBERNETES.
Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Bampis L, 2018, INT J ROBOT RES, V37, P62, DOI 10.1177/0278364917740639.
Barroso-Laguna A, 2019, IEEE I CONF COMP VIS, P5835, DOI 10.1109/ICCV.2019.00593.
Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302.
Bingyi Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P726, DOI 10.1007/978-3-030-58565-5\_43.
Blanco JL, 2009, AUTON ROBOT, V27, P327, DOI 10.1007/s10514-009-9138-7.
Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754.
CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142.
Choy CB, 2016, ADV NEUR IN, V29.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Nguyen DD, 2019, IEEE T INTELL TRANSP, V20, P4103, DOI 10.1109/TITS.2018.2881556.
DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060.
FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Garcia-Fidalgo Emilio, 2018, IEEE Robotics and Automation Letters, V3, P3051, DOI 10.1109/LRA.2018.2849609.
Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297.
Glover AJ, 2010, IEEE INT CONF ROBOT, P3507, DOI 10.1109/ROBOT.2010.5509547.
Han J, 2019, IEEE T INTELL TRANSP, V20, P4415, DOI 10.1109/TITS.2018.2885341.
Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57.
Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9.
Jiang XY, 2020, IEEE T IMAGE PROCESS, V29, P736, DOI 10.1109/TIP.2019.2934572.
Kazmi SMAM, 2019, IEEE T ROBOT, V35, P1352, DOI 10.1109/TRO.2019.2926475.
Khan S, 2015, IEEE INT CONF ROBOT, P5441, DOI 10.1109/ICRA.2015.7139959.
Krizhevsky A, 2012, ADV NEURAL INFORM PR, P1097, DOI 10.1145/3065386.
Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542.
Li XR, 2010, INT J COMPUT VISION, V89, P1, DOI 10.1007/s11263-010-0318-x.
Liu HR, 2010, PROC CVPR IEEE, P1609, DOI 10.1109/CVPR.2010.5539780.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662.
Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2.
Ma JY, 2019, INT J COMPUT VISION, V127, P512, DOI 10.1007/s11263-018-1117-z.
Ma JY, 2019, IEEE T NEUR NET LEAR, V30, P3584, DOI 10.1109/TNNLS.2018.2872528.
Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478.
MacQueen J.B., 1967, P 5 BERKELEY S MATH, P281.
Mishchuk A., 2017, ADV NEURAL INFORM PR, P4826.
Mur-Artal R, 2014, IEEE INT CONF ROBOT, P846, DOI 10.1109/ICRA.2014.6906953.
Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374.
Perronnin F, 2007, PROC CVPR IEEE, P2272.
Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300.
Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22.
Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663.
Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649.
Tola E, 2010, IEEE T PATTERN ANAL, V32, P815, DOI 10.1109/TPAMI.2009.77.
Tolias G, 2016, INT J COMPUT VISION, V116, P247, DOI 10.1007/s11263-015-0810-4.
Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832.
Tsintotas KA, 2019, IEEE ROBOT AUTOM LET, V4, P1737, DOI 10.1109/LRA.2019.2897151.
Tsintotas KA, 2018, IEEE INT CONF ROBOT, P5979, DOI 10.1109/ICRA.2018.8461146.
Wang TH, 2018, IEEE INT CONF ROBOT, P2341.
Williams B, 2009, ROBOT AUTON SYST, V57, P1188, DOI 10.1016/j.robot.2009.06.010.
Xia YF, 2016, IEEE IJCNN, P2274, DOI 10.1109/IJCNN.2016.7727481.
Yan JC, 2016, IEEE T PATTERN ANAL, V38, P1228, DOI 10.1109/TPAMI.2015.2477832.
Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4\_28.
Yu J, 2020, IEEE T NEUR NET LEAR, V31, P661, DOI 10.1109/TNNLS.2019.2908982.
Zetao Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3223, DOI 10.1109/ICRA.2017.7989366.
Zheng YF, 2006, IEEE T PATTERN ANAL, V28, P643, DOI 10.1109/TPAMI.2006.81.
Zhou B., 2014, P ADV NEUR INF PROC, P487, DOI DOI 10.1162/153244303322533223.},
  da = {2022-05-17},
  doc-delivery-number = {ZR6BI},
  earlyaccessdate = {JUN 2021},
  eissn = {1558-0016},
  funding-acknowledgement = {National Natural Science Foundation of China {[}61773295]; Key Research
and Development Program of Hubei Province {[}2020BAB113]; Natural
Science Foundation of Hubei Province {[}2019CFA037]},
  funding-text = {This work was supported in part by the National Natural Science
Foundation of China under Grant 61773295, in part by the Key Research
and Development Program of Hubei Province under Grant 2020BAB113, and in
part by the Natural Science Foundation of Hubei Province under Grant
2019CFA037. The Associate Editor for this article was T.-H. Kim.},
  issn = {1524-9050},
  journal-iso = {IEEE Trans. Intell. Transp. Syst.},
  keywords = {Feature extraction; Liquid crystal displays; Visualization; Task
analysis; Robots; Simultaneous localization and mapping; Kernel; SLAM;
loop closure detection; place recognition; feature matching; autonomous
vehicle},
  keywords-plus = {PLACE RECOGNITION; FAB-MAP; IMAGE; LOCALIZATION; KERNELS; VISION;
SEARCH; SCALE; SLAM},
  language = {English},
  number-of-cited-references = {64},
  orcid-numbers = {Zhang, Kaining/0000-0002-2033-2307
Jiang, Xingyu/0000-0001-9790-8856
Ma, Jiayi/0000-0003-3264-3265},
  research-areas = {Engineering; Transportation},
  times-cited = {3},
  type = {Article},
  unique-id = {WOS:000733469200001},
  usage-count-last-180-days = {8},
  usage-count-since-2013 = {8},
  web-of-science-categories = {Engineering, Civil; Engineering, Electrical \& Electronic;
Transportation Science \& Technology},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{clement-et-al:2020:2967659,
  author = {L. Clement and M. Gridseth and J. Tomasi and J. Kelly},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {Learning Matchable Image Transformations for Long-Term Metric Visual
Localization},
  volume = {5},
  number = {2},
  pages = {1492--1499},
  doi = {10.1109/LRA.2020.2967659},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2020},
  month = {4},
  abstract = {Long-term metric self-localization is an essential capability of
autonomous mobile robots, but remains challenging for vision-based
systems due to appearance changes caused by lighting, weather, or
seasonal variations. While experience-based mapping has proven to be an
effective technique for bridging the `appearance gap,' the number of
experiences required for reliable metric localization over days or
months can be very large, and methods for reducing the necessary number
of experiences are needed for this approach to scale. Taking inspiration
from color constancy theory, we learn a nonlinear RGB-to-grayscale
mapping that explicitly maximizes the number of inlier feature matches
for images captured under different lighting and weather conditions, and
use it as a pre-processing step in a conventional single-experience
localization pipeline to improve its robustness to appearance change. We
train this mapping by approximating the target non-differentiable
localization pipeline with a deep neural network, and find that
incorporating a learned low-dimensional context feature can further
improve cross-appearance feature matching. Using synthetic and
real-world datasets, we demonstrate substantial improvements in
localization performance across day-night cycles, enabling continuous
metric localization over a 30-hour period using a single mapping
experience, and allowing experience-based localization to scale to long
deployments with dramatically reduced data requirements.},
  affiliation = {Clement, L (Corresponding Author), Univ Toronto, Inst Aerosp Studies UTIAS, Space \& Terr Autonomous Robot Syst STARS Lab, Toronto, ON M3H 5T6, Canada.
Clement, Lee; Tomasi, Justin; Kelly, Jonathan, Univ Toronto, Inst Aerosp Studies UTIAS, Space \& Terr Autonomous Robot Syst STARS Lab, Toronto, ON M3H 5T6, Canada.
Gridseth, Mona, UTIAS, ASRL, N York, ON M3H 5T6, Canada.},
  affiliations = {University of Toronto},
  author-email = {lee.clement@mail.utoronto.ca
mona.gridseth@robotics.utias.utoronto.ca
justin.tomasi@mail.utoronto.ca
jkelly@utias.utoronto.ca},
  cited-references = {Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI 10.1109/ICRA.2019.8794387.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Clement L, 2018, IEEE ROBOT AUTOM LET, V3, P2447, DOI 10.1109/LRA.2018.2799741.
Clement L, 2017, J FIELD ROBOT, V34, P74, DOI 10.1002/rob.21655.
Corke P, 2013, IEEE INT C INT ROBOT, P2085, DOI 10.1109/IROS.2013.6696648.
Engel J, 2015, IEEE INT C INT ROBOT, P1935, DOI 10.1109/IROS.2015.7353631.
FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
GAIDON A, 2016, PROC CVPR IEEE, P4340, DOI DOI 10.1109/CVPR.2016.470.
Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297.
Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405.
Gomez-Ojeda R, 2018, IEEE INT CONF ROBOT, P805.
Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672.
Grzeszczuk R., 1998, P C NEUR INF PROC SY, P882.
He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123.
Ioffe S, 2015, PR MACH LEARN RES, V37, P448.
Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632.
Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6\_43.
Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90.
Kingma D.P., 2015, 3 INT C LEARNING REP.
Koziel S, 2011, STUD COMPUT INTELL, V356, P33.
Krajnik T, 2017, ROBOT AUTON SYST, V88, P127, DOI 10.1016/j.robot.2016.11.011.
Latif Y, 2018, IEEE INT CONF ROBOT, P2349, DOI 10.1109/ICRA.2018.8461081.
Linegar C, 2016, IEEE INT CONF ROBOT, P787, DOI 10.1109/ICRA.2016.7487208.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
McManus C, 2015, AUTON ROBOT, V39, P363, DOI 10.1007/s10514-015-9463-y.
McManus C, 2014, IEEE INT CONF ROBOT, P901, DOI 10.1109/ICRA.2014.6906961.
Paszke A, ADV NEURAL INFORM PR.
Paton M., 2018, FIELD SERVICE ROBOTI, P415.
Paton M, 2017, J FIELD ROBOT, V34, P98, DOI 10.1002/rob.21669.
Paton M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1918, DOI 10.1109/IROS.2016.7759303.
Peretroukhin V, 2018, IEEE ROBOT AUTOM LET, V3, P2424, DOI 10.1109/LRA.2017.2778765.
Porav H, 2018, IEEE INT CONF ROBOT, P1011, DOI 10.1109/ICRA.2018.8462894.
Ratnasingam S, 2010, J OPT SOC AM A, V27, P286, DOI 10.1364/JOSAA.27.000286.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Seonwook Park, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4523, DOI 10.1109/ICRA.2017.7989525.
van der Merwe R, 2007, NEURAL NETWORKS, V20, P462, DOI 10.1016/j.neunet.2007.04.023.
Zhang N, 2018, IEEE INT CONF ROBOT, P828, DOI 10.1109/ICRA.2018.8460674.
Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244.},
  da = {2022-05-17},
  doc-delivery-number = {LE4JU},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {Deep learning in robotics and automation; visual learning; visual-based
navigation; localization},
  keywords-plus = {NAVIGATION; VISION; TEACH},
  language = {English},
  number-of-cited-references = {40},
  oa = {Green Submitted},
  orcid-numbers = {Tomasi, Justin/0000-0002-4284-5269
Kelly, Jonathan/0000-0002-5528-6136},
  research-areas = {Robotics},
  times-cited = {3},
  type = {Article},
  unique-id = {WOS:000526686300021},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {3},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{murphy-sibley:2014:6907022,
  author = {L. Murphy and G. Sibley},
  booktitle = {2014 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {Incremental Unsupervised Topological Place Discovery},
  pages = {1312--1318},
  doi = {10.1109/ICRA.2014.6907022},
  note = {IEEE International Conference on Robotics and Automation (ICRA), Hong
Kong, PEOPLES R CHINA, MAY 31-JUN 07, 2014},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2014},
  abstract = {This paper describes an online place discovery and recognition engine
that fuses information over time to create topologically distinct
places. A key motivation is the recognition that a single image may be a
poor exemplar of what constitutes a place. Images are not `places' nor
are they `documents'. Instead, by treating image-sequences as a
multi-modal distribution over topics - and by discovering topics
incrementally and online - it is possible to both reduce the memory
footprint of place recognition systems, and to improve precision and
recall. Distinctive key-places are represented by a cluster topics found
from the covisibility graph of a relative simultaneous localization and
mapping engine - key-places inherently span many images. A dynamic
vocabulary of visual words and density based clustering is used to
continually estimate a set of visual topics, changes in which drive the
place-recognition process. The system is evaluated using an indoor robot
sequence, a standard outdoor robot sequence and a long-term sequence
from a static camera. Experiments demonstrate qualitatively distinct
themes associated with discovered places - from common place types such
as `hallway', or `desk-area', to temporal concepts such as `dusk',
`dawn' or `mid-day'. Compared to traditional image-based
place-recognition, this reduces the information that must be stored
without reducing place-recognition performance.},
  affiliation = {Murphy, L (Corresponding Author), George Washington Univ, Dept Comp Sci, Washington, DC 20052 USA.
Murphy, Liz; Sibley, Gabe, George Washington Univ, Dept Comp Sci, Washington, DC 20052 USA.},
  affiliations = {George Washington University},
  author-email = {liz\_murphy@gwu.edu},
  book-group-author = {IEEE},
  cited-references = {Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993.
Chapoulie A, 2012, IEEE INT C INT ROBOT, P4288, DOI 10.1109/IROS.2012.6385962.
Chou TC, 2008, IEEE T KNOWL DATA EN, V20, P289, DOI 10.1109/TKDE.2007.190702.
Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226.
Girdhar Y., 2012, 13 INT S EXP ROB ISE.
Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649.
Ila V, 2010, IEEE T ROBOT, V26, P78, DOI 10.1109/TRO.2009.2034435.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Korrapati H, 2012, IEEE INT CONF ROBOT, P1650, DOI 10.1109/ICRA.2012.6224892.
Kretzschmar H, 2012, INT J ROBOT RES, V31, P1219, DOI 10.1177/0278364912455072.
Mei C, 2011, INT J COMPUT VISION, V94, P198, DOI 10.1007/s11263-010-0361-7.
Nicosevici T, 2012, IEEE T ROBOT, V28, P886, DOI 10.1109/TRO.2012.2192013.
Paul Rohan, 2011, IEEE International Conference on Robotics and Automation, P445.
Paul R, 2012, IEEE INT CONF ROBOT, P4058, DOI 10.1109/ICRA.2012.6224762.
Ranganathan Ananth, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P2017, DOI 10.1109/ROBOT.2009.5152376.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.},
  da = {2022-05-17},
  doc-delivery-number = {BE9BP},
  eissn = {2577-087X},
  isbn = {978-1-4799-3685-4},
  issn = {1050-4729},
  language = {English},
  number-of-cited-references = {16},
  research-areas = {Automation \& Control Systems; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {17},
  type = {Proceedings Paper},
  unique-id = {WOS:000377221101048},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Automation \& Control Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{sun-et-al:2018:2856268,
  author = {L. Sun and Z. Yan and A. Zaganidis and C. Zhao and D. Tom},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {Recurrent-OctoMap: Learning State-Based Map Refinement for Long-Term
Semantic Mapping With 3-D-Lidar Data},
  volume = {3},
  number = {4},
  pages = {3749--3756},
  doi = {10.1109/LRA.2018.2856268},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2018},
  month = {10},
  abstract = {This letter presents a novel semantic mapping approach,
Recurrent-OctoMap, learned from long-term three-dimensional (3-D) Lidar
data. Most existing semantic mapping approaches focus on improving
semantic understanding of single frames, rather than 3-D refinement of
semantic maps (i.e. fusing semantic observations). The most widely used
approach for the 3-D semantic map refinement is ``Bayes update,{''}
which fuses the consecutive predictive probabilities following a
Markov-chain model. Instead, we propose a learning approach to fuse the
semantic features, rather than simply fusing predictions from a
classifier. In our approach, we represent and maintain our 3-D map as an
OctoMap, and model each cell as a recurrent neural network, to obtain a
Recurrent-OctoMap. In this case, the semantic mapping process can he
formulated as a sequence-to-sequence encoding-decoding problem.
Moreover, in order to extend the duration of observations in our
Recurrent-OctoMap, we developed a robust 3-D localization and mapping
system for successively mapping a dynamic environment using more than
two weeks of data, and the system can he trained and deployed with
arbitrary memory length. We validate our approach on the ETH long-term
3-D Lidar dataset. The experimental results show that our proposed
approach outperforms the conventional ``Bayes update{''} approach.},
  affiliation = {Sun, L (Corresponding Author), Univ Lincoln, L CAS, Lincoln LN6 7TS, England.
Sun, Li; Zaganidis, Anestis; Zhao, Cheng; Duckett, Tom, Univ Lincoln, L CAS, Lincoln LN6 7TS, England.
Yan, Zhi, UTBM, Lab Elect Informat \& Image, CNRS, F-90010 Belfort, France.},
  affiliations = {University of Lincoln; Centre National de la Recherche Scientifique
(CNRS); Universite de Bourgogne; Universite de Technologie de
Belfort-Montbeliard (UTBM)},
  author-email = {lsun@lincoln.ac.uk
zhi.yan@utbm.fr
azaganidis@lincoln.ac.uk
czhao@lincoln.ac.uk
tduckett@lincoln.ac.uk},
  cited-references = {Bogoslavskyi I, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P163, DOI 10.1109/IROS.2016.7759050.
Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691.
Cho Kyunghyun, 2014, COMPUT SCI, DOI DOI 10.3115/V1/D14-1179.
De Deuge M., 2013, P AUSTR C ROB AUT, V2, P1.
Dewan A, 2016, IEEE INT CONF ROBOT, P4508, DOI 10.1109/ICRA.2016.7487649.
Einhorn E, 2015, ROBOT AUTON SYST, V69, P28, DOI 10.1016/j.robot.2014.08.008.
ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402\_1.
Engelmann F, 2017, IEEE INT CONF COMP V, P716, DOI 10.1109/ICCVW.2017.90.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Hermans A, 2014, IEEE INT CONF ROBOT, P2631, DOI 10.1109/ICRA.2014.6907236.
Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735.
Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0.
Kidono K, 2011, IEEE INT VEH SYM, P405, DOI 10.1109/IVS.2011.5940433.
Krajnik T, 2017, IEEE T ROBOT, V33, P964, DOI 10.1109/TRO.2017.2665664.
Li B, 2017, IEEE INT C INT ROBOT, P1513, DOI 10.1109/IROS.2017.8205955.
Ma LN, 2017, IEEE INT C INT ROBOT, P598, DOI 10.1109/IROS.2017.8202213.
Maturana Daniel, 2018, FIELD SERVICE ROBOTI, P335, DOI DOI 10.1007/978-3-319-67361-5.
McCormac John, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4628, DOI 10.1109/ICRA.2017.7989538.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Navarro-Serment L. E., 2009, P C FIELD SERV ROB, P1516.
Pomerleau F, 2014, IEEE INT CONF ROBOT, P3712, DOI 10.1109/ICRA.2014.6907397.
Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16.
Sebastian T., PROBABILISTIC ROBOTI.
Sengupta S, 2013, IEEE INT CONF ROBOT, P580, DOI 10.1109/ICRA.2013.6630632.
Sengupta S, 2012, IEEE INT C INT ROBOT, P857, DOI 10.1109/IROS.2012.6385958.
Spinello L, 2011, IEEE INT CONF ROBOT, P1304, DOI 10.1109/ICRA.2011.5980085.
Sun L, 2018, IEEE INT CONF ROBOT, P5942, DOI 10.1109/ICRA.2018.8461228.
Sun L, 2016, IEEE INT CONF ROBOT, P2464, DOI 10.1109/ICRA.2016.7487399.
Sun L, 2015, IEEE INT CONF ROBOT, P185, DOI 10.1109/ICRA.2015.7138998.
Tateno K, 2016, IEEE INT CONF ROBOT, P2295, DOI 10.1109/ICRA.2016.7487378.
Wu BC, 2018, IEEE INT CONF ROBOT, P1887.
Xiang Y., 2017, P ROB SCI SYST RSS.
Xiong T, 2014, BIOMED CIRC SYST C, P9, DOI 10.1109/BioCAS.2014.6981632.
Yan Z, 2017, IEEE INT C INT ROBOT, P864, DOI 10.1109/IROS.2017.8202247.
Zaganidis A, 2018, IEEE ROBOT AUTOM LET, V3, P2942, DOI 10.1109/LRA.2018.2848308.
Zhao C., ARXIV171000132.
Zhao C, 2017, 2017 18TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P75, DOI 10.1109/ICAR.2017.8023499.},
  da = {2022-05-17},
  doc-delivery-number = {GQ1ZW},
  funding-acknowledgement = {European Union {[}732737, 645376]},
  funding-text = {This work was supported by the European Union's Horizon 2020 research
and innovation programme under Grant 732737 (ILIAD) and 645376 (FLOBOT).},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {Mapping; simultaneous localization and mapping (SLAM); deep learning in
robotics and automation; object detection; segmentation and
categorization},
  keywords-plus = {SEGMENTATION},
  language = {English},
  number-of-cited-references = {37},
  oa = {Green Accepted, Green Submitted},
  orcid-numbers = {Yan, Zhi/0000-0001-8251-9786
Sun, Li/0000-0002-0393-8665
Zhao, Cheng/0000-0001-8502-3233},
  research-areas = {Robotics},
  researcherid-numbers = {Yan, Zhi/W-5265-2019},
  times-cited = {28},
  type = {Article},
  unique-id = {WOS:000441444700010},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {29},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{tang-et-al:2021:17298814211037497,
  author = {L. Tang and Y. Wang and Q. Tan and R. Xiong},
  journal = {INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS},
  title = {Explicit feature disentanglement for visual place recognition across
appearance changes},
  volume = {18},
  number = {6},
  doi = {10.1177/17298814211037497},
  publisher = {SAGE PUBLICATIONS INC},
  address = {2455 TELLER RD, THOUSAND OAKS, CA 91320 USA},
  year = {2021},
  month = {11},
  abstract = {In the long-term deployment of mobile robots, changing appearance brings
challenges for localization. When a robot travels to the same place or
restarts from an existing map, global localization is needed, where
place recognition provides coarse position information. For visual
sensors, changing appearances such as the transition from day to night
and seasonal variation can reduce the performance of a visual place
recognition system. To address this problem, we propose to learn
domain-unrelated features across extreme changing appearance, where a
domain denotes a specific appearance condition, such as a season or a
kind of weather. We use an adversarial network with two discriminators
to disentangle domain-related features and domain-unrelated features
from images, and the domain-unrelated features are used as descriptors
in place recognition. Provided images from different domains, our
network is trained in a self-supervised manner which does not require
correspondences between these domains. Besides, our feature extractors
are shared among all domains, making it possible to contain more
appearance without increasing model complexity. Qualitative and
quantitative results on two toy cases are presented to show that our
network can disentangle domain-related and domain-unrelated features
from given data. Experiments on three public datasets and one proposed
dataset for visual place recognition are conducted to illustrate the
performance of our method compared with several typical algorithms.
Besides, an ablation study is designed to validate the effectiveness of
the introduced discriminators in our network. Additionally, we use a
four-domain dataset to verify that the network can extend to multiple
domains with one model while achieving similar performance.},
  affiliation = {Wang, Y (Corresponding Author), Zhejiang Univ, Dept Control Sci \& Engn, Hangzhou 30012, Peoples R China.
Tang, Li; Wang, Yue; Xiong, Rong, Zhejiang Univ, Dept Control Sci \& Engn, Hangzhou 30012, Peoples R China.
Tan, Qimeng, Beijing Inst Spacecraft Syst Engn, Beijing Key Lab Intelligent Space Robot Syst Tech, Beijing, Peoples R China.},
  affiliations = {Zhejiang University},
  article-number = {17298814211037497},
  author-email = {ywang24@zju.edu.cn},
  cited-references = {Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI 10.1109/ICRA.2019.8794387.
Ba J.L., 2016, ARXIV160706450.
Bresson G, 2017, IEEE T INTELL VEHICL, V2, P194, DOI 10.1109/TIV.2017.2749181.
Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754.
Chen ZT, 2018, IEEE ROBOT AUTOM LET, V3, P4015, DOI 10.1109/LRA.2018.2859916.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Clement L, 2018, IEEE ROBOT AUTOM LET, V3, P2447, DOI 10.1109/LRA.2018.2799741.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848.
Doan AD, 2019, IEEE I CONF COMP VIS, P9318, DOI 10.1109/ICCV.2019.00941.
Facil JM, 2019, ARXIV190209516.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Gao X, 2017, AUTON ROBOT, V41, P1, DOI 10.1007/s10514-015-9516-2.
Garg S., 2019, INT J ROBOT RES, DOI {[}10.1177/0278364919839761, DOI 10.1177/0278364919839761].
Garg S, 2020, IEEE INT CONF ROBOT, P3341, DOI 10.1109/ICRA40945.2020.9196827.
Gomezojeda Ruben, 2015, COMPUTER SCI.
Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672.
Hausler S, 2020, IEEE INT CONF ROBOT, P3327, DOI 10.1109/ICRA40945.2020.9197360.
Hausler S, 2019, IEEE INT C INT ROBOT, P3268, DOI 10.1109/IROS40897.2019.8967783.
Higgins I, 2018, ARXIV181202230.
Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647.
Hong ZY, 2019, IEEE I CONF COMP VIS, P2861, DOI 10.1109/ICCV.2019.00295.
Hu HJ, 2019, IEEE INT C INT ROBOT, P3684, DOI 10.1109/IROS40897.2019.8968047.
Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048.
Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632.
Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039.
Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6\_43.
Kanji, 2019, ARXIV PREPRINT ARXIV.
Khaliq A, 2020, IEEE T ROBOT, V36, P561, DOI 10.1109/TRO.2019.2956352.
Kingma D. P., 2014, 3 INT C LEARN REPR, DOI DOI 10.1145/1830483.1830503.
Krizhevsky A, 2012, ADV NEURAL INFORM PR, P1097, DOI 10.1145/3065386.
Latif Y, 2018, IEEE INT CONF ROBOT, P2349, DOI 10.1109/ICRA.2018.8461081.
Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791.
Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z.
Leonard J., 2019, LEARN LOC MAPP WOKSH.
Li FY, 2006, IEEE INT CONF ROBOT, P3405, DOI 10.1109/ROBOT.2006.1642222.
Liu MY, 2017, ADV NEUR IN, V30.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Lowry S, 2016, IEEE T ROBOT, V32, P600, DOI 10.1109/TRO.2016.2545711.
Maas A. L., 2013, P INT C MACH LEARN, DOI DOI 10.1016/0010-0277(84)90022-2.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
Mao Xudong, 2017, P IEEE INT C COMP VI, P2794.
McDonald-Maier, 2019, ARXIV PREPRINT ARXIV.
Merrill N, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mukherjee A, 2017, LECT NOTES COMPUT SC, V10597, P557, DOI 10.1007/978-3-319-69900-4\_71.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Murillo AC, 2013, IEEE T ROBOT, V29, P146, DOI 10.1109/TRO.2012.2220211.
Nowicki MR, 2017, WIRELESS PERS COMMUN, V97, P213, DOI 10.1007/s11277-017-4502-y.
Porav H, 2018, IEEE INT CONF ROBOT, P1011, DOI 10.1109/ICRA.2018.8462894.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897.
Schindler G., 2007, 2007 IEEE C COMP VIS, P1.
Schlegel D, 2018, IEEE ROBOT AUTOM LET, V3, P3741, DOI 10.1109/LRA.2018.2856542.
Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682.
Schubert Stefan, 2020, 2020 IEEE International Conference on Robotics and Automation (ICRA), P4372, DOI 10.1109/ICRA40945.2020.9197044.
Shanmugam Akshaya, 2015, 2015 41st Annual Northeast Biomedical Engineering Conference (NEBEC). Proceedings, P1, DOI 10.1109/NEBEC.2015.7117195.
Simonyan K, 2014, C TRACK P.
Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986.
Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.
Tang L, 2020, IEEE INT CONF ROBOT, P1301, DOI 10.1109/ICRA40945.2020.9196518.
Tang L, 2019, AUTON ROBOT, V43, P197, DOI 10.1007/s10514-018-9724-7.
Torii A., P IEEE C COMP VIS PA, P5297.
Torii A, 2015, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2015.7298790.
Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437.
van der Maaten L, 2008, J MACH LEARN RES, V9, P2579.
Xin Z, 2019, IEEE INT CONF ROBOT, P5979, DOI 10.1109/ICRA.2019.8794383.
Yin P, 2019, IEEE INT CONF ROBOT, P7137, DOI 10.1109/ICRA.2019.8793853.
Yin P, 2019, IEEE INT CONF ROBOT, P319, DOI 10.1109/ICRA.2019.8793752.
Yu J, 2020, IEEE T NEUR NET LEAR, V31, P661, DOI 10.1109/TNNLS.2019.2908982.
Zaffar M, 2021, IEEE T INTELL TRANSP, V22, P7355, DOI 10.1109/TITS.2020.3001228.
Zetao Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3223, DOI 10.1109/ICRA.2017.7989366.},
  da = {2022-05-17},
  doc-delivery-number = {XO3CL},
  funding-acknowledgement = {National Nature Science Foundation of China {[}61903332]; Natural
Science Foundation of Zhejiang Province {[}LGG21F030012]},
  funding-text = {The author(s) disclosed the receipt of the following financial support
for the research, authorship, and/or publication of this article: This
work was supported in part by the National Nature Science Foundation of
China under Grant 61903332, and in part by the Natural Science
Foundation of Zhejiang Province under grant number LGG21F030012.},
  issn = {1729-8814},
  journal-iso = {Int. J. Adv. Robot. Syst.},
  keywords = {Place recognition; feature disentanglement; adversarial;
self-supervised; changing appearance},
  keywords-plus = {SIMULTANEOUS LOCALIZATION; NAVIGATION; SLAM},
  language = {English},
  number-of-cited-references = {73},
  oa = {gold},
  research-areas = {Robotics},
  times-cited = {0},
  type = {Article},
  unique-id = {WOS:000730066500001},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{tang-et-al:2019:7,
  author = {L. Tang and Y. Wang and X. Ding and H. Yin and S. R. A. H. Xiong},
  journal = {AUTONOMOUS ROBOTS},
  title = {Topological local-metric framework for mobile robots navigation: a long
term perspective},
  volume = {43},
  number = {1},
  pages = {197--211},
  doi = {10.1007/s10514-018-9724-7},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2019},
  month = {1},
  abstract = {Long term mapping and localization are the primary components for mobile
robots in real world application deployment, of which the crucial
challenge is the robustness and stability. In this paper, we introduce a
topological local-metric framework (TLF), aiming at dealing with
environmental changes, erroneous measurements and achieving constant
complexity. TLF organizes the sensor data collected by the robot in a
topological graph, of which the geometry is only encoded in the edge,
i.e. the relative poses between adjacent nodes, relaxing the global
consistency to local consistency. Therefore the TLF is more robust to
unavoidable erroneous measurements from sensor information matching
since the error is constrained in the local. Based on TLF, as there is
no global coordinate, we further propose the localization and navigation
algorithms by switching across multiple local metric coordinates.
Besides, a lifelong memorizing mechanism is presented to memorize the
environmental changes in the TLF with constant complexity, as no global
optimization is required. In experiments, the framework and algorithms
are evaluated on 21-session data collected by stereo cameras, which are
sensitive to illumination, and compared with the state-of-art global
consistent framework. The results demonstrate that TLF can achieve
similar localization accuracy with that from global consistent
framework, but brings higher robustness with lower cost. The
localization performance can also be improved from sessions because of
the memorizing mechanism. Finally, equipped with TLF, the robot
navigates itself in a 1km session autonomously.},
  affiliation = {Wang, Y (Corresponding Author), Zhejiang Univ, State Key Lab Ind Control \& Technol, Hangzhou, Zhejiang, Peoples R China.
Wang, Y (Corresponding Author), Zhejiang Univ, Inst Cyber Syst \& Control, Hangzhou, Zhejiang, Peoples R China.
Wang, Y (Corresponding Author), iPlusBot, Hangzhou, Zhejiang, Peoples R China.
Tang, Li; Wang, Yue; Ding, Xiaqing; Yin, Huan; Xiong, Rong, Zhejiang Univ, State Key Lab Ind Control \& Technol, Hangzhou, Zhejiang, Peoples R China.
Tang, Li; Wang, Yue; Ding, Xiaqing; Yin, Huan; Xiong, Rong, Zhejiang Univ, Inst Cyber Syst \& Control, Hangzhou, Zhejiang, Peoples R China.
Wang, Yue, iPlusBot, Hangzhou, Zhejiang, Peoples R China.
Huang, Shoudong, Univ Technol Sydney, CAS, Sydney, NSW, Australia.},
  affiliations = {Zhejiang University; Zhejiang University; University of Technology
Sydney},
  author-email = {litang.cv@gmail.com
wangyue@iipc.zju.edu.cn
dingxiaq@gmail.com
zjuyinhuan@gmail.com
rxiong@zju.edu.cn
shoudong.huang@uts.edu.au},
  cited-references = {Angeli Adrien, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P4300, DOI 10.1109/ROBOT.2009.5152501.
BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791.
Blaer P, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P1031, DOI 10.1109/ROBOT.2002.1013491.
Cadena C, 2012, IEEE T ROBOT, V28, P871, DOI 10.1109/TRO.2012.2189497.
Carlevaris-Bianco N, 2014, IEEE T ROBOT, V30, P1371, DOI 10.1109/TRO.2014.2347571.
Choi J, 2016, IEEE T INTELL TRANSP, V17, P2440, DOI 10.1109/TITS.2016.2519536.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Corcoran P, 2011, IEEE T INTELL TRANSP, V12, P1177, DOI 10.1109/TITS.2011.2143706.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768.
Dissanayake G., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1009, DOI 10.1109/ROBOT.2000.844732.
Eustice RM, 2006, IEEE T ROBOT, V22, P1100, DOI 10.1109/TRO.2006.886264.
Fox D, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P343.
Furgale P, 2010, J FIELD ROBOT, V27, P534, DOI 10.1002/rob.20342.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405.
Huang GP, 2010, INT J ROBOT RES, V29, P502, DOI 10.1177/0278364909353640.
Huang GPQ, 2009, SPRINGER TRAC ADV RO, V54, P373.
Huang SD, 2007, IEEE T ROBOT, V23, P1036, DOI 10.1109/TRO.2007.903811.
Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706.
Konolige K., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P3041, DOI 10.1109/ICRA.2011.5980074.
Konolige K, 2008, IEEE T ROBOT, V24, P1066, DOI 10.1109/TRO.2008.2004832.
Konolige K, 2010, INT J ROBOT RES, V29, P941, DOI 10.1177/0278364910370376.
Krusi P, 2015, J FIELD ROBOT, V32, P534, DOI 10.1002/rob.21524.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Latif Y, 2013, INT J ROBOT RES, V32, P1611, DOI 10.1177/0278364913498910.
Lauer M, 2015, IEEE T INTELL TRANSP, V16, P970, DOI 10.1109/TITS.2014.2345498.
Lee GH, 2013, IEEE INT C INT ROBOT, P556, DOI 10.1109/IROS.2013.6696406.
Liao Y., 2016, IEEE T IMAGE PROCESS, P2839.
Liu M, 2012, IEEE INT C INT ROBOT, P567, DOI 10.1109/IROS.2012.6385640.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
McDonald J, 2013, ROBOT AUTON SYST, V61, P1144, DOI 10.1016/j.robot.2012.08.008.
McManus C, 2015, AUTON ROBOT, V39, P363, DOI 10.1007/s10514-015-9463-y.
McManus C, 2014, IEEE INT CONF ROBOT, P901, DOI 10.1109/ICRA.2014.6906961.
McManus C, 2013, J FIELD ROBOT, V30, P254, DOI 10.1002/rob.21444.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Montemerlo M, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P593.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Newman P, 2009, INT J ROBOT RES, V28, P1406, DOI 10.1177/0278364909341483.
Pascoe G., 2015, FARLAP FAST ROBUST L.
Paton M, 2017, J FIELD ROBOT, V34, P98, DOI 10.1002/rob.21669.
Paton M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1918, DOI 10.1109/IROS.2016.7759303.
Rybski PE, 2008, AUTON ROBOT, V24, P229, DOI 10.1007/s10514-007-9067-2.
Simhon S, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P1708, DOI 10.1109/IROS.1998.724844.
Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387.
Tully S, 2012, INT J ROBOT RES, V31, P271, DOI 10.1177/0278364911433617.
Walcott-Bryant A, 2012, IEEE INT C INT ROBOT, P1871, DOI 10.1109/IROS.2012.6385561.
Wang Y, 2015, ADV ROBOTICS, V29, P683, DOI 10.1080/01691864.2014.998707.
Wang Y, 2016, CAAI T INTELL TECHNO, V1, P90, DOI 10.1016/j.trit.2016.03.009.
Wang Y, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P32, DOI 10.1109/ECMR.2013.6698816.
Wolf DF, 2005, AUTON ROBOT, V19, P53, DOI 10.1007/s10514-005-0606-4.},
  da = {2022-05-17},
  doc-delivery-number = {HI0OW},
  eissn = {1573-7527},
  funding-acknowledgement = {National Nature Science Foundation of China {[}U1609210, 61473258,
61621002]; National Key Research and Development Program
{[}2017YFB1300400]; University of Technology, Sydney; Zhejiang
University},
  funding-text = {This work was supported by the National Nature Science Foundation of
China (Grant Nos. U1609210, 61473258 and 61621002), National Key
Research and Development Program (Grant No. 2017YFB1300400), and in part
by the Joint Centre for Robotics Research between Zhejiang University
and the University of Technology, Sydney.},
  issn = {0929-5593},
  journal-iso = {Auton. Robot.},
  keywords = {Mobile robot; Localization; Navigation; Lifelong learning},
  keywords-plus = {SIMULTANEOUS LOCALIZATION; PLACE RECOGNITION; VISUAL TEACH; SLAM;
VISION; REPEAT; TIME; EKF},
  language = {English},
  number-of-cited-references = {51},
  oa = {Green Submitted},
  orcid-numbers = {Yin, Huan/0000-0002-0872-8202
Yin, Huan/0000-0002-0872-8202
Huang, Shoudong/0000-0002-6124-4178},
  research-areas = {Computer Science; Robotics},
  researcherid-numbers = {Yin, Huan/AAD-8616-2019
Yin, Huan/ABC-9483-2020
Huang, Shoudong/B-4255-2013},
  times-cited = {17},
  type = {Article},
  unique-id = {WOS:000456142200010},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {20},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{wang-et-al:2020:9468884,
  author = {L. Wang and W. Chen and J. Wang},
  booktitle = {2020 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS)},
  title = {Long-Term Localization With Time Series Map Prediction for Mobile Robots
in Dynamic Environments},
  pages = {8587--8593},
  doi = {10.1109/IROS45743.2020.9468884},
  note = {IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), ELECTR NETWORK, OCT 24-JAN 24, 2020-2021},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2020},
  abstract = {In many applications of mobile robot, the environment is constantly
changing. How to use historical information to analysis environmental
changes and generate a map corresponding with current environment is
important to achieve high-precision localization. Inspired by predictive
mechanism of brain, this paper presents a long-term localization
approach named ArmMPU (ARMA-based Map Prediction and Update) based on
time series modeling and prediction. Autoregressive moving average model
(ARMA), a kind of time series modeling method, is employed for
environmental map modeling and prediction, then predicted map and
filtered observation are fused to fix the prediction error. The
simulation and experiment results show that the proposed method improves
long-term localization performance in dynamic environments.},
  affiliation = {Chen, WD (Corresponding Author), Shanghai Jiao Tong Univ, Inst Med Robot \& Dept Automat, Shanghai, Peoples R China.
Chen, WD (Corresponding Author), Minist Educ, Key Lab Syst Control \& Informat Proc, Shanghai, Peoples R China.
Wang, Lisai; Chen, Weidong; Wang, Jingchuan, Shanghai Jiao Tong Univ, Inst Med Robot \& Dept Automat, Shanghai, Peoples R China.
Wang, Lisai; Chen, Weidong; Wang, Jingchuan, Minist Educ, Key Lab Syst Control \& Informat Proc, Shanghai, Peoples R China.},
  affiliations = {Shanghai Jiao Tong University},
  author-email = {lisaiw@sjtu.edu.cn
wdchen@sjtu.edu.cn
jchwang@sjtu.edu.cn},
  book-group-author = {IEEE},
  cited-references = {AKAIKE H, 1969, ANN I STAT MATH, V21, P243, DOI 10.1007/BF02532251.
AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705.
Apruzzi F, 2016, J HIGH ENERGY PHYS, DOI 10.1007/JHEP07(2016)045.
Boateng P, 2017, MEGAPROJECT RISK ANALYSIS AND SIMULATION: A DYNAMIC SYSTEMS APPROACH, P223.
Dayoub F, 2011, ROBOT AUTON SYST, V59, P285, DOI 10.1016/j.robot.2011.02.013.
Dodge Y., 2008, CONCISE ENCY STAT.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Hu XW, 2018, CHIN AUTOM CONGR, P384, DOI 10.1109/CAC.2018.8623046.
Krajnik T, 2017, IEEE T ROBOT, V33, P964, DOI 10.1109/TRO.2017.2665664.
Krajnik T, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4558, DOI 10.1109/IROS.2016.7759671.
Krajnik T, 2014, IEEE INT CONF ROBOT, P3706, DOI 10.1109/ICRA.2014.6907396.
Meyer-Delius D, 2010, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2010.5648920.
Morris T, 2014, IEEE INT CONF ROBOT, P2765, DOI 10.1109/ICRA.2014.6907255.
Olson E, 2015, IEEE INT CONF ROBOT, P5815, DOI 10.1109/ICRA.2015.7140013.
Potter MC, 2014, ATTEN PERCEPT PSYCHO, V76, P270, DOI 10.3758/s13414-013-0605-z.
RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626.
Saarinen JP, 2013, INT J ROBOT RES, V32, P1627, DOI 10.1177/0278364913499415.
Schreiber M, 2019, IEEE INT CONF ROBOT, P9299, DOI 10.1109/ICRA.2019.8793582.
Song BW, 2019, IEEE INT C INT ROBOT, P5364, DOI 10.1109/IROS40897.2019.8968017.
Sun DL, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4517, DOI 10.1109/IROS.2016.7759665.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Tipaldi GD, 2013, INT J ROBOT RES, V32, P1662, DOI 10.1177/0278364913502830.
Wang Y, 2014, IND ROBOT, V41, P241, DOI 10.1108/IR-06-2013-371.},
  da = {2022-05-17},
  doc-delivery-number = {BS4YL},
  funding-acknowledgement = {National Natural Science Foundation of China {[}U1813206, 61773261]},
  funding-text = {This work is supported by the National Natural Science Foundation of
China (Grant U1813206 and 61773261).},
  isbn = {978-1-7281-6212-6},
  issn = {2153-0858},
  keywords-plus = {MODELS},
  language = {English},
  number-of-cited-references = {24},
  research-areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
  series = {IEEE International Conference on Intelligent Robots and Systems},
  times-cited = {0},
  type = {Proceedings Paper},
  unique-id = {WOS:000724145802112},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Engineering, Electrical \& Electronic; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{bosse-zlot:2009:009,
  author = {M. Bosse and R. Zlot},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {Keypoint design and evaluation for place recognition in 2D lidar maps},
  volume = {57},
  number = {12,SI},
  pages = {1211--1224},
  doi = {10.1016/j.robot.2009.07.009},
  note = {Workshop of the Inside-Data-Association, Zurich, SWITZERLAND, JUN 28,
2008},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2009},
  month = {12},
  abstract = {We address the place recognition problem, which we define as the problem
of establishing whether an observed location has been previously seen,
and if so, determining the transformation aligning the current
observations to an existing map. In the contexts of robot navigation and
mapping, place recognition amounts to globally localizing a robot or map
segment without being given any prior estimate. An efficient method of
solving this problem involves first selecting a set of keypoints in the
scene which store an encoding of their local region, and then utilizing
a sublinear-time search into a database of keypoints previously
generated from the global map to identify places with common features.
We present an algorithm to embed arbitrary keypoint descriptors in a
reduced-dimension metric space, in order to frame the problem as an
efficient nearest neighbor search. Given that there are a multitude of
possibilities for keypoint design, we propose a general methodology for
comparing keypoint location selection heuristics and descriptor models
that describe the region around the keypoint. With respect to selecting
keypoint locations, we introduce a metric that encodes how likely it is
that the keypoint will be found in the presence of noise and occlusions
during mapping passes. Metrics for keypoint descriptors are used to
assess the distinguishability between the distributions of matches and
non-matches and the probability the correct match will be found in an
approximate k-nearest neighbors search. Verification of the test
outcomes is done by comparing the various keypoint designs on a
kilometers-scale place recognition problem. We apply our design
evaluation methodology to three keypoint selection heuristics and six
keypoint descriptor models. A full place recognition system is
presented, including a series of match verification algorithms which
effectively filter out false positives. Results from city-scale and
long-term mapping problems illustrate our approach for both offline and
online SLAM, map merging, and global localization and demonstrate that
our algorithm is able to produce accurate maps over trajectories of
hundreds of kilometers. Crown Copyright (C) 2009 Published by Elsevier
B.V. All rights reserved.},
  affiliation = {Bosse, M (Corresponding Author), CSIRO, ICT Ctr, Autonomous Syst Lab, POB 883, Kenmore, Qld 4069, Australia.
Bosse, Michael; Zlot, Robert, CSIRO, ICT Ctr, Autonomous Syst Lab, Kenmore, Qld 4069, Australia.},
  affiliations = {Commonwealth Scientific \& Industrial Research Organisation (CSIRO)},
  author-email = {mike.bosse@csiro.au
robert.zlot@csiro.au},
  cited-references = {Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494.
Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348.
Bailey T., 2002, THESIS U SYDNEY SYDN.
BAY H, 2006, SURF SPEEDED UP ROBU.
Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558.
Bosse M, 2004, INT J ROBOT RES, V23, P1113, DOI 10.1177/0278364904049393.
Bosse M., 2009, INT C FIELD SERV ROB.
Bosse M, 2008, INT J ROBOT RES, V27, P667, DOI 10.1177/0278364908091366.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Estrada C, 2005, IEEE T ROBOT, V21, P588, DOI 10.1109/TRO.2005.844673.
FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/tit.1975.1055330.
HO K, 2005, EUR C MOB ROB.
Ho KL, 2007, INT J COMPUT VISION, V74, P261, DOI 10.1007/s11263-006-0020-1.
Howard A., 2003, ROBOTICS DATA SET RE.
LIN KI, 2005, INT DAT ENG APPL S.
LIU T, 2004, NEUR INF PROC SYST C.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
LU F, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P935, DOI 10.1109/CVPR.1994.323928.
MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9.
Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009.
PROCOPIUC O, 2003, INT S SPAT TEMP DAT.
Schindler G, 2007, IEEE COMP SOC C COMP.
TOMONO M, 2004, IEEE INT C ROB AUT.
Walthelm A., 2004, INT C INT AUT SYST.
WEISS G, 1994, IEEE RSJ INT C INT R.
Zlot R., 2008, INT S EXP ROB.},
  da = {2022-05-17},
  doc-delivery-number = {530PX},
  eissn = {1872-793X},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Place recognition; Data association; SLAM; Mapping; Localization;
Dimension reduction; Regional point descriptor},
  keywords-plus = {APPROXIMATE NEAREST-NEIGHBOR; LOCALIZATION},
  language = {English},
  number-of-cited-references = {26},
  orcid-numbers = {Zlot, Robert/0000-0002-1672-552X},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  researcherid-numbers = {Zlot, Robert/B-7546-2011
Bosse, Michael/B-7719-2011},
  times-cited = {61},
  type = {Article; Proceedings Paper},
  unique-id = {WOS:000272605900006},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {26},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{burki-et-al:2019:21870,
  author = {M. Burki and C. Cadena and I. Gilitschenski and S. Roland and J. Nieto},
  journal = {JOURNAL OF FIELD ROBOTICS},
  title = {Appearance-based landmark selection for visual localization},
  volume = {36},
  number = {6},
  pages = {1041--1073},
  doi = {10.1002/rob.21870},
  publisher = {WILEY},
  address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  year = {2019},
  month = {9},
  abstract = {Visual localization in outdoor environments is subject to varying
appearance conditions rendering it difficult to match current camera
images against a previously recorded map. Although it is possible to
extend the respective maps to allow precise localization across a wide
range of differing appearance conditions, these maps quickly grow in
size and become impractical to handle on a mobile robotic platform. To
address this problem, we present a landmark selection algorithm that
exploits appearance co-observability for efficient visual localization
in outdoor environments. Based on the appearance condition inferred from
recently observed landmarks, a small fraction of landmarks useful under
the current appearance condition is selected and used for localization.
This allows to greatly reduce the bandwidth consumption between the
mobile platform and a map backend in a shared-map scenario, and
significantly lowers the demands on the computational resources on said
mobile platform. We derive a landmark ranking function that exhibits
high performance under vastly changing appearance conditions and is
agnostic to the distribution of landmarks across the different map
sessions. Furthermore, we relate and compare our proposed
appearance-based landmark ranking function to popular ranking schemes
from information retrieval, and validate our results on the challenging
University of Michigan North Campus long-term vision and LIDAR data sets
(NCLT), including an evaluation of the localization accuracy using
ground-truth poses. In addition to that, we investigate the
computational and bandwidth resource demands. Our results show that by
selecting 20-30\% of landmarks using our proposed approach, a similar
localization performance as the baseline strategy using all landmarks is
achieved.},
  affiliation = {Burki, M (Corresponding Author), Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland.
Burki, Mathias; Cadena, Cesar; Siegwart, Roland; Nieto, Juan, Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland.
Gilitschenski, Igor, MIT, Comp Sci \& Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.},
  affiliations = {ETH Zurich; Massachusetts Institute of Technology (MIT)},
  author-email = {mathias.buerki@mavt.ethz.ch},
  cited-references = {Aizawa A, 2003, INFORM PROCESS MANAG, V39, P45, DOI 10.1016/S0306-4573(02)00021-3.
Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715.
Bay H., 2006, EUR C COMP VIS, P404.
Burki M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4137, DOI 10.1109/IROS.2016.7759609.
Burgard W, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2089, DOI 10.1109/IROS.2009.5354691.
Burki Mathias, 2018, 2018 IEEE Intelligent Vehicles Symposium (IV), P682, DOI 10.1109/IVS.2018.8500432.
Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222.
Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Clement L, 2017, J FIELD ROBOT, V34, P74, DOI 10.1002/rob.21655.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Dayoub F, 2011, ROBOT AUTON SYST, V59, P285, DOI 10.1016/j.robot.2011.02.013.
Dymczyk M, 2015, IEEE INT C INT ROBOT, P2536, DOI 10.1109/IROS.2015.7353722.
Hochdorfer S., 2009, INTENSIVMED, P1.
Johns E, 2014, INT J COMPUT VISION, V106, P297, DOI 10.1007/s11263-013-0648-6.
Johns E, 2013, IEEE INT CONF ROBOT, P3212, DOI 10.1109/ICRA.2013.6631024.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Lategahn H, 2014, IEEE INT VEH SYM, P756, DOI 10.1109/IVS.2014.6856421.
Li FY, 2006, IEEE INT CONF ROBOT, P3405, DOI 10.1109/ROBOT.2006.1642222.
Li YP, 2010, LECT NOTES COMPUT SC, V6312, P791.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410.
MacTavish Kirk, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2065, DOI 10.1109/ICRA.2017.7989238.
Maddern W., 2014, P VIS PLAC REC CHANG, V2, P5.
McManus C., 2014, SCENE SIGNATURES LOC.
McManus C, 2014, IEEE INT CONF ROBOT, P901, DOI 10.1109/ICRA.2014.6906961.
Milford M., 2005, P AUSTR C ROB AUT 20, P1.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
Milford MJ, 2004, IEEE INT CONF ROBOT, P403, DOI 10.1109/ROBOT.2004.1307183.
Muhlfellner P, 2016, J FIELD ROBOT, V33, P561, DOI 10.1002/rob.21595.
Muhlfellner P., 2015, DESIGNING RELATIONAL.
Paton M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1918, DOI 10.1109/IROS.2016.7759303.
Paton M, 2015, IEEE INT CONF ROBOT, P1519, DOI 10.1109/ICRA.2015.7139391.
Prasser D, 2006, SPRINGER TRAC ADV RO, V25, P143.
SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0.
Sattler T, 2011, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2011.6126302.
Schindler G., 2007, 2007 IEEE C COMP VIS, P1.
Stumm E, 2015, IEEE INT CONF ROBOT, P5475, DOI 10.1109/ICRA.2015.7139964.},
  da = {2022-05-17},
  doc-delivery-number = {IO2XF},
  eissn = {1556-4967},
  issn = {1556-4959},
  journal-iso = {J. Field Robot.},
  keywords = {landmark selection; long-term localization; multisession mapping; visual
localization; wheeled robots},
  keywords-plus = {RECOGNITION; NAVIGATION; SLAM},
  language = {English},
  number-of-cited-references = {38},
  orcid-numbers = {Cadena, Cesar/0000-0002-2972-6011
Burki, Mathias/0000-0002-6988-9990},
  research-areas = {Robotics},
  researcherid-numbers = {Cadena, Cesar/AAM-4987-2020},
  times-cited = {6},
  type = {Article},
  unique-id = {WOS:000479244400001},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {31},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{dymczyk-et-al:2015:7139575,
  author = {M. Dymczyk and S. Lynen and T. Cieslewsld and B. Michael and R. Siegwart and P. Furgale},
  booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {The Gist of Maps - Summarizing Experience for Lifelong Localization},
  pages = {2767--2773},
  doi = {10.1109/ICRA.2015.7139575},
  note = {IEEE International Conference on Robotics and Automation (ICRA),
Seattle, WA, MAY 26-30, 2015},
  publisher = {IEEE COMPUTER SOC},
  address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
  year = {2015},
  abstract = {Robust, scalable place recognition is a core competency for many robotic
applications. However, when revisiting places over and over, many
state-of-the-art approaches exhibit reduced performance in terms of
computation and memory complexity and in terms of accuracy. For
successful deployment of robots over long time scales, we must develop
algorithms that get better with repeated visits to the same environment,
while still working within a fixed computational budget.
This paper presents and evaluates an algorithm that alternates between
online place recognition and offline map maintenance with the goal of
producing the best performance with a fixed map size. At the core of the
algorithm is the concept of a Summary Map, a reduced map representation
that includes only the landmarks that are deemed most useful for place
recognition. To assign landmarks to the map, we use a scoring function
that ranks the utility of each landmark and a sampling policy that
selects the landmarks for each place. The Summary Map can then be used
by any descriptor-based inference method for constant-complexity online
place recognition. We evaluate a number of scoring functions and
sampling policies and show that it is possible to build and maintain
maps of a constant size and that place-recognition performance improves
over multiple visits.},
  affiliation = {Dymczyk, M (Corresponding Author), Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland.
Dymczyk, Martin; Lynen, Simon; Cieslewsld, Titus; Bosse, Michael; Siegwart, Roland; Furgale, Paul, Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland.},
  affiliations = {ETH Zurich},
  book-group-author = {IEEE},
  cited-references = {Agrawal M., 2008, IEEE T ROBOTICS.
Allen R., 2014, COMP ROB VIS CRV CAN.
Beinhofer M., 2013, ROBOTICS AUTONOMOUS.
Carlevaris-Bianco N., 2014, IEEE T ROBOTICS.
Churchill Winston, 2013, INT J ROBOTICS RES I.
Cieslewski T., 2015, ROB AUT ICRA 2015 IE.
Cummins M., 2008, INT J ROBOTICS RES.
Furgale P., 2010, J FIELD ROBOTICS.
Huang G., 2013, MOB ROB ECMR EUR C.
Jegou H., COMP VIS ECCV 2008 I.
Johannsson H., 2013, ROB AUT ICRA IEEE IN.
Johns E., 2013, ROB AUT ICRA IEEE IN.
Konolige K., 2010, INT J ROBOTICS RES.
Kretzschmar H., 2012, INT J ROBOTICS RES.
Lee J., GOOGLE PROJECT TANGO.
Lynen S., 2014, 3DV.
Maddern W., 2012, ROBOTICS SCI SYSTEMS.
Maddern W., 2012, INT ROB SYST IROS IE.
Milford M., 2012, ROB AUT ICRA IEEE IN.
Muehlfellner P., 2013, IEEE INT VEH S IV.
Muhlfellner P., 2014, SUMMARY MAPS L UNPUB.
Naseer Tayyab, 2014, AAAI.
Paul R., 2011, ROB AUT ICRA IEEE IN.
Pepperell E., 2014, ROB AUT ICRA IEEE IN.
Sattler T., 2011, COMP VIS ICCV IEEE I.
Sivic J., COMP VIS ICCV 2003 I.
Vitus M.P., 2010, ROBOTICS SCI SYSTEMS.},
  da = {2022-05-17},
  doc-delivery-number = {BE3MR},
  isbn = {978-1-4799-6923-4},
  issn = {1050-4729},
  language = {English},
  number-of-cited-references = {27},
  orcid-numbers = {Siegwart, Roland/0000-0002-2760-7983},
  research-areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
  researcherid-numbers = {Siegwart, Roland/A-4495-2008},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {29},
  type = {Proceedings Paper},
  unique-id = {WOS:000370974902115},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Engineering, Electrical \& Electronic; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{dymczyk-et-al:2016:7759673,
  author = {M. Dymczyk and T. Schneider and R. I. A. S. Gilitschenski and E. Stumm},
  booktitle = {2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS 2016)},
  title = {Erasing bad memories: agent-side summarization for long-term mapping},
  pages = {4572--4579},
  doi = {10.1109/IROS.2016.7759673},
  note = {IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Daejeon, SOUTH KOREA, OCT 09-14, 2016},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2016},
  abstract = {Precisely estimating the pose of an agent in a global reference frame is
a crucial goal that unlocks a multitude of robotic applications,
including autonomous navigation and collaboration. In order to achieve
this, current state-of-the-art localization approaches collect data
provided by one or more agents and create a single, consistent
localization map, maintained over time. However, with the introduction
of lengthier sorties and the growing size of the environments, data
transfers between the backend server where the global map is stored and
the agents are becoming prohibitively large. While some existing methods
partially address this issue by building compact summary maps, the data
transfer from the agents to the backend can still easily become
unmanageable.
In this paper, we propose a method that is designed to reduce the amount
of data that needs to be transferred from the agent to the backend,
functioning in large-scale, multi-session mapping scenarios. Our
approach is based upon a landmark selection method that exploits
information coming from multiple, possibly weak and correlated, landmark
utility predictors; fused using learned feature coefficients. Such a
selection yields a drastic reduction in data transfer while maintaining
localization performance and the ability to efficiently summarize
environments over time. We evaluate our approach on a data set that was
autonomously collected in a dynamic indoor environment over a period of
several months.},
  affiliation = {Dymczyk, M (Corresponding Author), Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland.
Dymczyk, Marcin; Schneider, Thomas; Gilitschenski, Igor; Siegwart, Roland; Stumm, Elena, Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland.},
  affiliations = {ETH Zurich},
  book-group-author = {IEEE},
  cited-references = {Arth C., 2011, IEEE INT S MIX AUGM.
Buoncompagni S, 2015, PATTERN RECOGN LETT, V62, P32, DOI 10.1016/j.patrec.2015.04.019.
Carneiro G, 2009, IMAGE VISION COMPUT, V27, P1143, DOI 10.1016/j.imavis.2008.10.015.
Churchill Winston, 2013, INT J ROBOTICS RES I.
Cieslewski T, 2015, IEEE INT CONF ROBOT, P6241, DOI 10.1109/ICRA.2015.7140075.
Dymczyk M., 2015, IEEE INT C ROB AUT.
Dymczyk M., 2015, IEEE RSJ INT C INT R.
Forster C., 2013, IEEE RSJ INT C INT R.
Galvez-Lopez D., 2012, IEEE T ROBOTICS, V28.
Hartmann W., 2014, IEEE C COMP VIS PATT.
Knopp J., 2010, EUR C COMP VIS.
Leutenegger S., 2011, INT C COMP VIS.
Lynen S., 2014, 3DV.
Lynen S, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.
Marder-Eppstein E, 2010, IEEE INT CONF ROBOT, P300, DOI 10.1109/ROBOT.2010.5509725.
Middelberg S., 2014, EUR C COMP VIS.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Muhlfellner P., 2015, J FIELD ROBOTICS.
Nikolic J, 2014, IEEE INT CONF ROBOT, P431, DOI 10.1109/ICRA.2014.6906892.
Park H. S., 2013, IEEE C COMP VIS PATT.
Riazuelo L., 2013, ROBOTICS AUTONOMOUS.
Sattler T., 2011, INT C COMP VIS.
Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.
Verdie Y., 2015, IEEE C COMP VIS PATT.
Walcott-Bryant A., 2012, IEEE RSJ INT C INT R.
Zhang W, 2007, IMAGE VISION COMPUT, V25, P704, DOI 10.1016/j.imavis.2006.05.016.},
  da = {2022-05-17},
  doc-delivery-number = {BG7XO},
  funding-acknowledgement = {Google},
  funding-text = {We would like to thank Mathias Gehrig for the preparation of the
Turtlebot, our autonomous mapping platform. The research leading to
these results has received funding from Google's project Tango.},
  isbn = {978-1-5090-3762-9},
  language = {English},
  number-of-cited-references = {26},
  orcid-numbers = {Schneider, Thomas/0000-0002-1383-769X},
  research-areas = {Computer Science; Robotics},
  times-cited = {8},
  type = {Proceedings Paper},
  unique-id = {WOS:000391921704089},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{gadd-newman:2016:7759843,
  author = {M. Gadd and P. Newman},
  booktitle = {2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS 2016)},
  title = {Checkout My Map: Version Control for Fleetwide Visual Localisation},
  pages = {5729--5736},
  doi = {10.1109/IROS.2016.7759843},
  note = {IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Daejeon, SOUTH KOREA, OCT 09-14, 2016},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2016},
  abstract = {This paper is about underpinning long-term operations of fleets of
vehicles using visual localisation. In particular it examines ways in
which vehicles, considered as independent agents, can share, update and
leverage each others' visual experiences in a mutually beneficial way.
We draw on our previous work in Experience-based Navigation (EBN) {[}1],
in which a visual map supporting multiple representations of the same
place is built, yielding real-time localisation capability for a
solitary vehicle. We now consider how any number of such agents might
operate in concert via data sharing policies that are germane to the
shared task of lifelong localisation. We rapidly construct considerable
maps by the conjoining of work distributed to asynchronous processes,
and share expertise amongst the team by the selective dispensing of
mission-specific map contents. We demonstrate and evaluate our system
against 100 km of data collected in North Oxford over a period of a
month featuring diverse deviation in appearance due to atmospheric,
lighting, and structural dynamics. We show that our framework is capable
of creating maps in a fraction of the time required by single-agent EBN,
with no significant loss in localisation robustness, and is able to
furnish robots on real-world forays with maps which require much less
storage.},
  affiliation = {Gadd, M (Corresponding Author), Univ Oxford, Mobile Robot Grp, Oxford, England.
Gadd, Matthew; Newman, Paul, Univ Oxford, Mobile Robot Grp, Oxford, England.},
  affiliations = {University of Oxford},
  author-email = {mattgadd@robots.ox.ac.uk
pnewman@robots.ox.ac.uk},
  book-group-author = {IEEE},
  cited-references = {Aragues R, 2012, IEEE T ROBOT, V28, P840, DOI 10.1109/TRO.2012.2192012.
Bahr A, 2009, IEEE INT CONF ROBOT, P4295.
Bailey Tim, 2011, IEEE International Conference on Robotics and Automation, P2859.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Churchill W, 2012, IEEE INT C INTELL TR, P1371, DOI 10.1109/ITSC.2012.6338716.
Cieslewski T, 2015, IEEE INT CONF ROBOT, P6241, DOI 10.1109/ICRA.2015.7140075.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cunningham A, 2012, IEEE INT CONF ROBOT, P1093, DOI 10.1109/ICRA.2012.6225356.
Dymczyk M., 2015, P IEEE INT C ROB AUT.
Glover A, 2012, IEEE INT CONF ROBOT, P4730, DOI 10.1109/ICRA.2012.6224843.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Leach P., 2005, UNIVERSALLY UNIQUE I.
Linegar C., 2015, P IEEE INT C ROB AUT.
Maddern W, 2015, P IEEE INT C ROB AUT.
McManus C, 2014, IEEE INT CONF ROBOT, P901, DOI 10.1109/ICRA.2014.6906961.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Nagel W., 2005, SUBVERSION VERSION C.
Nister D., 2004, COMP VIS PATT REC 20, V1, pI, DOI DOI 10.1109/CVPR.2004.1315094.
Paul R, 2012, IEEE INT CONF ROBOT, P4058, DOI 10.1109/ICRA.2012.6224762.
Pepperell E., 2015, AUTOMATIC IMAGE SCAL.
STORMO GD, 1982, NUCLEIC ACIDS RES, V10, P2997, DOI 10.1093/nar/10.9.2997.
Wingerd L., 2005, PRACTICAL PERFORCE.},
  da = {2022-05-17},
  doc-delivery-number = {BG7XO},
  funding-acknowledgement = {FirstRand Laurie Dip penar; Oppenheimer Memorial Trust; Keble Ian Palmer
scholarships; EPSRC Leadership Fellowship {[}EP/J012017/1]; EPSRC
Programme {[}EP/M019918/1]; European Community's Seventh Framework
Programme {[}FP7-610603]; EPSRC {[}EP/I005021/1, EP/M019918/1] Funding
Source: UKRI},
  funding-text = {Matthew Gadd is supported by the FirstRand Laurie Dip penar, Oppenheimer
Memorial Trust, and Keble Ian Palmer scholarships. Paul Newman is
supported by EPSRC Leadership Fellowship Grant EP/J012017/1 and EPSRC
Programme Grant EP/M019918/1. Additionally, the authors acknowledge the
support of this work by the European Community's Seventh Framework
Programme under grant agreement FP7-610603 (EUROPA2). The authors thank
Chris Linegar for his support in interfacing with core EBN software, as
well as Winston Churchill for his valuable suggestions.},
  isbn = {978-1-5090-3762-9},
  keywords-plus = {NAVIGATION},
  language = {English},
  number-of-cited-references = {23},
  oa = {Green Published},
  orcid-numbers = {Gadd, Matthew/0000-0001-9447-8619},
  research-areas = {Computer Science; Robotics},
  researcherid-numbers = {Gadd, Matthew/AAS-4274-2020},
  times-cited = {5},
  type = {Proceedings Paper},
  unique-id = {WOS:000391921705114},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{labbe-michaud:2019:21831,
  author = {M. Labbe and F. Michaud},
  journal = {JOURNAL OF FIELD ROBOTICS},
  title = {RTAB-Map as an open-source lidar and visual simultaneous localization
and mapping library for large-scale and long-term online operation},
  volume = {36},
  number = {2},
  pages = {416--446},
  doi = {10.1002/rob.21831},
  publisher = {WILEY},
  address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  year = {2019},
  month = {3},
  abstract = {Distributed as an open-source library since 2013, real-time
appearance-based mapping (RTAB-Map) started as an appearance-based loop
closure detection approach with memory management to deal with
large-scale and long-term online operation. It then grew to implement
simultaneous localization and mapping (SLAM) on various robots and
mobile platforms. As each application brings its own set of constraints
on sensors, processing capabilities, and locomotion, it raises the
question of which SLAM approach is the most appropriate to use in terms
of cost, accuracy, computation power, and ease of integration. Since
most of SLAM approaches are either visual- or lidar-based, comparison is
difficult. Therefore, we decided to extend RTAB-Map to support both
visual and lidar SLAM, providing in one package a tool allowing users to
implement and compare a variety of 3D and 2D solutions for a wide range
of applications with different robots and sensors. This paper presents
this extended version of RTAB-Map and its use in comparing, both
quantitatively and qualitatively, a large selection of popular
real-world datasets (e.g., KITTI, EuRoC, TUM RGB-D, MIT Stata Center on
PR2 robot), outlining strengths, and limitations of visual and lidar
SLAM configurations from a practical perspective for autonomous
navigation applications.},
  affiliation = {Labbe, M (Corresponding Author), Univ Sherbrooke, Interdisciplinary Inst Technol Innovat 3IT, Dept Elect Engn \& Comp Engn, Sherbrooke, PQ J1K 0A5, Canada.
Labbe, Mathieu; Michaud, Francois, Univ Sherbrooke, Interdisciplinary Inst Technol Innovat 3IT, Dept Elect Engn \& Comp Engn, Sherbrooke, PQ J1K 0A5, Canada.},
  affiliations = {University of Sherbrooke},
  author-email = {Mathieu.M.Labbe@USherbrooke.ca},
  cited-references = {Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791.
Bosse M, 2008, INT J ROBOT RES, V27, P667, DOI 10.1177/0278364908091366.
Bradski G., 2008, LEARNING OPENCV.
Burhanpurkar M, 2017, INT C REHAB ROBOT, P1079, DOI 10.1109/ICORR.2017.8009393.
Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033.
Calonder M., 2010, P EUR C COMP VIS ECC.
Carlone L, 2012, ROBOTICS: SCIENCE AND SYSTEMS VII, P41.
Chen YF, 2015, LECT NOTES ARTIF INT, V9513, P60, DOI 10.1007/978-3-319-29339-4\_5.
Cvisic I, 2018, J FIELD ROBOT, V35, P578, DOI 10.1002/rob.21762.
Dai A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3054739.
Della Corte B., 2017, GEN FRAMEWORK FLEXIB.
Dellaert F., 2012, GTRIMCPR2012002 GEOR.
Dube R., 2016, SEGMATCH SEGMENT BAS.
Dube R, 2017, IEEE INT C INT ROBOT, P1004, DOI 10.1109/IROS.2017.8202268.
Endres F, 2014, IEEE T ROBOT, V30, P177, DOI 10.1109/TRO.2013.2279412.
Engel J, 2015, IEEE INT C INT ROBOT, P1935, DOI 10.1109/IROS.2015.7353631.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
Fallon M, 2013, INT J ROBOT RES, V32, P1695, DOI 10.1177/0278364913509035.
Foote T., 2013, TECHN PRACT ROB APPL, P1, DOI DOI 10.1109/TEPRA.2013.6556373.
Foresti H., 2016, EMOTIVE ROBOTICS I Z.
Forster C., 2014, P IEEE INT C ROB AUT.
Fox D, 1999, SIXTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-99)/ELEVENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE (IAAI-99), P343.
Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Geiger A., 2011, P INT VEH S BAD BAD.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Goebel P., 2014, WINNING IROS 2014 MI.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925.
Gutierrez-Gomez D, 2016, ROBOT AUTON SYST, V75, P571, DOI 10.1016/j.robot.2015.09.026.
Harmat A, 2015, J INTELL ROBOT SYST, V78, P291, DOI 10.1007/s10846-014-0085-y.
Hess W, 2016, IEEE INT CONF ROBOT, P1271, DOI 10.1109/ICRA.2016.7487258.
Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0.
Huang A. S., 2011, P INT S ROB RES FLAG.
Kahler O, 2016, LECT NOTES COMPUT SC, V9912, P500, DOI 10.1007/978-3-319-46484-8\_30.
Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650.
Klein G., 2000, P IEEE ACM INT S MIX.
Kohlbrecher S., 2016, ROBOCUP RESCUE 2016.
Kohlbrecher S, 2011, P IEEE INT S SAF SEC.
Konolige K., 1998, Robotics Research. Eighth International Symposium, P203.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Labbe M., 2017, AUTONOMOUS ROBOTS.
Labbe M, 2014, IEEE INT C INT ROBOT, P2661, DOI 10.1109/IROS.2014.6942926.
Labbe M, 2013, IEEE T ROBOT, V29, P734, DOI 10.1109/TRO.2013.2242375.
Laniel Sebastien, 2017, IEEE Int Conf Rehabil Robot, V2017, P809, DOI 10.1109/ICORR.2017.8009347.
Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813.
Lin Y, 2018, J FIELD ROBOT, V35, P23, DOI 10.1002/rob.21732.
Liu Peidong, 2018, P IEEE INT C ROB AUT.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Lucas B. D., 1981, INT JOINT C ART INT, V2, P674, DOI DOI 10.5555/1623264.1623280.
Marder-Eppstein E, 2010, IEEE INT CONF ROBOT, P300, DOI 10.1109/ROBOT.2010.5509725.
Moore T, 2014, P INT C INT AUT SYST.
Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Pire T, 2017, ROBOT AUTON SYST, V93, P27, DOI 10.1016/j.robot.2017.03.019.
Pizzoli M., 2014, P IEEE INT C ROB AUT.
Pomerleau F, 2013, AUTON ROBOT, V34, P133, DOI 10.1007/s10514-013-9327-2.
Quigley M., 2009, P IEEE INT C ROB AUT.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233.
Schlegel D., 2017, PROSLAM GRAPH SLAM P.
Schneider Thomas, 2018, IEEE Robotics and Automation Letters, V3, P1418, DOI 10.1109/LRA.2018.2800113.
SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794.
Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663.
Stachniss C, 2016, SPRINGER HANDBOOK OF ROBOTICS, P1153.
Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187.
Steux B, 2010, I C CONT AUTOMAT ROB, P1975, DOI 10.1109/ICARCV.2010.5707402.
Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773.
Sun K, 2018, IEEE ROBOT AUTOM LET, V3, P965, DOI 10.1109/LRA.2018.2793349.
Thrun S., 2002, EXPLOR ARTIF INTELL, V1-35, P1.
Vincent R, 2010, P SOC PHOTO-OPT INS, V7664, DOI 10.1117/12.849593.
Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237.
Whelan T, 2015, INT J ROBOT RES, V34, P598, DOI 10.1177/0278364914551008.
Xu W, 2010, PROC CVPR IEEE, P263, DOI 10.1109/CVPR.2010.5540202.
Yun DS, 2014, I C INF COMM TECH CO, P609, DOI 10.1109/ICTC.2014.6983225.
Zhang J, 2017, AUTON ROBOT, V41, P401, DOI 10.1007/s10514-016-9548-2.
Zollhofer M, 2018, COMPUT GRAPH FORUM, V37, P625, DOI 10.1111/cgf.13386.},
  da = {2022-05-17},
  doc-delivery-number = {HK9TP},
  eissn = {1556-4967},
  esi-highly-cited-paper = {Y},
  esi-hot-paper = {N},
  funding-acknowledgement = {Natural Sciences and Engineering Research Council of Canada},
  funding-text = {Natural Sciences and Engineering Research Council of Canada},
  issn = {1556-4959},
  journal-iso = {J. Field Robot.},
  keywords = {perception; position estimation; SLAM},
  keywords-plus = {SLAM},
  language = {English},
  number-of-cited-references = {79},
  orcid-numbers = {Michaud, Francois/0000-0002-3639-7770
Labbe, Mathieu/0000-0003-0778-5595},
  research-areas = {Robotics},
  times-cited = {163},
  type = {Article},
  unique-id = {WOS:000458335100006},
  usage-count-last-180-days = {33},
  usage-count-since-2013 = {155},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{mazuran-et-al:2016:0278364915581629,
  author = {M. Mazuran and W. Burgard and G. D. Tipaldi},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {Nonlinear factor recovery for long-term SLAM},
  volume = {35},
  number = {1-3,SI},
  pages = {50--72},
  doi = {10.1177/0278364915581629},
  note = {10th Conference on Robotics - Science and Systems (RSS), Univ Calif,
Berkeley, CA, JUN, 2014},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2016},
  month = {1},
  abstract = {For long-term operations, graph-based simultaneous localization and
mapping (SLAM) approaches require nodes to be marginalized in order to
control the computational cost. In this paper, we present a method to
recover a set of nonlinear factors that best represents the marginal
distribution in terms of Kullback-Leibler divergence. The proposed
method, which we call nonlinear factor recovery (NFR), estimates both
the mean and the information matrix of the set of nonlinear factors,
where the recovery of the latter is equivalent to solving a convex
optimization problem. NFR is able to provide either the dense
distribution or a sparse approximation of it. In contrast to previous
algorithms, our method does not necessarily require a global
linearization point and can be used with any nonlinear measurement
function. Moreover, we are not restricted to only using tree-based
sparse approximations and binary factors, but we can include any
topology and correlations between measurements. Experiments performed on
several publicly available datasets demonstrate that our method
outperforms the state of the art with respect to the Kullback-Leibler
divergence and the sparsity of the solution.},
  affiliation = {Mazuran, M (Corresponding Author), Univ Freiburg, Dept Comp Sci, Georges Koehler Allee 79, D-79110 Freiburg, Germany.
Mazuran, Mladen; Burgard, Wolfram; Tipaldi, Gian Diego, Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany.},
  affiliations = {League of European Research Universities - LERU; University of Freiburg},
  author-email = {mazuran@informatik.uni-freiburg.de},
  cited-references = {Banerjee O., 2006, P INT C MACH LEARN.
Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016.
Boyd S., 2009, CONVEX OPTIMIZATION.
Carlevaris-Bianco N., 2013, P IEEE INT C ROB AUT.
Carlevaris-Bianco N, 2013, P IEEE RSJ INT C INT.
Carlevaris-Bianco N, 2014, IEEE T ROBO IN PRESS.
Carlevaris-Bianco N, 2014, P IEEE INT C ROB AUT.
CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142.
Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768.
Duchi J. C., 2008, P C UNC ART INT.
Eade E, 2010, P IEEE RSJ INT C INT.
Eustice R., 2005, P IEEE RSJ INT C INT.
FOLKESSON J, 2004, P IEEE INT C ROB AUT, V1.
FRESE U, 2007, P IEEE INT C ROB AUT.
Frese U, 2006, AUTON ROBOT, V21, P103, DOI 10.1007/s10514-006-9043-2.
Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045.
Golub G. H., 1996, MATRIX COMPUTATIONS.
Grisetti G., 2007, P ROB SCI SYST.
Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925.
HIGHAM NJ, 1988, LINEAR ALGEBRA APPL, V103, P103, DOI 10.1016/0024-3795(88)90223-6.
Huang G, 2013, P EUR C MOB ROB.
Huang SD, 2009, AUTON ROBOT, V27, P409, DOI 10.1007/s10514-009-9153-8.
Ila V, 2010, IEEE T ROBOT, V26, P78, DOI 10.1109/TRO.2009.2034435.
Johannsson H, 2013, P IEEE INT C ROB AUT.
Kaess M., 2007, P IEEE INT C ROB AUT.
Konolige K, 2009, P IEEE RSJ INT C INT.
Kretzschmar H., 2011, P IEEE RSJ INT C INT.
Kretzschmar H, 2012, INT J ROBOT RES, V31, P1219, DOI 10.1177/0278364912455072.
Kummerle R., 2011, P IEEE INT C ROB AUT.
Mazuran M, 2014, P ROB SCI SYST.
NOCEDAL J, 1980, MATH COMPUT, V35, P773, DOI 10.2307/2006193.
Olson E., 2006, P IEEE INT C ROB AUT.
Paskin MA, 2003, P INT JOINT C ART.
Schmidt M., 2009, P INT C ART INT STAT.
Thrun S, 2004, INT J ROBOT RES, V23, P693, DOI 10.1177/0278364904045479.
Vandenberghe L, 1998, SIAM J MATRIX ANAL A, V19, P499, DOI 10.1137/S0895479896303430.
Vial J, 2011, P IEEE RSJ INT C INT.},
  da = {2022-05-17},
  doc-delivery-number = {DA8CT},
  eissn = {1741-3176},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {Mobile robotics; SLAM; localization; mapping; graphical models;
nonlinear optimization},
  keywords-plus = {SIMULTANEOUS LOCALIZATION},
  language = {English},
  number-of-cited-references = {37},
  orcid-numbers = {Burgard, Wolfram/0000-0002-5680-6500},
  research-areas = {Robotics},
  researcherid-numbers = {Burgard, Wolfram/N-2381-2019},
  times-cited = {30},
  type = {Article; Proceedings Paper},
  unique-id = {WOS:000368032600004},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {12},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{mohan-et-al:2015:7139966,
  author = {M. Mohan and D. Galvez-Lopez and G. C. A. S. Montcleoni},
  booktitle = {2015 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {Environment Selection And Hierarchical Place Recognition},
  pages = {5487--5494},
  doi = {10.1109/ICRA.2015.7139966},
  note = {IEEE International Conference on Robotics and Automation (ICRA),
Seattle, WA, MAY 26-30, 2015},
  publisher = {IEEE COMPUTER SOC},
  address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
  year = {2015},
  abstract = {As robots continue to create long-term maps, the amount of information
that they need to handle increases over time. In terms of place
recognition, this implies that the number of images being considered may
increase until exceeding the computational resources of the robot. In
this paper we consider a scenario where, given multiple independent
large maps, possibly from different cities or locations, a robot must
effectively and in real time decide whether it can localize itself in
one of those known maps. Since the number of images to be handled by
such a system is likely to be extremely large, we find that it is
beneficial to decompose the set of images into independent groups or
environments. This raises a new question: Given a query image, how do we
select the best environment? This paper proposes a similarity criterion
that can be used to solve this problem. It is based on the observation
that, if each environment is described in terms of its co-occurrent
features, similarity between environments can be established by
comparing their co-occurrence matrices. We show that this leads to a
novel place recognition algorithm that divides the collection of images
into environments and arranges them in a hierarchy of inverted indices.
By selecting first the relevant environment for the operating robot, we
can reduce the number of images to perform the actual loop detection,
reducing the execution time while preserving the accuracy. The
practicality of this approach is shown through experimental results on
several large datasets covering a combined distance of more than 750Km.},
  affiliation = {Mohan, M (Corresponding Author), George Washington Univ, Dept Comp Sci, Washington, DC 20052 USA.
Mohan, Mahesh; Galvez-Lopez, Dorian; Montcleoni, Claire, George Washington Univ, Dept Comp Sci, Washington, DC 20052 USA.
Sibley, Gabe, Univ Colorado Boulder, Dept Comp Sci, Boulder, CO USA.},
  affiliations = {George Washington University; University of Colorado System; University
of Colorado Boulder},
  author-email = {mahesh\_mohan@gwu.edu
dorian@gwu.edu
cmontel@gwu.edu
GSibley@colorado.edu},
  book-group-author = {IEEE},
  cited-references = {{[}Anonymous], 2013, NORDLANDSBANEN MINUT.
Blanco JL, 2009, AUTON ROBOT, V27, P327, DOI 10.1007/s10514-009-9138-7.
Cummins M, 2010, INT J ROBOT RES.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9\_11.
Glover AJ, 2010, IEEE INT CONF ROBOT, P3507, DOI 10.1109/ROBOT.2010.5509547.
Kashima H., 2003, INT C MACHINE LEARNI, P321.
Labbe M, 2013, IEEE T ROBOT, V29, P734, DOI 10.1109/TRO.2013.2242375.
Li Maohai, 2011, Information Technology Journal, V10, P29.
MacTavish K., 2014, IEEE INT C ROB AUT I.
Mei Christopher, 2010, INT C INT ROB SYST T.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Morrison Jack, 2014, INT S DISTR IN PRESS.
Murphy L, 2014, IEEE INT CONF ROBOT, P1312, DOI 10.1109/ICRA.2014.6907022.
Nicosevici T, 2012, IEEE T ROBOT, V28, P886, DOI 10.1109/TRO.2012.2192013.
Nistr D., 2006, P IEEE COMP VIS PAT, V2, P2161.
Pandey G, 2011, INT J ROBOT RES, V30, P1543, DOI 10.1177/0278364911400640.
Ponce J., 2006, P IEEE C COMP VIS PA, V2, P2169, DOI DOI 10.1109/CVPR.2006.68.
Rawseeds, 2007, FP6IST045144 RAWS.
Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.
Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201.
Xiao J., 2014, INT J COMPUT VISION, P1.},
  da = {2022-05-17},
  doc-delivery-number = {BE3MR},
  isbn = {978-1-4799-6923-4},
  issn = {1050-4729},
  keywords-plus = {NAVIGATION; VISION; WORDS},
  language = {English},
  number-of-cited-references = {25},
  orcid-numbers = {Monteleoni, Claire/0000-0002-9488-0517},
  research-areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {14},
  type = {Proceedings Paper},
  unique-id = {WOS:000370974905063},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Engineering, Electrical \& Electronic; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{rapp-et-al:2015:77,
  author = {M. Rapp and M. Hahn and M. Thom and K. J. A. D. Dickmann},
  booktitle = {2015 IEEE 18TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION
SYSTEMS},
  title = {Semi-Markov Process Based Localization using Radar in Dynamic
Environments},
  pages = {423--429},
  doi = {10.1109/ITSC.2015.77},
  note = {18th IEEE International Conference on Intelligent Transportation
Systems, SPAIN, SEP 15-18, 2015},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2015},
  abstract = {Automotive localization in urban environment faces natural long-term
changes of the surroundings. In this work, a robust Monte-Carlo based
localization is presented. Robustness is achieved through a stochastic
analysis of previous observations of the area of interest. The model
uses a grid-based Markov chain to instantly model changes. An extension
of this model by a Levy process allows statements about reliability and
prediction for each cell of the grid. Experiments with a vehicle
equipped with four short range radars show the localization accuracy
performance improvement in a dynamic environment.},
  affiliation = {Rapp, M (Corresponding Author), Univ Ulm, Inst Measurement Control \& Microtechnol, D-89069 Ulm, Germany.
Rapp, Matthias; Thom, Markus; Dietmayer, Klaus, Univ Ulm, Inst Measurement Control \& Microtechnol, D-89069 Ulm, Germany.
Hahn, Markus; Dickmann, Juergen, Daimler AG, Ulm, Germany.},
  affiliations = {Ulm University; Daimler AG},
  author-email = {matthias.rapp@uni-ulm.de
markus.hahn@daimler.de
markus.thom@uni-ulm.de
jurgen.dickmann@daimler.de
klaus.dietmayer@uni-ulm.de},
  book-group-author = {IEEE},
  cited-references = {Adams M, 2012, ROBOTIC NAVIGATION AND MAPPING WITH RADAR, P1.
Anguelov D., 2002, P 18 C UNC ART INT, P10.
Birk A, 2006, P IEEE, V94, P1384, DOI 10.1109/JPROC.2006.876965.
Bouzouraa ME, 2010, IEEE INT VEH SYM, P294, DOI 10.1109/IVS.2010.5548106.
Coue C, 2006, INT J ROBOT RES, V25, P19, DOI 10.1177/0278364906061158.
Fink D., 1997, COMPENDIUM CONJUGATE.
Fox D, 1999, J ARTIF INTELL RES, V11, P391, DOI 10.1613/jair.616.
Hahnel D, 2003, IEEE INT CONF ROBOT, P1557, DOI 10.1109/ROBOT.2003.1241816.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Luber Matthias, 2011, INT J ROBOTICS RES.
MEDHI J, 1983, STOCHASTIC PROCESSES.
Meyer-Delius D., 2012, AAAI.
Meyer-Delius D, 2010, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2010.5648920.
Montemerlo M, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2436.
Moravec H., 1985, P 1985 IEEE INT C RO, V2, P116.
Morris Timothy, 2014, IEEE INT C ROB AUT 2.
Quigley M, 2009, IEEE INT CONF ROBOT, P3604.
Rapp Matthias, 2015, INT VEH S.
Saarinen J, 2012, IEEE INT C INT ROBOT, P3489, DOI 10.1109/IROS.2012.6385629.
Schreier M, 2014, IEEE INT CONF ROBOT, P3995, DOI 10.1109/ICRA.2014.6907439.
Shephard N, 2012, BASICS LEVY PROCESSE.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Werber Klaudius, 2015, 2015 IEEE MTTS INT C.
Wolf DF, 2003, PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS 2003, VOL 1-3, P594.
Ziegler J, 2014, IEEE INTEL TRANSP SY, V6, P8, DOI 10.1109/MITS.2014.2306552.},
  da = {2022-05-17},
  doc-delivery-number = {BE8OT},
  isbn = {978-1-4673-6596-3},
  issn = {2153-0009},
  keywords-plus = {MAPS},
  language = {English},
  number-of-cited-references = {25},
  research-areas = {Transportation},
  series = {IEEE International Conference on Intelligent Transportation Systems-ITSC},
  times-cited = {11},
  type = {Proceedings Paper},
  unique-id = {WOS:000376668800070},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Transportation Science \& Technology},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{lazaro-et-al:2018:8594310,
  author = {M. T. Lazaro and R. Capobianco and G. Grisetti},
  booktitle = {2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS)},
  title = {Efficient Long-term Mapping in Dynamic Environments},
  pages = {153--160},
  doi = {10.1109/IROS.2018.8594310},
  note = {25th IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Madrid, SPAIN, OCT 01-05, 2018},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2018},
  abstract = {As autonomous robots are increasingly being introduced in real-world
environments operating for long periods of time, the difficulties of
long-term mapping are attracting the attention of the robotics research
community. This paper proposes a full SLAM system capable of handling
the dynamics of the environment across a single or multiple mapping
sessions.
Using the pose graph SLAM paradigm, the system works on local maps in
the form of 2D point cloud data which are updated over time to store the
most up-to-date state of the environment. The core of our system is an
efficient ICP-based alignment and merging procedure working on the
clouds that copes with non-static entities of the environment.
Furthermore, the system retains the graph complexity by removing
out-dated nodes upon robust inter-and intra-session loop closure
detections while graph coherency is preserved by using condensed
measurements. Experiments conducted with real data from longterm SLAM
datasets demonstrate the efficiency, accuracy and effectiveness of our
system in the management of the mapping problem during long-term robot
operation.},
  affiliation = {Lazaro, MT (Corresponding Author), Sapienza Univ Rome, Dipartimento Ingn Informat Automat \& Gest Antonio, Rome, Italy.
Lazaro, Maria T.; Capobianco, Roberto; Grisetti, Giorgio, Sapienza Univ Rome, Dipartimento Ingn Informat Automat \& Gest Antonio, Rome, Italy.},
  affiliations = {Sapienza University Rome},
  author-email = {mtlazaro@diag.uniroma1.it
capobianco@diag.uniroma1.it
grisetti@diag.uniroma1.it},
  book-author = {Kosecka, J},
  cited-references = {Biber P, 2009, INT J ROBOT RES, V28, P20, DOI 10.1177/0278364908096286.
Biswas  R., 2002, IEEE RSJ INT C INT R.
Burgard W, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2089, DOI 10.1109/IROS.2009.5354691.
Fallon M, 2013, INT J ROBOT RES, V32, P1695, DOI 10.1177/0278364913509035.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Grisetti G, 2012, IEEE INT C INT ROBOT, P581, DOI 10.1109/IROS.2012.6385779.
Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925.
Hanheide  M., 2017, ACM IEEE INT C HUM R.
Hess W, 2016, IEEE INT CONF ROBOT, P1271, DOI 10.1109/ICRA.2016.7487258.
Iocchi L, 2015, LECT NOTES ARTIF INT, V9336, P465, DOI 10.1007/978-3-319-24309-2\_35.
Johannsson H, 2013, IEEE INT CONF ROBOT, P54, DOI 10.1109/ICRA.2013.6630556.
Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706.
Kohlbrecher S, 2011, P IEEE INT S SAF SEC.
Krajnik  T., 2016, IEEE RSJ INT C INT R.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Labbe M, 2014, IEEE INT C INT ROBOT, P2661, DOI 10.1109/IROS.2014.6942926.
Latif  Y., 2013, EUR C MOB ROB BARC S.
Lazaro M. T., 2013, IEEE RSJ INT C INT R.
McDonald  J., 2011, ECMR.
Serafin J, 2017, ROBOT AUTON SYST, V92, P91, DOI 10.1016/j.robot.2017.03.008.
Triebel R, 2016, SPRINGER TRAC ADV RO, V113, P607, DOI 10.1007/978-3-319-27702-8\_40.
Vallve J, 2018, IEEE ROBOT AUTOM LET, V3, P1322, DOI 10.1109/LRA.2018.2798283.
Walcott-Bryant A, 2012, IEEE INT C INT ROBOT, P1871, DOI 10.1109/IROS.2012.6385561.
Wolf D, 2004, IEEE INT CONF ROBOT, P1301, DOI 10.1109/ROBOT.2004.1308004.},
  da = {2022-05-17},
  doc-delivery-number = {BM0LT},
  editor = {Maciejewski, AA and Okamura, A and Bicchi, A and Stachniss, C and Song, DZ and Lee, DH and Chaumette, F and Ding, H and Li, JS and Wen, J and Roberts, J and Masamune, K and Chong, NY and Amato, N and Tsagwarakis, N and Rocco, P and Asfour, T and Chung, WK and Yasuyoshi, Y and Sun, Y and Maciekeski, T and Althoefer, K and AndradeCetto, J and Chung, WK and Demircan, E and Dias, J and Fraisse, P and Gross, R and Harada, H and Hasegawa, Y and Hayashibe, M and Kiguchi, K and Kim, K and Kroeger, T and Li, Y and Ma, S and Mochiyama, H and Monje, CA and Rekleitis, I and Roberts, R and Stulp, F and Tsai, CHD and Zollo, L},
  isbn = {978-1-5386-8094-0},
  issn = {2153-0858},
  language = {English},
  number-of-cited-references = {24},
  orcid-numbers = {GRAÑON, MARIA TERESA LAZARO/0000-0002-7742-2442
Capobianco, Roberto/0000-0002-2219-215X},
  research-areas = {Computer Science; Robotics},
  researcherid-numbers = {GRAÑON, MARIA TERESA LAZARO/AAI-6857-2021
Capobianco, Roberto/AAH-7703-2021},
  series = {IEEE International Conference on Intelligent Robots and Systems},
  times-cited = {10},
  type = {Proceedings Paper},
  unique-id = {WOS:000458872700017},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {3},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Computer Science, Information
Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{carlevaris-bianco-et-al:2014:2347571,
  author = {N. Carlevaris-Bianco and M. Kaess and R. M. Eustice},
  journal = {IEEE TRANSACTIONS ON ROBOTICS},
  title = {Generic Node Removal for Factor-Graph SLAM},
  volume = {30},
  number = {6},
  pages = {1371--1385},
  doi = {10.1109/TRO.2014.2347571},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2014},
  month = {12},
  abstract = {This paper reports on a generic factor-based method for node removal in
factor-graph simultaneous localization and mapping (SLAM), which we call
generic linear constraints (GLCs). The need for a generic node removal
tool is motivated by long-term SLAM applications, whereby nodes are
removed in order to control the computational cost of graph
optimization. GLC is able to produce a new set of linearized factors
over the elimination clique that can represent either the true
marginalization (i.e., dense GLC) or a sparse approximation of the true
marginalization using a Chow-Liu tree (i.e., sparse GLC). The proposed
algorithm improves upon commonly used methods in two key ways: First, it
is not limited to graphs with strictly full-state relative-pose factors
and works equally well with other low-rank factors, such as those
produced by monocular vision. Second, the new factors are produced in
such a way that accounts for measurement correlation, which is a problem
encountered in other methods that rely strictly upon pairwise
measurement composition. We evaluate the proposed method over multiple
real-world SLAM graphs and show that it outperforms other recently
proposed methods in terms of Kullback-Leibler divergence. Additionally,
we experimentally demonstrate that the proposed GLC method provides a
principled and flexible tool to control the computational complexity of
long-term graph SLAM, with results shown for 34.9 h of real-world
indoor-outdoor data covering 147.4 km collected over 27 mapping sessions
spanning a period of 15 months.},
  affiliation = {Carlevaris-Bianco, N (Corresponding Author), Univ Michigan, Dept Elect Engn \& Comp Sci, Ann Arbor, MI 48109 USA.
Carlevaris-Bianco, Nicholas, Univ Michigan, Dept Elect Engn \& Comp Sci, Ann Arbor, MI 48109 USA.
Kaess, Michael, Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
Eustice, Ryan M., Univ Michigan, Dept Naval Architecture \& Marine Engn, Ann Arbor, MI 48109 USA.},
  affiliations = {University of Michigan System; University of Michigan; Carnegie Mellon
University; University of Michigan System; University of Michigan},
  author-email = {carlevar@umich.edu
kaess@cmu.edu
eustice@umich.edu},
  cited-references = {Carlevaris-Bianco N, 2014, IEEE INT CONF ROBOT, P854, DOI 10.1109/ICRA.2014.6906954.
Carlevaris-Bianco N, 2013, IEEE INT C INT ROBOT, P1034, DOI 10.1109/IROS.2013.6696478.
Carlevaris-Bianco N, 2013, IEEE INT CONF ROBOT, P5748, DOI 10.1109/ICRA.2013.6631403.
CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142.
Cunningham A., 2013, P IEEE INT C ROB AUT, P5200.
Davison AJ, 2005, IEEE I CONF COMP VIS, P66.
Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768.
Eade E, 2010, IEEE INT C INT ROBOT, P3017, DOI 10.1109/IROS.2010.5649205.
Eustice R., 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P3281.
Eustice RM, 2006, IEEE T ROBOT, V22, P1100, DOI 10.1109/TRO.2006.886264.
Folkesson J, 2004, IEEE INT CONF ROBOT, P383, DOI 10.1109/ROBOT.2004.1307180.
FRESE U, 2004, SPATIAL COGNITION, V4.
Frese U, 2007, IEEE INT CONF ROBOT, P4814, DOI 10.1109/ROBOT.2007.364221.
Friedman N., 2009, PROBABILISTIC GRAPHI.
Hover FS, 2012, INT J ROBOT RES, V31, P1445, DOI 10.1177/0278364912461059.
Huang GQ, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P150, DOI 10.1109/ECMR.2013.6698835.
Ila V, 2010, IEEE T ROBOT, V26, P78, DOI 10.1109/TRO.2009.2034435.
Johannsson H, 2013, IEEE INT CONF ROBOT, P54, DOI 10.1109/ICRA.2013.6630556.
Kaess M., 2010, OPEN SOURCE IMPLEMEN.
Kaess M, 2009, ROBOT AUTON SYST, V57, P1198, DOI 10.1016/j.robot.2009.06.008.
Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706.
Konolige K, 2008, IEEE T ROBOT, V24, P1066, DOI 10.1109/TRO.2008.2004832.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Kretzschmar H, 2012, INT J ROBOT RES, V31, P1219, DOI 10.1177/0278364912455072.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733.
Magnusson M., 2009, THESIS.
Mazuran M., 2014, ROBOTICS SCI SYSTEMS, P1.
MINKA T, 2001, INFERRING GAUSSIAN D.
Neira J, 2001, IEEE T ROBOTIC AUTOM, V17, P890, DOI 10.1109/70.976019.
Olson E, 2006, IEEE INT CONF ROBOT, P2262, DOI 10.1109/ROBOT.2006.1642040.
Ozog P, 2014, IEEE INT CONF ROBOT, P3832, DOI 10.1109/ICRA.2014.6907415.
Rao C., 1971, GEN INVERSE MATRICES.
Smith R., 1990, AUTONOMOUS ROBOT VEH, P167, DOI DOI 10.1007/978-1-4613-8997-2\_14.
Thrun S, 2004, INT J ROBOT RES, V23, P693, DOI 10.1177/0278364904045479.
Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387.
Vial J, 2011, IEEE INT C INT ROBOT, P886, DOI 10.1109/IROS.2011.6048728.
Walcott-Bryant A, 2012, IEEE INT C INT ROBOT, P1871, DOI 10.1109/IROS.2012.6385561.
Walter MR, 2007, INT J ROBOT RES, V26, P335, DOI 10.1177/0278364906075026.
Wang Y, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P32, DOI 10.1109/ECMR.2013.6698816.},
  da = {2022-05-17},
  doc-delivery-number = {AW1SO},
  eissn = {1941-0468},
  funding-acknowledgement = {National Science Foundation {[}IIS-0746455]; Office of Naval Research
{[}N00014-12-1-0092, N00014-12-1-0093]; Naval Sea Systems Command
through the Naval Engineering Education Center {[}N65540-10-C-0003]},
  funding-text = {This work was supported in part by the National Science Foundation under
Award IIS-0746455, by the Office of Naval Research under Award
N00014-12-1-0092 and Award N00014-12-1-0093, and by the Naval Sea
Systems Command through the Naval Engineering Education Center under
Award N65540-10-C-0003. This paper was presented in part at the 2013
IEEE International Conference on Robotics and Automation {[}1] and at
the 2013 IEEE/RSJ International Conference on Intelligent Robots and
Systems {[}2].},
  issn = {1552-3098},
  journal-iso = {IEEE Trans. Robot.},
  keywords = {Factor-graphs; long-term autonomy; marginalization; mobile robotics;
simultaneous localization and mapping (SLAM)},
  keywords-plus = {SIMULTANEOUS LOCALIZATION; POSE GRAPHS; INFORMATION; ALIGNMENT; FILTERS},
  language = {English},
  number-of-cited-references = {40},
  oa = {Green Submitted},
  orcid-numbers = {Kaess, Michael/0000-0002-7590-3357
Eustice, Ryan/0000-0002-9989-4942},
  research-areas = {Robotics},
  times-cited = {47},
  type = {Article},
  unique-id = {WOS:000346070400007},
  usage-count-last-180-days = {5},
  usage-count-since-2013 = {25},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{chebrolu-et-al:2018:2849603,
  author = {N. Chebrolu and T. Laebe and C. Stachniss},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {Robust Long-Term Registration of UAV Images of Crop Fields for Precision
Agriculture},
  volume = {3},
  number = {4},
  pages = {3090--3097},
  doi = {10.1109/LRA.2018.2849603},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2018},
  month = {10},
  abstract = {Continuous crop monitoring is an important aspect of precision
agriculture and requires the registration of sensor data over longer
periods of time. Often, fields are monitored using cameras mounted on
unmanned aerial vehicles (UAVs) but strong changes in the visual
appearance of the growing crops and the field itself poses serious
challenges to conventional image registration methods. In this letter,
we present a method for registering images of agricultural fields taken
by an UAV over the crop season and present a complete pipeline for
computing temporally aligned three-dimensional (3-D) point clouds of the
field. Our approach exploits the inherent geometry of the crop
arrangement in the field, which remains mostly static over time. This
allows us to register the images even in the presence of strong visual
changes. To this end, we propose a scale invariant, geometric feature
descriptor that encodes the local plant arrangement geometry. The
experiments suggest that we are able to register images taken over the
crop season, including situations where matching with an off-the-shelf
visual descriptor fails. We evaluate the accuracy of our matching system
with respect to manually labeled ground truth. We furthermore illustrate
that the reconstructed 3-D models are qualitatively correct and the
registration results allow for monitoring growth parameters at a per
plant level.},
  affiliation = {Chebrolu, N (Corresponding Author), Univ Bonn, Inst Geodesy \& Geoinformat, D-53113 Bonn, Germany.
Chebrolu, Nived; Laebe, Thomas; Stachniss, Cyrill, Univ Bonn, Inst Geodesy \& Geoinformat, D-53113 Bonn, Germany.},
  affiliations = {University of Bonn},
  author-email = {nived.chebrolu@igg.uni-bonn.de
laebe@ipb.uni-bonn.de
cyrill.stachniss@igg.uni-bonn.de},
  cited-references = {Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293.
Bryson M, 2010, J FIELD ROBOT, V27, P632, DOI 10.1002/rob.20343.
Carcassoni M, 2003, PATTERN RECOGN, V36, P193, DOI 10.1016/S0031-3203(02)00054-7.
Das J, 2015, IEEE INT CON AUTO SC, P462, DOI 10.1109/CoASE.2015.7294123.
Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161.
Gold S, 1998, PATTERN RECOGN, V31, P1019, DOI 10.1016/S0031-3203(98)80010-1.
Jing Dong, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3878, DOI 10.1109/ICRA.2017.7989447.
Kusumam K, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P646, DOI 10.1109/IROS.2016.7759121.
Labe T., 2006, P TURK GERM JOINT GE.
Li YY, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508368.
Lottes Philipp, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3024, DOI 10.1109/ICRA.2017.7989347.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003.
OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076.
Pfeifer J., 2016, P INT C AGR ENG.
Vysotska O, 2016, IEEE ROBOT AUTOM LET, V1, P213, DOI 10.1109/LRA.2015.2512936.
Wolfson HJ, 1997, IEEE COMPUT SCI ENG, V4, P10, DOI 10.1109/99.641604.
Yang LM, 2015, IEEE T VIS COMPUT GR, V21, P1299, DOI 10.1109/TVCG.2015.2459897.},
  da = {2022-05-17},
  doc-delivery-number = {GN1DE},
  funding-acknowledgement = {EC {[}H2020-ICT-644227-Flourish]},
  funding-text = {This work was supported by the EC under Grant H2020-ICT-644227-Flourish.},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {Robotics in agriculture and forestry; SLAM},
  keywords-plus = {ALGORITHMS},
  language = {English},
  number-of-cited-references = {20},
  orcid-numbers = {Stachniss, Cyrill/0000-0003-1173-6972
Laebe, Thomas/0000-0003-4873-513X},
  research-areas = {Robotics},
  times-cited = {27},
  type = {Article},
  unique-id = {WOS:000438724400011},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {15},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{piasco-et-al:2021:6,
  author = {N. Piasco and D. Sidibe and C. V. A. D. Gouet-Brunet},
  journal = {INTERNATIONAL JOURNAL OF COMPUTER VISION},
  title = {Improving Image Description with Auxiliary Modality for Visual
Localization in Challenging Conditions},
  volume = {129},
  number = {1},
  pages = {185--202},
  doi = {10.1007/s11263-020-01363-6},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2021},
  month = {1},
  abstract = {Image indexing for lifelong localization is a key component for a large
panel of applications, including robot navigation, autonomous driving or
cultural heritage valorization. The principal difficulty in long-term
localization arises from the dynamic changes that affect outdoor
environments. In this work, we propose a new approach for outdoor large
scale image-based localization that can deal with challenging scenarios
like cross-season, cross-weather and day/night localization. The key
component of our method is a new learned global image descriptor, that
can effectively benefit from scene geometry information during training.
At test time, our system is capable of inferring the depth map related
to the query image and use it to increase localization accuracy. We show
through extensive evaluation that our method can improve localization
performances, especially in challenging scenarios when the visual
appearance of the scene has changed. Our method is able to leverage both
visual and geometric clues from monocular images to create
discriminative descriptors for cross-season localization and effective
matching of images acquired at different time periods. Our method can
also use weakly annotated data to localize night images across a
reference dataset of daytime images. Finally we extended our method to
reflectance modality and we compare multi-modal descriptors respectively
based on geometry, material reflectance and a combination of both.},
  affiliation = {Piasco, N (Corresponding Author), Univ Bourgogne Franche Comte, ImViA, VIBOT ERL CNRS 6000, Dijon, France.
Piasco, N (Corresponding Author), Univ Paris Est, ENSG, IGN, LaSTIG, F-94160 St Mande, France.
Piasco, Nathan; Demonceaux, Cedric, Univ Bourgogne Franche Comte, ImViA, VIBOT ERL CNRS 6000, Dijon, France.
Piasco, Nathan; Gouet-Brunet, Valerie, Univ Paris Est, ENSG, IGN, LaSTIG, F-94160 St Mande, France.
Sidibe, Desire, Univ Evry, Univ Paris Saclay, IBISC, F-91020 Evry, France.},
  affiliations = {Universite de Bourgogne; Universite Gustave-Eiffel; UDICE-French
Research Universities; League of European Research Universities - LERU;
Universite Paris Saclay; Universite d'Evry-Val-d'Essonne},
  author-email = {nathan.piasco@gmail.com},
  cited-references = {Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI 10.1109/ICRA.2019.8794387.
Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122.
Arandjelovi R., 2014, AS C COMP VIS ACCV.
Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Ardeshir S, 2014, LECT NOTES COMPUT SC, V8694, P602, DOI 10.1007/978-3-319-10599-4\_39.
Aubry M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2591009.
Azzi C., 2016, BRIT MACH VIS C BMVC, V2, P1.
Bansal A, 2014, IEEE INT VEH SYM, P800, DOI 10.1109/IVS.2014.6856605.
Bevilacqua M, 2017, ISPRS J PHOTOGRAMM, V125, P16, DOI 10.1016/j.isprsjprs.2017.01.005.
Bhowmik N, 2017, JOINT URB REMOTE SEN.
Brachmann E, 2018, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2018.00489.
Cao Y, 2016, THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3457.
Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598.
Chevalier M, 2018, PATTERN RECOGN LETT, V116, P29, DOI 10.1016/j.patrec.2018.09.007.
Christie G., 2016, ARXIV160904794.
Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172.
Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601.
Cord, 2018, ADV NEURAL INFORM PR, P1310.
Croissant JG, 2016, FRONT MOL BIOSCI, V3, DOI 10.3389/fmolb.2016.00001.
Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921.
Eigen D., 2014, ADV NEURAL INFORM PR.
Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446.
Garg S, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV.
Garg S, 2018, IEEE INT CONF ROBOT, P3645, DOI 10.1109/ICRA.2018.8461051.
Germain H., 2018, ARXIV181203707.
Germain H, 2019, INT CONF 3D VISION, P513, DOI 10.1109/3DV.2019.00063.
Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699.
Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8.
Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4\_15.
Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0\_23.
Hays J., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587784.
Hinton G., 2015, ARXIV150302531, V14, P38.
Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96.
Iscen A., 2018, MINING MANIFOLDS MET.
Jegou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609.
Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348.
Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6\_43.
Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572.
Kim HJ, 2017, PROC CVPR IEEE, P3251, DOI 10.1109/CVPR.2017.346.
Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947.
Li W, 2018, IEEE T PATTERN ANAL, V40, P2030, DOI 10.1109/TPAMI.2017.2734890.
Liu L, 2019, IEEE I CONF COMP VIS, P2570, DOI 10.1109/ICCV.2019.00266.
Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685.
Loo SY, 2019, IEEE INT CONF ROBOT, P5218, DOI 10.1109/ICRA.2019.8794425.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Morago B, 2016, IEEE T IMAGE PROCESS, V7149, P12.
Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331.
Naseer Tayyab, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2614, DOI 10.1109/ICRA.2017.7989305.
Naseer T, 2018, IEEE T ROBOT, V34, P289, DOI 10.1109/TRO.2017.2788045.
Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724.
Paulin M, 2017, INT J COMPUT VISION, V121, P149, DOI 10.1007/s11263-016-0924-3.
Piasco N., 2019, BRIT MACH VIS C BMVC.
Piasco N, 2019, IEEE IMAGE PROC, P2561, DOI 10.1109/ICIP.2019.8803014.
Piasco N, 2019, IEEE INT CONF ROBOT, P9094, DOI 10.1109/ICRA.2019.8794221.
Piasco N, 2018, PATTERN RECOGN, V74, P90, DOI 10.1016/j.patcog.2017.09.013.
Porav H, 2019, IEEE INT CONF ROBOT, P7087, DOI 10.1109/ICRA.2019.8793486.
Porav H, 2018, IEEE INT CONF ROBOT, P1011, DOI 10.1109/ICRA.2018.8462894.
Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566.
Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0\_1.
Riba E., 2016, BRIT MACH VIS C.
Russell BC, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS).
Sarlin P.-E., 2018, P 2 C ROB LEARN CORL, P1.
Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300.
Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897.
Sattler T, 2016, PROC CVPR IEEE, P1582, DOI 10.1109/CVPR.2016.175.
Schonberger JL, 2018, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR.2018.00721.
Seymour M, 2019, COMMUN BIOL, V2, DOI 10.1038/s42003-019-0330-9.
Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107.
Shotton J, 2013, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2013.377.
Sizikova E., 2016, EUR C COMP VIS WORKS, P1.
Stenborg E, 2018, IEEE INT CONF ROBOT, P6484, DOI 10.1109/ICRA.2018.8463150.
Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.
Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695.
Toft C, 2018, LECT NOTES COMPUT SC, V11206, P391, DOI 10.1007/978-3-030-01216-8\_24.
Torii A, 2015, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2015.7298790.
Torii A, 2015, IEEE T PATTERN ANAL, V37, P2346, DOI 10.1109/TPAMI.2015.2409868.
Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316.
Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470.
Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042.
Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451.
Zamir AR, 2014, IEEE T PATTERN ANAL, V36, P1546, DOI 10.1109/TPAMI.2014.2299799.
Zamir AR, 2010, LECT NOTES COMPUT SC, V6314, P255, DOI 10.1007/978-3-642-15561-1\_19.
Zhou T., 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.632.
Zwald Laurent, 2012, ARXIV12076868.},
  da = {2022-05-17},
  doc-delivery-number = {PU0KV},
  earlyaccessdate = {AUG 2020},
  eissn = {1573-1405},
  funding-acknowledgement = {French ANR project pLaTINUM {[}ANR-15-CE23-0010]; NVIDIA Corporation},
  funding-text = {We would like to acknowledge the French ANR project pLaTINUM
(ANR-15-CE23-0010) for its financial support and Marco Bevilacqua for
kindly sharing the code of his inpainting algorithm used in this
research. We also gratefully acknowledge the support of NVIDIA
Corporation with the donation of the Titan Xp GPU used for this
research.},
  issn = {0920-5691},
  journal-iso = {Int. J. Comput. Vis.},
  keywords = {Localization; Image retrieval; Side modality learning; Depth from
monocular; Global image descriptor},
  keywords-plus = {REPRESENTATIONS; RECOGNITION},
  language = {English},
  number-of-cited-references = {87},
  oa = {Green Submitted},
  orcid-numbers = {SIDIBE, DESIRE/0000-0002-5843-7139
Demonceaux, Cedric/0000-0001-6916-1273},
  research-areas = {Computer Science},
  researcherid-numbers = {SIDIBE, DESIRE/AFQ-8070-2022
Demonceaux, Cedric/S-5643-2017},
  times-cited = {2},
  type = {Article},
  unique-id = {WOS:000565043900001},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {9},
  web-of-science-categories = {Computer Science, Artificial Intelligence},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{biber-duckett:2009:0278364908096286,
  author = {P. Biber and T. Duckett},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {Experimental Analysis of Sample-Based Maps for Long-Term SLAM},
  volume = {28},
  number = {1},
  pages = {20--33},
  doi = {10.1177/0278364908096286},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2009},
  month = {1},
  abstract = {This paper presents a system for long-term SLAM (simultaneous
localization and mapping) by mobile service robots and its experimental
evaluation in a real dynamic environment. To deal with the
stability-plasticity dilemma (the trade-off between adaptation to new
patterns and preservation of old patterns), the environment is
represented by multiple timescales simultaneously (five in our
experiments). A sample-based representation is proposed, where older
memories fade at different rates depending on the timescale and robust
statistics are used to interpret the samples. The dynamics of this
representation are analyzed in a five-week experiment, measuring the
relative influence of short- and long-term memories over time and
further demonstrating the robustness of the approach.},
  affiliation = {Biber, P (Corresponding Author), Univ Tubingen, Dept Comp Sci, WSI GRIS, Tubingen, Germany.
Biber, Peter, Univ Tubingen, Dept Comp Sci, WSI GRIS, Tubingen, Germany.
Duckett, Tom, Lincoln Univ, Dept Comp \& Informat, Lincoln LN6 7TS, England.},
  affiliations = {Eberhard Karls University of Tubingen; University of Lincoln},
  author-email = {dr.peter.biber@googleemail.com
tduckett@lincoln.ac.uk},
  cited-references = {Andrade-Cetto J, 2002, INT J PATTERN RECOGN, V16, P361, DOI 10.1142/S0218001402001745.
ANGUELOV D, 2002, P 17 ANN C UNC AI UA.
BIBER P, 2003, INT C INT ROB SYST I.
BIBER P, 2005, P ROB SCI SYST CAMBR, V1.
BIBER P, 2007, THESIS U TUBINGEN GE.
Burgard W, 1999, ARTIF INTELL, V114, P3, DOI 10.1016/S0004-3702(99)00070-3.
Dennis J.E, 1996, SIAM CLASSICS APPL M.
Duda R., 2001, PATTERN CLASSIFICATI, Vxx.
GROSSBERG S, 1988, ADAPTIVE BRAIN.
HAHNEL D, 2003, MAP BUILDING MOBILE.
Huber P. J, 2011, ROBUST STAT.
Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733.
STACHNISS C, 2005, P NAT C ART INT AAAI.
Sutton R., 1998, INTRO REINFORCEMENT.
Thrun S., 1999, ICRA.
Thrun S., 2003, ICRA.
Yamauchi B, 1996, IEEE T SYST MAN CY B, V26, P496, DOI 10.1109/3477.499799.
ZIMMER U, 1995, THESIS U KAISERSLAUT.},
  da = {2022-05-17},
  doc-delivery-number = {390DF},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {simultaneous localization and mapping; dynamic environments; mobile
robot navigation; lifelong learning; multi-timescale representations},
  keywords-plus = {DYNAMIC ENVIRONMENTS},
  language = {English},
  number-of-cited-references = {18},
  oa = {Green Submitted, Green Accepted},
  research-areas = {Robotics},
  times-cited = {44},
  type = {Article},
  unique-id = {WOS:000262143800002},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {8},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{egger-et-al:2018:8593854,
  author = {P. Egger and P. V. K. Borges and G. Catt and P. Andreas and R. Siegwart and R. Dube},
  booktitle = {2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS)},
  title = {PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization},
  pages = {3430--3437},
  doi = {10.1109/IROS.2018.8593854},
  note = {25th IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Madrid, SPAIN, OCT 01-05, 2018},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2018},
  abstract = {Reliable long-term localization is key for robotic systems in dynamic
environments. In this paper, we propose a novel approach for long-term
localization using 3D LiDARs, coined PoseMap. In essence, we extract
distinctive features from range measurements and bundle these into local
views along with observation poses. The sensor's trajectory is then
estimated in a sliding window fashion by matching current and old
features and minimizing the distances in-between. The map representation
facilitates finding a suitable set of old features, by selecting the
closest local map(s) for matching. Similarly to a visibility analysis,
this procedure provides a suitable set of features for localization but
at a fraction of the computational cost. PoseMap also allows for updates
and extensions of the map at any time by replacing and adding local maps
when necessary. We evaluate our approach using two platforms both
equipped with a 3D LiDAR and an IMU, demonstrating localization at 8 Hz
and robustness to changes in the environment such as moving vehicles and
changing vegetation. PoseMap was implemented on an autonomous vehicle
allowing it to drive autonomously over a period of 18 months through a
mix of industrial and unstructured off-road environments, covering more
than 100 kms without a single localization failure.},
  affiliation = {Egger, P (Corresponding Author), CSIRO, Data61, Robot \& Autonomous Syst Grp, Canberra, ACT, Australia.
Egger, P (Corresponding Author), Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland.
Egger, Philipp; Borges, Paulo V. K.; Catt, Gavin, CSIRO, Data61, Robot \& Autonomous Syst Grp, Canberra, ACT, Australia.
Egger, Philipp; Pfrunder, Andreas; Siegwart, Roland; Dube, Renaud, Swiss Fed Inst Technol, Autonomous Syst Lab, Zurich, Switzerland.},
  affiliations = {Commonwealth Scientific \& Industrial Research Organisation (CSIRO); ETH
Zurich},
  author-email = {philipp-egger@outlook.com
paulo.borges@csiro.au
gavin.catt@csiro.au
andrepfr@ethz.ch
rdube@ethz.ch},
  book-author = {Kosecka, J},
  cited-references = {Biber P, 2009, INT J ROBOT RES, V28, P20, DOI 10.1177/0278364908096286.
Bosse M., 2009, 2009 IEEE INT C ROBO, P4312, DOI {[}10.1109/ROBOT.2009.5152851, DOI 10.1109/ROBOT.2009.5152851].
Bosse M., 2013, IEEE INT C ROB AUT I.
Bosse M, 2008, INT J ROBOT RES, V27, P667, DOI 10.1177/0278364908091366.
Bosse M, 2012, IEEE T ROBOT, V28, P1104, DOI 10.1109/TRO.2012.2200990.
Cadena C., 2017, PROC IEEE INT C ROBO, P5266.
Dube R., 2017, IEEE RSJ INT C INT R.
Dube R, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV.
ELFES A, 1989, COMPUTER, V22, P46, DOI 10.1109/2.30720.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Hess W, 2016, IEEE INT CONF ROBOT, P1271, DOI 10.1109/ICRA.2016.7487258.
Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0.
Kohlbrecher S., 2011, 2011 Proceedings of IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR 2011), P155, DOI 10.1109/SSRR.2011.6106777.
Maddern W, 2015, IEEE INT CONF ROBOT, P1684, DOI 10.1109/ICRA.2015.7139414.
McManus C, 2015, AUTON ROBOT, V39, P363, DOI 10.1007/s10514-015-9463-y.
Meyer-Delius D., 2012, AAAI.
Muhlfellner P, 2016, J FIELD ROBOT, V33, P561, DOI 10.1002/rob.21595.
Pfrunder A, 2017, IEEE INT C INT ROBOT, P2601, DOI 10.1109/IROS.2017.8206083.
Saarinen J, 2012, IEEE INT C INT ROBOT, P3489, DOI 10.1109/IROS.2012.6385629.
Walcott-Bryant A, 2012, IEEE INT C INT ROBOT, P1871, DOI 10.1109/IROS.2012.6385561.
Zhang J., 2014, ROBOTICS SCI SYSTEMS, V2.},
  da = {2022-05-17},
  doc-delivery-number = {BM0LT},
  editor = {Maciejewski, AA and Okamura, A and Bicchi, A and Stachniss, C and Song, DZ and Lee, DH and Chaumette, F and Ding, H and Li, JS and Wen, J and Roberts, J and Masamune, K and Chong, NY and Amato, N and Tsagwarakis, N and Rocco, P and Asfour, T and Chung, WK and Yasuyoshi, Y and Sun, Y and Maciekeski, T and Althoefer, K and AndradeCetto, J and Chung, WK and Demircan, E and Dias, J and Fraisse, P and Gross, R and Harada, H and Hasegawa, Y and Hayashibe, M and Kiguchi, K and Kim, K and Kroeger, T and Li, Y and Ma, S and Mochiyama, H and Monje, CA and Rekleitis, I and Roberts, R and Stulp, F and Tsai, CHD and Zollo, L},
  isbn = {978-1-5386-8094-0},
  issn = {2153-0858},
  keywords-plus = {MAPS},
  language = {English},
  number-of-cited-references = {21},
  research-areas = {Computer Science; Robotics},
  series = {IEEE International Conference on Intelligent Robots and Systems},
  times-cited = {23},
  type = {Proceedings Paper},
  unique-id = {WOS:000458872703031},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {5},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Computer Science, Information
Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{ganti-waslander:2019:00024,
  author = {P. Ganti and S. L. Waslander},
  booktitle = {2019 16TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2019)},
  title = {Network Uncertainty Informed Semantic Feature Selection for Visual SLAM},
  pages = {121--128},
  doi = {10.1109/CRV.2019.00024},
  note = {16th Conference on Computer and Robot Vision (CRV), Kingston, CANADA,
MAY 29-31, 2019},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2019},
  abstract = {In order to facilitate long-term localization using a visual
simultaneous localization and mapping (SLAM) algorithm, careful feature
selection can help ensure that reference points persist over long
durations and the runtime and storage complexity of the algorithm remain
consistent. We present SIVO (Semantically Informed Visual Odometry and
Mapping), a novel information-theoretic feature selection method for
visual SLAM which incorporates semantic segmentation and neural network
uncertainty into the feature selection pipeline. Our algorithm selects
points which provide the highest reduction in Shannon entropy between
the entropy of the current state and the joint entropy of the state,
given the addition of the new feature with the classification entropy of
the feature from a Bayesian neural network. Each selected feature
significantly reduces the uncertainty of the vehicle state and has been
detected to be a static object (building, traffic sign, etc.) repeatedly
with a high confidence. This selection strategy generates a sparse map
which can facilitate long-term localization. The KITTI odometry dataset
is used to evaluate our method, and we also compare our results against
ORB\_SLAM2. Overall, SIVO performs comparably to the baseline method
while reducing the map size by almost 70\%.},
  affiliation = {Ganti, P (Corresponding Author), Univ Waterloo, Dept Mech \& Mechatron Engn, Waterloo, ON, Canada.
Ganti, Pranav, Univ Waterloo, Dept Mech \& Mechatron Engn, Waterloo, ON, Canada.
Waslander, Steven L., Univ Toronto, Inst Aerosp Studies, Toronto, ON, Canada.},
  affiliations = {University of Waterloo; University of Toronto},
  author-email = {pganti@uwaterloo.ca
stevenw@utias.utoronto.ca},
  book-group-author = {IEEE},
  cited-references = {An LF, 2017, INT J ADV ROBOT SYST, V14, DOI 10.1177/1729881417735667.
Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Bowman Sean L., 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1722, DOI 10.1109/ICRA.2017.7989203.
Chli M., 2010, THESIS.
Choudhary S, 2015, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ICRA.2015.7139839.
Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350.
Cover T. M., 2012, ELEMENTS INFORM THEO.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Davison AJ, 2005, IEEE I CONF COMP VIS, P66.
Dissanayake G., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1009, DOI 10.1109/ROBOT.2000.844732.
FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
Gal Y., 2015, ARXIV150602158.
Gal Y., 2016, UNCERTAINTY DEEP LEA.
Gal Y., 2015, ARXIV150602157.
Gal Y, 2016, PR MACH LEARN RES, V48.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Hochdorfer S, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P382, DOI 10.1109/IROS.2009.5354433.
Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889.
Kaess M, 2009, ROBOT AUTON SYST, V57, P1198, DOI 10.1016/j.robot.2009.06.008.
Kendall A., 2015, ARXIV151102680.
Kendall A., 2017, P ADV NEUR INF PROC, P5574.
Kendall A, 2016, IEEE INT CONF ROBOT, P4762, DOI 10.1109/ICRA.2016.7487679.
Kumar RT., 2017, P 2017 IEEE 20 INT C, P1.
Li X., 2016, ARXIV161104144.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mu BP, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4602, DOI 10.1109/IROS.2016.7759677.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Neal R. M., 1995, THESIS.
Nister David, 2004, IEEE C COMP VIS PATT, V1.
Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023\_34.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Salas-Moreno RF, 2013, PROC CVPR IEEE, P1352, DOI 10.1109/CVPR.2013.178.
Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
Stenborg E, 2018, IEEE INT CONF ROBOT, P6484, DOI 10.1109/ICRA.2018.8463150.
Strasdat H., 2009, IEEE INT C ROB AUT K, P1410.
Zhang F, 2005, 2005 IEEE International Conference on Mechatronics and Automations, Vols 1-4, Conference Proceedings, P2117.},
  da = {2022-05-17},
  doc-delivery-number = {BO1TX},
  isbn = {978-1-7281-1838-3},
  keywords = {Localization; Mapping; SLAM; Deep Learning; Information Theory; Semantic
Segmentation},
  language = {English},
  number-of-cited-references = {38},
  oa = {Green Submitted},
  research-areas = {Computer Science; Engineering; Robotics},
  times-cited = {4},
  type = {Proceedings Paper},
  unique-id = {WOS:000502543900016},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {10},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Engineering, Electrical \&
Electronic; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{gao-zhang:2020:9196906,
  author = {P. Gao and H. Zhang},
  booktitle = {2020 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {Long-term Place Recognition through Worst-case Graph Matching to
Integrate Landmark Appearances and Spatial Relationships},
  pages = {1070--1076},
  doi = {10.1109/ICRA40945.2020.9196906},
  note = {IEEE International Conference on Robotics and Automation (ICRA), ELECTR
NETWORK, MAY 31-JUN 15, 2020},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2020},
  abstract = {Place recognition is an important component for simultaneously
localization and mapping in a variety of robotics applications.
Recently, several approaches using landmark information to represent a
place showed promising performance to address long-term environment
changes. However, previous approaches do not explicitly consider changes
of the landmarks, i,e., old landmarks may disappear and new ones often
appear over time. In addition, representations used in these approaches
to represent landmarks are limited, based upon visual or spatial cues
only. In this paper, we introduce a novel worst-case graph matching
approach that integrates spatial relationships of landmarks with their
appearances for long-term place recognition. Our method designs a graph
representation to encode distance and angular spatial relationships as
well as visual appearances of landmarks in order to represent a place.
Then, we formulate place recognition as a graph matching problem under
the worst-case scenario. Our approach matches places by computing the
similarities of distance and angular spatial relationships of the
landmarks that have the least similar appearances (i.e., worst-case). If
the worst appearance similarity of landmarks is small, two places are
identified to be not the same, even though their graph representations
have high spatial relationship similarities. We evaluate our approach
over two public benchmark datasets for long-term place recognition,
including St. Lucia and CMU-VL. The experimental results have validated
that our approach obtains the state-of-the-art place recognition
performance, with a changing number of landmarks.},
  affiliation = {Gao, P (Corresponding Author), Colorado Sch Mines, Dept Comp Sci, Human Ctr Robot Lab, Golden, CO 80401 USA.
Gao, Peng; Zhang, Hao, Colorado Sch Mines, Dept Comp Sci, Human Ctr Robot Lab, Golden, CO 80401 USA.},
  affiliations = {Colorado School of Mines},
  author-email = {gaopeng@mines.edu
hzhang@mines.edu},
  book-group-author = {IEEE},
  cited-references = {Arandjelovic R., 2016, IEEE C COMPUTER VISI.
Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150.
Badino H, 2012, IEEE INT CONF ROBOT, P1635, DOI 10.1109/ICRA.2012.6224716.
Cadena C, 2012, IEEE T ROBOT, V28, P871, DOI 10.1109/TRO.2012.2189497.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Endres F, 2012, IEEE INT CONF ROBOT, P1691, DOI 10.1109/ICRA.2012.6225199.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Gao X, 2017, AUTON ROBOT, V41, P1, DOI 10.1007/s10514-015-9516-2.
Glover AJ, 2010, IEEE INT CONF ROBOT, P3507, DOI 10.1109/ROBOT.2010.5509547.
Han F, 2018, IEEE ROBOT AUTOM LET, V3, P3669, DOI 10.1109/LRA.2018.2856274.
Han F, 2018, AUTON ROBOT, V42, P1323, DOI 10.1007/s10514-018-9736-3.
Han F, 2017, IEEE ROBOT AUTOM LET, V2, P1172, DOI 10.1109/LRA.2017.2662061.
Hansen P, 2014, IEEE INT C INT ROBOT, P4549, DOI 10.1109/IROS.2014.6943207.
Ho KL, 2006, ROBOT AUTON SYST, V54, P740, DOI 10.1016/j.robot.2006.04.016.
Hou Y, 2018, J INTELL ROBOT SYST, V92, P505, DOI 10.1007/s10846-017-0735-y.
Johns E, 2013, IEEE INT CONF ROBOT, P3212, DOI 10.1109/ICRA.2013.6631024.
Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650.
Kitt B. M., 2011, MONOCULAR VISUAL ODO.
Lategahn H, 2011, IEEE INT CONF ROBOT, P1732.
Latif Y., 2014, ROBOTICS SCI SYSTEMS.
Lee D, 2013, ROBOT INTELLIGENCE T.
Liu K, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P8034.
Lowry S, 2016, IEEE T ROBOT, V32, P600, DOI 10.1109/TRO.2016.2545711.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Milford MJ, 2004, IEEE INT CONF ROBOT, P403, DOI 10.1109/ROBOT.2004.1307183.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2017, IEEE ROBOT AUTOM LET, V2, P796, DOI 10.1109/LRA.2017.2653359.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Naseer T, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564.
Naseer T, 2015, IEEE INT C INT ROBOT, P2529, DOI 10.1109/IROS.2015.7353721.
Neubert P, 2015, ROBOT AUTON SYST, V69, P15, DOI 10.1016/j.robot.2014.08.005.
Newman P, 2006, IEEE INT CONF ROBOT, P1180, DOI 10.1109/ROBOT.2006.1641869.
Panphattarasap P., 2016, P AS C COMP VIS, P487.
Pronobis A, 2010, INT J ROBOT RES, V29, P298, DOI 10.1177/0278364909356483.
Rabanser S., 2015, MACH LEARN, V98, P1.
Romeijn HE, 2000, DISCRETE APPL MATH, V103, P209, DOI 10.1016/S0166-218X(99)00224-3.
Siam Sayem Mohammad, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5702, DOI 10.1109/ICRA.2017.7989671.
Siva S, 2018, IEEE INT CONF ROBOT, P5175.
Strasdat H, 2012, IMAGE VISION COMPUT, V30, P65, DOI 10.1016/j.imavis.2012.02.009.
Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986.
Sunderhauf N, 2011, IEEE INT C INT ROBOT, P1234, DOI 10.1109/IROS.2011.6048590.
Ulrich I., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1023, DOI 10.1109/ROBOT.2000.844734.
Yang SW, 2017, ROBOT AUTON SYST, V93, P116, DOI 10.1016/j.robot.2017.03.018.
Zetao Chen, 2017, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), P9, DOI 10.1109/IROS.2017.8202131.
Zhang H, 2016, ROBOTICS: SCIENCE AND SYSTEMS XII.
Zhou HZ, 2015, IEEE T VEH TECHNOL, V64, P1364, DOI 10.1109/TVT.2015.2388780.
Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1\_26.},
  da = {2022-05-17},
  doc-delivery-number = {BS3LB},
  eissn = {2577-087X},
  funding-acknowledgement = {DOT PHMSA {[}693JK31850005CAAP]; DOE {[}DE-FE0031650];  {[}IIS-1942056];
{[}IIS-1849348];  {[}IIS1849359]},
  funding-text = {This work was partially supported by IIS-1942056, IIS-1849348,
IIS1849359, DOT PHMSA 693JK31850005CAAP, and DOE DE-FE0031650.},
  isbn = {978-1-7281-7395-5},
  issn = {1050-4729},
  keywords-plus = {LOOP CLOSURE DETECTION; VISUAL SLAM; FEATURES},
  language = {English},
  number-of-cited-references = {48},
  research-areas = {Automation \& Control Systems; Engineering; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {3},
  type = {Proceedings Paper},
  unique-id = {WOS:000712319500114},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Automation \& Control Systems; Engineering, Electrical \& Electronic;
Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{muehlfellner-et-al:2016:21595,
  author = {P. Muehlfellner and M. Burki and M. Bosse and D. Wojciech and R. Philippsen and P. Furgale},
  journal = {JOURNAL OF FIELD ROBOTICS},
  title = {Summary Maps for Lifelong Visual Localization},
  volume = {33},
  number = {5},
  pages = {561--590},
  doi = {10.1002/rob.21595},
  publisher = {WILEY},
  address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  year = {2016},
  month = {8},
  abstract = {Robots that use vision for localization need to handle environments that
are subject to seasonal and structural change, and operate under
changing lighting and weather conditions. We present a framework for
lifelong localization and mapping designed to provide robust and
metrically accurate online localization in these kinds of changing
environments. Our system iterates between offline map building, map
summary, and online localization. The offline mapping fuses data from
multiple visually varied datasets, thus dealing with changing
environments by incorporating new information. Before passing these data
to the online localization system, the map is summarized, selecting only
the landmarks that are deemed useful for localization. This Summary Map
enables online localization that is accurate and robust to the variation
of visual information in natural environments while still being
computationally efficient. We present a number of summary policies for
selecting useful features for localization from the multisession map,
and we explore the tradeoff between localization performance and
computational complexity. The system is evaluated on 77 recordings, with
a total length of 30 kilometers, collected outdoors over 16 months.
These datasets cover all seasons, various times of day, and changing
weather such as sunshine, rain, fog, and snow. We show that it is
possible to build consistent maps that span data collected over an
entire year, and cover day-to-night transitions. Simple statistics
computed on landmark observations are enough to produce a Summary Map
that enables robust and accurate localization over a wide range of
seasonal, lighting, and weather conditions.},
  affiliation = {Muhlfellner, P (Corresponding Author), Halmstad Univ, Dept Driver Assistance \& Integrated Safety, Volkswagen AG, Letter Box 011-1777, Wolfsburg, Germany.
Muehlfellner, Peter, Halmstad Univ, Dept Driver Assistance \& Integrated Safety, Volkswagen AG, Letter Box 011-1777, Wolfsburg, Germany.
Burki, Mathias; Bosse, Michael; Furgale, Paul, ETH, Autonomous Syst Lab, Leonhardstr 21, Zurich, Switzerland.
Derendarz, Wojciech, Volkswagen AG, Dept Driver Assistance \& Integrated Safety, Letter Box 011-1777, Wolfsburg, Germany.
Philippsen, Roland, Halmstad Univ, Intelligent Syst Lab, Kristian IVs Vag 3, Halmstad, Sweden.},
  affiliations = {Volkswagen; Volkswagen Germany; ETH Zurich; Volkswagen; Volkswagen
Germany; Halmstad University},
  author-email = {peter.muehlfellner@volkswagen.de
mathias.buerki@mavt.ethz.ch
mike.bosse@mavt.ethz.ch
wojciech.derendarz@volkswagen.de
roland.philippsen@hh.se
paul.furgale@mavt.ethz.ch},
  cited-references = {Agarwal S, 2010, LECT NOTES COMPUT SC, V6312, P29, DOI 10.1007/978-3-642-15552-9\_3.
Bosse M, 2003, IEEE INT CONF ROBOT, P1899, DOI 10.1109/ROBOT.2003.1241872.
Bosse M, 2013, IEEE INT CONF ROBOT, P2677, DOI 10.1109/ICRA.2013.6630945.
Brubaker MA, 2013, PROC CVPR IEEE, P3057, DOI 10.1109/CVPR.2013.393.
Carlevaris-Bianco N., 2014, IEEE RSJ INT C INT R.
Carlevaris-Bianco N, 2013, IEEE INT C INT ROBOT, P1034, DOI 10.1109/IROS.2013.6696478.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Corke P, 2013, IEEE INT C INT ROBOT, P2085, DOI 10.1109/IROS.2013.6696648.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Dayoub F, 2011, ROBOT AUTON SYST, V59, P285, DOI 10.1016/j.robot.2011.02.013.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Furgale P, 2013, IEEE INT VEH SYM, P809, DOI 10.1109/IVS.2013.6629566.
Furgale P, 2010, J FIELD ROBOT, V27, P534, DOI 10.1002/rob.20342.
Heng L., 2014, J FIELD ROBOTICS.
Johannsson H., 2012, RSS WORKSH LONGT OP.
Johns E, 2014, INT J COMPUT VISION, V106, P297, DOI 10.1007/s11263-013-0648-6.
Johns E, 2013, IEEE INT CONF ROBOT, P3212, DOI 10.1109/ICRA.2013.6631024.
Kneip L, 2014, IEEE INT CONF ROBOT, P1, DOI 10.1109/ICRA.2014.6906582.
Kneip L, 2013, IEEE I CONF COMP VIS, P2352, DOI 10.1109/ICCV.2013.292.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Krajnik T., 2013, P 2013 16 INT C ADV, P1.
Krajnik T, 2014, IEEE INT C INT ROBOT, P4537, DOI 10.1109/IROS.2014.6943205.
Krajnik T, 2010, J FIELD ROBOT, V27, P511, DOI 10.1002/rob.20354.
Lategahn H., 2012, 2012 IEEE International Conference on Vehicular Electronics and Safety (ICVES 2012), P1, DOI 10.1109/ICVES.2012.6294279.
Lategahn H, 2014, IEEE T INTELL TRANSP, V15, P1246, DOI 10.1109/TITS.2014.2298492.
Lategahn H, 2013, IEEE INT VEH SYM, P285, DOI 10.1109/IVS.2013.6629483.
Lee G., 2014, THESIS.
Lee GH, 2013, IEEE INT C INT ROBOT, P564, DOI 10.1109/IROS.2013.6696407.
Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542.
Li YP, 2010, LECT NOTES COMPUT SC, V6312, P791.
Lovegrove S, 2011, IEEE INT VEH SYM, P788, DOI 10.1109/IVS.2011.5940546.
Maddern W., 2012, P ROB SCI SYST C 201.
Maddern W, 2014, IEEE INT VEH SYM, P330, DOI 10.1109/IVS.2014.6856471.
Mazuran M., 2014, P ROB SCI SYST BERK.
McManus C, 2014, P ROB SCI SYST RSS B.
McManus C., 2014, 2014 IEEE INT C ROB.
McManus C, 2013, J FIELD ROBOT, V30, P254, DOI 10.1002/rob.21444.
Milford M, 2014, J FIELD ROBOT, V31, P814, DOI 10.1002/rob.21532.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Muehlfellner P, 2013, 2013 IEEE INTELLIGENT VEHICLES SYMPOSIUM WORKSHOPS (IV WORKSHOPS), P57.
Naseer T., 2014, P NAT C ART INT AAAI.
Neubert P, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P198, DOI 10.1109/ECMR.2013.6698842.
Ni K, 2007, IEEE I CONF COMP VIS, P2009.
Pepperell E, 2014, IEEE INT CONF ROBOT, P1612, DOI 10.1109/ICRA.2014.6907067.
Sibley G., 2009, ROBOTICS SCI SYSTEMS.
Sibley G, 2010, INT J ROBOT RES, V29, P958, DOI 10.1177/0278364910369268.
Snavely N., 2008, CVPR, V1, P2.
Stewart AD, 2012, IEEE INT CONF ROBOT, P2625, DOI 10.1109/ICRA.2012.6224750.
Valgren C, 2010, ROBOT AUTON SYST, V58, P149, DOI 10.1016/j.robot.2009.09.010.
Zhang ZY, 1997, IMAGE VISION COMPUT, V15, P59, DOI 10.1016/S0262-8856(96)01112-2.
Ziegler J, 2014, IEEE INT VEH SYM, P1231, DOI 10.1109/IVS.2014.6856560.
Ziegler J, 2014, IEEE INTEL TRANSP SY, V6, P8, DOI 10.1109/MITS.2014.2306552.},
  da = {2022-05-17},
  doc-delivery-number = {DR7TU},
  eissn = {1556-4967},
  funding-acknowledgement = {European Community {[}269916, 610603]},
  funding-text = {This work is supported in part by the European Community's Seventh
Framework Programme (FP7/2007-2013) under Grants No. 269916 (V-Charge)
and No. 610603 (EU-ROPA2).},
  issn = {1556-4959},
  journal-iso = {J. Field Robot.},
  keywords-plus = {FAB-MAP; APPEARANCE; NAVIGATION; REPEAT; SCALE; TEACH},
  language = {English},
  number-of-cited-references = {53},
  research-areas = {Robotics},
  times-cited = {48},
  type = {Article},
  unique-id = {WOS:000380103400001},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {19},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{neubert-et-al:2015:005,
  author = {P. Neubert and N. Suenderhauf and P. Protzel},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {Superpixel-based appearance change prediction for long-term navigation
across seasons},
  volume = {69},
  number = {SI},
  pages = {15--27},
  doi = {10.1016/j.robot.2014.08.005},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2015},
  month = {7},
  abstract = {Changing environments pose a serious problem to current robotic systems
aiming at long term operation under varying seasons or local weather
conditions. This paper is built on our previous work where we propose to
learn to predict the changes in an environment. Our key insight is that
the occurring scene changes are in part systematic, repeatable and
therefore predictable. The goal of our work is to support existing
approaches to place recognition by learning how the visual appearance of
an environment changes over time and by using this learned knowledge to
predict its appearance under different environmental conditions. We
describe the general idea of appearance change prediction (ACP) and
investigate properties of our novel implementation based on vocabularies
of superpixels (SP-ACP). Our previous work showed that the proposed
approach significantly improves the performance of SeqSLAM and
BRIEF-Gist for place recognition on a subset of the Nordland dataset
under extremely different environmental conditions in summer and winter.
This paper deepens the understanding of the proposed SP-ACP system and
evaluates the influence of its parameters. We present the results of a
large-scale experiment on the complete 10 h Nordland dataset and
appearance change predictions between different combinations of seasons.
(C) 2014 Elsevier B.V. All rights reserved.},
  affiliation = {Neubert, P (Corresponding Author), Tech Univ Chemnitz, Dept Elect Engn \& Informat Technol, D-09111 Chemnitz, Germany.
Neubert, Peer; Suenderhauf, Niko; Protzel, Peter, Tech Univ Chemnitz, Dept Elect Engn \& Informat Technol, D-09111 Chemnitz, Germany.},
  affiliations = {Technische Universitat Chemnitz},
  author-email = {peer.neubert@etit.tu-chemnitz.de},
  cited-references = {Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120.
Badino H, 2011, IEEE INT CONF ROBOT.
Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214.
Biber P, 2009, INT J ROBOT RES, V28, P20, DOI 10.1177/0278364908096286.
Churchill W., 2012, INT C ROB AUT ICRA.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Efros AA, 2001, COMP GRAPH, P341.
Glover A., 2010, INT C ROB AUT ICRA.
Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x.
He XM, 2006, J FIELD ROBOT, V23, P1091, DOI 10.1002/rob.20170.
Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295.
Johannsson H, 2012, RSS WORKSH LONG TERM.
Konolige K., 2009, INT C INT ROB SYST I.
Liu C., 2008, EUR C COMP VIS ECCV.
Maddern W., 2012, ROB SCI SYST C RSS.
Milford M., 2013, IEEE INT C ROB AUT.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
Neubert P., 2013, EUR C MOB ROB ECMR.
Protzel P., 2013, P INT C ROB AUT ICRA.
Ren X., 2003, INT C COMP VIS ICCV.
Sunderhauf N., 2013, WORKSH LONG TERM AUT.
Sunderhauf N., 2011, INT C INT ROB SYST I.
Tighe J., 2010, EUR C COMP VIS ECCV.
Torralba A., 2003, INT C COMP VIS ICCV.
Valgren C, 2010, ROBOT AUTON SYST, V58, P149, DOI 10.1016/j.robot.2009.09.010.
Zhang W., 2005, C COMP VIS PATT REC.},
  da = {2022-05-17},
  doc-delivery-number = {CG2CO},
  eissn = {1872-793X},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Appearance change prediction; Long term navigation; Place recognition;
Appearance based localization; Changing environments},
  keywords-plus = {MAP},
  language = {English},
  number-of-cited-references = {26},
  orcid-numbers = {Sünderhauf, Niko/0000-0001-5286-3789},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  researcherid-numbers = {Sünderhauf, Niko/O-2192-2017},
  times-cited = {47},
  type = {Article},
  unique-id = {WOS:000353082700003},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {12},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ozog-et-al:2016:21582,
  author = {P. Ozog and N. Carlevaris-Bianco and A. Kim and E. R. M. },
  journal = {JOURNAL OF FIELD ROBOTICS},
  title = {Long-term Mapping Techniques for Ship Hull Inspection and Surveillance
using an Autonomous Underwater Vehicle},
  volume = {33},
  number = {3,1,SI},
  pages = {265--289},
  doi = {10.1002/rob.21582},
  publisher = {WILEY},
  address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  year = {2016},
  month = {5},
  abstract = {This paper reports on a system for an autonomous underwater vehicle to
perform in situ, multiple session hull inspection using long-term
simultaneous localization and mapping (SLAM). Our method assumes very
little a priori knowledge, and it does not require the aid of acoustic
beacons for navigation, which is a typical mode of navigation in this
type of application. Our system combines recent techniques in underwater
saliency-informed visual SLAM and a method for representing the ship
hull surface as a collection of many locally planar surface features.
This methodology produces accurate maps that can be constructed in
real-time on consumer-grade computing hardware. A single-session SLAM
result is initially used as a prior map for later sessions, where the
robot automatically merges the multiple surveys into a common
hull-relative reference frame. To perform the relocalization step, we
use a particle filter that leverages the locally planar representation
of the ship hull surface, and a fast visual descriptor matching
algorithm. Finally, we apply the recently developed graph sparsification
tool, generic linear constraints, as a way to manage the computational
complexity of the SLAM system as the robot accumulates information
across multiple sessions. We show results for 20 SLAM sessions for two
large vessels over the course of days, months, and even up to three
years, with a total path length of approximately 10.2 km.},
  affiliation = {Eustice, RM (Corresponding Author), Univ Michigan, Dept Naval Architecture \& Marine Engn, Ann Arbor, MI 48109 USA.
Ozog, Paul; Carlevaris-Bianco, Nicholas, Univ Michigan, Dept Elect Engn \& Comp Sci, Ann Arbor, MI 48109 USA.
Kim, Ayoung, Korea Adv Inst Sci \& Technol, Dept Civil \& Environm Engn, Daejeon, South Korea.
Eustice, Ryan M., Univ Michigan, Dept Naval Architecture \& Marine Engn, Ann Arbor, MI 48109 USA.},
  affiliations = {University of Michigan System; University of Michigan; Korea Advanced
Institute of Science \& Technology (KAIST); University of Michigan
System; University of Michigan},
  author-email = {paulozog@umich.edu
carlevar@umich.edu
ayoungk@kaist.ac.kr
eustice@umich.edu},
  cited-references = {Agarwal P, 2013, IEEE INT CONF ROBOT, P62, DOI 10.1109/ICRA.2013.6630557.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Belcher E, 2001, OCEANS 2001 MTS/IEEE: AN OCEAN ODYSSEY, VOLS 1-4, CONFERENCE PROCEEDINGS, P6, DOI 10.1109/OCEANS.2001.968656.
Bonnin-Pascual F, 2010, FRONT ARTIF INTEL AP, V220, P111, DOI 10.3233/978-1-60750-643-0-111.
Boon B., 2009, INT SHIP OFFSH STRUC, V2, P313.
Bosse M, 2004, INT J ROBOT RES, V23, P1113, DOI 10.1177/0278364904049393.
Bosse M, 2008, INT J ROBOT RES, V27, P667, DOI 10.1177/0278364908091366.
Bowen A. D., 2009, P IEEE MTS OCEANS C, P1, DOI DOI 10.23919/OCEANS.2009.5422311.
Carlevaris-Bianco Nicholas, 2011, IEEE International Conference on Robotics and Automation, P423.
Carlevaris-Bianco N, 2014, IEEE INT CONF ROBOT, P854, DOI 10.1109/ICRA.2014.6906954.
Carlevaris-Bianco N, 2013, IEEE INT CONF ROBOT, P5748, DOI 10.1109/ICRA.2013.6631403.
Carvalho AA, 2003, APPL OCEAN RES, V25, P235, DOI 10.1016/j.apor.2004.02.004.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768.
Eade E, 2010, IEEE INT C INT ROBOT, P3017, DOI 10.1109/IROS.2010.5649205.
Eustice RM, 2008, IEEE J OCEANIC ENG, V33, P103, DOI 10.1109/JOE.2008.923547.
Glover A, 2012, IEEE INT CONF ROBOT, P4730, DOI 10.1109/ICRA.2012.6224843.
Grisetti G., 2007, ROBOTICS SCI SYSTEMS, V3, P65.
Gustafson E., 2011, POL PETR POT C EXH, P714.
Harris SE, 1999, OCEANS `99 MTS/IEEE : RIDING THE CREST INTO THE 21ST CENTURY, VOLS 1-3, P493, DOI 10.1109/OCEANS.1999.799792.
Hover FS, 2007, MAR TECHNOL SOC J, V41, P44, DOI 10.4031/002533207787442196.
Hover FS, 2012, INT J ROBOT RES, V31, P1445, DOI 10.1177/0278364912461059.
Ishizu Kensei, 2012, 2012 OCEANS YEOSU, P1.
Johannsson H, 2010, IEEE INT C INT ROBOT, P4396, DOI 10.1109/IROS.2010.5650831.
Julier SJ, 2002, P AMER CONTR CONF, V1-6, P4555, DOI 10.1109/ACC.2002.1025369.
Kaess M., 2010, OPEN SOURCE IMPLEMEN.
Kaess M, 2009, ROBOT AUTON SYST, V57, P1198, DOI 10.1016/j.robot.2009.06.008.
Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706.
Kim A, 2013, IEEE T ROBOT, V29, P719, DOI 10.1109/TRO.2012.2235699.
Kim A, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1559, DOI 10.1109/IROS.2009.5354132.
Kim B, 2010, IEEE INT CONF ROBOT, P3185, DOI 10.1109/ROBOT.2010.5509154.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Kretzschmar H, 2012, INT J ROBOT RES, V31, P1219, DOI 10.1177/0278364912455072.
KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Kunz C, 2009, J FIELD ROBOT, V26, P411, DOI 10.1002/rob.20288.
Leonard J., 1999, P INT S ROB RES SALT.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Menegaldo LL, 2008, AMC `08: 10TH INTERNATIONAL WORKSHOP ON ADVANCED MOTION CONTROL, VOLS 1 AND 2, PROCEEDINGS, P27.
Menegaldo LL, 2009, IEEE T IND ELECTRON, V56, P3717, DOI 10.1109/TIE.2009.2025716.
Milne P. H., 1983, UNDERWATER ACOUSTIC.
Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331.
Negahdaripour S, 2006, IEEE J OCEANIC ENG, V31, P551, DOI 10.1109/JOE.2005.851391.
Neira J, 2001, IEEE T ROBOTIC AUTOM, V17, P890, DOI 10.1109/70.976019.
Ni K, 2007, IEEE INT CONF ROBOT, P1678, DOI 10.1109/ROBOT.2007.363564.
Nistr D., 2006, P IEEE COMP VIS PAT, V2, P2161.
Ozog P, 2014, IEEE INT CONF ROBOT, P3832, DOI 10.1109/ICRA.2014.6907415.
Ozog P, 2013, IEEE INT C INT ROBOT, P1042, DOI 10.1109/IROS.2013.6696479.
Ozog P, 2013, IEEE INT CONF ROBOT, P3777, DOI 10.1109/ICRA.2013.6631108.
Pizarro O, 2009, IEEE J OCEANIC ENG, V34, P150, DOI 10.1109/JOE.2009.2016071.
Ridao P, 2010, J FIELD ROBOT, V27, P759, DOI 10.1002/rob.20351.
Roman C, 2005, 2005 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2422, DOI 10.1109/IROS.2005.1545340.
Segal A., 2009, P ROB SCI SYST C SEA.
Singh H., 2004, SUBSURF SENS TECHNOL, V5, P25, DOI {[}10.1023/B:SSTA.0000018445.25977.f3, DOI 10.1023/B:SSTA.0000018445.25977.F3].
Smith R., 1986, P 2 C ANN C UNC ART, P267.
Sunderhauf N, 2012, IEEE INT C INT ROBOT, P1879, DOI 10.1109/IROS.2012.6385590.
Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387.
Trevor AJB, 2012, IEEE INT CONF ROBOT, P3041, DOI 10.1109/ICRA.2012.6225287.
Trimble GM, 2002, OCEANS 2002 MTS/IEEE CONFERENCE \& EXHIBITION, VOLS 1-4, CONFERENCE PROCEEDINGS, P1172.
VanMiddlesworth M, 2013, C FIELD SERV ROB FSR, P17.
Walter M, 2008, IEEE INT CONF ROBOT, P1463, DOI 10.1109/ROBOT.2008.4543408.
Weingarten J, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P3062, DOI 10.1109/IROS.2006.282245.},
  da = {2022-05-17},
  doc-delivery-number = {DK3VR},
  eissn = {1556-4967},
  funding-acknowledgement = {Office of Naval Research {[}N00014-12-1-0092]},
  funding-text = {This work was supported by the Office of Naval Research under award
N00014-12-1-0092.},
  issn = {1556-4959},
  journal-iso = {J. Field Robot.},
  keywords-plus = {SIMULTANEOUS LOCALIZATION; DATA ASSOCIATION; POSE GRAPHS; INFORMATION;
NAVIGATION; SLAM; SYSTEM; MAP; SAM},
  language = {English},
  number-of-cited-references = {62},
  oa = {Green Submitted},
  research-areas = {Robotics},
  times-cited = {37},
  type = {Article},
  unique-id = {WOS:000374846000002},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {27},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{schmuck-chli:2019:00071,
  author = {P. Schmuck and M. Chli},
  booktitle = {2019 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2019)},
  title = {On the Redundancy Detection in Keyframe-based SLAM},
  pages = {594--603},
  doi = {10.1109/3DV.2019.00071},
  note = {7th International Conference on 3D Vision (3DV), Quebec City, CANADA,
SEP 15-18, 2019},
  publisher = {IEEE COMPUTER SOC},
  address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
  year = {2019},
  abstract = {Egomotion and scene estimation is a key component in automating robot
navigation, as well as in virtual reality applications for mobile phones
or head-mounted displays. It is well known, however, that with long
exploratory trajectories and multi-session mapping for long-term
autonomy or collaborative applications, the maintenance of the
ever-increasing size of these maps quickly becomes a bottleneck. With
the explosion of data resulting in increasing runtime of the
optimization algorithms ensuring the accuracy of the Simultaneous
Localization And Mapping (SLAM) estimates, the large quantity of
collected experiences is imposing hard limits on the scalability of such
techniques. Considering the keyframe-based paradigm of SLAM techniques,
this paper investigates the redundancy inherent in SLAM maps, by
quantifying the information of different experiences of the scene as
encoded in keyframes. Here we propose and evaluate different
information-theoretic and heuristic metrics to remove dispensable scene
measurements with minimal impact on the accuracy of the SLAM estimates.
Evaluating the proposed metrics in two state-of-the-art centralized
collaborative SLAM systems, we provide our key insights into how to
identify redundancy in keyframe-based SLAM.},
  affiliation = {Schmuck, P (Corresponding Author), Swiss Fed Inst Technol, Vis Robot Lab, CH-8092 Zurich, Switzerland.
Schmuck, Patrik; Chli, Margarita, Swiss Fed Inst Technol, Vis Robot Lab, CH-8092 Zurich, Switzerland.},
  affiliations = {ETH Zurich},
  book-group-author = {IEEE Comp Soc},
  cited-references = {Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033.
Carlevaris-Bianco N, 2014, IEEE T ROBOT, V30, P1371, DOI 10.1109/TRO.2014.2347571.
Carlone L, 2014, IEEE INT C INT ROBOT, P2667, DOI 10.1109/IROS.2014.6942927.
Chli M, 2008, LECT NOTES COMPUT SC, V5302, P72, DOI 10.1007/978-3-540-88682-2\_7.
Choudhary S, 2015, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ICRA.2015.7139839.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Davison AJ, 2005, IEEE I CONF COMP VIS, P66.
Deutsch I., 2016, IEEE INT C REAL TIM.
Eade E, 2010, IEEE INT C INT ROBOT, P3017, DOI 10.1109/IROS.2010.5649205.
Forster C, 2013, DROUGHT AND THE HUMAN STORY: BRAVING THE BULL OF HEAVEN, P1.
Hepp B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3233794.
Hsiung J, 2018, IEEE INT C INT ROBOT, P1146, DOI 10.1109/IROS.2018.8594007.
Huang GQ, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P150, DOI 10.1109/ECMR.2013.6698835.
Ila V, 2010, IEEE T ROBOT, V26, P78, DOI 10.1109/TRO.2009.2034435.
Johannsson H, 2013, IEEE INT CONF ROBOT, P54, DOI 10.1109/ICRA.2013.6630556.
Karrer M, 2018, IEEE ROBOT AUTOM LET, V3, P2762, DOI 10.1109/LRA.2018.2837226.
Konolige K, 2008, IEEE T ROBOT, V24, P1066, DOI 10.1109/TRO.2008.2004832.
Kretzschmar H, 2012, INT J ROBOT RES, V31, P1219, DOI 10.1177/0278364912455072.
Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813.
Mazuran M, 2016, INT J ROBOT RES, V35, P50, DOI 10.1177/0278364915581629.
Mu BP, 2017, IEEE T ROBOT, V33, P124, DOI 10.1109/TRO.2016.2623344.
Mur-Artal R, 2017, IEEE ROBOT AUTOM LET, V2, P796, DOI 10.1109/LRA.2017.2653359.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Paull L, 2016, IEEE INT CONF ROBOT, P1346, DOI 10.1109/ICRA.2016.7487268.
Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729.
Riazuelo L, 2014, ROBOT AUTON SYST, V62, P401, DOI 10.1016/j.robot.2013.11.007.
Schmuck P., 2017, P IEEE INT C ROB AUT.
Schmuck P, 2019, J FIELD ROBOT, V36, P763, DOI 10.1002/rob.21854.
Schneider Thomas, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P6487, DOI 10.1109/ICRA.2017.7989766.
Snavely N., 2008, CVPR, V1, P2.
Strasdat H, 2012, IMAGE VISION COMPUT, V30, P65, DOI 10.1016/j.imavis.2012.02.009.
Usenko Vladyslav, 2019, ARXIV190406504.
Vallve J, 2018, IEEE ROBOT AUTOM LET, V3, P1322, DOI 10.1109/LRA.2018.2798283.
Vial J, 2011, IEEE INT C INT ROBOT, P886, DOI 10.1109/IROS.2011.6048728.},
  da = {2022-05-17},
  doc-delivery-number = {BQ7OG},
  eissn = {2475-7888},
  funding-acknowledgement = {Swiss National Science Foundation (SNSF) {[}PP00P2183720]; NCCR Robotics},
  funding-text = {This research was supported by the Swiss National Science Foundation
(SNSF, Agreement no. PP00P2183720) and NCCR Robotics.},
  isbn = {978-1-7281-3131-3},
  issn = {2378-3826},
  keywords-plus = {GRAPH SLAM; VERSATILE},
  language = {English},
  number-of-cited-references = {34},
  oa = {Green Accepted},
  orcid-numbers = {Chli, Margarita/0000-0001-5611-7492},
  research-areas = {Computer Science; Engineering; Imaging Science \& Photographic
Technology},
  series = {International Conference on 3D Vision},
  times-cited = {3},
  type = {Proceedings Paper},
  unique-id = {WOS:000618059700062},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Computer Science, Software
Engineering; Engineering, Electrical \& Electronic; Imaging Science \&
Photographic Technology},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{yin-et-al:2021:3061375,
  author = {P. Yin and L. Xu and J. Zhang and H. Choset},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {FusionVLAD: A Multi-View Deep Fusion Networks for Viewpoint-Free 3D
Place Recognition},
  volume = {6},
  number = {2},
  pages = {2304--2310},
  doi = {10.1109/LRA.2021.3061375},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2021},
  month = {4},
  abstract = {Real-time 3D place recognition is a crucial technology to recover from
localization failure in applications like autonomous driving, last-mile
delivery, and service robots. However, it is challenging for 3D place
retrieval methods to be accurate, efficient, and robust to the variant
viewpoints differences. In this letter, we propose FusionVLAD, a
fusion-based network that encodes a multiview representation of sparse
3D point clouds into viewpoint-free global descriptors. The system
consists of two parallel branches: a spherical-view branch for
orientation-invariant feature extraction, and the top-down view branch
for translation-insensitive feature extraction. Furthermore, we design a
parallel fusion module to enhance the combination of region-wise feature
connection between the two branches. Experiments on two public datasets
and two generated datasets show that our method outperforms
state-of-the-art with robust place recognition accuracy and efficient
inference time. Besides, FusionVLAD requires limited computation
resources and makes it extremely suitable for low-cost robots' long-term
place recognition task.},
  affiliation = {Xu, LY (Corresponding Author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
Yin, Peng; Xu, Lingyun; Zhang, Ji; Choset, Howie, Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.},
  affiliations = {Carnegie Mellon University},
  author-email = {pyin2@andrew.cmu.edu
hitmaxtom@gmail.com
zhangji@cmu.edu
choset@cs.cmu.edu},
  cited-references = {Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
Cadena C., 2017, PROC IEEE INT C ROBO, P5266.
Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693.
Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8\_4.
He L, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P231, DOI 10.1109/IROS.2016.7759060.
Kim G, 2018, IEEE INT C INT ROBOT, P4802, DOI 10.1109/IROS.2018.8593953.
Liu Z, 2019, IEEE I CONF COMP VIS, P2831, DOI 10.1109/ICCV.2019.00292.
Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Qi CR, 2017, NEURIPS.
Rho E., 2018, P 6 INT C BRAIN COMP, P1.
Rohling T, 2015, IEEE INT C INT ROBOT, P736, DOI 10.1109/IROS.2015.7353454.
Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x.
Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1\_26.
Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470.
Wohlkinger W., 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2987, DOI 10.1109/ROBIO.2011.6181760.
Xiong T, 2014, BIOMED CIRC SYST C, P9, DOI 10.1109/BioCAS.2014.6981632.
Zhang W., 2019, P IEEE C COMP VIS PA, P12436.},
  da = {2022-05-17},
  doc-delivery-number = {QY0LG},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {Recognition; SLAM; visual learning},
  language = {English},
  number-of-cited-references = {19},
  research-areas = {Robotics},
  researcherid-numbers = {Yin, Peng/AEY-2004-2022},
  times-cited = {3},
  type = {Article},
  unique-id = {WOS:000629731200026},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {11},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{yin-et-al:2018:8593562,
  author = {P. Yin and L. Xu and Z. Liu and L. Li and Y. H. A. H. Salman and W. Xu and H. Wang and H. Choset},
  booktitle = {2018 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS)},
  title = {Stabilize an Unsupervised Feature Learning for LiDAR-based Place
Recognition},
  pages = {1162--1167},
  doi = {10.1109/IROS.2018.8593562},
  note = {25th IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Madrid, SPAIN, OCT 01-05, 2018},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2018},
  abstract = {Place recognition is one of the major challenges for the LiDAR-based
effective localization and mapping task. Traditional methods are usually
relying on geometry matching to achieve place recognition, where a
global geometry map need to be restored. In this paper, we accomplish
the place recognition task based on an end-to-end feature learning
framework with the LiDAR inputs. This method consists of two core
modules, a dynamic octree mapping module that generates local 2D maps
with the consideration of the robot's motion; and an unsupervised place
feature learning module which is an improved adversarial feature
learning network with additional assistance for the long-term place
recognition requirement. More specially, in place feature learning, we
present an additional Generative Adversarial Network with a designed
Conditional Entropy Reduction module to stabilize the feature learning
process in an unsupervised manner. We evaluate the proposed method on
the Kitti dataset and North Campus Long-Term LiDAR dataset. Experimental
results show that the proposed method outperforms state-of-the-art in
place recognition tasks under long-term applications. What's more, the
feature size and inference efficiency in the proposed method are
applicable in real-time performance on practical robotic platforms.},
  affiliation = {Xu, LY (Corresponding Author), Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang, Liaoning, Peoples R China.
Yin, Peng; Xu, Lingyun; He, Yuqing, Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang, Liaoning, Peoples R China.
Yin, Peng; Xu, Lingyun; He, Yuqing, Univ Chinese Acad Sci, Beijing, Peoples R China.
Liu, Zhe, Chinese Univ Hong Kong, Dept Mech \& Automat Engn, Hong Kong, Peoples R China.
Li, Lu; Salman, Hadi; Choset, Howie, Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
Xu, Weiliang, Univ Auckland, Dept Mech Engn, Auckland, New Zealand.
Wang, Hesheng, Shanghai Jiao Tong Univ, Dept Automat, Shanghai, Peoples R China.},
  affiliations = {Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
Chinese Academy of Sciences; University of Chinese Academy of Sciences,
CAS; Chinese University of Hong Kong; Carnegie Mellon University;
University of Auckland; Shanghai Jiao Tong University},
  author-email = {pyin2@andrew.cmu.edu
121067240@qq.com
zheli-u@cuhk.edu.hk
luli2@cmu.edu
hadis@cmu.edu
heyuqing@sia.cn
p.xu@auckland.ac.nz
wanghesheng@sjtu.edu.cn
choset@cmu.edu},
  book-author = {Kosecka, J},
  cited-references = {Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791.
Cadena C., 2017, PROC IEEE INT C ROBO, P5266.
Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Donahue Jeff, 2016, ADVERSARIAL FEATURE.
Falliat D, 2007, IEEE INT CONF ROBOT, P3921.
Fossel J, 2017, IEEE INT C INT ROBOT, P6764, DOI 10.1109/IROS.2017.8206594.
Garg S, 2017, IEEE INT C INT ROBOT, P6863, DOI 10.1109/IROS.2017.8206608.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672.
Hess W, 2016, IEEE INT CONF ROBOT, P1271, DOI 10.1109/ICRA.2016.7487258.
Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0.
Krizhevsky A, 2012, ADV NEURAL INFORM PR, P1097, DOI 10.1145/3065386.
Latif Y., 2017, ARXIV170908810.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Martin A, 2017, ARXIV170104862.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Ng PC, 2003, NUCLEIC ACIDS RES, V31, P3812, DOI 10.1093/nar/gkg509.
Sunderhauf N., 2013, WORKSH LONG TERM AUT.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Xiong T, 2014, BIOMED CIRC SYST C, P9, DOI 10.1109/BioCAS.2014.6981632.},
  da = {2022-05-17},
  doc-delivery-number = {BM0LT},
  editor = {Maciejewski, AA and Okamura, A and Bicchi, A and Stachniss, C and Song, DZ and Lee, DH and Chaumette, F and Ding, H and Li, JS and Wen, J and Roberts, J and Masamune, K and Chong, NY and Amato, N and Tsagwarakis, N and Rocco, P and Asfour, T and Chung, WK and Yasuyoshi, Y and Sun, Y and Maciekeski, T and Althoefer, K and AndradeCetto, J and Chung, WK and Demircan, E and Dias, J and Fraisse, P and Gross, R and Harada, H and Hasegawa, Y and Hayashibe, M and Kiguchi, K and Kim, K and Kroeger, T and Li, Y and Ma, S and Mochiyama, H and Monje, CA and Rekleitis, I and Roberts, R and Stulp, F and Tsai, CHD and Zollo, L},
  isbn = {978-1-5386-8094-0},
  issn = {2153-0858},
  keywords-plus = {LOCALIZATION},
  language = {English},
  number-of-cited-references = {22},
  orcid-numbers = {Xu, Peter/0000-0002-1960-0992},
  research-areas = {Computer Science; Robotics},
  researcherid-numbers = {Yin, Peng/AEY-2004-2022},
  series = {IEEE International Conference on Intelligent Robots and Systems},
  times-cited = {7},
  type = {Proceedings Paper},
  unique-id = {WOS:000458872701042},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {3},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Computer Science, Information
Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{meng-et-al:2021:3062647,
  author = {Q. Meng and H. Guo and X. Zhao and H. D. A. C. Cao},
  journal = {IEEE-ASME TRANSACTIONS ON MECHATRONICS},
  title = {Loop-Closure Detection With a Multiresolution Point Cloud Histogram Mode
in Lidar Odometry and Mapping for Intelligent Vehicles},
  volume = {26},
  number = {3},
  pages = {1307--1317},
  doi = {10.1109/TMECH.2021.3062647},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2021},
  month = {6},
  abstract = {Precise positioning is the basic condition for intelligent vehicles to
complete perception, decision making and control tasks. In response to
this challenge, in this article, lidar simultaneous localization and
mapping (SLAM) is taken as the research object, and a SLAM system is
designed that integrates motion compensation and ground information
removal functions, and can construct a real-time environment map and
determine its own position on the map while the vehicle is driving. A
loop-closure detection method with a multiresolution point cloud
histogram mode is proposed, which can effectively detect whether the
vehicle passes through the same position and perform optimization to
obtain globally consistent pose and map information in the urban
conditions with more driving loops. We conduct experiments on the
well-known KITTI dataset and compare the results with those of
state-of-the-art systems. The experiments confirm that the lidar SLAM
system designed in this article can provide accurate and effective
positioning information for intelligent vehicles. The proposed
loop-closure detection algorithm has an excellent real-time performance
and accuracy, which can guarantee the long-term driving operation of
these vehicles.},
  affiliation = {Guo, HY (Corresponding Author), Jilin Univ, State Key Lab Automot Simulat \& Control, Campus Nanling, Changchun 130025, Peoples R China.
Guo, HY (Corresponding Author), Jilin Univ, Dept Control Sci \& Engn, Campus Nanling, Changchun 130025, Peoples R China.
Meng, Qingyu; Guo, Hongyan; Zhao, Xiaoming, Jilin Univ, State Key Lab Automot Simulat \& Control, Campus Nanling, Changchun 130025, Peoples R China.
Meng, Qingyu; Guo, Hongyan; Zhao, Xiaoming, Jilin Univ, Dept Control Sci \& Engn, Campus Nanling, Changchun 130025, Peoples R China.
Cao, Dongpu, Univ Waterloo, Dept Mech \& Mechatron Engn, Waterloo, ON N2L 3G1, Canada.
Chen, Hong, Tongji Univ, Clean Energy Automot Engn Ctr, Shanghai 201804, Peoples R China.},
  affiliations = {Jilin University; Jilin University; University of Waterloo; Tongji
University},
  author-email = {qymeng19@mails.jlu.edu.cn
guohy11@jlu.edu.cn
zhaoxm19@mails.jlu.edu.cn
dongpu.cao@uwaterloo.ca
chenh@jlu.edu.cn},
  cited-references = {Agarwal S., 2019, CERES SOLVER A LARGE.
Cadena Cesar, 2018, ARXIV180409557.
Chen LH, 2019, IEEE SENS J, V19, P11475, DOI 10.1109/JSEN.2019.2931368.
Chen YB, 2020, IEEE-ASME T MECH, V25, P1182, DOI 10.1109/TMECH.2019.2963439.
Dellaert F., 2012, GTRIMCPR2012002.
Duan JM, 2016, PROCEDIA ENGINEER, V138, P267, DOI 10.1016/j.proeng.2016.01.258.
Dube R, 2020, INT J ROBOT RES, V39, P339, DOI 10.1177/0278364919863090.
Eckenhoff K, 2019, INT J ROBOT RES, V38, P563, DOI 10.1177/0278364919835021.
Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297.
Grisetti G., 2011, IEEE INT C ROB AUT I, P9.
Grupp M., 2017, EVO PYTHON PACKAGE E.
Guclu O, 2020, VISUAL COMPUT, V36, P1271, DOI 10.1007/s00371-019-01720-8.
Handa A., 2014, SIMPLIFIED JACOBIANS SIMPLIFIED JACOBIANS, P8.
Huang YJ, 2017, J POWER SOURCES, V341, P91, DOI 10.1016/j.jpowsour.2016.11.106.
Jin LQ, 2020, IEEE-ASME T MECH, V25, P1803, DOI 10.1109/TMECH.2020.2997606.
Jo H, 2018, IEEE-ASME T MECH, V23, P714, DOI 10.1109/TMECH.2018.2795252.
Kim G, 2019, IEEE ROBOT AUTOM LET, V4, P1948, DOI 10.1109/LRA.2019.2897340.
Kim MD, 2018, IEEE-ASME T MECH, V23, P491, DOI 10.1109/TMECH.2018.2791473.
Li SP, 2019, ROBOT AUTON SYST, V112, P201, DOI 10.1016/j.robot.2018.11.009.
Lin CF, 2014, IET INTELL TRANSP SY, V8, P550, DOI 10.1049/iet-its.2013.0056.
Lin J., 2019, ARXIV190911811.
Lin JR, 2020, IEEE INT CONF ROBOT, P3126, DOI 10.1109/ICRA40945.2020.9197440.
Liu WL, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19224945.
Magnusson M, 2009, J FIELD ROBOT, V26, P892, DOI 10.1002/rob.20314.
McDonald J, 2013, ROBOT AUTON SYST, V61, P1144, DOI 10.1016/j.robot.2012.08.008.
Memon AR, 2020, ROBOT AUTON SYST, V126, DOI 10.1016/j.robot.2020.103470.
More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Neuhaus F., 2018, GERM C PATT REC, P60.
Nguyen T, 2020, J ROBOT, V2020, DOI 10.1155/2020/7362952.
Ortiz-Gonzalez A, 2020, J COMMUN TECHNOL EL+, V65, P690, DOI 10.1134/S1064226920060224.
Quigley M, 2009, IEEE INT CONF ROBOT, P3604.
Rohling T, 2015, IEEE INT C INT ROBOT, P736, DOI 10.1109/IROS.2015.7353454.
Schops T, 2020, IEEE T PATTERN ANAL, V42, P2494, DOI 10.1109/TPAMI.2019.2947048.
Shan TX, 2018, IEEE INT C INT ROBOT, P4758, DOI 10.1109/IROS.2018.8594299.
Song WJ, 2018, IEEE T INTELL TRANSP, V19, P758, DOI 10.1109/TITS.2017.2700628.
Soualmi B, 2014, CONTROL ENG PRACT, V24, P106, DOI 10.1016/j.conengprac.2013.11.015.
Tang DQ, 2020, IEEE-ASME T MECH, V25, P1555, DOI 10.1109/TMECH.2020.2976794.
Tomono M, 2020, ADV ROBOTICS, V34, P1530, DOI 10.1080/01691864.2020.1824809.
Wen WS, 2020, IEEE INTEL TRANSP SY, V12, P53, DOI 10.1109/MITS.2020.2994131.
Wen WS, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18113928.
Zhang FQ, 2020, MULTIMED TOOLS APPL, V79, P16421, DOI 10.1007/s11042-019-7438-2.
Zhang J., 2014, ROBOT SCI SYST, V2, P9.
Zhang J, 2017, AUTON ROBOT, V41, P401, DOI 10.1007/s10514-016-9548-2.},
  da = {2022-05-17},
  doc-delivery-number = {TL9XM},
  eissn = {1941-014X},
  funding-acknowledgement = {National Nature Science Foundation of China {[}U19A2069, 61790563];
Project of the Science and Technology Department of Jilin Province
{[}20200401088GX, 20200501011GX]; National Development and Reform
Commission of Jilin Province {[}2019C0365]},
  funding-text = {This work was supported in part by the National Nature Science
Foundation of China under Grant U19A2069 and Grant 61790563, in part by
the Project of the Science and Technology Department of Jilin Province
(20200401088GX, 20200501011GX), and in part by the Project of the
National Development and Reform Commission of Jilin Province under Grant
2019C0365.},
  issn = {1083-4435},
  journal-iso = {IEEE-ASME Trans. Mechatron.},
  keywords = {Three-dimensional displays; Laser radar; Intelligent vehicles;
Simultaneous localization and mapping; Histograms; Trajectory; Feature
extraction; KITTI dataset; lidar simultaneous localization and mapping
(SLAM); loop-closure detection; multiresolution histogram; pose
estimation},
  keywords-plus = {OBJECT-DETECTION; SLAM; VISION; MAP},
  language = {English},
  number-of-cited-references = {44},
  orcid-numbers = {Chen, Hong/0000-0002-1724-8649},
  research-areas = {Automation \& Control Systems; Engineering},
  researcherid-numbers = {Chen, Hong/A-2851-2012},
  times-cited = {0},
  type = {Article},
  unique-id = {WOS:000675207800014},
  usage-count-last-180-days = {10},
  usage-count-since-2013 = {14},
  web-of-science-categories = {Automation \& Control Systems; Engineering, Manufacturing; Engineering,
Electrical \& Electronic; Engineering, Mechanical},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{arroyo-et-al:2018:7,
  author = {R. Arroyo and P. F. Alcantarilla and E. L. M. A. R. Bergasa},
  journal = {AUTONOMOUS ROBOTS},
  title = {Are you ABLE to perform a life-long visual topological localization?},
  volume = {42},
  number = {3},
  pages = {665--685},
  doi = {10.1007/s10514-017-9664-7},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2018},
  month = {3},
  abstract = {Visual topological localization is a process typically required by
varied mobile autonomous robots, but it is a complex task if long
operating periods are considered. This is because of the appearance
variations suffered in a place: dynamic elements, illumination or
weather. Due to these problems, long-term visual place recognition
across seasons has become a challenge for the robotics community. For
this reason, we propose an innovative method for a robust and efficient
life-long localization using cameras. In this paper, we describe our
approach (ABLE), which includes three different versions depending on
the type of images: monocular, stereo and panoramic. This distinction
makes our proposal more adaptable and effective, because it allows to
exploit the extra information that can be provided by each type of
camera. Besides, we contribute a novel methodology for identifying
places, which is based on a fast matching of global binary descriptors
extracted from sequences of images. The presented results demonstrate
the benefits of using ABLE, which is compared to the most representative
state-of-the-art algorithms in long-term conditions.},
  affiliation = {Arroyo, R (Corresponding Author), Univ Alcala UAH, Dept Elect, Madrid 28871, Spain.
Arroyo, Roberto; Bergasa, Luis M.; Romera, Eduardo, Univ Alcala UAH, Dept Elect, Madrid 28871, Spain.
Alcantarilla, Pablo F., iRobot Corp, 10 Greycoat Pl, London, England.},
  affiliations = {Universidad de Alcala},
  author-email = {roberto.arroyo@depeca.uah.es
palcantarilla@irobot.com
luism.bergasa@uah.es
eduardo.romera@depeca.uah.es},
  cited-references = {Alahi A, 2012, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2012.6247715.
Alcantarilla P., 2016, P ROB SCI SYST, DOI {[}10.15607/RSS.2016.XII.044, DOI 10.15607/RSS.2016.XII.044].
Alcantarilla PF, 2013, AUTON ROBOT, V34, P47, DOI 10.1007/s10514-012-9312-1.
Arroyo R, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P965, DOI 10.1109/ITSC.2016.7795672.
Arroyo R, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4656, DOI 10.1109/IROS.2016.7759685.
Arroyo R, 2015, IEEE INT CONF ROBOT, P6328, DOI 10.1109/ICRA.2015.7140088.
Arroyo R, 2014, IEEE INT C INT ROBOT, P3089, DOI 10.1109/IROS.2014.6942989.
Arroyo R, 2014, IEEE INT VEH SYM, P1378, DOI 10.1109/IVS.2014.6856457.
Badino H, 2012, IEEE INT CONF ROBOT, P1635, DOI 10.1109/ICRA.2012.6224716.
Bailey T, 2006, IEEE ROBOT AUTOM MAG, V13, P108, DOI 10.1109/MRA.2006.1678144.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Cadena C, 2012, IEEE T ROBOT, V28, P871, DOI 10.1109/TRO.2012.2189497.
Cadena C, 2010, IEEE INT C INT ROBOT, P5182, DOI 10.1109/IROS.2010.5650234.
Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222.
Campos FM, 2013, LECT NOTES ARTIF INT, V8154, P247, DOI 10.1007/978-3-642-40669-0\_22.
Caramazana L., 2016, OP C FUT TRENDS ROB, P97.
Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638.
Carlevaris-Bianco N, 2014, IEEE INT C INT ROBOT, P2769, DOI 10.1109/IROS.2014.6942941.
Ceriani S, 2009, AUTON ROBOT, V27, P353, DOI 10.1007/s10514-009-9156-5.
Clemente L.A., 2007, P ROB SCI SYST, P297.
Corke P, 2013, IEEE INT C INT ROBOT, P2085, DOI 10.1109/IROS.2013.6696648.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Cummins M, 2010, IEEE T ROBOT, V26, P1042, DOI 10.1109/TRO.2010.2080390.
Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177.
Drouilly R, 2015, IEEE INT CONF ROBOT, P1106, DOI 10.1109/ICRA.2015.7139314.
Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022.
Dymczyk M, 2015, IEEE INT CONF ROBOT, P2767, DOI 10.1109/ICRA.2015.7139575.
Erkent O, 2015, IEEE INT CONF ROBOT, P5462, DOI 10.1109/ICRA.2015.7139962.
Fraundorfer F, 2012, IEEE ROBOT AUTOM MAG, V19, P78, DOI 10.1109/MRA.2012.2182810.
Fuentes-Pacheco J, 2015, ARTIF INTELL REV, V43, P55, DOI 10.1007/s10462-012-9365-8.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Gao X, 2017, AUTON ROBOT, V41, P1, DOI 10.1007/s10514-015-9516-2.
Garcia-Fidalgo E, 2015, ROBOT AUTON SYST, V64, P1, DOI 10.1016/j.robot.2014.11.009.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Geiger A, 2011, LECT NOTES COMPUT SC, V6492, P25, DOI 10.1007/978-3-642-19315-6\_3.
Glover A, 2012, IEEE INT CONF ROBOT, P4730, DOI 10.1109/ICRA.2012.6224843.
Glover AJ, 2010, IEEE INT CONF ROBOT, P3507, DOI 10.1109/ROBOT.2010.5509547.
Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI {[}10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166].
Johns E, 2014, INT J COMPUT VISION, V106, P297, DOI 10.1007/s11263-013-0648-6.
Korrapati H, 2017, AUTON ROBOT, V41, P967, DOI 10.1007/s10514-016-9560-6.
Korrapati H, 2013, IEEE INT C INT ROBOT, P3684, DOI 10.1109/IROS.2013.6696882.
Lee GH, 2014, IEEE INT CONF ROBOT, P1510, DOI 10.1109/ICRA.2014.6907052.
Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Liu Y, 2012, IEEE INT C INT ROBOT, P1051, DOI 10.1109/IROS.2012.6386145.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Lowry S., 2015, WORKSH VIS PLAC REC.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Lv Q, 2007, VLDB, DOI DOI 10.1145/1143844.1143857.
Masatoshi A, 2015, IEEE INT CONF ROBOT, P5455, DOI 10.1109/ICRA.2015.7139961.
McManus C, 2014, IEEE INT CONF ROBOT, P901, DOI 10.1109/ICRA.2014.6906961.
Milford M., 2012, P 2012 ROB SCI SYST, P297, DOI {[}10.15607/RSS.2012.VIII.038, DOI 10.15607/RSS.2012.VIII.038].
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mohan M, 2015, IEEE INT CONF ROBOT, P5487, DOI 10.1109/ICRA.2015.7139966.
Mousavian A, 2015, IEEE INT CONF ROBOT, P4882, DOI 10.1109/ICRA.2015.7139877.
Muja M., 2012, 2012 Canadian Conference on Computer and Robot Vision, P404, DOI 10.1109/CRV.2012.60.
Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Murillo AC, 2013, IEEE T ROBOT, V29, P146, DOI 10.1109/TRO.2012.2220211.
Carrasco PLN, 2016, AUTON ROBOT, V40, P1403, DOI 10.1007/s10514-015-9522-4.
Nelson P, 2015, IEEE INT CONF ROBOT, P5245, DOI 10.1109/ICRA.2015.7139930.
Neubert P, 2015, ROBOT AUTON SYST, V69, P15, DOI 10.1016/j.robot.2014.08.005.
Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4.
Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2.
Pandey G, 2011, INT J ROBOT RES, V30, P1543, DOI 10.1177/0278364911400640.
Paul R, 2010, IEEE INT CONF ROBOT, P2649, DOI 10.1109/ROBOT.2010.5509587.
Pepperell E, 2015, IEEE INT CONF ROBOT, P1118, DOI 10.1109/ICRA.2015.7139316.
Pepperell E, 2014, IEEE INT CONF ROBOT, P1612, DOI 10.1109/ICRA.2014.6907067.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.
Sunderhauf N, 2011, IEEE INT C INT ROBOT, P1234, DOI 10.1109/IROS.2011.6048590.
Sunderhauf N., 2013, WORKSH LONG TERM AUT.
Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.
Ulrich I., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1023, DOI 10.1109/ROBOT.2000.844734.
Upcroft B, 2014, IEEE INT CONF ROBOT, P1712, DOI 10.1109/ICRA.2014.6907082.
Valgren C, 2010, ROBOT AUTON SYST, V58, P149, DOI 10.1016/j.robot.2009.09.010.
Williams B, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P2053, DOI 10.1109/IROS.2008.4650996.
Williams B, 2009, ROBOT AUTON SYST, V57, P1188, DOI 10.1016/j.robot.2009.06.010.
Yang X, 2014, IEEE T PATTERN ANAL, V36, P188, DOI 10.1109/TPAMI.2013.150.},
  da = {2022-05-17},
  doc-delivery-number = {FW2DY},
  eissn = {1573-7527},
  funding-acknowledgement = {Spanish MINECO through the SmartElderlyCar project
{[}TRA2015-70501-C2-1-R]; RoboCity2030-III-CM project (Robotica aplicada
a la mejora de la calidad de vida de los ciudadanos. fase III)
{[}S2013/MIT-2748]; Programas de actividades I+D (CAM); EU Structural
Funds},
  funding-text = {This work has been funded in part from the Spanish MINECO through the
SmartElderlyCar project (TRA2015-70501-C2-1-R) and from the
RoboCity2030-III-CM project (Robotica aplicada a la mejora de la calidad
de vida de los ciudadanos. fase III; S2013/MIT-2748), funded by
Programas de actividades I+D (CAM) and cofunded by EU Structural Funds.},
  issn = {0929-5593},
  journal-iso = {Auton. Robot.},
  keywords = {Localization across seasons; Visual place recognition; Loop closure
detection; Image matching; Binary descriptors},
  keywords-plus = {ROBUST PLACE RECOGNITION; LOOP CLOSURE; FAB-MAP; IMAGE FEATURES; SLAM;
VISION; BINARY; SCALE; GIST},
  language = {English},
  number-of-cited-references = {81},
  oa = {Green Accepted},
  orcid-numbers = {Bergasa, Luis M./0000-0002-0087-3077
Fernandez Alcantarilla, Pablo/0000-0001-7185-2911},
  research-areas = {Computer Science; Robotics},
  researcherid-numbers = {Bergasa, Luis M./H-9810-2013},
  times-cited = {14},
  type = {Article},
  unique-id = {WOS:000425113800010},
  usage-count-last-180-days = {3},
  usage-count-since-2013 = {19},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{paul-newman:2013:0278364913509859,
  author = {R. Paul and P. Newman},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {Self-help: Seeking out perplexing images for ever improving topological
mapping},
  volume = {32},
  number = {14,SI},
  pages = {1742--1766},
  doi = {10.1177/0278364913509859},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2013},
  month = {12},
  abstract = {In this work, we present a novel approach that allows a robot to improve
its own navigation performance through introspection and then targeted
data retrieval. It is a step in the direction of life-long learning and
adaptation and is motivated by the desire to build robots that have
plastic competencies which are not baked in. They should react to and
benefit from use. We consider a particular instantiation of this problem
in the context of place recognition. Based on a topic-based
probabilistic representation for images, we use a measure of perplexity
to evaluate how well a working set of background images explain the
robot's online view of the world. Offline, the robot then searches an
external resource to seek out additional background images that bolster
its ability to localize in its environment when used next. In this way
the robot adapts and improves performance through use. We demonstrate
this approach using data collected from a mobile robot operating in
outdoor workspaces.},
  affiliation = {Paul, R (Corresponding Author), Univ Oxford, Dept Engn Sci, Robot Res Grp, Oxford OX1 3PJ, England.
Paul, Rohan; Newman, Paul, Univ Oxford, Mobile Robot Res Grp, Oxford OX1 3PJ, England.},
  affiliations = {League of European Research Universities - LERU; University of Oxford},
  author-email = {rohanp@robots.ox.ac.uk},
  cited-references = {Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514.
Banerjee A, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P431.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Blei DM, 2007, ANN APPL STAT, V1, P17, DOI 10.1214/07-AOAS114.
Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993.
Boiman O, 2007, INT J COMPUT VISION, V74, P17, DOI 10.1007/s11263-006-0009-9.
Boyd-Graber J, 2010, ARXIV10024665.
Chengxiang Zhai, 2001, SIGIR Forum, P334.
Wang C, 2009, PROC CVPR IEEE, P1903, DOI {[}10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800].
Churchill W, 2012, IEEE INT CONF ROBOT, P4525, DOI 10.1109/ICRA.2012.6224596.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cummins M, 2007, IEEE INT CONF ROBOT, P2042, DOI 10.1109/ROBOT.2007.363622.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Cussens J., 1993, Machine Learning: ECML-93. European Conference on Machine Learning Proceedings, P136.
Dayoub F, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3364, DOI 10.1109/IROS.2008.4650701.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Endres F, 2009, P ROB SCI SYST SEATT, P34.
Fei-Fei L, 2005, PROC CVPR IEEE, P524.
Girdhar Y, 2010, 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2010), P746, DOI 10.1109/IROS.2010.5650315.
Girdhar Y, 2012, INT S EXP ROB QUEB C.
Girdhar Y, 2010, IEEE INT CONF ROBOT, P5035, DOI 10.1109/ROBOT.2010.5509464.
Glover AJ, 2010, IEEE INT CONF ROBOT, P3507, DOI 10.1109/ROBOT.2010.5509547.
Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101.
Heinrich G, 2005, PARAMETER ESTIMATION.
Hendel A, 2011, LECT NOTES COMPUT SC, V6494, P448, DOI 10.1007/978-3-642-19318-7\_35.
Ho KL, 2007, INT J COMPUT VISION, V74, P261, DOI 10.1007/s11263-006-0020-1.
Hoffman M., 2010, ADV NEURAL INFORM PR, P856, DOI DOI 10.1073/PNAS.0307750100.
Hoi S. C. H., 2006, P 23 INT C MACH LEAR, P417, DOI DOI 10.1145/1143844.1143897.
Holub A, 2008, PROC CVPR IEEE, P885.
Horster E., 2007, CIVR 07, P17.
Hospedales T, 2011, IEEE T KNOWL DATA EN, P1.
Itti L, 2005, PROC CVPR IEEE, P631.
Joho D, 2012, P ROB SCI SYST RSS S.
Joshi Ajay J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2372, DOI 10.1109/CVPRW.2009.5206627.
Kapoor A, 2010, INT J COMPUT VISION, V88, P169, DOI 10.1007/s11263-009-0268-3.
Konolige K, 2010, INT J ROBOT RES, V29, P941, DOI 10.1177/0278364910370376.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Lavrenko V, 2009, INFORM RETRIEVAL SER, V26, P1.
Lawrence N. D., 2002, ADV NEURAL INF PROCE, P609.
Levin A, 2004, PROC CVPR IEEE, P611.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
MacKay D. J. C., 1995, NAT LANG ENG, V1, P1.
Maddern W, 2012, INT J ROBOT RES, V31, P429, DOI 10.1177/0278364912438273.
Manning C.D., 2008, INTRO INFORM RETRIEV.
Mei C, 2010, IEEE INT C INT ROBOT, P3738, DOI 10.1109/IROS.2010.5652266.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
Murillo A C, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2196, DOI 10.1109/ICCVW.2009.5457552.
Paul R, 2011, 2011 IEEE INT C ROB, P445.
Paul R, 2010, IEEE INT CONF ROBOT, P2649, DOI 10.1109/ROBOT.2010.5509587.
Philbin J, 2008, P BRIT MACH VIS C BM.
Ranganathan Ananth, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P2017, DOI 10.1109/ROBOT.2009.5152376.
Seeger M, 2003, 9 INT WORKSH AI STAT.
SETTLES B, 2010, 1648 U WISC.
Silpa - Anan C., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587638.
Silpa-Anan C, 2004, P AUSTR C ROB AUT CA.
Singh G, 2010, OMN ROB VIS WORKSH I.
Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663.
Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950.
Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302.
Tellex S., 2012, P ROB SCI SYST SYDN.
Valgren C., 2007, EUR C MOB ROB.
Varadarajan Jagannadan, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1338, DOI 10.1109/ICCVW.2009.5457456.
Wallach MH, 2006, INT C MACHINE LEARNI, P977, DOI DOI 10.1145/1143844.1143967.
Wang C, 2011, P 14 INT C ART INT S.
Wang X., 2007, ADV NEURAL INFORM PR, P1577.
Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87.
Xiaoyong Liu, 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P186.
Xing Wei, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P178.
Yi Zhang, 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P81.
Zhu Jun, 2010, P 27 INT C MACH LEAR, P1239.},
  da = {2022-05-17},
  doc-delivery-number = {287AL},
  eissn = {1741-3176},
  funding-acknowledgement = {Rhodes Trust, Oxford, UK; EPSRC {[}EP/I005021/1]; Engineering and
Physical Sciences Research Council {[}EP/I005021/1] Funding Source:
researchfish},
  funding-text = {This work was supported by the Rhodes Trust, Oxford, UK, and by the
EPSRC (grant number EP/I005021/1) and an EPSRC Leadership Fellowship.},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {Life-long learning; topological mapping; topic models; perplexity},
  keywords-plus = {PROBABILISTIC LOCALIZATION; SURPRISING EVENTS; LOOP-CLOSURE; FAB-MAP;
SLAM; NAVIGATION; MODEL},
  language = {English},
  number-of-cited-references = {70},
  research-areas = {Robotics},
  times-cited = {2},
  type = {Article},
  unique-id = {WOS:000329510300010},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {7},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{hochdorfer-et-al:2009:5339626,
  author = {S. Hochdorfer and M. Lutz and C. Schlegel},
  booktitle = {2009 IEEE INTERNATIONAL CONFERENCE ON TECHNOLOGIES FOR PRACTICAL ROBOT
APPLICATIONS (TEPRA 2009)},
  title = {Lifelong Localization of a Mobile Service-Robot in Everyday Indoor
Environments Using Omnidirectional Vision},
  pages = {161--166},
  doi = {10.1109/TEPRA.2009.5339626},
  note = {IEEE International Conference on Technologies for Practical Robot
Applications, Woburn, MA, NOV 09-10, 2009},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2009},
  abstract = {SLAM (Simultaneous Localization and Mapping) mechanisms are a key
component towards advanced service robotics applications. Currently, a
major hurdle on the way to lifelong localization is the handling of the
ever growing amount of landmarks over time. Therefore, the required
resources in terms of memory and processing power are also growing over
time.
An approach to restrict the absolute number of landmarks by an upper
bound was presented in {[}1]. The key was a method to specifically
select and replace landmarks once an upper bound has been reached. In
this paper, we extend that landmark rating and selection approach. The
here presented extension improves the landmark rating and selection
process. Landmarks are kept such that their visibility regions better
approximate the robot's operational area.
A landmark with a low information content in a sparsely known region is
often more useful than a landmark with a higher information content in a
well-known region. Clustering algorithms are used to identify regions in
the environment with a high landmark density. Removing a landmark from a
cluster with high localization support will have the smallest degradal
ion of robot localization quality.
Real-world experiments are used to demonstrate the performance of our
approach. These experiments are performed on a P3DX-platform with a
bearing-only SLAM approach. All three approaches of handling landmarks
(the standard approach without upper bound on the number of landmarks,
the improved and the previous landmark rating and selection process) are
compared against each other.},
  affiliation = {Hochdorfer, S (Corresponding Author), Univ Appl Sci Ulm, Dept Comp Sci, Prittwitzstr 10, D-89075 Ulm, Germany.
Hochdorfer, Siegfried; Lutz, Matthias; Schlegel, Christian, Univ Appl Sci Ulm, Dept Comp Sci, D-89075 Ulm, Germany.},
  affiliations = {Ulm University},
  author-email = {hochdorfer@hs-ulm.de
lutz@hs-ulm.de
schlegel@hs-ulm.de},
  book-group-author = {IEEE},
  cited-references = {Bailey T, 2003, IEEE INT CONF ROBOT, P1966, DOI 10.1109/ROBOT.2003.1241882.
Bay H, 2006, 9 EUR C COMP VIS GRA.
Dissanayake G., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1009, DOI 10.1109/ROBOT.2000.844732.
Ester M., 1996, KDD, V96, P226, DOI DOI 10.5555/3001460.3001507.
HOCHDORFER S, 2007, AUTONOME MOBILE SYST, P8.
HOCHDORFER S, 2009, IEEE RSJ INT C INT R.
SCHLEGEL C, 2005, AUTONOME MOBILE SYST, P99.
Seber GAF, 1984, MULTIVARIATE OBSERVA.
Spath H, 1985, CLUSTER DISSECTION A.
Strasdat H., 2009, IEEE INT C ROB AUT I.
Tran TN, 2005, CHEMOMETR INTELL LAB, V77, P3, DOI 10.1016/j.chemolab.2004.07.011.
Yip AM, 2006, IEEE T PATTERN ANAL, V28, P877, DOI 10.1109/TPAMI.2006.117.},
  da = {2022-05-17},
  doc-delivery-number = {BUO02},
  isbn = {978-1-4244-4991-0},
  language = {English},
  number-of-cited-references = {12},
  research-areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
  times-cited = {1},
  type = {Proceedings Paper},
  unique-id = {WOS:000289878100027},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Engineering, Electrical \& Electronic; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{luthardt-et-al:2018:8569323,
  author = {S. Luthardt and V. Willert and J. Adamy},
  booktitle = {2018 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS
(ITSC)},
  title = {LLama-SLAM: Learning High-Quality Visual Landmarks for Long-Term Mapping
and Localization},
  pages = {2645--2652},
  doi = {10.1109/ITSC.2018.8569323},
  note = {21st IEEE International Conference on Intelligent Transportation Systems
(ITSC), Maui, HI, NOV 04-07, 2018},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2018},
  abstract = {The precise localization of vehicles is an important requirement for
autonomous driving or advanced driver assistance systems. Using common
GNSS the ego position can be measured but not with the reliability and
precision necessary. An alternative approach to achieve precise
localization is the usage of visual landmarks observed by a camera
mounted in the vehicle. However, this raises the necessity of reliable
visual landmarks that are easily recognizable and persistent. We propose
a novel SLAM algorithm that focuses on learning and mapping such visual
long-term landmarks (LLamas). The algorithm therefore processes stereo
image streams from several recording sessions in the same spatial area.
The key part within LLama-SLAM is the assessment of the landmarks with
quality values that are inferred as viewpoint dependent probabilities
from observation statistics. By adding solely landmarks of high quality
to the final LLama Map, it can be kept compact while still allowing
reliable localization. Due to the long-term evaluation of the GNSS
measurement during the sessions, the landmarks can be positioned
precisely in a global referenced coordinate system. For a first
assessment of the algorithm's capabilities, we present some experimental
results from the mapping process combining three sessions recorded over
two months on the same route.},
  affiliation = {Luthardt, S (Corresponding Author), Tech Univ Darmstadt, Control Methods \& Robot, Darmstadt, Germany.
Luthardt, Stefan; Willert, Volker; Adamy, Juergen, Tech Univ Darmstadt, Control Methods \& Robot, Darmstadt, Germany.},
  affiliations = {Technical University of Darmstadt},
  book-group-author = {IEEE},
  cited-references = {Bishop, 2006, INFORM SCI STAT, V1.
Bresson G, 2017, IEEE T INTELL VEHICL, V2, P194, DOI 10.1109/TIV.2017.2749181.
Buczko M, 2017, IEEE INT VEH SYM, P739, DOI 10.1109/IVS.2017.7995805.
Buczko M, 2016, IEEE INT VEH SYM, P478, DOI 10.1109/IVS.2016.7535429.
Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Cvisic I., 2017, J FIELD ROBOT, V13, P99.
Dayoub F, 2011, ROBOT AUTON SYST, V59, P285, DOI 10.1016/j.robot.2011.02.013.
Delobel L, 2017, IEEE INT VEH SYM, P1342, DOI 10.1109/IVS.2017.7995898.
Dymczyk M, 2015, IEEE INT CONF ROBOT, P2767, DOI 10.1109/ICRA.2015.7139575.
Fraundorfer F, 2012, IEEE ROBOT AUTOM MAG, V19, P78, DOI 10.1109/MRA.2012.2182810.
Grisetti G, 2010, IEEE INTEL TRANSP SY, V2, P31, DOI 10.1109/MITS.2010.939925.
Johns E, 2014, INT J COMPUT VISION, V106, P297, DOI 10.1007/s11263-013-0648-6.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Krajnik T, 2017, IEEE T ROBOT, V33, P964, DOI 10.1109/TRO.2017.2665664.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Levinson J, 2010, IEEE INT CONF ROBOT, P4372, DOI 10.1109/ROBOT.2010.5509700.
Muhlfellner P, 2016, J FIELD ROBOT, V33, P561, DOI 10.1002/rob.21595.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233.
Schuster F., 2017, IEEE 20 INT C INT TR, P1.
Sons M, 2017, IEEE INT VEH SYM, P1158, DOI 10.1109/IVS.2017.7995869.
Stubler M, 2017, 2017 SENSOR DATA FUSION: TRENDS, SOLUTIONS, APPLICATIONS (SDF).
Willert Volker, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P711, DOI 10.1109/ICCVW.2009.5457632.
Willert V, 2010, VISAPP 2010: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P117.},
  da = {2022-05-17},
  doc-delivery-number = {BL9OB},
  isbn = {978-1-7281-0323-5},
  issn = {2153-0009},
  language = {English},
  number-of-cited-references = {26},
  oa = {Green Submitted},
  research-areas = {Automation \& Control Systems; Computer Science; Engineering;
Transportation},
  series = {IEEE International Conference on Intelligent Transportation Systems-ITSC},
  times-cited = {4},
  type = {Proceedings Paper},
  unique-id = {WOS:000457881302097},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Engineering, Electrical \& Electronic; Transportation
Science \& Technology},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{nuske-et-al:2009:20306,
  author = {S. Nuske and J. Roberts and G. Wyeth},
  journal = {JOURNAL OF FIELD ROBOTICS},
  title = {Robust Outdoor Visual Localization Using a Three-Dimensional-Edge Map},
  volume = {26},
  number = {9},
  pages = {728--756},
  doi = {10.1002/rob.20306},
  publisher = {WILEY},
  address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  year = {2009},
  month = {9},
  abstract = {Visual localization systems that are practical for autonomous vehicles
in outdoor industrial applications must perform reliably in a wide range
of conditions. Changing outdoor conditions cause difficulty by
drastically altering the information available in the camera images. To
confront the problem, we have developed a visual localization system
that uses a surveyed three-dimensional (3D)-edge map of permanent
structures in the environment. The map has the invariant properties
necessary to achieve long-term robust operation. Previous 3D-edge map
localization systems usually maintain a single pose hypothesis, making
it difficult to initialize without an accurate prior pose estimate and
also making them susceptible to misalignment with unmapped edges
detected in the camera image. A multihypothesis particle filter is
employed here to perform the initialization procedure with significant
uncertainty in the vehicle's initial pose. A novel observation function
for the particle filter is developed and evaluated against two existing
functions. The new function is shown to further improve the abilities of
the particle filter to converge given a very coarse estimate of the
vehicle's initial pose. An intelligent exposure control algorithm is
also developed that improves the quality of the pertinent information in
the image. Results gathered over an entire sunny day and also during
rainy weather illustrate that the localization system can operate in a
wide range of outdoor conditions. The conclusion is that an invariant
map, a robust multihypothesis localization algorithm, and an intelligent
exposure control algorithm all combine to enable reliable visual
localization through challenging outdoor conditions. (C) 2009 Wiley
Periodicals, Inc.},
  affiliation = {Nuske, S (Corresponding Author), Univ Queensland, Sch Informat Technol \& Elect Engn, St Lucia, Qld 4072, Australia.
Nuske, Stephen; Wyeth, Gordon, Univ Queensland, Sch Informat Technol \& Elect Engn, St Lucia, Qld 4072, Australia.
Nuske, Stephen; Roberts, Jonathan, CSIRO ICT Ctr, Autonomous Syst Lab, Kenmore, Qld 4069, Australia.},
  affiliations = {University of Queensland; Commonwealth Scientific \& Industrial Research
Organisation (CSIRO)},
  author-email = {nuske@cmu.edu
jonathan.roberts@csiro.au},
  cited-references = {{*}1394 TRAD ASS, 2000, IIDC 1394 BAS DIG CA.
CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049.
Drummond T, 2002, IEEE T PATTERN ANAL, V24, P932, DOI 10.1109/TPAMI.2002.1017620.
Fox D, 2003, INT J ROBOT RES, V22, P985, DOI 10.1177/0278364903022012001.
Georgiev A, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P472, DOI 10.1109/IRDS.2002.1041435.
Geyer C, 2001, INT J COMPUT VISION, V45, P223, DOI 10.1023/A:1013610201135.
KLEIN G, 2006, FULL 3D EDGE TRACKIN.
KOSAKA A, 1992, P INT C INT ROB SYST.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Maimone M, 2007, J FIELD ROBOT, V24, P169, DOI 10.1002/rob.20184.
Marks TK, 2008, IEEE INT CONF ROBOT, P3717, DOI 10.1109/ROBOT.2008.4543781.
MICHEL R, 2007, IEEE RSJ INT C INT R, P463.
Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188.
Nister D, 2006, J FIELD ROBOT, V23, P3, DOI 10.1002/rob.20103.
NUSKE S, 2008, P IEEE INT C ROB AUT.
Nuske S, 2006, IEEE INT CONF ROBOT, P162, DOI 10.1109/ROBOT.2006.1641178.
{*}NVIDIA CORP, 2007, NVIDIA OPENGL EXT SP.
Paz LM, 2008, IEEE T ROBOT, V24, P946, DOI 10.1109/TRO.2008.2004637.
Pradalier C, 2008, J FIELD ROBOT, V25, P243, DOI 10.1002/rob.20240.
Reitmayr G., 2006, INT S MIX AUGM REAL, P109, DOI DOI 10.1109/ISMAR.2006.297801.
ROBERTS J, 2008, P 6 IARP IEEE RAS EU.
Roberts J, 2007, IEEE INT CONF ROBOT, P2770, DOI 10.1109/ROBOT.2007.363888.
Se S, 2002, INT J ROBOT RES, V21, P735, DOI 10.1177/027836402761412467.
SHIMIZU S, 1992, IEEE T CONSUM ELECTR, V38, P617, DOI 10.1109/30.156745.
SIM R, 2003, INT JOINT C ART INT.
Tews A, 2007, IEEE INT CONF ROBOT, P1176, DOI 10.1109/ROBOT.2007.363144.
Thrun S., 2005, PROBABALISTIC ROBOTI.
VALGREN C, 2008, IEEE INT C ROB AUT P.
Valgren C., 2007, EUR C MOB ROB FREIB.
YANG M, 2006, IEEE INT C COMP VIS.
Ying XG, 2004, LECT NOTES COMPUT SC, V3021, P442.},
  da = {2022-05-17},
  doc-delivery-number = {495MO},
  eissn = {1556-4967},
  funding-acknowledgement = {CSIRO ICT Centre; School of Information Technology and Electrical
Engineering at the University of Queensland},
  funding-text = {This work was a part of Stephen Nuske's Ph.D. research that was funded
equally by the CSIRO ICT Centre and the School of Information Technology
and Electrical Engineering at the University of Queensland. The material
resources required for this work were provided by CSIRO's Light Metals
Flagship Autonomous Hot Metal Carrier Project. The authors gratefully
acknowledge the following members of the CSIRO ICT Centre's Autonomous
Systems Lab team for their assistance with the setup, calibration, and
experimental work conducted during this project: Cedric Pradalier,
Ashley Tews, Peter Hansen, Paul Hick, Polly Alexander, Felix Duvallet,
and Felix Ruess.},
  issn = {1556-4959},
  journal-iso = {J. Field Robot.},
  keywords-plus = {ODOMETRY; VEHICLE; VISION; SLAM},
  language = {English},
  number-of-cited-references = {33},
  orcid-numbers = {Wyeth, Gordon/0000-0002-4996-3612},
  research-areas = {Robotics},
  researcherid-numbers = {Roberts, Jonathan/B-7485-2011
Wyeth, Gordon/B-7902-2010},
  times-cited = {20},
  type = {Article},
  unique-id = {WOS:000269897400004},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {2},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ouerghi-et-al:2018:s18040939,
  author = {S. Ouerghi and R. Boutteau and X. Savatier and F. Thai},
  journal = {SENSORS},
  title = {Visual Odometry and Place Recognition Fusion for Vehicle Position
Tracking in Urban Environments},
  volume = {18},
  number = {4},
  pages = {939},
  doi = {10.3390/s18040939},
  publisher = {MDPI},
  address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  year = {2018},
  month = {4},
  abstract = {In this paper, we address the problem of vehicle localization in urban
environments. We rely on visual odometry, calculating the incremental
motion, to track the position of the vehicle and on place recognition to
correct the accumulated drift of visual odometry, whenever a location is
recognized. The algorithm used as a place recognition module is SeqSLAM,
addressing challenging environments and achieving quite remarkable
results. Specifically, we perform the long-term navigation of a vehicle
based on the fusion of visual odometry and SeqSLAM. The template library
for this latter is created online using navigation information from the
visual odometry module. That is, when a location is recognized, the
corresponding information is used as an observation of the filter. The
fusion is done using the EKF and the UKF, the well-known nonlinear state
estimation methods, to assess the superior alternative. The algorithm is
evaluated using the KITTI dataset and the results show the reduction of
the navigation errors by loop-closure detection. The overall position
error of visual odometery with SeqSLAM is 0.22\% of the trajectory,
which is much smaller than the navigation errors of visual odometery
alone 0.45\%. In addition, despite the superiority of the UKF in a
variety of estimation problems, our results indicate that the UKF
performs as efficiently as the EKF at the expense of an additional
computational overhead. This leads to the conclusion that the EKF is a
better choice for fusing visual odometry and SeqSlam in a long-term
navigation context.},
  affiliation = {Ouerghi, S (Corresponding Author), Carthage Univ, SUPCOM, GRESCOM, El Ghazela 2083, Tunisia.
Ouerghi, Safa; Thai, Fethi, Carthage Univ, SUPCOM, GRESCOM, El Ghazela 2083, Tunisia.
Boutteau, Remi; Savatier, Xavier, Normandie Univ, UNIROUEN, ESIGELEC, IRSEEM, F-76000 Rouen, France.},
  affiliations = {Universite de Carthage},
  article-number = {939},
  author-email = {safa.ouerghi@supcom.tn
remi.boutteau@esigelec.fr
xavier.savatier@esigelec.fr
fethi.tlili@supcom.tn},
  cited-references = {Baatz G, 2012, INT J COMPUT VISION, V96, P315, DOI 10.1007/s11263-011-0458-7.
Badino H, 2011, IEEE INT VEH SYM, P794, DOI 10.1109/IVS.2011.5940504.
Bonardi F, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17051167.
Brubaker MA, 2016, IEEE T PATTERN ANAL, V38, P652, DOI 10.1109/TPAMI.2015.2453975.
Chu H., 2015, ARXIV151009171.
Clipp B, 2010, IEEE INT C INT ROBOT, P3961, DOI 10.1109/IROS.2010.5653696.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
Floros G, 2013, IEEE INT CONF ROBOT, P1054, DOI 10.1109/ICRA.2013.6630703.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Geiger A, 2011, IEEE INT VEH SYM, P963, DOI 10.1109/IVS.2011.5940405.
Hentschel M, 2010, 2010 13th International IEEE Conference on Intelligent Transportation Systems (ITSC 2010), P1645, DOI 10.1109/ITSC.2010.5625092.
Kaess M., 2009, P IEEE INT C ROB AUT.
Kaess M, 2012, INT J ROBOT RES, V31, P216, DOI 10.1177/0278364911430419.
Kalman R. E., 1960, J FLUIDS ENG, V82, P34, DOI {[}https://doi.org/10.1115/1.3662552, DOI 10.1115/1.3662552].
Kneip L, 2014, IEEE INT CONF ROBOT, P1, DOI 10.1109/ICRA.2014.6906582.
Kummerle R., 2011, P 2011 IEEE INT C RO.
Majdik AL, 2014, IEEE INT CONF ROBOT, P920, DOI 10.1109/ICRA.2014.6906964.
Mei C, 2011, INT J COMPUT VISION, V94, P198, DOI 10.1007/s11263-010-0361-7.
Milford M., 2012, P ROB SCI SYST RSS S.
Milford M, 2013, INT J ROBOT RES, V32, P766, DOI 10.1177/0278364913490323.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513.
OH S, 2004, P IEEE RSJ INT C INT.
Ouerghi S., 2017, P 25 INT C COMP GRAP.
Paul R, 2010, IEEE INT CONF ROBOT, P2649, DOI 10.1109/ROBOT.2010.5509587.
Pepperell E., 2013, P AUSTR C ROB AUT AR.
Ranganathan A, 2011, INT J ROBOT RES, V30, P755, DOI 10.1177/0278364910393287.
Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233.
Stone T., 2014, P ROB SCI SYST 10 RS.
Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986.
Sunderhauf N, 2013, P IEEE INT C ROB AUT.
Thrun S, 2001, ARTIF INTELL, V128, P99, DOI 10.1016/S0004-3702(01)00069-8.
Thrun Sebastian, 2005, PROBABILISTIC ROBOTI, V1.
Zhou DF, 2016, IEEE INT VEH SYM, P490, DOI 10.1109/IVS.2016.7535431.},
  da = {2022-05-17},
  doc-delivery-number = {GJ7NS},
  eissn = {1424-8220},
  journal-iso = {Sensors},
  keywords = {real-time navigation; visual-odometry; SeqSLAM; loop-closure; EKF; UKF},
  keywords-plus = {FAB-MAP; LOCALIZATION},
  language = {English},
  number-of-cited-references = {36},
  oa = {Green Published, Green Submitted, gold},
  orcid-numbers = {Boutteau, Rémi/0000-0003-1078-5043},
  research-areas = {Chemistry; Engineering; Instruments \& Instrumentation},
  researcherid-numbers = {SAVATIER, Xavier/AAG-6093-2019
Boutteau, Rémi/U-7674-2019},
  times-cited = {6},
  type = {Article},
  unique-id = {WOS:000435574800011},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {12},
  web-of-science-categories = {Chemistry, Analytical; Engineering, Electrical \& Electronic;
Instruments \& Instrumentation},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{siva-zhang:2018:8461042,
  author = {S. Siva and H. Zhang},
  booktitle = {2018 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {Omnidirectional Multisensory Perception Fusion for Long-Term Place
Recognition},
  pages = {5175--5181},
  doi = {10.1109/ICRA.2018.8461042},
  note = {IEEE International Conference on Robotics and Automation (ICRA),
Brisbane, AUSTRALIA, MAY 21-25, 2018},
  publisher = {IEEE COMPUTER SOC},
  address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
  year = {2018},
  abstract = {Over the recent years, long-term place recognition has attracted an
increasing attention to detect loops for largescale Simultaneous
Localization and Mapping (SLAM) in loopy environments during long-term
autonomy. Almost all existing methods are designed to work with
traditional cameras with a limited field of view. Recent advances in
omnidirectional sensors offer a robot an opportunity to perceive the
entire surrounding environment. However, no work has existed thus far to
research how omnidirectional sensors can help long-term place
recognition, especially when multiple types of omnidirectional sensory
data are available. In this paper, we propose a novel approach to
integrate observations obtained from multiple sensors from different
viewing angles in the omnidirectional observation in order to perform
multi-directional place recognition in longterm autonomy. Our approach
also answers two new questions when omnidirectional multisensory data is
available for place recognition, including whether it is possible to
recognize a place with long-term appearance variations when robots
approach it from various directions, and whether observations from
various viewing angles are the same informative. To evaluate our
approach and hypothesis, we have collected the first largescale dataset
that consists of omnidirectional multisensory (intensity and depth) data
collected in urban and suburban environments across a year. Experimental
results have shown that our approach is able to achieve
multi-directional long-term place recognition, and identifies the most
discriminative viewing angles from the omnidirectional observation.},
  affiliation = {Siva, S (Corresponding Author), Colorado Sch Mines, Human Ctr Robot Lab, Dept Comp Sci, Golden, CO 80401 USA.
Siva, Sriram; Zhang, Hao, Colorado Sch Mines, Human Ctr Robot Lab, Dept Comp Sci, Golden, CO 80401 USA.},
  affiliations = {Colorado School of Mines},
  author-email = {sivasriram@mines.edu
hzhang@mines.edu},
  book-group-author = {IEEE},
  cited-references = {Arroyo R., 2015, INT C ROB AUT.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Chen C, 2006, INT J ROBOT RES, V25, P953, DOI 10.1177/0278364906068375.
Chen Zetao, 2014, AUSTR C ROB AUT.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dalal Navneet, 2005, C COMP VIS PATT REC.
Han F., 2018, LEARNING INTEGRATED.
Han F, 2017, IEEE ROBOT AUTOM LET, V2, P1172, DOI 10.1109/LRA.2017.2662061.
Labbe M, 2013, IEEE T ROBOT, V29, P734, DOI 10.1109/TRO.2013.2242375.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Milford M. J., 2004, INT C ROB AUT.
Milford M. J., 2012, INT C ROB AUT.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Naseer T., 2014, C ART INT.
Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724.
Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2.
Salas-Moreno R. F., 2013, C COMP VIS PATT REC.
Sunderhauf N., 2015, INTELLIGENT ROBOTS S.
Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.
Zhang H, 2016, ROBOTICS: SCIENCE AND SYSTEMS XII.},
  da = {2022-05-17},
  doc-delivery-number = {BL0QZ},
  eissn = {2577-087X},
  funding-acknowledgement = {ARO {[}W911NF-17-1-0447]},
  funding-text = {This work was funded in part by the ARO grant W911NF-17-1-0447.},
  isbn = {978-1-5386-3081-5},
  issn = {1050-4729},
  keywords-plus = {SCENE},
  language = {English},
  number-of-cited-references = {20},
  orcid-numbers = {Zhang, Hao/0000-0001-8043-9184},
  research-areas = {Automation \& Control Systems; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {12},
  type = {Proceedings Paper},
  unique-id = {WOS:000446394503135},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Automation \& Control Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{williams-et-al:2014:0278364914531056,
  author = {S. Williams and V. Indelman and M. Kaess and R. Richard and J. J. Leonard and F. Dellaert},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {Concurrent filtering and smoothing: A parallel architecture for
real-time navigation and full smoothing},
  volume = {33},
  number = {12},
  pages = {1544--1568},
  doi = {10.1177/0278364914531056},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2014},
  month = {10},
  abstract = {We present a parallelized navigation architecture that is capable of
running in real-time and incorporating long-term loop closure
constraints while producing the optimal Bayesian solution. This
architecture splits the inference problem into a low-latency update that
incorporates new measurements using just the most recent states
(filter), and a high-latency update that is capable of closing long
loops and smooths using all past states (smoother). This architecture
employs the probabilistic graphical models of factor graphs, which
allows the low-latency inference and high-latency inference to be viewed
as sub-operations of a single optimization performed within a single
graphical model. A specific factorization of the full joint density is
employed that allows the different inference operations to be performed
asynchronously while still recovering the optimal solution produced by a
full batch optimization. Due to the real-time, asynchronous nature of
this algorithm, updates to the state estimates from the high-latency
smoother will naturally be delayed until the smoother calculations have
completed. This architecture has been tested within a simulated aerial
environment and on real data collected from an autonomous ground
vehicle. In all cases, the concurrent architecture is shown to recover
the full batch solution, even while updated state estimates are produced
in real-time.},
  affiliation = {Indelman, V (Corresponding Author), Coll Comp, Inst Robot \& Intelligent Machines, 801 Atlantic Dr, Atlanta, GA 30309 USA.
Williams, Stephen; Indelman, Vadim; Roberts, Richard; Dellaert, Frank, Georgia Inst Technol, Inst Robot \& Intelligent Machines, Atlanta, GA 30332 USA.
Kaess, Michael, Carnegie Mellon Univ, Sch Comp Sci, Field Robot Ctr, Inst Robot, Pittsburgh, PA 15213 USA.
Leonard, John J., MIT, Comp Sci \& Artificial Intelligence Lab, Cambridge, MA 02139 USA.},
  affiliations = {University System of Georgia; Georgia Institute of Technology; Carnegie
Mellon University; Massachusetts Institute of Technology (MIT)},
  author-email = {indelman@cc.gatech.edu},
  cited-references = {Aharoni R, 2009, INVENT MATH, V176, P1, DOI 10.1007/s00222-008-0157-3.
Bar-Shalom Y, 2002, IEEE T AERO ELEC SYS, V38, P769, DOI 10.1109/TAES.2002.1039398.
Bar-Shalom Y., 1995, MULTITARGET MULTISEN.
Davis TA, 2004, ACM T MATH SOFTWARE, V30, P353, DOI 10.1145/1024074.1024079.
Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768.
Eustice RM, 2006, IEEE T ROBOT, V22, P1100, DOI 10.1109/TRO.2006.886264.
Farrell J.A., 2008, AIDED NAVIGATION GPS.
Folkesson J, 2004, IEEE INT CONF ROBOT, P383, DOI 10.1109/ROBOT.2004.1307180.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Grisetti G, 2007, P ROB SCI SYST RSS.
GTSAM, GTSAM 2 3 1.
Heggernes P, 1996, P 2 SIAM C SPARS MAT.
Indelman V., 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P2154.
Indelman V, 2013, IEEE INT C INT ROBOT, P1952, DOI 10.1109/IROS.2013.6696615.
Indelman V, 2013, ROBOT AUTON SYST, V61, P721, DOI 10.1016/j.robot.2013.05.001.
Jones ES, 2011, INT J ROBOT RES, V30, P407, DOI 10.1177/0278364910388963.
Kaess Michael, 2011, 2011 IEEE International Conference on Robotics and Automation, P3281.
Kaess M, 2010, P INT WORKSH ALG FDN.
Kaess M, 2012, P INT C INF FUS FUS, P2154.
Kaess M, 2012, INT J ROBOT RES, V31, P216, DOI 10.1177/0278364911430419.
Kaess M, 2010, SPRINGER TRAC ADV RO, V68, P157.
Klein George, 2007, P1.
Konolige K., 2010, 2010 IEEERSJ INT C I, P22.
Konolige K, 2008, IEEE T ROBOT, V24, P1066, DOI 10.1109/TRO.2008.2004832.
Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572.
KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694.
Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410.
Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733.
Lupton T, 2012, IEEE T ROBOT, V28, P61, DOI 10.1109/TRO.2011.2170332.
Mahon I, 2008, IEEE T ROBOT, V24, P1002, DOI 10.1109/TRO.2008.2004888.
Maybeck P. S., 1979, STOCHASTIC MODELS ES, V1.
Mei C, 2011, INT J COMPUT VISION, V94, P198, DOI 10.1007/s11263-010-0361-7.
Menger K., 1927, FUND MATH, V10, P96, DOI DOI 10.4064/FM-10-1-96-115.
Mourikis Anastasios I, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563131.
Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024.
Moutarlier P., 1989, EXPT ROBOTICS 1, P327.
Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513.
Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378.
Ranganathan A, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P2492.
Shen XJ, 2009, IEEE T AUTOMAT CONTR, V54, P1928, DOI 10.1109/TAC.2009.2023777.
SIBLEY G, 2009, P ROB SCI SYST RSS S.
Smith D, 2006, IEEE T KNOWL DATA EN, V18, P1696, DOI 10.1109/TKDE.2006.183.
Smith R., 1990, AUTONOMOUS ROBOT VEH, P167, DOI DOI 10.1007/978-1-4613-8997-2\_14.
Smith Randall, 1988, P 4 INT S ROB RES, P467.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Triggs Bill, 1999, LECT NOTES COMPUTER, P298, DOI DOI 10.1007/3-540-44480-7\_21.
Vial J, 2011, IEEE INT C INT ROBOT, P886, DOI 10.1109/IROS.2011.6048728.
Zahng S, 2011, P SPIE.
Zhu Z., 2007, IEEE C COMP VIS PATT, P1.},
  da = {2022-05-17},
  doc-delivery-number = {AQ4UM},
  eissn = {1741-3176},
  funding-acknowledgement = {All Source Positioning and Navigation (ASPN) program of the Air Force
Research Laboratory (AFRL) {[}FA8650-11-C-7137]},
  funding-text = {This work was supported by the All Source Positioning and Navigation
(ASPN) program of the Air Force Research Laboratory (AFRL) (contract
number FA8650-11-C-7137). The views expressed in this work have not been
endorsed by the sponsors.},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {Information fusion; smoothing; filtering; real time navigation; SLAM;
probabilistic graphical models},
  keywords-plus = {TRACKING; FUSION; SLAM},
  language = {English},
  number-of-cited-references = {49},
  oa = {Green Submitted},
  orcid-numbers = {Kaess, Michael/0000-0002-7590-3357
Dellaert, Frank/0000-0002-5532-3566},
  research-areas = {Robotics},
  times-cited = {11},
  type = {Article},
  unique-id = {WOS:000342795200003},
  usage-count-last-180-days = {3},
  usage-count-since-2013 = {20},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{yang-et-al:2020:s20082432,
  author = {S. Yang and G. Fan and L. Bai and C. Zhao and L. Dexin},
  journal = {SENSORS},
  title = {SGC-VSLAM: A Semantic and Geometric Constraints VSLAM for Dynamic Indoor
Environments},
  volume = {20},
  number = {8},
  pages = {2432},
  doi = {10.3390/s20082432},
  publisher = {MDPI},
  address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  year = {2020},
  month = {4},
  abstract = {As one of the core technologies for autonomous mobile robots, Visual
Simultaneous Localization and Mapping (VSLAM) has been widely researched
in recent years. However, most state-of-the-art VSLAM adopts a strong
scene rigidity assumption for analytical convenience, which limits the
utility of these algorithms for real-world environments with independent
dynamic objects. Hence, this paper presents a semantic and geometric
constraints VSLAM (SGC-VSLAM), which is built on the RGB-D mode of
ORB-SLAM2 with the addition of dynamic detection and static point cloud
map construction modules. In detail, a novel improved quadtree-based
method was adopted for SGC-VSLAM to enhance the performance of the
feature extractor in ORB-SLAM (Oriented FAST and Rotated BRIEF-SLAM).
Moreover, a new dynamic feature detection method called semantic and
geometric constraints was proposed, which provided a robust and fast way
to filter dynamic features. The semantic bounding box generated by YOLO
v3 (You Only Look Once, v3) was used to calculate a more accurate
fundamental matrix between adjacent frames, which was then used to
filter all of the truly dynamic features. Finally, a static point cloud
was estimated by using a new drawing key frame selection strategy.
Experiments on the public TUM RGB-D (Red-Green-Blue Depth) dataset were
conducted to evaluate the proposed approach. This evaluation revealed
that the proposed SGC-VSLAM can effectively improve the positioning
accuracy of the ORB-SLAM2 system in high-dynamic scenarios and was also
able to build a map with the static parts of the real environment, which
has long-term application value for autonomous mobile robots.},
  affiliation = {Yang, SQ (Corresponding Author), Xian Univ Technol, Sch Mech \& Precis Instrument Engn, Xian 710048, Shaanxi, Peoples R China.
Yang, Shiqiang; Fan, Guohao; Bai, Lele; Zhao, Cheng; Li, Dexin, Xian Univ Technol, Sch Mech \& Precis Instrument Engn, Xian 710048, Shaanxi, Peoples R China.},
  affiliations = {Xi'an University of Technology},
  article-number = {2432},
  author-email = {yangsq@xaut.edu.cn
2170220051@stu.xaut.edu.cn
2180220045@stu.xaut.edu.cn
2190221126@stu.xaut.edu.cn
lidexin@xaut.edu.cn},
  cited-references = {Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615.
Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039.
Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4.
Fan YC, 2019, PATTERN RECOGN LETT, V127, P191, DOI 10.1016/j.patrec.2018.10.024.
Han SQ, 2020, IEEE ACCESS, V8, P43563, DOI 10.1109/ACCESS.2020.2977684.
He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322.
Kerl C., 2013, P 2013 IEEE INT C RO.
Kim DH, 2016, IEEE T ROBOT, V32, P1565, DOI 10.1109/TRO.2016.2609395.
Klein George, 2007, P1.
Kovacs A, 2013, IEEE GEOSCI REMOTE S, V10, P796, DOI 10.1109/LGRS.2012.2224315.
Li P., 2019, P 2019 INT C ADV MEC.
Li RH, 2018, COGN COMPUT, V10, P875, DOI 10.1007/s12559-018-9591-8.
Li SL, 2017, IEEE ROBOT AUTOM LET, V2, P2263, DOI 10.1109/LRA.2017.2724759.
Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0\_2.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Paul S, 2016, IEEE GEOSCI REMOTE S, V13, P1300, DOI 10.1109/LGRS.2016.2582528.
Pritts J, 2013, INT CONF IMAG VIS, P106, DOI 10.1109/IVCNZ.2013.6727000.
Redmon J., 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.91.
Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Sedaghat A, 2011, IEEE T GEOSCI REMOTE, V49, P4516, DOI 10.1109/TGRS.2011.2144607.
Stuhmer J, 2010, LECT NOTES COMPUT SC, V6376, P11.
Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773.
Sun YX, 2018, ROBOT AUTON SYST, V108, P115, DOI 10.1016/j.robot.2018.07.002.
Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012.
Taketomi T., 2017, IPSJ T COMPUTER VISI, V9, P1, DOI {[}10.1186/s41074-016-0012-1, DOI 10.1186/S41074-017-0027-2, 10.1186/s41074-017-0027-2].
Vidal AR, 2018, IEEE ROBOT AUTOM LET, V3, P994, DOI 10.1109/LRA.2018.2793357.
Wangsiripitak Somkiat, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P375, DOI 10.1109/ROBOT.2009.5152290.
Xie H., 2016, APPL SCI TECHNOL, V3, P23.
Yazdi M, 2018, COMPUT SCI REV, V28, P157, DOI 10.1016/j.cosrev.2018.03.001.
Yi KM, 2013, IEEE COMPUT SOC CONF, P27, DOI 10.1109/CVPRW.2013.9.
Yu C, 2018, IEEE INT C INT ROBOT, P1168, DOI 10.1109/IROS.2018.8593691.
Zhang L, 2018, IEEE ACCESS, V6, P75545, DOI {[}10.1109/ACCESS.2018.2873617, 10.1109/TCBB.2018.2848633].
Zhao LL, 2019, IEEE ACCESS, V7, P75604, DOI 10.1109/ACCESS.2019.2922733.
Zhong FW, 2018, IEEE WINT CONF APPL, P1001, DOI 10.1109/WACV.2018.00115.},
  da = {2022-05-17},
  doc-delivery-number = {LO0VA},
  eissn = {1424-8220},
  funding-acknowledgement = {National Natural Science Foundation of China {[}51475365]},
  funding-text = {This research was supported by the National Natural Science Foundation
of China under Grant No. 51475365.},
  journal-iso = {Sensors},
  keywords = {Visual SLAM; ORB-SLAM2; dynamic indoor environment; dynamic feature
filtering; point cloud map},
  keywords-plus = {RGB-D SLAM; MOTION REMOVAL; IMAGES},
  language = {English},
  number-of-cited-references = {38},
  oa = {Green Published, gold},
  research-areas = {Chemistry; Engineering; Instruments \& Instrumentation},
  times-cited = {1},
  type = {Article},
  unique-id = {WOS:000533346400274},
  usage-count-last-180-days = {14},
  usage-count-since-2013 = {36},
  web-of-science-categories = {Chemistry, Analytical; Engineering, Electrical \& Electronic;
Instruments \& Instrumentation},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{zhu-et-al:2021:9561584,
  author = {S. Zhu and X. Zhang and S. Guo and J. Li and L. Huaping},
  booktitle = {2021 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA
2021)},
  title = {Lifelong Localization in Semi-Dynamic Environment},
  pages = {14389--14395},
  doi = {10.1109/ICRA48506.2021.9561584},
  note = {IEEE International Conference on Robotics and Automation (ICRA), Xian,
PEOPLES R CHINA, MAY 30-JUN 05, 2021},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2021},
  abstract = {Mapping and localization in non-static environments are fundamental
problems in robotics. Most of previous methods mainly focus on static
and highly dynamic objects in the environment, which may suffer from
localization failure in semi-dynamic scenarios without considering
objects with lower dynamics, such as parked cars and stopped
pedestrians. In this paper, we introduce semantic mapping and lifelong
localization approaches to recognize semi-dynamic objects in non-static
environments. We also propose a generic framework that can integrate
mainstream object detection algorithms with mapping and localization
algorithms. The mapping method combines an object detection algorithm
and a SLAM algorithm to detect semi-dynamic objects and constructs a
semantic map that only contains semi-dynamic objects in the environment.
During navigation, the localization method can classify observation
corresponding to static and non-static objects respectively and evaluate
whether those semi-dynamic objects have moved, to reduce the weight of
invalid observation and localization fluctuation. Real-world experiments
show that the proposed method can improve the localization accuracy of
mobile robots in non-static scenarios.},
  affiliation = {Zhang, XY (Corresponding Author), Tsinghua Univ, State Key Lab Automot Safety \& Energy, Beijing 100084, Peoples R China.
Zhang, XY (Corresponding Author), Tsinghua Univ, Sch Vehicle \& Mobil, Beijing 100084, Peoples R China.
Zhu, Shifan; Zhang, Xinyu; Guo, Shichun; Li, Jun, Tsinghua Univ, State Key Lab Automot Safety \& Energy, Beijing 100084, Peoples R China.
Zhu, Shifan; Zhang, Xinyu; Guo, Shichun; Li, Jun, Tsinghua Univ, Sch Vehicle \& Mobil, Beijing 100084, Peoples R China.
Liu, Huaping, Tsinghua Univ, Dept Comp Sci \& Technol, Beijing 100084, Peoples R China.},
  affiliations = {Tsinghua University; Tsinghua University; Tsinghua University},
  author-email = {shifzhu@gmail.com
xyzhang@tsinghua.edu
shichunguo@gmail.com
lijun19580326@126.com
hpliu@mail.tsinghua.edu.cn},
  book-group-author = {IEEE},
  cited-references = {Akai N, 2018, IEEE INT C INT ROBOT, P3159, DOI 10.1109/IROS.2018.8594146.
Aldibaja M, 2017, IEEE T IND INFORM, V13, P2369, DOI 10.1109/TII.2017.2713836.
Andrade-Cetto J, 2002, INT J PATTERN RECOGN, V16, P361, DOI 10.1142/S0218001402001745.
Aycard O, 1998, IEEE INT CONF ROBOT, P3135, DOI 10.1109/ROBOT.1998.680907.
Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039.
Brasch N, 2018, IEEE INT C INT ROBOT, P393, DOI 10.1109/IROS.2018.8593828.
Burgard W, 1999, ARTIF INTELL, V114, P3, DOI 10.1016/S0004-3702(99)00070-3.
Burgard W., 2012, 26 AAAI C ART INT, P2024.
Chen SW, 2020, IEEE ROBOT AUTOM LET, V5, P612, DOI 10.1109/LRA.2019.2963823.
Ding W., 2020, 2020 P IEEE INT C RO.
Doherty K, 2019, IEEE INT CONF ROBOT, P2419, DOI 10.1109/ICRA.2019.8794244.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Egger P, 2018, IEEE INT C INT ROBOT, P3430, DOI 10.1109/IROS.2018.8593854.
Fox D, 1999, J ARTIF INTELL RES, V11, P391, DOI 10.1613/jair.616.
Gallagher G, 2009, IEEE INT CONF ROBOT, P4322.
Grupp M., 2017, EVO PYTHON PACKAGE E.
Hahnel D, 2003, IEEE INT CONF ROBOT, P1557, DOI 10.1109/ROBOT.2003.1241816.
Hahnel D, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P496, DOI 10.1109/IRDS.2002.1041439.
Hess W, 2016, IEEE INT CONF ROBOT, P1271, DOI 10.1109/ICRA.2016.7487258.
Krajnik T, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4558, DOI 10.1109/IROS.2016.7759671.
Li AQ, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1902, DOI 10.1109/IROS.2016.7759301.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Meyer-Delius D, 2010, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2010.5648920.
Montesano L, 2005, IEEE INT CONF ROBOT, P4556.
Redmon Joseph, 2018, ARXIV ABS 180402767.
Stenborg E, 2018, IEEE INT CONF ROBOT, P6484, DOI 10.1109/ICRA.2018.8463150.
Tardos JD, 2002, INT J ROBOT RES, V21, P311, DOI 10.1177/027836402320556340.
Thrun S, 1998, AUTON ROBOT, V5, P253, DOI 10.1023/A:1008806205438.
Tipaldi GD, 2013, INT J ROBOT RES, V32, P1662, DOI 10.1177/0278364913502830.
Valencia R, 2014, IEEE INT CONF ROBOT, P3956, DOI 10.1109/ICRA.2014.6907433.
Vineet V, 2015, IEEE INT CONF ROBOT, P75, DOI 10.1109/ICRA.2015.7138983.
Wan GW, 2018, IEEE INT CONF ROBOT, P4670, DOI 10.1109/ICRA.2018.8461224.
Wang BH, 2019, IEEE ROBOT AUTOM LET, V4, P2902, DOI 10.1109/LRA.2019.2922582.
Wang CC, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P2918, DOI 10.1109/ROBOT.2002.1013675.
Wang CC, 2007, INT J ROBOT RES, V26, P889, DOI 10.1177/0278364907081229.
Wolf DF, 2005, AUTON ROBOT, V19, P53, DOI 10.1007/s10514-005-0606-4.
Xiao LH, 2019, ROBOT AUTON SYST, V117, P1, DOI 10.1016/j.robot.2019.03.012.
Yu C, 2018, IEEE INT C INT ROBOT, P1168, DOI 10.1109/IROS.2018.8593691.},
  da = {2022-05-17},
  doc-delivery-number = {BS8DT},
  eissn = {2577-087X},
  funding-acknowledgement = {National High Technology Research and Development Program of China
{[}2018YFE0204300]; Beijing Science and Technology Plan Project
{[}Z191100007419008, 2019GQG1010]; National Natural Science Foundation
of China {[}U1964203]},
  funding-text = {This work was supported by the National High Technology Research and
Development Program of China under Grant No. 2018YFE0204300, and the
Beijing Science and Technology Plan Project (Z191100007419008), and the
Guoqiang Research Institute Project (2019GQG1010), and the National
Natural Science Foundation of China under Grant No. U1964203.},
  isbn = {978-1-7281-9077-8},
  issn = {1050-4729},
  keywords-plus = {MOBILE ROBOTS},
  language = {English},
  number-of-cited-references = {38},
  research-areas = {Automation \& Control Systems; Robotics},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {0},
  type = {Proceedings Paper},
  unique-id = {WOS:000771405405115},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Automation \& Control Systems; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{an-et-al:2016:0,
  author = {S.-Y. An and L.-K. Lee and S.-Y. Oh},
  journal = {AUTONOMOUS ROBOTS},
  title = {Ceiling vision-based active SLAM framework for dynamic and wide-open
environments},
  volume = {40},
  number = {2},
  pages = {291--324},
  doi = {10.1007/s10514-015-9453-0},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2016},
  month = {2},
  abstract = {A typical indoor environment can be divided into three categories;
office (or room), hallway, and wide-open space such as lobby and hall.
There have been numerous approaches for solving simultaneous
localization and mapping (SLAM) problem in office (or room) and hallway.
However, direct application of the existing approaches to wide-open
space may be failed, because it has some distinguished features compared
to other indoor places. To solve this problem, this paper proposes a new
ceiling vision-based active SLAM framework, with an emphasis on
practical deployment of service robot for commercial use in dynamically
changing and wide-open environments by adopting the ceiling vision.
First, for defining ceiling feature which can be extracted regardless of
complexity of ceiling pattern we introduce a model-free landmark, i.e.,
visual node descriptor, which consists of edge points and their
orientations in image space. Second, a recursive `explore and exploit'
is proposed for autonomous mapping. It is recursively performed by
spreading out mapped area gradually while the robot is actively
localized in the map. It can improve map accuracy due to frequent small
loop closing. Third, a dynamic edge link (DEL) is proposed to cope with
environmental changes in the map. Owing to DEL, we do not need to filter
out corrupted sensor data and to distinguish moving object from static
one. Also, a self-repairing map mechanism is introduced to deal with
unexpected installation or removal of inner structures. We therefore
achieve long-term navigation. Several simulations and real experiments
in various places show that the proposed active SLAM framework could
build a topologically consistent map, and demonstrated that it can be
applied well to real environments such as wide-open space in a city hall
and railway station.},
  affiliation = {An, SY (Corresponding Author), Elect \& Telecommun Res Inst ETRI, Daegu 711883, South Korea.
An, Su-Yong, Elect \& Telecommun Res Inst ETRI, Daegu 711883, South Korea.
Lee, Lae-Kyoung; Oh, Se-Young, Pohang Univ Sci \& Technol POSTECH, Dept Elect Engn, Pohang 790784, Gyungbuk, South Korea.},
  affiliations = {Electronics \& Telecommunications Research Institute - Korea (ETRI);
Pohang University of Science \& Technology (POSTECH)},
  author-email = {syong.an@etri.re.kr},
  cited-references = {Albrecht S, 2009, THESIS.
An SY, 2012, ADV ROBOTICS, V26, P437, DOI 10.1163/156855311X617452.
Bailey T, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P3562, DOI 10.1109/IROS.2006.281644.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
BREU H, 1995, IEEE T PATTERN ANAL, V17, P529, DOI 10.1109/34.391389.
Burgard W, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2089, DOI 10.1109/IROS.2009.5354691.
Callmer J, 2008, P 2008 AUSTR C ROB A, P8.
Choi J, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P4048, DOI 10.1109/IROS.2006.281866.
Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049.
Delaunay Boris, 1934, B LACADEMIE SCI LURS, V6, P793.
Dijkstra, 1959, NUMER MATH, V1, P269, DOI 10.1007/BF01386390.
Diosi A., 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P3317.
Dissanayake MWMG, 2001, IEEE T ROBOTIC AUTOM, V17, P229, DOI 10.1109/70.938381.
Doucet A., 2000, P 16 C UNC ART INT S, P176.
Estrada C, 2005, IEEE T ROBOT, V21, P588, DOI 10.1109/TRO.2005.844673.
Gaspar J, 2000, IEEE T ROBOTIC AUTOM, V16, P890, DOI 10.1109/70.897802.
Grisetti G, 2005, IEEE INT CONF ROBOT, P2432.
Guivant JE, 2001, IEEE T ROBOTIC AUTOM, V17, P242, DOI 10.1109/70.938382.
Hahnel D, 2003, IEEE INT CONF ROBOT, P1557, DOI 10.1109/ROBOT.2003.1241816.
Holz D., 2010, ISR 2010 41 INT S RO, P1.
Hwang SY, 2011, IEEE T IND ELECTRON, V58, P4804, DOI 10.1109/TIE.2011.2109333.
Jeong WY, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P2570, DOI 10.1109/IROS.2006.281708.
Jeong-Gwan Kang, 2007, 2007 International Conference on Control, Automation and Systems - ICCAS `07, P1092, DOI 10.1109/ICCAS.2007.4407062.
Julier SJ, 2007, ROBOT AUTON SYST, V55, P3, DOI 10.1016/j.robot.2006.06.011.
Karlsson N, 2005, IEEE INT CONF ROBOT, P24.
Kim A, 2013, IEEE T ROBOT, V29, P719, DOI 10.1109/TRO.2012.2235699.
Kim J, 2007, ROBOT AUTON SYST, V55, P62, DOI 10.1016/j.robot.2006.06.006.
Krose BJA, 2001, IMAGE VISION COMPUT, V19, P381, DOI 10.1016/S0262-8856(00)00086-X.
Kundu A, 2011, IEEE I CONF COMP VIS, P2080, DOI 10.1109/ICCV.2011.6126482.
Launay F, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS I-IV, PROCEEDINGS, P3918, DOI 10.1109/ROBOT.2002.1014338.
Lee JS, 2010, AUTON ROBOT, V29, P1, DOI 10.1007/s10514-010-9184-1.
Leung C, 2008, IEEE INT CONF ROBOT, P1898, DOI 10.1109/ROBOT.2008.4543484.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Martinez-Cantin R, 2009, AUTON ROBOT, V27, P93, DOI 10.1007/s10514-009-9130-2.
Montemerlo M., 2002, AAAI IAAI, DOI DOI 10.1007/S00244-005-7058-X.
Nieto J, 2007, ROBOT AUTON SYST, V55, P39, DOI 10.1016/j.robot.2006.06.008.
Nieto J, 2006, SPRINGER TRAC ADV RO, V25, P167.
Paz LM, 2008, IEEE T ROBOT, V24, P1107, DOI 10.1109/TRO.2008.2004639.
Roda Jose Pascual, 2007, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, P3898, DOI 10.1109/IROS.2007.4399134.
Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275.
Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423.
Seo-Yeon Hwang, 2008, 2008 International Conference on Control, Automation and Systems (ICCAS), P704, DOI 10.1109/ICCAS.2008.4694592.
Shafait F, 2008, PROC SPIE, V6815, DOI 10.1117/12.767755.
Sim R, 2005, IEEE INT CONF ROBOT, P661.
Su-Yong An, 2013, 2013 IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), P1, DOI 10.1109/ROMAN.2013.6628522.
Sunderhauf N, 2012, IEEE INT CONF ROBOT, P1254, DOI 10.1109/ICRA.2012.6224709.
Tardos JD, 2002, INT J ROBOT RES, V21, P311, DOI 10.1177/027836402320556340.
Thrun S, 2000, INT J ROBOT RES, V19, P972, DOI 10.1177/02783640022067922.
Thrun S, 1999, ICRA `99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1999, DOI 10.1109/ROBOT.1999.770401.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Thrun S, 2006, INT J ROBOT RES, V25, P403, DOI 10.1177/0278364906065387.
Ulrich I., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1023, DOI 10.1109/ROBOT.2000.844734.
Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb.
Wang CC, 2007, INT J ROBOT RES, V26, P889, DOI 10.1177/0278364907081229.
Wilson S. W., 1996, From Animals to Animats 4. Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, P325.
Wolf DF, 2005, AUTON ROBOT, V19, P53, DOI 10.1007/s10514-005-0606-4.
WooYeon Jeong, 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P3195.
Xu D, 2009, IEEE T IND ELECTRON, V56, P1617, DOI 10.1109/TIE.2009.2012457.},
  da = {2022-05-17},
  doc-delivery-number = {DB8XZ},
  eissn = {1573-7527},
  issn = {0929-5593},
  journal-iso = {Auton. Robot.},
  keywords = {Ceiling vision; Mobile robot; SLAM; Dynamic environment; Wide-open area},
  keywords-plus = {SIMULTANEOUS LOCALIZATION; ALGORITHM},
  language = {English},
  number-of-cited-references = {58},
  research-areas = {Computer Science; Robotics},
  times-cited = {3},
  type = {Article},
  unique-id = {WOS:000368801800006},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {31},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{krajnik-et-al:2017:2665664,
  author = {T. Krajnik and J. P. Fentanes and J. M. Santos and D. Tom},
  journal = {IEEE TRANSACTIONS ON ROBOTICS},
  title = {FreMEn: Frequency Map Enhancement for Long-Term Mobile Robot Autonomy in
Changing Environments},
  volume = {33},
  number = {4},
  pages = {964--977},
  doi = {10.1109/TRO.2017.2665664},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2017},
  month = {8},
  abstract = {We present a new approach to long-term mobile robot mapping in dynamic
indoor environments. Unlike traditional world models that are tailored
to represent static scenes, our approach explicitly models environmental
dynamics. We assume that some of the hidden processes that influence the
dynamic environment states are periodic and model the uncertainty of the
estimated state variables by their frequency spectra. The spectral model
can represent arbitrary timescales of environment dynamics with low
memory requirements. Transformation of the spectral model to the time
domain allows for the prediction of the future environment states, which
improves the robot's long-term performance in changing environments.
Experiments performed over time periods of months to years demonstrate
that the approach can efficiently represent large numbers of
observations and reliably predict future environment states. The
experiments indicate that the model's predictive capabilities improve
mobile robot localization and navigation in changing environments.},
  affiliation = {Krajnik, T (Corresponding Author), Univ Lincoln, Lincoln Ctr Autonomous Syst, Lincoln LN6 7TS, England.
Krajnik, Tomas; Fentanes, Jaime P.; Santos, Joao M.; Duckett, Tom, Univ Lincoln, Lincoln Ctr Autonomous Syst, Lincoln LN6 7TS, England.
Krajnik, Tomas, Czech Tech Univ, Fac Elect Engn, Prague 16636, Czech Republic.},
  affiliations = {University of Lincoln; Czech Technical University Prague},
  author-email = {tkrajnik@lincoln.ac.uk
jpulidofentanes@lincoln.ac.uk
jsantos@lincoln.ac.uk
tduckett@lincoln.ac.uk},
  cited-references = {Ambrus R, 2014, IEEE INT C INT ROBOT, P1854, DOI 10.1109/IROS.2014.6942806.
{[}Anonymous], 2017, STROMOVKA DATASET.
{[}Anonymous], 2016, THE FFTW C LIB.
Arbuckle D, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P409, DOI 10.1109/IRDS.2002.1041424.
Austin D, 2001, IROS 2001: PROCEEDINGS OF THE 2001 IEEE/RJS INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P613, DOI 10.1109/IROS.2001.976237.
Bracewell R.N., 1986, FOURIER TRANSFORM IT.
Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754.
Calonder Michael, 2010, Computer Vision - ECCV 2010. Proceedings 11th European Conference on Computer Vision, P778, DOI 10.1007/978-3-642-15561-1\_56.
Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638.
Carlevaris-Bianco N, 2014, IEEE INT C INT ROBOT, P2769, DOI 10.1109/IROS.2014.6942941.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Dayoub F, 2011, ROBOT AUTON SYST, V59, P285, DOI 10.1016/j.robot.2011.02.013.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Fentanes JP, 2015, IEEE INT CONF ROBOT, P1112, DOI 10.1109/ICRA.2015.7139315.
Hahnel D, 2003, ADV ROBOTICS, V17, P579, DOI 10.1163/156855303769156965.
Hawes N., 2017, IEEE ROBOT IN PRESS.
Hochdorfer S., 2009, P INT C ADV ROB, P1.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Krajnik T., 2015, P AAAI C ART INT VID.
Krajnik T., 2014, ADV AUTONOMOUS ROBOT, P281.
Krajnik T., 2016, P ICRA 2016 WORKSH A.
Krajnik T., 2013, 2013 16 INT C ADV RO, P1, DOI {[}10.1109/icar.2013.6766520, DOI 10.1109/ICAR.2013.6766520].
Krajnik T., 2015, P 2015 EUR C MOB ROB, P1.
Krajnik T, 2017, ROBOT AUTON SYST, V88, P127, DOI 10.1016/j.robot.2016.11.011.
Krajnik T, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4558, DOI 10.1109/IROS.2016.7759671.
Krajnik T, 2015, IEEE INT CONF ROBOT, P2140, DOI 10.1109/ICRA.2015.7139481.
Krajnik T, 2014, IEEE INT C INT ROBOT, P4537, DOI 10.1109/IROS.2014.6943205.
Krajnik T, 2014, IEEE INT CONF ROBOT, P3706, DOI 10.1109/ICRA.2014.6907396.
Krajnik T, 2010, J FIELD ROBOT, V27, P511, DOI 10.1002/rob.20354.
Kucner T., 2013, P IEEE RSJ INT C INT, P1.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Migliore D., 2009, P ICRA WORKSH SAF NA.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
Mishkin D., 2015, P CVPR WORKSH VIS PL.
Mitsou N.C., 2007, CONTR AUT 2007 MED 0, P1.
Muhlfellner P, 2016, J FIELD ROBOT, V33, P561, DOI 10.1002/rob.21595.
Neubert P, 2015, ROBOT AUTON SYST, V69, P15, DOI 10.1016/j.robot.2014.08.005.
Rosen DM, 2016, IEEE INT CONF ROBOT, P1063, DOI 10.1109/ICRA.2016.7487237.
Santos JM, 2016, IEEE ROBOT AUTOM LET, V1, P684, DOI 10.1109/LRA.2016.2516594.
Santos M. J., 2016, ROBOT AUTON SYST.
Stachniss C., 2005, P C ART INT, P1324.
Thrun S., 2005, PROBABILISTIC ROBOTI.
Thrun S. a. o., 2002, EXPLORING ARTIFICIAL, V1, P1.
Tipaldi GD, 2013, INT J ROBOT RES, V32, P1662, DOI 10.1177/0278364913502830.
Wang CC, 2007, INT J ROBOT RES, V26, P889, DOI 10.1177/0278364907081229.
Wolf DF, 2005, AUTON ROBOT, V19, P53, DOI 10.1007/s10514-005-0606-4.
Yguel M, 2006, SPRINGER TRAC ADV RO, V25, P219.},
  da = {2022-05-17},
  doc-delivery-number = {FD2WJ},
  eissn = {1941-0468},
  funding-acknowledgement = {EU ICT {[}600623]; Czech Science Foundation {[}17-27006Y]},
  funding-text = {This work was supported by the EU ICT Project 600623 ``STRANDS{''} and
the Czech Science Foundation under Project 17-27006Y.},
  issn = {1552-3098},
  journal-iso = {IEEE Trans. Robot.},
  keywords = {Localization; long-term autonomy; mapping},
  keywords-plus = {SIMULTANEOUS LOCALIZATION; NAVIGATION},
  language = {English},
  number-of-cited-references = {47},
  oa = {Green Submitted, Green Accepted},
  orcid-numbers = {Krajník, Tomáš/0000-0002-4408-7916},
  research-areas = {Robotics},
  researcherid-numbers = {Krajník, Tomáš/O-2339-2013},
  times-cited = {48},
  type = {Article},
  unique-id = {WOS:000407395200015},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {19},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{naseer-et-al:2015:7324181,
  author = {T. Naseer and B. Suger and M. Ruhnke and B. Wolfram},
  booktitle = {2015 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR)},
  title = {Vision-Based Markov Localization Across Large Perceptual Changes},
  pages = {1--6},
  doi = {10.1109/ECMR.2015.7324181},
  note = {European Conference on Mobile Robots, Lincoln, ENGLAND, SEP 02-04, 2015},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2015},
  abstract = {Recently, there has been significant progress towards lifelong,
autonomous operation of mobile robots, especially in the field of
localization and mapping. One important challenge in this context is
visual localization under substantial perceptual changes, for example,
coming from different seasons. In this paper, we present an approach to
localize a mobile robot with a low frequency camera with respect to an
image sequence, recorded previously within a different season. Our
approach uses a discrete Bayes filter and a sensor model based on whole
image descriptors. Thereby it exploits sequential information to model
the dynamics of the system. Since we compute a probability distribution
over the whole state space, our approach can handle more complex
trajectories that may include same season loop-closures as well as
fragmented sub-sequences. Throughout an extensive experimental
evaluation on challenging datasets, we demonstrate that our approach
outperforms state-of-the-art techniques.},
  affiliation = {Naseer, T (Corresponding Author), Univ Freiburg, Autonomous Intelligent Syst Grp, Freiburg, Germany.
Naseer, Tayyab; Suger, Benjamin; Ruhnke, Michael; Burgard, Wolfram, Univ Freiburg, Autonomous Intelligent Syst Grp, Freiburg, Germany.},
  affiliations = {University of Freiburg},
  author-email = {naseer@informatik.uni-freiburg.de
suger@informatik.uni-freiburg.de
ruhnke@informatik.uni-freiburg.de
burgard@informatik.uni-freiburg.de},
  book-group-author = {IEEE},
  cited-references = {Badino H, 2012, IEEE INT CONF ROBOT, P1635, DOI 10.1109/ICRA.2012.6224716.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Bennewitz M., 2006, EUR ROB S, P143.
Cadena C, 2012, IEEE T ROBOT, V28, P871, DOI 10.1109/TRO.2012.2189497.
Churchill W, 2012, P IEEE INT C ROB AUT.
Churchill W., 2009, P ROB SCI SYST SEATT.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
DALAL N, 2005, P IEEE INT C COMP VI.
Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049.
Fox D, 2003, IEEE PERVAS COMPUT, V2, P24, DOI 10.1109/MPRV.2003.1228524.
Glover AJ, 2010, IEEE INT CONF ROBOT, P3507, DOI 10.1109/ROBOT.2010.5509547.
Hansen P, 2014, IEEE INT C INT ROBOT, P4549, DOI 10.1109/IROS.2014.6943207.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Milford M, 2012, P IEEE INT C ROB AUT.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
Naseer T., 2014, AAAI C ART INT AAAI.
Neubert Peer, 2014, ROBOTICS AUTONOMOUS.
Pepperell E., 2013, P AUSTR C ROB P AUST.
Valgren C, 2010, ROBOT AUTON SYST, V58, P149, DOI 10.1016/j.robot.2009.09.010.
Vysotska O., 2015, P IEEE INT C ROB AUT.},
  da = {2022-05-17},
  doc-delivery-number = {BF1AR},
  isbn = {978-1-4673-9163-4},
  language = {English},
  number-of-cited-references = {20},
  orcid-numbers = {Burgard, Wolfram/0000-0002-5680-6500},
  research-areas = {Robotics},
  researcherid-numbers = {Burgard, Wolfram/N-2381-2019},
  times-cited = {1},
  type = {Proceedings Paper},
  unique-id = {WOS:000380213600015},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{qin-et-al:2020:9340939,
  author = {T. Qin and T. Chen and Y. Chen and Q. Su},
  booktitle = {2020 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS)},
  title = {AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous
Vehicles in the Parking Lot},
  pages = {5939--5945},
  doi = {10.1109/IROS45743.2020.9340939},
  note = {IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), ELECTR NETWORK, OCT 24-JAN 24, 2020-2021},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2020},
  abstract = {Autonomous valet parking is a specific application for autonomous
vehicles. In this task, vehicles need to navigate in narrow, crowded and
GPS-denied parking lots. Accurate localization ability is of great
importance. Traditional visual-based methods suffer from tracking lost
due to texture-less regions, repeated structures, and appearance
changes. In this paper, we exploit robust semantic features to build the
map and localize vehicles in parking lots. Semantic features contain
guide signs, parking lines, speed bumps, etc, which typically appear in
parking lots. Compared with traditional features, these semantic
features are long-term stable and robust to the perspective and
illumination change. We adopt four surround-view cameras to increase the
perception range. Assisting by an IMU (Inertial Measurement Unit) and
wheel encoders, the proposed system generates a global visual semantic
map. This map is further used to localize vehicles at the centimeter
level. We analyze the accuracy and recall of our system and compare it
against other methods in real experiments. Furthermore, we demonstrate
the practicability of the proposed system by the autonomous parking
application.},
  affiliation = {Qin, T (Corresponding Author), Huawei Technol, IAS BU, Shanghai, Peoples R China.
Qin, Tong; Chen, Tongqing; Chen, Yilun; Su, Qing, Huawei Technol, IAS BU, Shanghai, Peoples R China.},
  affiliations = {Huawei Technologies},
  author-email = {qintong@huawei.com
chentongqing@huawei.com
chengyilun@huawei.com
suqing@huawei.com},
  book-group-author = {IEEE},
  cited-references = {Badrinarayanan V., 2015, ARXIV150507293.
Burki M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4137, DOI 10.1109/IROS.2016.7759609.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
Forster C, 2017, IEEE T ROBOT, V33, P1, DOI 10.1109/TRO.2016.2597321.
Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584.
Jeong J, 2017, IEEE INT VEH SYM, P1736, DOI 10.1109/IVS.2017.7995958.
Kitt B, 2010, IEEE INT VEH SYM, P486, DOI 10.1109/IVS.2010.5548123.
Klein George, 2007, P1.
Le Gentil C, 2019, IEEE INT CONF ROBOT, P6388, DOI 10.1109/ICRA.2019.8794429.
Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813.
Li MY, 2013, INT J ROBOT RES, V32, P690, DOI 10.1177/0278364913481251.
Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965.
Lu Y, 2017, IEEE INT VEH SYM, P468, DOI 10.1109/IVS.2017.7995762.
Lynen S., GET OUT MY LAB LARGE.
Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Qin T, 2018, IEEE INT CONF ROBOT, P1197.
Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729.
Ranganathan A, 2013, IEEE INT C INT ROBOT, P921, DOI 10.1109/IROS.2013.6696460.
Rehder E, 2015, IEEE INT VEH SYM, P1393, DOI 10.1109/IVS.2015.7225910.
Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4\_28.
Schneider Thomas, 2018, IEEE Robotics and Automation Letters, V3, P1418, DOI 10.1109/LRA.2018.2800113.
Schreiber M, 2013, IEEE INT VEH SYM, P449, DOI 10.1109/IVS.2013.6629509.
Yin H, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING AND ROBOTICS (IEEE-RCAR 2020), P1, DOI 10.1109/RCAR49640.2020.9303291.
Zhang J., 2014, ROBOT SCI SYST, V2, P9.},
  da = {2022-05-17},
  doc-delivery-number = {BS3PB},
  isbn = {978-1-7281-6212-6},
  issn = {2153-0858},
  language = {English},
  number-of-cited-references = {25},
  oa = {Green Submitted},
  research-areas = {Automation \& Control Systems; Computer Science; Engineering; Robotics},
  series = {IEEE International Conference on Intelligent Robots and Systems},
  times-cited = {6},
  type = {Proceedings Paper},
  unique-id = {WOS:000714033803090},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {4},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Engineering, Electrical \& Electronic; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{taisho-kanji:2016:7866383,
  author = {T. Taisho and T. Kanji},
  booktitle = {2016 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO)},
  title = {Mining DCNN Landmarks for Long-term Visual SLAM},
  pages = {570--576},
  doi = {10.1109/ROBIO.2016.7866383},
  note = {IEEE International Conference on Robotics and Biomimetics (IEEE ROBIO),
Qingdao, PEOPLES R CHINA, DEC 03-07, 2016},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2016},
  abstract = {Long-term visual SLAM, in familiar, semidynamic, and partially changing
environments is an important area of research in robotics. The main
problem we faced is the question of how to describe a scene
discriminatively and compactly-both of which are necessary in order to
cope with changes in appearance and a large amount of visual
information. In this study, we address the above issues by mining visual
experience. Our strategy is to mine a library of raw visual images,
termed visual experience, to find the relevant visual patterns to
effectively explain the input scene. From a practical point of view, our
work offers three main contributions over the previous work. First, it
is the first application of discriminative visual features from deep
convolutional neural networks (DCNN) to the task of visual landmark
mining. Second, we show how to interpret a high-dimensional DCNN feature
to a compact semantic representation of visual word. Third, we show that
our approach can turn the scene description task with any feature
(including the DCNN feature) into the task of mining visual experience.
Experiments on a challenging cross-domain visual place recognition
validate efficacy of the proposed approach.},
  affiliation = {Kanji, T (Corresponding Author), Univ Fukui, Grad Sch Engn, Fukui, Japan.
Taisho, Tsukamoto; Kanji, Tanaka, Univ Fukui, Grad Sch Engn, Fukui, Japan.},
  affiliations = {University of Fukui},
  author-email = {tnkknj@u-fukui.ac.jp},
  book-group-author = {IEEE},
  cited-references = {Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28.
Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018.
Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1\_38.
Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598.
Bruce J., 2015, ICRA15 WS VPRICE.
Carlevaris-Bianco N., 2015, I J ROBOTICS RES.
Carrillo H, 2014, IEEE INT C INT ROBOT, P4950, DOI 10.1109/IROS.2014.6943266.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Isola P, 2013, IEEE I CONF COMP VIS, P3048, DOI 10.1109/ICCV.2013.457.
Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235.
Kanji T, 2015, IEEE INT C INT ROBOT, P729, DOI 10.1109/IROS.2015.7353453.
Kanji T, 2015, IEEE INT CONF ROBOT, P6359, DOI 10.1109/ICRA.2015.7140092.
Kanji T, 2014, IEEE INT C INT ROBOT, P136, DOI 10.1109/IROS.2014.6942552.
Kentaro Y., 2015, ICRA15 WS VPRICE.
Krajnik T., 2015, ICRA15 WS VPRICE.
Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Lowry S., 2015, ICRA15 WS VPRICE.
Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478.
Masatoshi A., 2015, ICRA.
Morita H., 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P2965.
Naseer T, 2015, IEEE INT C INT ROBOT, P2529, DOI 10.1109/IROS.2015.7353721.
Rematas K, 2013, ACCV, P176.
Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663.
Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986.
Sunderhauf N., 2015, ICRA 2015 WORKSH VIS.
Tanaka K., 2016, IEEE RSJ INT C IROS.
Tommasi T, 2013, IEEE I CONF COMP VIS, P897, DOI 10.1109/ICCV.2013.116.
Tuytelaars T, 2011, IEEE I CONF COMP VIS, P1824, DOI 10.1109/ICCV.2011.6126449.
Vysotska O., 2015, ICRA15 WS VPRICE.},
  da = {2022-05-17},
  doc-delivery-number = {BI1FW},
  funding-acknowledgement = {JSPS KAKENHI {[}23700229, 26330297]},
  funding-text = {Our work has been supported in part by JSPS KAKENHI Grant-in-Aid for
Young Scientists (B) 23700229, and for Scientific Research (C) 26330297
({''}The realization of next-generation, discriminative and succinct
SLAM technique: PartSLAM{''}).},
  isbn = {978-1-5090-4364-4},
  language = {English},
  number-of-cited-references = {30},
  research-areas = {Robotics},
  times-cited = {2},
  type = {Proceedings Paper},
  unique-id = {WOS:000405724600096},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {3},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{zeng-si:2021:6,
  author = {T. Zeng and B. Si},
  journal = {COGNITIVE NEURODYNAMICS},
  title = {A brain-inspired compact cognitive mapping system},
  volume = {15},
  number = {1,SI},
  pages = {91--101},
  doi = {10.1007/s11571-020-09621-6},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2021},
  month = {2},
  abstract = {In many simultaneous localization and mapping (SLAM) systems, the map of
the environment grows over time as the robot explores the environment.
The ever-growing map prevents long-term mapping, especially in
large-scale environments. In this paper, we develop a compact cognitive
mapping approach inspired by neurobiological experiments. Mimicking the
firing activities of neighborhood cells, neighborhood fields determined
by movement information, i.e. translation and rotation, are modeled to
describe one of the distinct segments of the explored environment. The
vertices with low neighborhood field activities are avoided to be added
into the cognitive map. The optimization of the cognitive map is
formulated as a robust non-linear least squares problem constrained by
the transitions between vertices, and is numerically solved efficiently.
According to the cognitive decision-making of place familiarity, loop
closure edges are clustered depending on time intervals, and then batch
global optimization of the cognitive map is performed to satisfy the
combined constraint of the whole cluster. After the loop closure
process, scene integration is performed, in which revisited vertices are
removed subsequently to further reduce the size of the cognitive map.
The compact cognitive mapping approach is tested on a monocular visual
SLAM system in a naturalistic maze for a biomimetic animated robot. Our
results demonstrate that the proposed method largely restricts the
growth of the size of the cognitive map over time, and meanwhile, the
compact cognitive map correctly represents the overall layout of the
environment. The compact cognitive mapping method is well suitable for
the representation of large-scale environments to achieve long-term
robot navigation.},
  affiliation = {Si, BL (Corresponding Author), Beijing Normal Univ, Sch Syst Sci, Beijing 100875, Peoples R China.
Zeng, Taiping, Fudan Univ, Inst Sci \& Technol Brain Inspired Intelligence, Shanghai, Peoples R China.
Zeng, Taiping, Fudan Univ, Minist Educ, Key Lab Computat Neurosci \& Brain Inspired Intell, Shanghai, Peoples R China.
Si, Bailu, Beijing Normal Univ, Sch Syst Sci, Beijing 100875, Peoples R China.},
  affiliations = {Fudan University; Fudan University; Beijing Normal University},
  author-email = {zengtaiping@fudan.edu.cn
bailusi@bnu.edu.cn},
  cited-references = {Ball DR, 2010, IEEE INT SOI CONF.
Ball D, 2013, AUTON ROBOT, V34, P149, DOI 10.1007/s10514-012-9317-9.
Bos JJ, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15602.
Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754.
Carlevaris-Bianco N, 2013, IEEE INT CONF ROBOT, P5748, DOI 10.1109/ICRA.2013.6631403.
Carr MF, 2011, NAT NEUROSCI, V14, P147, DOI 10.1038/nn.2732.
Eichenbaum H, 2014, NAT REV NEUROSCI, V15, P732, DOI 10.1038/nrn3827.
Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721.
Ila V, 2010, IEEE T ROBOT, V26, P78, DOI 10.1109/TRO.2009.2034435.
Johannsson H, 2013, IEEE INT CONF ROBOT, P54, DOI 10.1109/ICRA.2013.6630556.
Kretzschmar H, 2012, INT J ROBOT RES, V31, P1219, DOI 10.1177/0278364912455072.
Kropff E, 2015, NATURE, V523, P419, DOI 10.1038/nature14622.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Larkin MC, 2014, HIPPOCAMPUS, V24, P773, DOI 10.1002/hipo.22268.
Lever C, 2009, J NEUROSCI, V29, P9771, DOI 10.1523/JNEUROSCI.1319-09.2009.
Lu F, 1997, AUTON ROBOT, V4, P333, DOI 10.1023/A:1008854305733.
MacDonald CJ, 2011, NEURON, V71, P737, DOI 10.1016/j.neuron.2011.07.012.
Mazuran M, 2016, INT J ROBOT RES, V35, P50, DOI 10.1177/0278364915581629.
McNaughton BL, 2006, NAT REV NEUROSCI, V7, P663, DOI 10.1038/nrn1932.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
MITTELSTAEDT ML, 1980, NATURWISSENSCHAFTEN, V67, P566, DOI 10.1007/BF00450672.
Moser EI, 2008, ANNU REV NEUROSCI, V31, P69, DOI 10.1146/annurev.neuro.31.061307.090723.
Moser MB, 2015, CSH PERSPECT BIOL, V7, DOI 10.1101/cshperspect.a021808.
Naidoo R, 2016, ORYX, V50, P138, DOI 10.1017/S0030605314000222.
OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1.
TAUBE JS, 1990, J NEUROSCI, V10, P420.
TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626.
Zeng TP, 2020, NEURAL NETWORKS, V126, P21, DOI 10.1016/j.neunet.2020.02.023.
Zeng TP, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00061.
Zhao D, 2019, P 2019 INT JOINT C N.},
  da = {2022-05-17},
  doc-delivery-number = {QU6QJ},
  earlyaccessdate = {JUL 2020},
  eissn = {1871-4099},
  funding-acknowledgement = {National Key Research and Development Program of China
{[}2016YFC0801808]},
  funding-text = {The authors would like to thank the support from the National Key
Research and Development Program of China (No. 2016YFC0801808).},
  issn = {1871-4080},
  journal-iso = {Cogn. Neurodynamics},
  keywords = {SLAM; Compact cognitive map; Long-term mapping; Neighborhood cells;
Neighborhood fields},
  keywords-plus = {PATH-INTEGRATION; SPATIAL MAP; TIME CELLS; INFORMATION; HIPPOCAMPUS;
MEMORY; RATS},
  language = {English},
  number-of-cited-references = {30},
  oa = {Green Submitted, Green Published},
  orcid-numbers = {Si, Bailu/0000-0002-0260-3433},
  research-areas = {Neurosciences \& Neurology},
  researcherid-numbers = {Si, Bailu/O-9575-2014},
  times-cited = {2},
  type = {Article},
  unique-id = {WOS:000554055600001},
  usage-count-last-180-days = {8},
  usage-count-since-2013 = {29},
  web-of-science-categories = {Neurosciences},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{nguyen-et-al:2022:3094157,
  author = {T.-M. Nguyen and M. Cao and S. Yuan and T. H. Y. A. N. Lyu and L. Xie},
  journal = {IEEE TRANSACTIONS ON ROBOTICS},
  title = {VIRAL-Fusion: A Visual-Inertial-Ranging-Lidar Sensor Fusion Approach},
  volume = {38},
  number = {2},
  pages = {958--977},
  doi = {10.1109/TRO.2021.3094157},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2022},
  month = {4},
  abstract = {In recent years, onboard self-localization (OSL) methods based on
cameras or lidar have achieved many significant progresses. However,
some issues such as estimation drift and robustness in low-texture
environment still remain inherent challenges for OSL methods. On the
other hand, infrastructure-based methods can generally overcome these
issues, but at the expense of some installation cost. This poses an
interesting problem of how to effectively combine these methods, so as
to achieve localization with long-term consistency as well as
flexibility compared to any single method. To this end, we propose a
comprehensive optimization-based estimator for the 15-D state of an
unmanned aerial vehicle (UAV), fusing data from an extensive set of
sensors: inertial measurement unit (IMU), ultrawideband (UWB) ranging
sensors, and multiple onboard visual-inertial and lidar odometry
subsystems. In essence, a sliding window is used to formulate a sequence
of robot poses, where relative rotational and translational constraints
between these poses are observed in the IMU preintegration and OSL
observations, while orientation and position are coupled in the
body-offset UWB range observations. An optimization-based approach is
developed to estimate the trajectory of the robot in this sliding
window. We evaluate the performance of the proposed scheme in multiple
scenarios, including experiments on public datasets, high-fidelity
graphical-physical simulation, and field-collected data from UAV flight
tests. The result demonstrates that our integrated localization method
can effectively resolve the drift issue, while incurring minimal
installation requirements.},
  affiliation = {Nguyen, TM (Corresponding Author), Nanyang Technol Univ, Sch Elect \& Elect Engn, Singapore 639798, Singapore.
Nguyen, Thien-Minh; Cao, Muqing; Yuan, Shenghai; Lyu, Yang; Nguyen, Thien Hoang; Xie, Lihua, Nanyang Technol Univ, Sch Elect \& Elect Engn, Singapore 639798, Singapore.},
  affiliations = {Nanyang Technological University \& National Institute of Education
(NIE) Singapore; Nanyang Technological University},
  author-email = {thienminh.npn@gmail.com
mqcao@ntu.edu.sg
shyuan@ntu.edu.sg
lyu.yang@ntu.edu.sg
e180071@e.ntu.edu.sg
elhxie@ntu.edu.sg},
  cited-references = {Agarwal S., 2018, CERES SOLVER TUTORIA.
Bloesch M, 2015, IEEE INT C INT ROBOT, P298, DOI 10.1109/IROS.2015.7353389.
Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033.
Campos Carlos, 2021, IEEE Transactions on Robotics, V37, P1874, DOI 10.1109/TRO.2021.3075644.
Cao MQ, 2020, IEEE INT CONF CON AU, P1149, DOI 10.1109/ICCA51439.2020.9264577.
Cao YJ, 2020, VIR SLAM VISUAL INER.
Chirikjian GS, 2012, APPL NUMER HARMON AN, P1, DOI 10.1007/978-0-8176-4944-9.
Dellaert F, 2017, FDN TRENDS ROBOT, V6, P1.
Djugash J, 2012, INT J ROBOT RES, V31, P604, DOI 10.1177/0278364912441039.
Eckenhoff K, 2019, INT J ROBOT RES, V38, P563, DOI 10.1177/0278364919835021.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
Fang X, 2021, IEEE T SYST MAN CY-S, V51, P6830, DOI 10.1109/TSMC.2020.2964713.
Fang X, 2018, I C CONT AUTOMAT ROB, P1973, DOI 10.1109/ICARCV.2018.8581124.
Forster C, 2017, IEEE T ROBOT, V33, P1, DOI 10.1109/TRO.2016.2597321.
Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584.
Guo KX, 2016, UNMANNED SYST, V4, P23, DOI 10.1142/S2301385016400033.
Huang WB, 2020, IEEE T ROBOT, V36, P1153, DOI 10.1109/TRO.2019.2959161.
Karrer M., 2020, DISTRIBUTED VARIABLE.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Li JX, 2018, IEEE INT CONF CON AU, P100, DOI 10.1109/ICCA.2018.8444329.
Lynen S, 2013, IEEE INT C INT ROBOT, P3923, DOI 10.1109/IROS.2013.6696917.
Mautz R., 2012, THESIS, DOI {[}10.3929/ethz-a-007313554, DOI 10.3929/ETHZ-A-007313554].
Mueller MW, 2015, IEEE INT CONF ROBOT, P1730, DOI 10.1109/ICRA.2015.7139421.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Nguyen T.-M., INT J ROBOT RES, V2021.
Nguyen T.-M., 2016, 2016 INT MICR VEH C, P56.
Nguyen TM, 2020, IEEE T ROBOT, V36, P553, DOI 10.1109/TRO.2019.2954677.
Nguyen TM, 2020, IEEE T CONTR SYST T, V28, P2021, DOI 10.1109/TCST.2019.2916089.
Oleynikova H, 2020, J FIELD ROBOT, V37, P642, DOI 10.1002/rob.21950.
Paredes JA, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010089.
Qin T, 2018, IEEE INT C INT ROBOT, P3662, DOI 10.1109/IROS.2018.8593603.
Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729.
Queralta J. P., 2020, VIO UWBBASED COLLABO.
Shah S., 2017, FIELD SERVICE ROBOTI, P621, DOI DOI 10.1007/978-3-319-67361-5.
Shan TX, 2020, IEEE INT C INT ROBOT, P5135, DOI 10.1109/IROS45743.2020.9341176.
Sola J., 2018, ARXIV PREPRINT ARXIV.
Song Y, 2019, IEEE INT CONF ROBOT, P6568, DOI 10.1109/ICRA.2019.8794222.
Nguyen TH, 2020, IEEE INT CONF ROBOT, P665, DOI 10.1109/ICRA40945.2020.9196794.
Nguyen TH, 2020, AUTON ROBOT, V44, P1519, DOI 10.1007/s10514-020-09944-7.
Nguyen TH, 2020, UNMANNED SYST, V8, P179, DOI 10.1142/S2301385020500119.
Nguyen TM, 2019, IEEE ROBOT AUTOM LET, V4, P3641, DOI 10.1109/LRA.2019.2926671.
Tiemann J, 2017, INT C INDOOR POSIT, DOI 10.1109/ipin.2017.8115937.
Wang C, 2017, IEEE INT C INT ROBOT, P1602, DOI 10.1109/IROS.2017.8205968.
Weinstein Aaron, 2018, IEEE Robotics and Automation Letters, V3, P1801, DOI 10.1109/LRA.2018.2800119.
Xu H, 2020, IEEE INT CONF ROBOT, P8776, DOI 10.1109/ICRA40945.2020.9196944.
Ye HY, 2019, IEEE INT CONF ROBOT, P3144, DOI 10.1109/ICRA.2019.8793511.
Yuan SH, 2021, UNMANNED SYST, V9, P129, DOI 10.1142/S230138502150014X.
Zhang J, 2018, J FIELD ROBOT, V35, P1242, DOI 10.1002/rob.21809.
Zhang ZC, 2018, IEEE INT C INT ROBOT, P7244, DOI 10.1109/IROS.2018.8593941.},
  da = {2022-05-17},
  doc-delivery-number = {0H8BQ},
  earlyaccessdate = {JUL 2021},
  eissn = {1941-0468},
  funding-acknowledgement = {Autonomous Systems and Software Program (WASP) - Knut and Alice
Wallenberg Foundation, under the Wallenberg-NTU Presidential
Postdoctoral Fellowship Program},
  funding-text = {This work was supported by theWallenberg AI, Autonomous Systems and
Software Program (WASP), funded by the Knut and Alice Wallenberg
Foundation, under the Wallenberg-NTU Presidential Postdoctoral
Fellowship Program. This paper was recommended for publication by
Associate Editor R. Tron and Editor F. Chaumette upon evaluation of the
reviewers' comments.},
  issn = {1552-3098},
  journal-iso = {IEEE Trans. Robot.},
  keywords = {Quaternions; Location awareness; Matrix converters; Visualization;
Simultaneous localization and mapping; Laser radar; Distance
measurement; Aerial robots; localization; optimization},
  keywords-plus = {LOCALIZATION; VERSATILE; ODOMETRY; SLAM},
  language = {English},
  number-of-cited-references = {50},
  oa = {Green Submitted},
  orcid-numbers = {Nguyen, Thien/0000-0003-1218-0910
Nguyen, Thien-Minh/0000-0003-1315-0967
Xie, Lihua/0000-0002-7137-4136},
  research-areas = {Robotics},
  times-cited = {4},
  type = {Article},
  unique-id = {WOS:000733501100001},
  usage-count-last-180-days = {10},
  usage-count-since-2013 = {10},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{nguyen-et-al:2013:004,
  author = {V. A. Nguyen and J. A. Starzyk and W.-B. Goh},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {A spatio-temporal Long-term Memory approach for visual place recognition
in mobile robotic navigation},
  volume = {61},
  number = {12},
  pages = {1744--1758},
  doi = {10.1016/j.robot.2012.12.004},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2013},
  month = {12},
  abstract = {This paper proposes a solution to the problem of mobile robotic
localization using visual indoor image sequences with a biologically
inspired spatio-temporal neural network approach. The system contains
three major subsystems: a feature extraction module, a scene
quantization module and a spatio-temporal long-term memory (LTM) module.
During learning, the scene quantization module clusters the visual
images set into scene tokens. A K-Iteration Fast Learning Artificial
Neural Network (KFLANN) is employed as the core unit of the quantization
module. The KFLANN network is driven by intrinsic statistics of the data
stream and therefore does not require the number of clusters to be
predefined. In addition, the KFLANN performance is less sensitive to
data presentation ordering compared to popular clustering methods such
as k-means, and can therefore produce a consistent number of stable
centroids. Using scene tokens, the topological structure of the
environment can be composed into sequences of tokens. These sequences
are then learnt and stored in memory units in an LTM architecture, which
is able to continuously and robustly recognize the visual input stream.
The design of memory units addresses two critical problems in
spatio-temporal learning, namely error tolerance and memory forgetting.
The primary objective of this work is to explore the synergy between the
strength of KFLANN and LTM models to address the visual topological
localization problem. We demonstrate the efficiency and efficacy of the
proposed framework on the challenging COsy Localization Dataset. (C)
2013 Elsevier B.V. All rights reserved.},
  affiliation = {Nguyen, VA (Corresponding Author), Nanyang Technol Univ, Sch Comp Engn, CeMNet, Singapore 639798, Singapore.
Vu Anh Nguyen; Goh, Wool-Boon, Nanyang Technol Univ, Sch Comp Engn, CeMNet, Singapore 639798, Singapore.
Starzyk, Janusz A., Ohio Univ, Sch Elect Engn \& Comp Sci, Russ Coll Engn \& Technol, Athens, OH 45701 USA.
Starzyk, Janusz A., Univ Informat Technol \& Management, Dept Appl Informat Syst, Rzeszow, Poland.},
  affiliations = {Nanyang Technological University \& National Institute of Education
(NIE) Singapore; Nanyang Technological University; Ohio University;
University of Information Technology \& Management Rzeszow},
  author-email = {anhngv102@gmail.com
starzykj@gmail.com
aswbgoh@ntu.edu.sg},
  cited-references = {Ackerman C, 2005, IEEE T ROBOT, V21, P247, DOI 10.1109/TRO.2004.837241.
Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514.
Arleo A, 2004, IEEE T NEURAL NETWOR, V15, P639, DOI 10.1109/TNN.2004.826221.
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555.
Burgess N, 2005, TRENDS COGN SCI, V9, P535, DOI 10.1016/j.tics.2005.09.011.
CARPENTER GA, 1987, COMPUT VISION GRAPH, V37, P54, DOI 10.1016/S0734-189X(87)80014-2.
Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199.
Cover T. M., 2006, ELEMENTS INFORM THEO, V2nd.
DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909.
Dissanayake MWMG, 2001, IEEE T ROBOTIC AUTOM, V17, P229, DOI 10.1109/70.938381.
Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022.
Eichenbaum H, 1999, NEURON, V23, P209, DOI 10.1016/S0896-6273(00)80773-4.
Fei-Fei L, 2005, PROC CVPR IEEE, P524.
Filliat D., 2003, COGN SYST RES, V4, P243, DOI DOI 10.1016/S1389-0417(03)00008-1.
FILLIAT D, 2003, J COGN SYST RES, V4, P283.
Gnadt W, 2008, NEURAL NETWORKS, V21, P699, DOI 10.1016/j.neunet.2007.09.016.
Goedeme T., 2008, MOBILE ROBOTS MOTION, P63.
Grossberg S, 2009, J VISION, V9, DOI 10.1167/9.4.6.
Harris C. G., 1988, ALVEY VISION C, P1, DOI DOI 10.5244/C.2.23.
Hebb D., 1949, ORG BEHAV.
Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650.
Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007.
Kremer SC, 2001, NEURAL COMPUT, V13, P249, DOI 10.1162/089976601300014538.
Kuipers B., 1991, Robotics and Autonomous Systems, V8, P47, DOI 10.1016/0921-8890(91)90014-C.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112.
McGaugh JL, 2000, SCIENCE, V287, P248, DOI 10.1126/science.287.5451.248.
Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188.
Nguyen V.A., 2009, INT C COGN NEUR SYST.
Nguyen V. H., 2010, IEEE 25 INT S DEF FA, P1, DOI DOI 10.1109/RIVF.2010.5632316.
OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1.
Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724.
OReilly R.C., 2000, COMPUTATIONAL EXPLOR.
Ping WL, 2003, IEEE IJCNN, P1517.
Ponce J., 2006, P IEEE C COMP VIS PA, V2, P2169, DOI DOI 10.1109/CVPR.2006.68.
Poucet B, 2003, J PHYSIOL-PARIS, V97, P537, DOI 10.1016/j.jphysparis.2004.01.011.
Pronobis A, 2008, IEEE INT CONF ROBOT, P522, DOI 10.1109/ROBOT.2008.4543260.
Pronobis A, 2010, ROBOT AUTON SYST, V58, P81, DOI 10.1016/j.robot.2009.07.025.
Pronobis A, 2010, IMAGE VISION COMPUT, V28, P1080, DOI 10.1016/j.imavis.2010.01.015.
Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537.
Redish Aaron David, 1999, COGNITIVE MAP PLACE.
Ridella S, 1999, IEEE T NEURAL NETWOR, V10, P31, DOI 10.1109/72.737491.
Rottmann A., 2005, P NAT C ART INT AAAI, P1306.
Shastri L, 2001, EPISODIC MEMORY TRAC.
Siagian C, 2007, IEEE T PATTERN ANAL, V29, P300, DOI 10.1109/TPAMI.2007.40.
Starner T, 1998, SECOND INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS - DIGEST OF PAPERS, P50, DOI 10.1109/ISWC.1998.729529.
STARNER T, 1999, THESIS MIT.
Starzyk J. A., 2008, FRONTIERS ROBOTICS A, P83.
Starzyk JA, 2007, IEEE T NEURAL NETWOR, V18, P344, DOI 10.1109/TNN.2006.884681.
Starzyk JA, 2009, IEEE T NEURAL NETWOR, V20, P768, DOI 10.1109/TNN.2009.2012854.
Sun R, 2001, IEEE INTELL SYST, V16, P67, DOI 10.1109/MIS.2001.1463065.
Tay ALP, 2007, IEEE T NEURAL NETWOR, V18, P1645, DOI 10.1109/TNN.2007.900231.
Tay ALP, 2006, IEEE IJCNN, P4201.
Thrun S, 1998, ARTIF INTELL, V99, P21, DOI 10.1016/S0004-3702(97)00078-7.
Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273.
Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128.
Tse R, 2008, IEEE SYS MAN CYBERN, P3033.
Tulving E., 1972, ORG MEMORY, P381.
Ullah MM, 2008, IEEE INT CONF ROBOT, P530, DOI 10.1109/ROBOT.2008.4543261.
Ulrich I., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1023, DOI 10.1109/ROBOT.2000.844734.
Vapnik, 1998, STAT LEARNING THEORY.
Nguyen VA, 2012, IEEE T NEUR NET LEAR, V23, P971, DOI 10.1109/TNNLS.2012.2191419.
WANG DL, 1993, IEEE T SYST MAN CYB, V23, P993, DOI 10.1109/21.247884.
WANG DL, 1995, IEEE T SYST MAN CYB, V25, P615, DOI 10.1109/21.370192.
WANG DL, 1990, P IEEE, V78, P1536, DOI 10.1109/5.58329.
Wolf J, 2005, IEEE T ROBOT, V21, P208, DOI 10.1109/TRO.2004.835453.
Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224.
Wu JX, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P4763, DOI 10.1109/IROS.2009.5354164.
Zabih R., 1994, Computer Vision - ECCV `94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151.},
  da = {2022-05-17},
  doc-delivery-number = {268NS},
  eissn = {1872-793X},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {Visual place recognition; Spatio-temporal neural networks; Long-term
Memory; Topological robotic mapping},
  keywords-plus = {SIMULTANEOUS LOCALIZATION; NEURAL SYSTEM; SCENE; REPRESENTATION;
HIPPOCAMPUS; SEQUENCES; NETWORKS; CELLS; SVM; MAP},
  language = {English},
  number-of-cited-references = {70},
  orcid-numbers = {Starzyk, Janusz/0000-0003-2678-5515},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  times-cited = {5},
  type = {Article},
  unique-id = {WOS:000328178200038},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {15},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ila-et-al:2017:0278364917691110,
  author = {V. Ila and L. Polok and M. Solony and P. Svoboda},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {SLAM plus plus -A highly efficient and temporally scalable incremental
SLAM framework},
  volume = {36},
  number = {2},
  pages = {210--230},
  doi = {10.1177/0278364917691110},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2017},
  month = {2},
  abstract = {The most common way to deal with the uncertainty present in noisy
sensorial perception and action is to model the problem with a
probabilistic framework. Maximum likelihood estimation is a well-known
estimation method used in many robotic and computer vision applications.
Under Gaussian assumption, the maximum likelihood estimation converts to
a nonlinear least squares problem. Efficient solutions to nonlinear
least squares exist and they are based on iteratively solving sparse
linear systems until convergence. In general, the existing solutions
provide only an estimation of the mean state vector, the resulting
covariance being computationally too expensive to recover. Nevertheless,
in many simultaneous localization and mapping (SLAM) applications,
knowing only the mean vector is not enough. Data association, obtaining
reduced state representations, active decisions and next best view are
only a few of the applications that require fast state covariance
recovery. Furthermore, computer vision and robotic applications are in
general performed online. In this case, the state is updated and
recomputed every step and its size is continuously growing, therefore,
the estimation process may become highly computationally demanding. This
paper introduces a general framework for incremental maximum likelihood
estimation called SLAM++, which fully benefits from the incremental
nature of the online applications, and provides efficient estimation of
both the mean and the covariance of the estimate. Based on that, we
propose a strategy for maintaining a sparse and scalable state
representation for large scale mapping, which uses information theory
measures to integrate only informative and non-redundant contributions
to the state representation. SLAM++ differs from existing
implementations by performing all the matrix operations by blocks. This
led to extremely fast matrix manipulation and arithmetic operations used
in nonlinear least squares. Even though this paper tests SLAM++
efficiency on SLAM problems, its applicability remains general.},
  affiliation = {Ila, V (Corresponding Author), 115 North Rd, Canberra, ACT 2100, Australia.
Ila, Viorela, Australian Natl Univ, Canberra, ACT, Australia.
Polok, Lukas; Solony, Marek; Svoboda, Pavel, Brno Univ Technol, Fac Informat Technol, Brno, Czech Republic.},
  affiliations = {Australian National University; Brno University of Technology},
  author-email = {viorela.ila@anu.edu.au},
  cited-references = {Agarwal S., 2012, CERES SOLVER.
Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148.
Barfoot TD, 2014, IEEE T ROBOT, V30, P679, DOI 10.1109/TRO.2014.2298059.
Beall C, 2010, IEEE INT C INT ROBOT, P4418, DOI 10.1109/IROS.2010.5649213.
Bjorck A, 1996, NUMERICAL METHODS LE.
Blanco J. - L., 2010, TECHNICAL REPORT.
Carlevaris-Bianco N, 2014, IEEE T ROBOT, V30, P1371, DOI 10.1109/TRO.2014.2347571.
Carlevaris-Bianco N, 2014, IEEE INT CONF ROBOT, P854, DOI 10.1109/ICRA.2014.6906954.
Carlevaris-Bianco N, 2013, IEEE INT CONF ROBOT, P5748, DOI 10.1109/ICRA.2013.6631403.
CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Davis T., 2006, CSPARSE.
Davis T., 2006, DIRECT METHODS SPARS.
Davis TA, 1999, SIAM J MATRIX ANAL A, V20, P606, DOI 10.1137/S0895479897321076.
Davison AJ, 2002, IEEE T PATTERN ANAL, V24, P865, DOI 10.1109/TPAMI.2002.1017615.
Dellaert F, 2006, INT J ROBOT RES, V25, P1181, DOI 10.1177/0278364906072768.
Dissanayake G, 2002, AUTON ROBOT, V12, P267, DOI 10.1023/A:1015217631658.
Eustice RM, 2006, INT J ROBOT RES, V25, P1223, DOI 10.1177/0278364906072512.
Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297.
GOLUB GH, 1980, LINEAR ALGEBRA APPL, V34, P3, DOI 10.1016/0024-3795(80)90156-1.
Grisetti G., 2007, ROBOTICS SCI SYSTEMS, P27.
HAGER WW, 1989, SIAM REV, V31, P221, DOI 10.1137/1031049.
Haner S, 2012, LECT NOTES COMPUT SC, V7573, P545, DOI 10.1007/978-3-642-33709-3\_39.
Huang GQ, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P150, DOI 10.1109/ECMR.2013.6698835.
Huang GQP, 2011, IEEE INT C INT ROBOT, P65, DOI 10.1109/IROS.2011.6048760.
Ila V, 2015, IEEE INT CONF ROBOT, P4636, DOI 10.1109/ICRA.2015.7139841.
Ila V, 2010, IEEE T ROBOT, V26, P78, DOI 10.1109/TRO.2009.2034435.
Indelman V, 2012, BRIT MACH VIS C BMVC, P134.
Johannsson H, 2013, IEEE INT CONF ROBOT, P54, DOI 10.1109/ICRA.2013.6630556.
KABSCH W, 1976, ACTA CRYSTALLOGR A, V32, P922, DOI 10.1107/S0567739476001873.
Kaess Michael, 2011, 2011 IEEE International Conference on Robotics and Automation, P3281.
Kaess M, 2010, INT WORKSH ALG FDN R, P150.
Kaess M, 2012, INT J ROBOT RES, V31, P216, DOI 10.1177/0278364911430419.
Kaess M, 2009, ROBOT AUTON SYST, V57, P1198, DOI 10.1016/j.robot.2009.06.008.
Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706.
Klein George, 2007, P1.
Konolige, 2010, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.24.102.
Konolige K., 2010, 2010 IEEERSJ INT C I, P22.
Kretzschmar H, 2012, INT J ROBOT RES, V31, P1219, DOI 10.1177/0278364912455072.
Kretzschmar H, 2011, IEEE INT C INT ROBOT, P865, DOI 10.1109/IROS.2011.6048060.
Kummerle R, 2009, AUTON ROBOT, V27, P387, DOI 10.1007/s10514-009-9155-6.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Neira J, 2001, IEEE T ROBOTIC AUTOM, V17, P890, DOI 10.1109/70.976019.
Polok L, 2013, ROBOTICS SCI SYSTEMS.
Polok L., 2013, P 21 HIGH PERF COMP, P698.
Polok L, 2013, IEEE INT CONF ROBOT, P2263, DOI 10.1109/ICRA.2013.6630883.
Prentice S, 2010, SPRINGER TRAC ADV RO, V66, P293.
Salas-Moreno RF, 2013, PROC CVPR IEEE, P1352, DOI 10.1109/CVPR.2013.178.
Sibley G, 2008, LECT NOTES ELECTR EN, V8, P103.
Sim R, 2005, IEEE INT CONF ROBOT, P2411.
SMITH RC, 1986, INT J ROBOT RES, V5, P56, DOI 10.1177/027836498600500404.
Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773.
Thrun S, 2004, INT J ROBOT RES, V23, P693, DOI 10.1177/0278364904045479.
Tipaldi GD, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P3466.
Valencia R, 2013, IEEE T ROBOT, V29, P1050, DOI 10.1109/TRO.2013.2257577.
Vidal-Calleja T, 2006, IEEE INT CONF ROBOT, P1930, DOI 10.1109/ROBOT.2006.1641988.},
  da = {2022-05-17},
  doc-delivery-number = {ES5DX},
  eissn = {1741-3176},
  funding-acknowledgement = {ARC Centre of Excellence for Robotic Vision {[}CE140100016]; European
Union {[}316564-IMPART]; IT4-Innovations Centre of Excellence project -
European Regional Development Fund {[}CZ.1.05/1.1.00/02.0070]; national
budget of the Czech Republic via the Research and Development for
Innovations Operational Programme; Czech Ministry of Education, Youth
and Sports via the project Large Research, Development and Innovations
Infrastructures {[}LM2011033]},
  funding-text = {The author(s) disclosed receipt of the following financial support for
the research, authorship, and/or publication of this article: Dr.
Viorela Ila was supported by the ARC Centre of Excellence for Robotic
Vision, project number CE140100016.; The authors from Brno University of
Technology were supported by the European Union, 7th Framework Programme
grant 316564-IMPART and the IT4-Innovations Centre of Excellence project
(CZ.1.05/1.1.00/02.0070), funded by the European Regional Development
Fund and the national budget of the Czech Republic via the Research and
Development for Innovations Operational Programme, as well as Czech
Ministry of Education, Youth and Sports via the project Large Research,
Development and Innovations Infrastructures (LM2011033).},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {Nonlinear least squares; incremental covariance recovery; long-term
SLAM; loop closure; compact state representation},
  keywords-plus = {SIMULTANEOUS LOCALIZATION},
  language = {English},
  number-of-cited-references = {56},
  orcid-numbers = {Ila, Viorela/0000-0002-8137-0833},
  research-areas = {Robotics},
  times-cited = {42},
  type = {Article},
  unique-id = {WOS:000399558300006},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {19},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{ali-et-al:2021:3100882,
  author = {W. Ali and P. Liu and R. Ying and Z. Gong},
  journal = {IEEE SENSORS JOURNAL},
  title = {A Life-Long SLAM Approach Using Adaptable Local Maps Based on Rasterized
LIDAR Images},
  volume = {21},
  number = {19},
  pages = {21740--21749},
  doi = {10.1109/JSEN.2021.3100882},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2021},
  month = {10},
  abstract = {Most real-time autonomous robot applications require a robot to traverse
through a dynamic space for a long time. In some cases, a robot needs to
work in the same environment. Such applications give rise to the problem
of a life-long SLAM system. Life-long SLAM presents two main challenges
i.e. the tracking should not fail in a dynamic environment and the need
for a robust and efficient mapping strategy. The system should update
maps with new information; while also keeping track of older
observations. But, mapping for a long time can require higher
computational requirements. In this paper, we propose a solution to the
problem of life-long SLAM. We represent the global map as a set of
rasterized images of local maps along with a map management system
responsible for updating local maps and keeping track of older values.
We also present an efficient approach of using the bag of visual words
method for loop closure detection and relocalization. We evaluate the
performance of our system on the KITTI dataset and an indoor dataset.
Our loop closure system reported recall and precision of above 90
percent. The computational cost of our system is much lower as compared
to state-of-the-art methods. Our method reports lower computational
requirements even for long-term operation.},
  affiliation = {Ali, W (Corresponding Author), Shanghai Jiao Tong Univ, Sch Elect Informat \& Elect Engn, Shanghai 200240, Peoples R China.
Ali, Waqas; Liu, Peilin; Ying, Rendong; Gong, Zheng, Shanghai Jiao Tong Univ, Sch Elect Informat \& Elect Engn, Shanghai 200240, Peoples R China.},
  affiliations = {Shanghai Jiao Tong University},
  author-email = {vaqas11@sjtu.edu.cn},
  cited-references = {Ali W., ARXIV210310678, V2021.
Angeli A, 2008, IEEE INT CONF ROBOT, P1842, DOI 10.1109/ROBOT.2008.4543475.
Banerjee N, 2020, PARASITOLOGY, V147, P841, DOI 10.1017/S0031182019001422.
Behley J, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV.
Biber P, 2009, INT J ROBOT RES, V28, P20, DOI 10.1177/0278364908096286.
Bosse M, 2004, INT J ROBOT RES, V23, P1113, DOI 10.1177/0278364904049393.
Chen XYL, 2019, IEEE INT C INT ROBOT, P4530, DOI 10.1109/IROS40897.2019.8967704.
Duckett T., 2005, ROBOTICS SCI SYSTEMS, P17.
Fentanes JP, 2015, IEEE INT CONF ROBOT, P1112, DOI 10.1109/ICRA.2015.7139315.
FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Haehnel D., 2009, P ROBOT SCI SYST, P435, DOI 10.15607/RSS.2009.V.021.
Hess W, 2016, IEEE INT CONF ROBOT, P1271, DOI 10.1109/ICRA.2016.7487258.
Kejriwal N, 2016, ROBOT AUTON SYST, V77, P55, DOI 10.1016/j.robot.2015.12.003.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Kretzschmar H, 2011, IEEE INT C INT ROBOT, P865, DOI 10.1109/IROS.2011.6048060.
Magnusson Martin, 2009, 2009 IEEE International Conference on Robotics and Automation (ICRA), P23, DOI 10.1109/ROBOT.2009.5152712.
McDonald J., 2011, P EUR C MOB ROB ECMR, P69.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Naima A., 2011, THESIS DEP ELECT ENG.
Nicosevici T, 2012, IEEE T ROBOT, V28, P886, DOI 10.1109/TRO.2012.2192013.
Pan Y., ARXIV210203771, V2021.
Qin T., 2019, ARXIV190103638.
Rohling T, 2015, IEEE INT C INT ROBOT, P736, DOI 10.1109/IROS.2015.7353454.
Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023\_34.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Stachniss C., 2005, P C ART INT, P1324.
Stachniss C, 2017, SPRINGER TRAC ADV RO, V100, DOI 10.1007/978-3-319-29363-9\_16.
Steder B, 2011, IEEE INT C INT ROBOT, P1249, DOI 10.1109/IROS.2011.6048325.
Steder B, 2010, IEEE INT CONF ROBOT, P1400, DOI 10.1109/ROBOT.2010.5509401.
Thrun S, 2002, COMMUN ACM, V45, P52.
Wang H, 2020, IEEE INT CONF ROBOT, P2095, DOI 10.1109/ICRA40945.2020.9196764.
Wolf DF, 2005, AUTON ROBOT, V19, P53, DOI 10.1007/s10514-005-0606-4.
Yamauchi B, 1996, IEEE T SYST MAN CY B, V26, P496, DOI 10.1109/3477.499799.
Zhang J., 2014, ROBOT SCI SYST, V2, P9.},
  da = {2022-05-17},
  doc-delivery-number = {WA2IU},
  eissn = {1558-1748},
  issn = {1530-437X},
  journal-iso = {IEEE Sens. J.},
  keywords = {Simultaneous localization and mapping; Three-dimensional displays;
Feature extraction; Robots; Databases; Laser radar; Sensors; Laser
scanning; place recognition; bag of words; rasterization; mapping;
simultaneous localization; mapping},
  keywords-plus = {SIMULTANEOUS LOCALIZATION; NAVIGATION; WORDS; BAG},
  language = {English},
  number-of-cited-references = {36},
  oa = {Green Submitted},
  research-areas = {Engineering; Instruments \& Instrumentation; Physics},
  times-cited = {0},
  type = {Article},
  unique-id = {WOS:000702716000071},
  usage-count-last-180-days = {7},
  usage-count-since-2013 = {9},
  web-of-science-categories = {Engineering, Electrical \& Electronic; Instruments \& Instrumentation;
Physics, Applied},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{churchill-newman:2013:0278364913499193,
  author = {W. Churchill and P. Newman},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {Experience-based navigation for long-term localisation},
  volume = {32},
  number = {14,SI},
  pages = {1645--1661},
  doi = {10.1177/0278364913499193},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2013},
  month = {12},
  abstract = {This paper is about long-term navigation in environments whose
appearance changes over time, suddenly or gradually. We describe,
implement and validate an approach which allows us to incrementally
learn a model whose complexity varies naturally in accordance with
variation of scene appearance. It allows us to leverage the state of the
art in pose estimation to build over many runs, a world model of
sufficient richness to allow simple localisation despite a large
variation in conditions. As our robot repeatedly traverses its
workspace, it accumulates distinct visual experiences that in concert,
implicitly represent the scene variation: each experience captures a
visual mode. When operating in a previously visited area, we continually
try to localise in these previous experiences while simultaneously
running an independent vision-based pose estimation system. Failure to
localise in a sufficient number of prior experiences indicates an
insufficient model of the workspace and instigates the laying down of
the live image sequence as a new distinct experience. In this way, over
time we can capture the typical time-varying appearance of an
environment and the number of experiences required tends to a constant.
Although we focus on vision as a primary sensor throughout, the ideas we
present here are equally applicable to other sensor modalities. We
demonstrate our approach working on a road vehicle operating over a
3-month period at different times of day, in different weather and
lighting conditions. We present extensive results analysing different
aspects of the system and approach, in total processing over 136,000
frames captured from 37 km of driving.},
  affiliation = {Churchill, W (Corresponding Author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.
Churchill, Winston; Newman, Paul, Univ Oxford, Mobile Robot Grp, Oxford OX1 3PJ, England.},
  affiliations = {League of European Research Universities - LERU; University of Oxford},
  author-email = {winston.churchill@eng.ox.ac.uk},
  cited-references = {Bailey T, 2006, IEEE ROBOT AUTOM MAG, V13, P108, DOI 10.1109/MRA.2006.1678144.
Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
Biber P, 2005, P IEEE ROB SCI SYST.
Borrmann D, 2010, IEEE INT C INT ROB S.
Burgard W, 2007, SPRINGER TRACTS ADV, V35, P27.
Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222.
Cummins M., 2009, ROBOTICS SCI SYSTEMS.
Dayoub F, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3364, DOI 10.1109/IROS.2008.4650701.
Dayoub F, 2011, ROBOT AUTON SYST, V59, P285, DOI 10.1016/j.robot.2011.02.013.
Furgale P, 2010, J FIELD ROBOT, V27, P534, DOI 10.1002/rob.20342.
Konolige K, 2007, INT S RES ROB ISRR.
Lategahn H, 2012, P IEEE INT C VEH EL.
McManus C., 2012, P IEEE INT C ROB AUT.
Mei C, 2008, IEEE T ROBOT, V24, P1352, DOI 10.1109/TRO.2008.2007941.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Newman P, 2009, INT J ROBOT RES, V28, P1406, DOI 10.1177/0278364909341483.
Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275.
Schindler K, 2010, ISPRS J PHOTOGRAMM, V65, P523, DOI 10.1016/j.isprsjprs.2010.06.006.
Sibley G, 2010, INT J ROBOT RES, V29, P958, DOI 10.1177/0278364910369268.
Taneja A, 2011, P IEEE INT C COMP VI.
Wolf D, 2004, P IEEE INT C ROB AUT.},
  da = {2022-05-17},
  doc-delivery-number = {287AL},
  eissn = {1741-3176},
  funding-acknowledgement = {EPSRC {[}EP/I005021/1]; Oxford Technologies Ltd.; BAE SYSTEMS;
Engineering and Physical Sciences Research Council {[}EP/J012017/1,
EP/I005021/1] Funding Source: researchfish; EPSRC {[}EP/J012017/1]
Funding Source: UKRI},
  funding-text = {Winston Churchill is supported by an EPSRC Case Studentship with Oxford
Technologies Ltd. Paul Newman is supported by an EPSRC Leadership
Fellowship (number EP/I005021/1). This work has also been supported by
BAE SYSTEMS.},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {Localisation; mobile and distributed robotics; SLAM; mapping; field
robots; field and service robotics},
  keywords-plus = {TRACKING},
  language = {English},
  number-of-cited-references = {22},
  research-areas = {Robotics},
  times-cited = {121},
  type = {Article},
  unique-id = {WOS:000329510300004},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {15},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{maddern-et-al:2012:6224622,
  author = {W. Maddern and M. Milford and G. Wyeth},
  booktitle = {2012 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
  title = {Capping Computation Time and Storage Requirements for Appearance-based
Localization with CAT-SLAM},
  pages = {822--827},
  doi = {10.1109/ICRA.2012.6224622},
  note = {IEEE International Conference on Robotics and Automation (ICRA), St
Paul, MN, MAY 14-18, 2012},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2012},
  abstract = {Appearance-based localization is increasingly used for loop closure
detection in metric SLAM systems. Since it relies only upon the
appearance-based similarity between images from two locations, it can
perform loop closure regardless of accumulated metric error. However,
the computation time and memory requirements of current appearance-based
methods scale linearly not only with the size of the environment but
also with the operation time of the platform. These properties impose
severe restrictions on long-term autonomy for mobile robots, as loop
closure performance will inevitably degrade with increased operation
time. We present a set of improvements to the appearance-based SLAM
algorithm CAT-SLAM to constrain computation scaling and memory usage
with minimal degradation in performance over time. The appearance-based
comparison stage is accelerated by exploiting properties of the particle
observation update, and nodes in the continuous trajectory map are
removed according to minimal information loss criteria. We demonstrate
constant time and space loop closure detection in a large urban
environment with recall performance exceeding FAB-MAP by a factor of 3
at 100\% precision, and investigate the minimum computational and memory
requirements for maintaining mapping performance.},
  affiliation = {Maddern, W (Corresponding Author), Queensland Univ Technol, Fac Sci \& Engn, Sch Elect Engn \& Comp Sci, Brisbane, Qld 4001, Australia.
Maddern, Will; Milford, Michael; Wyeth, Gordon, Queensland Univ Technol, Fac Sci \& Engn, Sch Elect Engn \& Comp Sci, Brisbane, Qld 4001, Australia.},
  affiliations = {Queensland University of Technology (QUT)},
  author-email = {w.maddern@qut.edu.au
michael.milford@qut.edu.au
gordon.wyeth@qut.edu.au},
  book-group-author = {IEEE},
  cited-references = {Biber P, 2009, INT J ROBOT RES, V28, P20, DOI 10.1177/0278364908096286.
Cummins M., 2009, ROB SCI SYST C SEATT.
Cummins M., 2008, IEEE INT C ROB AUT P.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Cummins M, 2010, IEEE T ROBOT, V26, P1042, DOI 10.1109/TRO.2010.2080390.
Glover A, 2012, IEEE INT CONF ROBOT, P4730, DOI 10.1109/ICRA.2012.6224843.
Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178.
Knight J., 2001, IEEE RSJ INT C INT R.
LEONARD J, 2003, INT JOINT C ART INT.
Liu JS, 2001, STAT ENG IN, P225.
Maddern W., INT J ROBOT IN PRESS.
Maddern W., 2010, AUSTR C ROB AUT BRIS.
Milford M., 2010, INT J ROBOTICS RES.
Milford M., 2013, IEEE INT C ROB AUT.
Milford M., 2004, IEEE INT C ROB AUT N.
Nerurkar ED, 2011, INT J ROBOT RES, V30, P772, DOI 10.1177/0278364910390539.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.
Teynor A, 2007, LECT NOTES COMPUT SC, V4841, P610.
Thrun S., 2002, WORKSH ALG FDN ROB N.
Torii A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS).},
  da = {2022-05-17},
  doc-delivery-number = {BCA25},
  eissn = {2577-087X},
  isbn = {978-1-4673-1405-3},
  issn = {1050-4729},
  language = {English},
  number-of-cited-references = {20},
  orcid-numbers = {Milford, Michael/0000-0002-5162-1793
Wyeth, Gordon/0000-0002-4996-3612},
  research-areas = {Automation \& Control Systems; Engineering; Robotics},
  researcherid-numbers = {Milford, Michael/J-1304-2012},
  series = {IEEE International Conference on Robotics and Automation ICRA},
  times-cited = {31},
  type = {Proceedings Paper},
  unique-id = {WOS:000309406700122},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Automation \& Control Systems; Engineering, Electrical \& Electronic;
Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@inproceedings{ding-et-al:2019:8968550,
  author = {X. Ding and Y. Wang and L. Tang and H. Yin and R. Xiong},
  booktitle = {2019 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS)},
  title = {Communication constrained cloud-based long-term visual localization in
real time},
  pages = {2159--2166},
  doi = {10.1109/IROS40897.2019.8968550},
  note = {IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS), Macau, PEOPLES R CHINA, NOV 04-08, 2019},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2019},
  abstract = {Visual localization is one of the primary capabilities for mobile
robots. Long-term visual localization in real time is particularly
challenging, in which the robot is required to efficiently localize
itself using visual data where appearance may change significantly over
time. In this paper, we propose a cloud-based visual localization system
targeting at long-term localization in real time. On the robot, we
employ two estimators to achieve accurate and real-time performance. One
is a sliding-window based visual inertial odometry, which integrates
constraints from consecutive observations and selfmotion measurements,
as well as the constraints induced by localization results from the
cloud. This estimator builds a local visual submap as the virtual
observation which is then sent to the cloud as new localization
constraints. The other one is a delayed state Extended Kalman Filter to
fuse the pose of the robot localized from the cloud, the local odometry
and the high-frequency inertial measurements. On the cloud, we propose a
longer sliding-window based localization method to aggregate the virtual
observations for larger field of view, leading to more robust alignment
between virtual observations and the map. Under this architecture, the
robot can achieve drift-free and real-time localization using onboard
resources even in a network with limited bandwidth, high latency and
existence of package loss, which enables the autonomous navigation in
real-world environment. We evaluate the effectiveness of our system on a
dataset with challenging seasonal and illuminative variations. We
further validate the robustness of the system under challenging network
conditions.},
  affiliation = {Wang, Y; Xiong, R (Corresponding Author), Zhejiang Univ, State Key Lab Ind Control \& Technol, Hangzhou, Peoples R China.
Ding, Xiaqing; Wang, Yue; Tang, Li; Yin, Huan; Xiong, Rong, Zhejiang Univ, State Key Lab Ind Control \& Technol, Hangzhou, Peoples R China.},
  affiliations = {Zhejiang University},
  author-email = {wangyue@iipc.zju.edu.cn
rxiong@zju.edu.cn},
  book-group-author = {IEEE},
  cited-references = {Burki M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4137, DOI 10.1109/IROS.2016.7759609.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Ding XQ, 2018, IEEE INT C INT ROBOT, P4794, DOI 10.1109/IROS.2018.8593846.
Forster C, 2017, IEEE T ROBOT, V33, P1, DOI 10.1109/TRO.2016.2597321.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
GUO B, 2018, IEEE T ROBOTICS, P1.
Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039.
Kim Y, 2018, IEEE INT C INT ROBOT, P5826.
Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Lynen S., 2013, P IEEE RSJ C INT ROB.
Maddern W, 2012, INT J ROBOT RES, V31, P429, DOI 10.1177/0278364912438273.
Middelberg S, 2014, LECT NOTES COMPUT SC, V8690, P268, DOI 10.1007/978-3-319-10605-2\_18.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mohanarajah G, 2015, IEEE T AUTOM SCI ENG, V12, P423, DOI 10.1109/TASE.2015.2408456.
Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024.
Mur-Artal R, 2017, IEEE ROBOT AUTOM LET, V2, P796, DOI 10.1109/LRA.2017.2653359.
Naseer T., 2017, IEEE INT C ROB AUT.
Paton M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1918, DOI 10.1109/IROS.2016.7759303.
Paull L, 2015, IEEE INT CONF ROBOT, P509, DOI 10.1109/ICRA.2015.7139227.
Sattler T., 2018, P CVPR, V1.
Sinha RS, 2017, ICT EXPRESS, V3, P14, DOI 10.1016/j.icte.2017.03.004.
Sturm J, 2012, WORKSH COL DEPTH CAM.
Tang L, 2019, AUTON ROBOT, V43, P197, DOI 10.1007/s10514-018-9724-7.
Wang Y, 2015, ADV ROBOTICS, V29, P683, DOI 10.1080/01691864.2014.998707.
Wu K. J., 2015, 2015 ROB SCI SYST C.
Zhu X., 2017, J FIELD ROBOTICS, V34.},
  da = {2022-05-17},
  doc-delivery-number = {BP2QS},
  funding-acknowledgement = {National Nature Science Foundation of China {[}U1609210]; Science and
Technology Project of Zhejiang Province {[}2019C01043]; State Key
Laboratory of Industrial Control Technology {[}ITC1904]; Science and
Technology on Space Intelligent Control Laboratory {[}HTKJ2019KL502002]},
  funding-text = {This work was supported by the National Nature Science Foundation of
China (Grant No. U1609210) and Science and Technology Project of
Zhejiang Province (Grant No. 2019C01043), the State Key Laboratory of
Industrial Control Technology (ITC1904) and Science and Technology on
Space Intelligent Control Laboratory (Grant No. HTKJ2019KL502002).},
  isbn = {978-1-7281-4004-9},
  issn = {2153-0858},
  keywords-plus = {NAVIGATION},
  language = {English},
  number-of-cited-references = {27},
  oa = {Green Submitted},
  orcid-numbers = {Yin, Huan/0000-0002-0872-8202},
  research-areas = {Computer Science; Robotics},
  researcherid-numbers = {Yin, Huan/ABC-9483-2020},
  series = {IEEE International Conference on Intelligent Robots and Systems},
  times-cited = {0},
  type = {Proceedings Paper},
  unique-id = {WOS:000544658401117},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Computer Science, Information
Systems; Computer Science, Theory \& Methods; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{ding-et-al:2020:2942760,
  author = {X. Ding and Y. Wang and R. Xiong and D. Li and T. Li and H. Yin and L. Zhao},
  journal = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS},
  title = {Persistent Stereo Visual Localization on Cross-Modal Invariant Map},
  volume = {21},
  number = {11},
  pages = {4646--4658},
  doi = {10.1109/TITS.2019.2942760},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2020},
  month = {11},
  abstract = {Autonomous mobile vehicles are expected to perform persistent and
accurate localization with low-cost equipment. To achieve this goal, we
propose a stereo camera based visual localization method using a
modified laser map, which takes the advantage of both the low cost of
camera, and high geometric precision of laser data to achieve long-term
performance. Considering that LiDAR and camera give measurements of the
same environment in different modalities, the cross-modal invariance is
investigated to modify the laser map for visual localization.
Specifically, a map learning algorithm is introduced to sample the
robust subsets in laser maps that are useful for visual localization
using multi-session visual and laser data. Further, a generative map
model is derived to describe this crossmodal invariance, based on which
two types of measurements are defined to model the laser map points as
appropriate visual observations. Tightly coupling these measurements
within the local bundle adjustment during online sliding-window based
visual odometry, the vehicle can achieve robust localization even one
year after the map was built. The effectiveness of the proposed method
is evaluated on both the public KITTI datasets and self-collected
datasets in our campus, which include seasonal, illumination and object
variations. On all experimental localization sessions, our method
provides satisfactory results, even when the direction is opposite to
that in the mapping session, verifying the superior performance of the
laser map based visual localization method.},
  affiliation = {Wang, Y; Xiong, R (Corresponding Author), Zhejiang Univ, State Key Lab Ind Control \& Technol, Hangzhou 310007, Peoples R China.
Ding, Xiaqing; Wang, Yue; Xiong, Rong; Li, Dongxuan; Tang, Li; Yin, Huan, Zhejiang Univ, State Key Lab Ind Control \& Technol, Hangzhou 310007, Peoples R China.
Zhao, Liang, Univ Technol Sydney, Ctr Autonomous Syst CAS, Sydney, NSW 2007, Australia.},
  affiliations = {Zhejiang University; University of Technology Sydney},
  author-email = {wangyue@iipc.zju.edu.cn
rxiong@zju.edu.cn},
  cited-references = {ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965.
Badino H, 2011, IEEE INT VEH SYM, P794, DOI 10.1109/IVS.2011.5940504.
Bay H., 2006, EUR C COMP VIS, P404.
BESAG J, 1986, J R STAT SOC B, V48, P259.
Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222.
Caselitz T, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1926, DOI 10.1109/IROS.2016.7759304.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Corke P, 2013, IEEE INT C INT ROBOT, P2085, DOI 10.1109/IROS.2013.6696648.
Dymczyk M, 2015, IEEE INT CONF ROBOT, P2767, DOI 10.1109/ICRA.2015.7139575.
Fehr Marius, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5237, DOI 10.1109/ICRA.2017.7989614.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Hata AY, 2018, IEEE T INTELL TRANSP, V19, P2893, DOI 10.1109/TITS.2017.2761774.
Heyde ChristopherC., 2008, QUASI LIKELIHOOD ITS.
Hu F, 2010, ENVIRON TOXICOL CHEM, V29, P683, DOI 10.1002/etc.73.
Klein G, 2007, ADV CANCER RES, V98, P1, DOI 10.1016/S0065-230X(06)98001-4.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Labbe M, 2018, AUTON ROBOT, V42, P1133, DOI 10.1007/s10514-017-9682-5.
Lategahn H, 2014, IEEE T INTELL TRANSP, V15, P1246, DOI 10.1109/TITS.2014.2298492.
Levinson J, 2010, IEEE INT CONF ROBOT, P4372, DOI 10.1109/ROBOT.2010.5509700.
Li DX, 2017, P AMER CONTR CONF, P3579, DOI 10.23919/ACC.2017.7963501.
Linegar C, 2016, IEEE INT CONF ROBOT, P787, DOI 10.1109/ICRA.2016.7487208.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
McManus C., 2014, ROBOTICS SCI SYSTEMS.
McManus C, 2015, AUTON ROBOT, V39, P363, DOI 10.1007/s10514-015-9463-y.
McManus C, 2013, J FIELD ROBOT, V30, P254, DOI 10.1002/rob.21444.
Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592.
Milford MJ, 2008, IEEE T ROBOT, V24, P1038, DOI 10.1109/TRO.2008.2004520.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024.
Muhlfellner P, 2016, J FIELD ROBOT, V33, P561, DOI 10.1002/rob.21595.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Naseer T, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564.
Ozog P, 2017, ROBOT AUTON SYST, V87, P329, DOI 10.1016/j.robot.2016.09.006.
Pascoe G., 2015, P BRIT MACH VIS C.
Pomerleau F, 2014, IEEE INT CONF ROBOT, P3712, DOI 10.1109/ICRA.2014.6907397.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Santoso F, 2017, IEEE T AUTOM SCI ENG, V14, P260, DOI 10.1109/TASE.2016.2582752.
Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897.
Schneider Thomas, 2018, IEEE Robotics and Automation Letters, V3, P1418, DOI 10.1109/LRA.2018.2800113.
Sons M, 2017, IEEE INT VEH SYM, P1158, DOI 10.1109/IVS.2017.7995869.
Sturm J., 2012, P WORKSH COL DEPTH C.
Surber Julian, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P6300, DOI 10.1109/ICRA.2017.7989745.
Tang L., 2018, IEEE INT C ROB BIOM, P787.
Tang L, 2019, AUTON ROBOT, V43, P197, DOI 10.1007/s10514-018-9724-7.
Torii A., 2017, P IEEE C COMP VIS PA, P1637.
Valgren C, 2010, ROBOT AUTON SYST, V58, P149, DOI 10.1016/j.robot.2009.09.010.
Walcott-Bryant A, 2012, IEEE INT C INT ROBOT, P1871, DOI 10.1109/IROS.2012.6385561.
Wang R, 2017, IEEE I CONF COMP VIS, P3923, DOI 10.1109/ICCV.2017.421.
Wang Y, 2016, CAAI T INTELL TECHNO, V1, P90, DOI 10.1016/j.trit.2016.03.009.
Wang Y, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P32, DOI 10.1109/ECMR.2013.6698816.
Wang Y, 2013, INT J ROBOT AUTOM, V28, P234, DOI 10.2316/Journal.206.2013.3.206-3806.
Withers Dan, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P6233, DOI 10.1109/ICRA.2017.7989738.
Wolcott RW, 2014, IEEE INT C INT ROBOT, P176, DOI 10.1109/IROS.2014.6942558.
Xu YQ, 2017, IEEE INT VEH SYM, P487, DOI 10.1109/IVS.2017.7995765.
Yang ZF, 2017, IEEE T AUTOM SCI ENG, V14, P39, DOI 10.1109/TASE.2016.2550621.},
  da = {2022-05-17},
  doc-delivery-number = {OO9QU},
  eissn = {1558-0016},
  funding-acknowledgement = {National Key Research and Development Program of China
{[}2017YFC0806501]; Key Research and Development Program of Zhejiang
Province, China {[}2019C01043]; National Nature Science Foundation of
China {[}61903332]},
  funding-text = {This work was supported in part by the National Key Research and
Development Program of China under Grant 2017YFC0806501, in part by the
Key Research and Development Program of Zhejiang Province, China, Grant
2019C01043, and in part by the National Nature Science Foundation of
China under Grant 61903332.},
  issn = {1524-9050},
  journal-iso = {IEEE Trans. Intell. Transp. Syst.},
  keywords = {Visual localization; persistent autonomy; map maintenance; map
incorporated bundle adjustment},
  keywords-plus = {SLAM; NAVIGATION; FRAMEWORK},
  language = {English},
  number-of-cited-references = {57},
  orcid-numbers = {Yin, Huan/0000-0002-0872-8202
Zhao, Liang/0000-0003-4063-8183},
  research-areas = {Engineering; Transportation},
  researcherid-numbers = {Yin, Huan/ABC-9483-2020},
  times-cited = {5},
  type = {Article},
  unique-id = {WOS:000587709700015},
  usage-count-last-180-days = {2},
  usage-count-since-2013 = {13},
  web-of-science-categories = {Engineering, Civil; Engineering, Electrical \& Electronic;
Transportation Science \& Technology},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{xu-et-al:2021:3060741,
  author = {X. Xu and H. Yin and Z. Chen and Y. Li and R. Y. A. X. Wang},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {DiSCO: Differentiable Scan Context With Orientation},
  volume = {6},
  number = {2},
  pages = {2791--2798},
  doi = {10.1109/LRA.2021.3060741},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2021},
  month = {4},
  abstract = {Global localization is essential for robot navigation, of which the
first step is to retrieve a query from the map database. This problem is
called place recognition. In recent years, LiDAR scan based place
recognition has drawn attention as it is robust against the appearance
change. In this letter, we propose a LiDAR-based place recognition
method, named Differentiable Scan Context with Orientation (DiSCO),
which simultaneously finds the scan at a similar place and estimates
their relative orientation. The orientation can further be used as the
initial value for the down-stream local optimal metric pose estimation,
improving the pose estimation especially when a large orientation
between the current scan and retrieved scan exists. Our key idea is to
transform the feature into the frequency domain. We utilize the
magnitude of the spectrum as the place descriptor, which is
theoretically rotation-invariant. In addition, based on the
differentiable phase correlation, we can efficiently estimate the global
optimal relative orientation using the spectrum. With such structural
constraints, the network can be learned in an end-to-end manner, and the
backbone is fully shared by the two tasks, achieving better
interpretability and lightweight. Finally, DiSCO is validated on three
datasets with long-term outdoor conditions, showing better performance
than the compared methods. Codes are released at
https://github.com/MaverickPeter/DiSCO-pytorch.},
  affiliation = {Wang, Y (Corresponding Author), Zhejiang Univ, State Key Lab Ind Control Technol, Hangzhou 310027, Zhejiang, Peoples R China.
Wang, Y (Corresponding Author), Zhejiang Univ, Inst Cyber Syst \& Control, Hangzhou 310027, Zhejiang, Peoples R China.
Xu, Xuecheng; Yin, Huan; Chen, Zexi; Wang, Yue; Xiong, Rong, Zhejiang Univ, State Key Lab Ind Control Technol, Hangzhou 310027, Zhejiang, Peoples R China.
Xu, Xuecheng; Yin, Huan; Chen, Zexi; Wang, Yue; Xiong, Rong, Zhejiang Univ, Inst Cyber Syst \& Control, Hangzhou 310027, Zhejiang, Peoples R China.
Li, Yuehua, Zhejiang Lab, Hangzhou 310014, Zhejiang, Peoples R China.},
  affiliations = {Zhejiang University; Zhejiang University},
  author-email = {xuechengxu@zju.edu.cn
zjuyinhuan@gmail.com
chenzexi@zju.edu.cn
liyh@zhejianglab.com
ywang24@zju.edu.cn
rxiong\_zju@hotmail.com},
  cited-references = {Andrei I., 2018, P C ROB LEARN, P605.
Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Barnes D., 2020, C ROB LEARN, P303.
BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791.
Bulow H, 2020, IEEE INT CONF ROBOT, P8594, DOI 10.1109/ICRA40945.2020.9197453.
Bulow H, 2018, INT J COMPUT VISION, V126, P731, DOI 10.1007/s11263-018-1067-5.
Cadena C., 2017, PROC IEEE INT C ROBO, P5266.
Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638.
Chen XYL, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693.
Dube Renaud, 2018, IEEE Robotics and Automation Letters, V3, P1832, DOI 10.1109/LRA.2018.2803213.
He L, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P231, DOI 10.1109/IROS.2016.7759060.
Kim G, 2020, IEEE INT CONF ROBOT, P6246.
Kim G, 2019, IEEE ROBOT AUTOM LET, V4, P1948, DOI 10.1109/LRA.2019.2897340.
Kim G, 2018, IEEE INT C INT ROBOT, P4802, DOI 10.1109/IROS.2018.8593953.
Kingma D. P., 2015, P INT C LEARN REPR.
Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3\_43.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
Rusu RB, 2010, IEEE INT C INT ROBOT, P2155, DOI 10.1109/IROS.2010.5651280.
Schaupp L, 2019, IEEE INT C INT ROBOT, P3255, DOI 10.1109/IROS40897.2019.8968094.
Steder B, 2011, IEEE INT C INT ROBOT, P1249, DOI 10.1109/IROS.2011.6048325.
STEIN F, 1992, IEEE T PATTERN ANAL, V14, P1198, DOI 10.1109/34.177385.
Tang L, 2020, IEEE INT CONF ROBOT, P1301, DOI 10.1109/ICRA40945.2020.9196518.
Tombari F, 2011, IEEE IMAGE PROC, P809, DOI 10.1109/ICIP.2011.6116679.
Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470.
Wang J, 2015, SOIL USE MANAGE, V31, P375, DOI 10.1111/sum.12202.
Wang Y., 2020, ARXIV200809474.
Yin H, 2018, IEEE INT VEH SYM, P728.
Yin H, 2020, IEEE T INTELL TRANSP, V21, P1380, DOI 10.1109/TITS.2019.2905046.},
  da = {2022-05-17},
  doc-delivery-number = {RD7ET},
  funding-acknowledgement = {National Key R\&D Program of China {[}2018AAA0102700]; State
Administration of Science, Technology and Industry for National Defence
Grant, PRC {[}HTKJ2019KL502005]},
  funding-text = {This work was supported in part by the National Key R\&D Program of
China underGrant 2018AAA0102700 and in part by the stable support
project of State Administration of Science, Technology and Industry for
National Defence Grant, PRC under Grant HTKJ2019KL502005. (Corresponding
author: Yue Wang.)},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {Localization; range sensing; SLAM},
  keywords-plus = {LOCALIZATION},
  language = {English},
  number-of-cited-references = {31},
  oa = {Green Submitted},
  orcid-numbers = {Yin, Huan/0000-0002-0872-8202
Chen, Zexi/0000-0002-9782-6022
Xu, Xuecheng/0000-0002-0762-6714},
  research-areas = {Robotics},
  researcherid-numbers = {Yin, Huan/ABC-9483-2020},
  times-cited = {5},
  type = {Article},
  unique-id = {WOS:000633637000021},
  usage-count-last-180-days = {3},
  usage-count-since-2013 = {11},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{bouaziz-et-al:2022:4,
  author = {Y. Bouaziz and E. Royer and G. Bresson and D. Michel},
  journal = {MULTIMEDIA TOOLS AND APPLICATIONS},
  title = {Map management for robust long-term visual localization of an autonomous
shuttle in changing conditions},
  doi = {10.1007/s11042-021-11870-4},
  publisher = {SPRINGER},
  address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
  year = {2022},
  abstract = {Changes in appearance present a tremendous problem for the visual
localization of an autonomous vehicle in outdoor environments. Data
association between the current image and the landmarks in the map can
be challenging in cases where the map was built with different
environmental conditions. This paper introduces a solution to build and
use multi-session maps incorporating sequences recorded in different
conditions (day, night, fog, snow, rain, change of season, etc.). During
visual localization, we exploit a ranking function to extract the most
relevant keyframes from the map. This ranking function is designed to
take into account the pose of the vehicle as well as the current
environmental condition. In the mapping phase, covering all conditions
by constantly adding data to the map leads to a continuous growth in the
map size which in turn deteriorates the localization speed and
performance. Our map management strategy is an incremental approach that
aims to limit the size of the map while keeping it as diverse as
possible. Our experiments were performed on real data collected with our
autonomous shuttle as well as on a widely used public dataset. The
results demonstrate that our keyframe-based ranking function is suitable
for long-term scenarios. Our map management algorithm aims to build a
map with as much diversity as possible whereas some state of the art
approaches tend to filter out the less observed landmarks. This strategy
shows a reduction of localization failures while maintaining real-time
performance.},
  affiliation = {Bouaziz, Y (Corresponding Author), Inst VEDECOM, Inst Pascal, CNRS, Versailles, France.
Bouaziz, Youssef, Inst VEDECOM, Inst Pascal, CNRS, Versailles, France.
Royer, Eric; Dhome, Michel, Inst Pascal, SIGMA Clermont, CNRS, Clermont Ferrand, France.
Bresson, Guillaume, Inst VEDECOM, Versailles, France.},
  affiliations = {Centre National de la Recherche Scientifique (CNRS); Centre National de
la Recherche Scientifique (CNRS); Universite Clermont Auvergne (UCA)},
  author-email = {youssef.bouaziz@etu.uca.fr
eric.royer@uca.fr
guillaume.bresson@vedecom.fr
michel.dhome@uca.fr},
  cited-references = {Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023\_32.
Berrio JS, 2019, IEEE INT VEH SYM, P1166, DOI 10.1109/IVS.2019.8814289.
Bouaziz Y, 2021, 18 INT C INF CONTR A.
Bouaziz Y, 2021, 2021 IEEE 19TH WORLD SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE AND INFORMATICS (SAMI 2021), P93, DOI 10.1109/SAMI50585.2021.9378614.
Burki M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4137, DOI 10.1109/IROS.2016.7759609.
Burki M, 2018, IEEE INT VEH SYM, P682.
Burki M, 2019, J FIELD ROBOT, V36, P1041, DOI 10.1002/rob.21870.
Carlevaris-Bianco N, 2016, INT J ROBOT RES, V35, P1023, DOI 10.1177/0278364915614638.
Chen C, 2020, ARXIV200612567.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Churchill W, 2012, IEEE INT CONF ROBOT, P4525, DOI 10.1109/ICRA.2012.6224596.
Clark R, 2017, PROC CVPR IEEE, P2652, DOI 10.1109/CVPR.2017.284.
Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350.
Diaz-Escobar J, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/3758102.
Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828.
Dymczyk M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4572, DOI 10.1109/IROS.2016.7759673.
Dymczyk M, 2015, IEEE INT CONF ROBOT, P2767, DOI 10.1109/ICRA.2015.7139575.
Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297.
Gridseth M, 2020, IEEE INT CONF ROBOT, P1674, DOI 10.1109/ICRA40945.2020.9197362.
Halodova L, 2019, IEEE INT C INT ROBOT, P7033, DOI 10.1109/IROS40897.2019.8967994.
Harris C. G., 1988, ALVEY VISION C, P1, DOI DOI 10.5244/C.2.23.
Jatzkowski I, 2018, IEEE INT C INTELL TR, P2030, DOI 10.1109/ITSC.2018.8569692.
Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336.
Krajnik T, 2019, IEEE ROBOT AUTOM LET, V4, P3310, DOI 10.1109/LRA.2019.2926682.
Krajnik T, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4558, DOI 10.1109/IROS.2016.7759671.
Laskar Z, 2017, IEEE INT CONF COMP V, P920, DOI 10.1109/ICCVW.2017.113.
Lebraly Pierre, 2011, 2011 IEEE International Conference on Robotics and Automation, P221.
Linegar C, 2015, IEEE INT CONF ROBOT, P90, DOI 10.1109/ICRA.2015.7138985.
Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94.
MacTavish K, 2018, J FIELD ROBOT, V35, P1265, DOI 10.1002/rob.21838.
Maddern W, 2020, ARXIV200210152.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
Magnago V, 2019, IEEE T INSTRUM MEAS, V68, P4443, DOI 10.1109/TIM.2018.2887071.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Muhlfellner P, 2016, J FIELD ROBOT, V33, P561, DOI 10.1002/rob.21595.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Murillo A C, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2196, DOI 10.1109/ICCVW.2009.5457552.
Naseer Tayyab, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2614, DOI 10.1109/ICRA.2017.7989305.
Pascoe G, 2017, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2017.158.
Pepperell E, 2016, INT J ROBOT RES, V35, DOI 10.1177/0278364915618766.
Rosen DM, 2016, IEEE INT CONF ROBOT, P1063, DOI 10.1109/ICRA.2016.7487237.
Royer E, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2248, DOI 10.1109/ITSC.2016.7795919.
Schneider Thomas, 2018, IEEE Robotics and Automation Letters, V3, P1418, DOI 10.1109/LRA.2018.2800113.
Schonberger JL, 2018, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR.2018.00721.
Stenborg E, 2020, INT CONF 3D VISION, P938, DOI 10.1109/3DV50981.2020.00104.
Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127.
Walch F, 2017, IEEE I CONF COMP VIS, P627, DOI 10.1109/ICCV.2017.75.
Yan Z, 2020, IEEE INT C INT ROBOT, P10697, DOI 10.1109/IROS45743.2020.9341406.
Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4\_28.},
  da = {2022-05-17},
  doc-delivery-number = {0K9RK},
  earlyaccessdate = {APR 2022},
  eissn = {1573-7721},
  funding-acknowledgement = {French government research program ``Investissements d'Avenir{''}
through the IMobS3 Laboratory of Excellence {[}ANR-10-LABX-16-01];
French government research program ``Investissements d'Avenir{''}
through the RobotEx Equipment of Excellence {[}ANR-10-EQPX-44]; European
Union through the Regional Competitiveness and Employment program
2014-2020 (ERDF - AURA region); AURA region},
  funding-text = {This work has been sponsored by the French government research program
``Investissements d'Avenir{''} through the IMobS3 Laboratory of
Excellence (ANR-10-LABX-16-01) and the RobotEx Equipment of Excellence
(ANR-10-EQPX-44), by the European Union through the Regional
Competitiveness and Employment program 2014-2020 (ERDF - AURA region)
and by the AURA region.},
  issn = {1380-7501},
  journal-iso = {Multimed. Tools Appl.},
  keywords = {Visual-based navigation; Computer vision for transportation; SLAM},
  keywords-plus = {EXPERIENCE},
  language = {English},
  number-of-cited-references = {50},
  research-areas = {Computer Science; Engineering},
  times-cited = {0},
  type = {Article; Early Access},
  unique-id = {WOS:000781124000002},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {0},
  web-of-science-categories = {Computer Science, Information Systems; Computer Science, Software
Engineering; Computer Science, Theory \& Methods; Engineering,
Electrical \& Electronic},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@inproceedings{latif-et-al:2012:6385879,
  author = {Y. Latif and C. Cadena and J. Neira},
  booktitle = {2012 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
(IROS)},
  title = {Realizing, Reversing, Recovering : Incremental Robust Loop Closing over
time using the iRRR algorithm},
  pages = {4211--4217},
  doi = {10.1109/IROS.2012.6385879},
  note = {25th IEEE\textbackslash{}RSJ International Conference on Intelligent
Robots and Systems (IROS), Algarve, PORTUGAL, OCT 07-12, 2012},
  publisher = {IEEE},
  address = {345 E 47TH ST, NEW YORK, NY 10017 USA},
  year = {2012},
  abstract = {The ability to reconsider information over time allows to detect
failures and is crucial for long term robust autonomous robot
applications. This applies to loop closure decisions in localization and
mapping systems. This paper describes a method to analyze all available
information up to date in order to robustly remove past incorrect loop
closures from the optimization process. The main novelties of our
algorithm are: 1. incrementally reconsidering loop closures and 2.
handling multi-session, spatially related or unrelated experiments. We
validate our proposal in real multi-session experiments showing better
results than those obtained by state of the art methods.},
  affiliation = {Latif, Y (Corresponding Author), Univ Zaragoza, I3A, Zaragoza 50018, Spain.
Latif, Yasir; Cadena, Cesar; Neira, Jose, Univ Zaragoza, I3A, Zaragoza 50018, Spain.},
  affiliations = {University of Zaragoza},
  author-email = {ylatif@unizar.es
ccadena@unizar.es
jneira@unizar.es},
  book-group-author = {IEEE
Robotics Society of Japan},
  cited-references = {Bar-Shalom Y., 2001, ESTIMATION APPL TRAC.
Cadena C, 2012, IEEE T ROBOT, V28, P871, DOI 10.1109/TRO.2012.2189497.
Cummins M, 2010, INT J ROBOT RES.
Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706.
Konolige K, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P1156, DOI 10.1109/IROS.2009.5354121.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Latif Y., 2012, P ROB SCI SYST.
McDonald J., 2011, P EUR C MOB ROB ECMR, P69.
Mei C, 2011, INT J COMPUT VISION, V94, P198, DOI 10.1007/s11263-010-0361-7.
Ranganathan A, 2011, INT J ROBOT RES, V30, P755, DOI 10.1177/0278364910393287.
RAWSEEDS, 2009, ROB ADV WEBP SENS EL.
Sibley G, 2010, INT J ROBOT RES, V29, P958, DOI 10.1177/0278364910369268.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.
Strasdat H, 2011, IEEE I CONF COMP VIS, P2352, DOI 10.1109/ICCV.2011.6126517.
Sunderhauf N, 2012, IEEE INT CONF ROBOT, P1254, DOI 10.1109/ICRA.2012.6224709.
Tully S., 2012, INT J ROBOT RES.},
  da = {2022-05-17},
  doc-delivery-number = {BEK21},
  isbn = {978-1-4673-1736-8},
  issn = {2153-0858},
  language = {English},
  number-of-cited-references = {16},
  oa = {Green Submitted},
  orcid-numbers = {Cadena, Cesar/0000-0002-2972-6011
Neira, Jose/0000-0003-0668-977X
Lerma, Cesar D Cadena/0000-0002-2972-6011
Latif, Yasir/0000-0002-2529-5322},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  researcherid-numbers = {Cadena, Cesar/AAM-4987-2020
Neira, Jose/F-8887-2013
Neira, Jose/AAM-6571-2020
Lerma, Cesar D Cadena/X-4739-2018},
  series = {IEEE International Conference on Intelligent Robots and Systems},
  times-cited = {17},
  type = {Proceedings Paper},
  unique-id = {WOS:000317042704119},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {7},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Conference Proceedings Citation Index - Science (CPCI-S)},
}

@article{latif-et-al:2017:016,
  author = {Y. Latif and G. Huang and J. Leonard and J. Neira},
  journal = {ROBOTICS AND AUTONOMOUS SYSTEMS},
  title = {Sparse optimization for robust and efficient loop closing},
  volume = {93},
  pages = {13--26},
  doi = {10.1016/j.robot.2017.03.016},
  publisher = {ELSEVIER},
  address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
  year = {2017},
  month = {7},
  abstract = {It is essential for a robot to be able to detect revisits or loop
closures for long-term visual navigation. A key insight explored in this
work is that the loop-closing event inherently occurs sparsely, i.e.,
the image currently being taken matches with only a small subset (if
any) of previous images. Based on this observation, we formulate the
problem of loop-closure detection as a sparse, convex l(1)-minimization
problem. By leveraging fast convex optimization techniques, we are able
to efficiently find loop closures, thus enabling real-time robot
navigation. This novel formulation requires no offline dictionary
learning, as required by most existing approaches, and thus allows
online incremental operation. Our approach ensures a unique hypothesis
by choosing only a single globally optimal match when making a
loop-closure decision. Furthermore, the proposed formulation enjoys a
flexible representation with no restriction imposed on how images should
be represented, while requiring only that the representations are
``close{''} to each other when the corresponding images are visually
similar. The proposed algorithm is validated extensively using
real-world datasets. (C) 2017 Elsevier B.V. All rights reserved.},
  affiliation = {Latif, Y (Corresponding Author), Univ Adelaide, ARC Ctr Robot Vis, Adelaide, SA 5005, Australia.
Latif, Yasir, Univ Adelaide, ARC Ctr Robot Vis, Adelaide, SA 5005, Australia.
Huang, Guoquan, Univ Delaware, Dept Mech Engn, Newark, DE 19716 USA.
Leonard, John, MIT, Comp Sci \& Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
Neira, Jose, Univ Zaragoza, I3A, Zaragoza, Spain.},
  affiliations = {University of Adelaide; University of Delaware; Massachusetts Institute
of Technology (MIT); University of Zaragoza},
  author-email = {yasir.latif@adelaide.edu.au},
  cited-references = {Amaldi E, 1998, THEOR COMPUT SCI, V209, P237, DOI 10.1016/S0304-3975(97)00115-1.
Asif M., 2008, PRIMAL DUAL PURSUIT.
Bach F, 2012, OPTIMIZATION FOR MACHINE LEARNING, P19.
Bengio Y, 2012, UNSUPERVISED TRANSF, V7, P19.
Boyd S., 2004, CONVEX OPTIMIZATION.
Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1\_56.
Cannell CJ, 2005, OCEANS-IEEE, P1514.
Capezio Francesco, 2009, Journal of Computing and Information Technology - CIT, V17, P95, DOI 10.2498/cit.1001180.
Casafranca JJ, 2013, IEEE INT C INT ROBOT, P17, DOI 10.1109/IROS.2013.6696326.
Casafranca J.J., 2013, ROB MULT INF FACT GR.
Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764.
Churchill W, 2013, INT J ROBOT RES, V32, P1645, DOI 10.1177/0278364913499193.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Dalal N., 2005, 2005 IEEE COMPUTER S, P886, DOI {[}10.1109/CVPR.2005.177, DOI 10.1109/CVPR.2005.177].
Donoho D., 2006, TECHNICAL REPORT.
Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132.
Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582.
Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969.
Elad M, 2010, P IEEE, V98, P972, DOI 10.1109/JPROC.2009.2037655.
Everingham M, 2012, PASCAL VISUAL OBJECT.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Kumar B., 2016, IEEE C COMP VIS PATT.
Latif Y., 2014, P ROB SCI SYST BERK.
Latif Y, 2014, IEEE INT C INT ROBOT, P2683, DOI 10.1109/IROS.2014.6942929.
Latif Y, 2013, INT J ROBOT RES, V32, P1611, DOI 10.1177/0278364913498910.
LeCun Y., 1995, HDB BRAIN THEORY NEU.
Lee JH, 2014, IEEE INT CONF ROBOT, P5550, DOI 10.1109/ICRA.2014.6907675.
Lee JH, 2013, IEEE INT CONF ROBOT, P3799, DOI 10.1109/ICRA.2013.6631111.
Lowry S, 2016, IEEE T ROBOT, V32, P1, DOI 10.1109/TRO.2015.2496823.
Malioutov D.M., 2005, IEEE INT C AC SPEECH.
Milford M, 2013, INT J ROBOT RES, V32, P766, DOI 10.1177/0278364913490323.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Nistr D., 2006, P IEEE COMP VIS PAT, V2, P2161.
Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724.
Paul R, 2013, INT J ROBOT RES, V32, P1742, DOI 10.1177/0278364913509859.
RAWSEEDS, 2009, FP6IST045144 RAWSEED.
Rosten E, 2005, IEEE I CONF COMP VIS, P1508.
Sfinderhauf N., 2015, P IEEE RJS INT C INT.
Shakeri M., 2015, ONLINE LOOP CLOSURE.
Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.
Stinderhauf N., 2015, VISUAL PLACE RECOGNI.
Sugiyama H., 2005, INT C COLL COMP NETW.
Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.
Zhang H, 2016, ROBOTICS: SCIENCE AND SYSTEMS XII.},
  da = {2022-05-17},
  doc-delivery-number = {EX2AJ},
  eissn = {1872-793X},
  funding-acknowledgement = {MINECO-FEDER {[}DPI2015-68905-P]; ONR {[}N0001410-1-0936,
N00014-11-1-0688, N00014-13-1-0588]; NSF {[}IIS-1318392, IIS-1566129];
DTRA {[}HDTRA 1-16-1-0039];  {[}BES-2010-033116];  {[}EEBB-I-13-07010];
Direct For Computer \& Info Scie \& Enginr {[}1566129] Funding Source:
National Science Foundation},
  funding-text = {This work was partially supported by the MINECO-FEDER project
DPI2015-68905-P, by the research grant BES-2010-033116, by the travel
grant EEBB-I-13-07010, by the ONR grants N0001410-1-0936,
N00014-11-1-0688 and N00014-13-1-0588, by the NSF awards IIS-1318392 and
IIS-1566129, and by the DTRA award HDTRA 1-16-1-0039.},
  issn = {0921-8890},
  journal-iso = {Robot. Auton. Syst.},
  keywords = {SLAM; Place recognition; Relocalization; Sparse optimization},
  keywords-plus = {PLACE RECOGNITION; VISION; NAVIGATION},
  language = {English},
  number-of-cited-references = {46},
  oa = {Green Submitted},
  orcid-numbers = {Huang, Guoquan/0000-0001-9932-0685
/0000-0002-8863-6550
Neira Parra, Jose/0000-0003-0668-977X},
  research-areas = {Automation \& Control Systems; Computer Science; Robotics},
  researcherid-numbers = {Neira, Jose/AAM-6571-2020
Huang, Guoquan/AAF-2094-2019
Neira Parra, Jose/F-8887-2013},
  times-cited = {10},
  type = {Article},
  unique-id = {WOS:000403027600002},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {21},
  web-of-science-categories = {Automation \& Control Systems; Computer Science, Artificial
Intelligence; Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{chen-et-al:2018:2859916,
  author = {Z. Chen and L. Liu and I. Sa and Z. Ge and C. Margarita},
  journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  title = {Learning Context Flexible Attention Model for Long-Term Visual Place
Recognition},
  volume = {3},
  number = {4},
  pages = {4015--4022},
  doi = {10.1109/LRA.2018.2859916},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
  address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
  year = {2018},
  month = {10},
  abstract = {Identifying regions of interest in an image has long been of great
importance in a wide range of tasks, including place recognition. In
this letter, we propose a novel attention mechanism with flexible
context, which can be incorporated into existing feed-forward network
architecture to learn image representations for long-term place
recognition. In particular, in order to focus on regions that contribute
positively to place recognition, we introduce a multiscale
context-flexible network to estimate the importance of each spatial
region in the feature map. Our model is trained end-to-end for place
recognition and can detect regions of interest of arbitrary shape.
Extensive experiments have been conducted to verify the effectiveness of
our approach and the results demonstrate that our model can achieve
consistently better performance than the state of the art on standard
benchmark datasets. Finally, we visualize the learned attention maps to
generate insights into what attention the network has learned.},
  affiliation = {Chen, ZT (Corresponding Author), Swiss Fed Inst Technol, Vis Robot Lab, CH-8092 Zurich, Switzerland.
Chen, Zetao; Chli, Margarita, Swiss Fed Inst Technol, Vis Robot Lab, CH-8092 Zurich, Switzerland.
Liu, Lingqiao, Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
Sa, Inkyu, Swiss Fed Inst Technol, Autonomous Syst Lab, CH-8092 Zurich, Switzerland.
Ge, Zongyuan, Monash Univ, ERes Ctr, Melbourne, Vic 3800, Australia.},
  affiliations = {ETH Zurich; University of Adelaide; ETH Zurich; Monash University},
  author-email = {chenze@ethz.ch
lingqiao.liu@adelaide.edu.au
inkyu.sa@mavt.ethz.ch
Zongyuan.Ge@monash.edu
chlim@ethz.ch},
  cited-references = {Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI {[}10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572].
Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161.
Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014.
Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396.
Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667.
Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856.
Chen Z., 2014, AUSTR C ROB AUT MELB.
Chen ZT, 2017, IEEE INT C INT ROBOT, P9.
Choi J, 2016, PROC CVPR IEEE, P4321, DOI 10.1109/CVPR.2016.468.
Chu Xiao, 2017, PROC CVPR IEEE, P1831, DOI DOI 10.1109/CVPR.2017.601.
CUMMINS M, 2009, P ROB SCI SYST SEATT.
Cummins M, 2011, INT J ROBOT RES, V30, P1100, DOI 10.1177/0278364910385483.
Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754.
Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135.
Girshick R, 2015, PROC CVPR IEEE, P437, DOI 10.1109/CVPR.2015.7298641.
Glover AJ, 2010, IEEE INT CONF ROBOT, P3507, DOI 10.1109/ROBOT.2010.5509547.
He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90.
Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336.
Kim HJ, 2017, PROC CVPR IEEE, P3251, DOI 10.1109/CVPR.2017.346.
Krizhevsky A, 2012, ADV NEURAL INFORM PR, P1097, DOI 10.1145/3065386.
Liu LQ, 2017, IEEE T PATTERN ANAL, V39, P2305, DOI 10.1109/TPAMI.2016.2637921.
Liu LQ, 2015, PROC CVPR IEEE, P4749, DOI 10.1109/CVPR.2015.7299107.
Lowry S, 2018, IEEE ROBOT AUTOM LET, V3, P957, DOI 10.1109/LRA.2018.2793308.
McManus C., 2014, P ROB SCI SYST.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mousavian A, 2015, IEEE INT CONF ROBOT, P4882, DOI 10.1109/ICRA.2015.7139877.
Naseer T, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564.
Neubert P, 2015, ROBOT AUTON SYST, V69, P15, DOI 10.1016/j.robot.2014.08.005.
Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374.
Siagian C, 2009, IEEE T ROBOT, V25, P861, DOI 10.1109/TRO.2009.2022424.
Simonyan K, 2014, C TRACK P.
Sunderhauf N., 2015, IEEE RSJ INT C INT R.
Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.
Szegedy Christian., 2014, PROC CVPR IEEE, V1409, P4842, DOI DOI 10.1109/CVPR.2015.7298594.
Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683.
Wang P, 2017, PROC CVPR IEEE, P6212, DOI 10.1109/CVPR.2017.658.
Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446.
Zetao Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3223, DOI 10.1109/ICRA.2017.7989366.
Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009.},
  da = {2022-05-17},
  doc-delivery-number = {GQ7PI},
  funding-acknowledgement = {Swiss National Science Foundation {[}PP00P2 157585]; EC's Horizon 2020
Programme {[}644128]; European Union's Horizon 2020 research and
innovation Programme {[}644227]; Swiss State Secretariat for Education,
Research and Innovation {[}15.0029]},
  funding-text = {This work was supported in part by the Swiss National Science Foundation
(Agreement PP00P2 157585), in part by the EC's Horizon 2020 Programme
under Grant Agreement 644128 (AEROWORKS), in part by the European
Union's Horizon 2020 research and innovation Programme under Grant
Agreement 644227 (Flourish), and in part by the Swiss State Secretariat
for Education, Research and Innovation under Contract 15.0029.},
  issn = {2377-3766},
  journal-iso = {IEEE Robot. Autom. Lett.},
  keywords = {Localization; deep learning in robotics and automation; visual-based
navigation},
  language = {English},
  number-of-cited-references = {39},
  oa = {Green Accepted},
  orcid-numbers = {Sa, Inkyu/0000-0001-5429-0515
Chli, Margarita/0000-0001-5611-7492
Ge, Zongyuan/0000-0002-5880-8673
Chen, Zetao/0000-0002-5596-5008},
  research-areas = {Robotics},
  times-cited = {40},
  type = {Article},
  unique-id = {WOS:000441935900004},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {8},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{hong-et-al:2022:02783649221080483,
  author = {Z. Hong and Y. Petillot and A. Wallace and S. Wang},
  journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
  title = {RadarSLAM: A robust simultaneous localization and mapping system for all
weather conditions},
  doi = {10.1177/02783649221080483},
  publisher = {SAGE PUBLICATIONS LTD},
  address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
  year = {2022},
  abstract = {A Simultaneous Localization and Mapping (SLAM) system must be robust to
support long-term mobile vehicle and robot applications. However, camera
and LiDAR based SLAM systems can be fragile when facing challenging
illumination or weather conditions which degrade the utility of imagery
and point cloud data. Radar, whose operating electromagnetic spectrum is
less affected by environmental changes, is promising although its
distinct sensor model and noise characteristics bring open challenges
when being exploited for SLAM. This paper studies the use of a Frequency
Modulated Continuous Wave radar for SLAM in large-scale outdoor
environments. We propose a full radar SLAM system, including a novel
radar motion estimation algorithm that leverages radar geometry for
reliable feature tracking. It also optimally compensates motion
distortion and estimates pose by joint optimization. Its loop closure
component is designed to be simple yet efficient for radar imagery by
capturing and exploiting structural information of the surrounding
environment. Extensive experiments on three public radar datasets,
ranging from city streets and residential areas to countryside and
highways, show competitive accuracy and reliability performance of the
proposed radar SLAM system compared to the state-of-the-art LiDAR,
vision and radar methods. The results show that our system is
technically viable in achieving reliable SLAM in extreme weather
conditions on the RADIATE Dataset, for example, heavy snow and dense
fog, demonstrating the promising potential of using radar for
all-weather localization and mapping.},
  affiliation = {Wang, S (Corresponding Author), Heriot Watt Univ, Sch Engn \& Phys Sci, Inst Sensors Signals \& Syst, Edinburgh EH14 4AS, Midlothian, Scotland.
Hong, Ziyang; Petillot, Yvan; Wallace, Andrew; Wang, Sen, Heriot Watt Univ, Edinburgh Ctr Robot, Edinburgh, Midlothian, Scotland.},
  affiliations = {Heriot Watt University},
  article-number = {02783649221080483},
  author-email = {s.wang@hw.ac.uk},
  cited-references = {Aldera R, 2019, IEEE INT C INTELL TR, P2835, DOI 10.1109/ITSC.2019.8917111.
Aldera R, 2019, IEEE INT CONF ROBOT, P1190, DOI 10.1109/ICRA.2019.8794014.
Aldibaja M, 2016, IEEE/SICE I S SYS IN, P212, DOI 10.1109/SII.2016.7844000.
Almalioglu Y, 2021, IEEE SENS J, V21, P3314, DOI 10.1109/JSEN.2020.3023243.
Bailo O, 2018, PATTERN RECOGN LETT, V106, P53, DOI 10.1016/j.patrec.2018.02.020.
Barnes D., 2020, C ROB LEARN, P303.
Barnes D, 2020, IEEE INT CONF ROBOT, P6433, DOI 10.1109/ICRA40945.2020.9196884.
Barnes D, 2020, IEEE INT CONF ROBOT, P9484, DOI 10.1109/ICRA40945.2020.9196835.
Behley J, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV.
Burnett K, 2021, IEEE ROBOT AUTOM LET, V6, P771, DOI 10.1109/LRA.2021.3052439.
Campos C., 2020, ORB SLAM3 ACCURATE O.
Carballo A., 2020, ARXIV PREPRINT ARXIV.
Cen SH, 2019, IEEE INT CONF ROBOT, P298, DOI 10.1109/ICRA.2019.8793990.
Cen SH, 2018, IEEE INT CONF ROBOT, P6045, DOI 10.1109/ICRA.2018.8460687.
CHALLIS JH, 1995, J BIOMECH, V28, P733, DOI 10.1016/0021-9290(94)00116-L.
Chandran M, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P808, DOI 10.1109/IROS.2006.281673.
Charron N, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P254, DOI 10.1109/CRV.2018.00043.
Checchin P, 2010, SPRINGER TRAC ADV RO, V62, P151.
Clark S, 1998, IEEE INT CONF ROBOT, P3697, DOI 10.1109/ROBOT.1998.681411.
De Martini D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20216002.
Dissanayake MWMG, 2001, IEEE T ROBOTIC AUTOM, V17, P229, DOI 10.1109/70.938381.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
Gadd Matthew, 2020, 2020 IEEE/ION Position, Location and Navigation Symposium (PLANS), P270, DOI 10.1109/PLANS46316.2020.9109951.
Gadd M., 2021, ARXIV PREPRINT ARXIV.
Garg K, 2004, PROC CVPR IEEE, P528.
He L, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P231, DOI 10.1109/IROS.2016.7759060.
Himstedt M, 2014, IEEE INT C INT ROBOT, P5030, DOI 10.1109/IROS.2014.6943277.
Holder M, 2019, IEEE INT VEH SYM, P1145, DOI 10.1109/IVS.2019.8813841.
Hong ZY, 2020, IEEE INT C INT ROBOT, P5164, DOI 10.1109/IROS45743.2020.9341287.
Howard A, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3946, DOI 10.1109/IROS.2008.4651147.
Huang HY, 2019, IEEE INT C INTELL TR, P1290, DOI 10.1109/ITSC.2019.8916977.
Jokela M, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9112341.
Jose E., 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P3087, DOI 10.1109/IROS.2005.1545232.
Kim G, 2020, IEEE INT CONF ROBOT, P6246.
Konc J, 2007, MATCH-COMMUN MATH CO, V58, P569.
Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607.
Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299.
Lucas BD, 1981, ITERATIVE IMAGE REGI.
Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498.
Marck JW, 2013, EUROP RADAR CONF, P471.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Ort T, 2020, IEEE ROBOT AUTOM LET, V5, P3267, DOI 10.1109/LRA.2020.2976310.
Park YS, 2020, IEEE INT CONF ROBOT, P2617, DOI 10.1109/ICRA40945.2020.9197231.
Park YS, 2019, IEEE INT C INT ROBOT, P1307, DOI 10.1109/IROS40897.2019.8967633.
Porav H, 2019, IEEE INT CONF ROBOT, P7087, DOI 10.1109/ICRA.2019.8793486.
Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729.
Ren WH, 2017, PROC CVPR IEEE, P2838, DOI 10.1109/CVPR.2017.303.
Rouveure R., 2009, INT RAD C SURV SAF W, P1.
Saftescu S, 2020, IEEE INT CONF ROBOT, P4358, DOI 10.1109/ICRA40945.2020.9196682.
Schubert D, 2018, LECT NOTES COMPUT SC, V11212, P699, DOI 10.1007/978-3-030-01237-3\_42.
Schuster F, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2559, DOI 10.1109/ITSC.2016.7795967.
Shan TX, 2020, IEEE INT C INT ROBOT, P5135, DOI 10.1109/IROS45743.2020.9341176.
Shan TX, 2018, IEEE INT C INT ROBOT, P4758, DOI 10.1109/IROS.2018.8594299.
Sheeny M., 2021, IEEE INT C ROB AUT I.
Sola J., 2018, ABS181201537 CORR.
Tang TY, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI.
Tang TYQ, 2020, IEEE ROBOT AUTOM LET, V5, P1087, DOI 10.1109/LRA.2020.2965907.
Vivet D, 2013, INT J ADV ROBOT SYST, V10, DOI 10.5772/56636.
Vivet D, 2012, IEEE INT CONF ROBOT, P2618, DOI 10.1109/ICRA.2012.6224573.
Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2\_11.
Yamada M, 2019, IEEE INT C INTELL TR, P293, DOI 10.1109/ITSC.2019.8917241.
Yoneda K, 2018, IEEE INT VEH SYM, P971, DOI 10.1109/IVS.2018.8500378.
Zhang C, 2018, IEEE INT C INT ROBOT, P3409, DOI 10.1109/IROS.2018.8593703.
Zhang J., 2014, ROBOT SCI SYST, V2, P9.
Zhang ZC, 2018, IEEE INT C INT ROBOT, P7244, DOI 10.1109/IROS.2018.8593941.},
  da = {2022-05-17},
  doc-delivery-number = {0U4LQ},
  earlyaccessdate = {APR 2022},
  eissn = {1741-3176},
  funding-acknowledgement = {EPSRC Robotics and Artificial Intelligence ORCA Hub {[}EP/R026173/1]; EU
H2020 Programme under EUMarineRobots project {[}ID 731103]},
  funding-text = {The author(s) disclosed receipt of the following financial support for
the research, authorship, and/or publication of this article: This work
was supported by EPSRC Robotics and Artificial Intelligence ORCA Hub
(grant No. EP/R026173/1) and EU H2020 Programme under EUMarineRobots
project (grant ID 731103).},
  issn = {0278-3649},
  journal-iso = {Int. J. Robot. Res.},
  keywords = {radar sensing; simultaneous localization and mapping; all-weather
perception},
  keywords-plus = {SLAM; VERSATILE},
  language = {English},
  number-of-cited-references = {66},
  oa = {hybrid},
  research-areas = {Robotics},
  times-cited = {0},
  type = {Article; Early Access},
  unique-id = {WOS:000787623300001},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {1},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{pan-et-al:2019:s19194252,
  author = {Z. Pan and H. Chen and S. Li and Y. Liu},
  journal = {SENSORS},
  title = {ClusterMap Building and Relocalization in Urban Environments for
Unmanned Vehicles},
  volume = {19},
  number = {19},
  pages = {4252},
  doi = {10.3390/s19194252},
  publisher = {MDPI},
  address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  year = {2019},
  month = {10},
  abstract = {Map building and map-based relocalization techniques are important for
unmanned vehicles operating in urban environments. The existing
approaches require expensive high-density laser range finders and suffer
from relocalization problems in long-term applications. This study
proposes a novel map format called the ClusterMap, on the basis of which
an approach to achieving relocalization is developed. The ClusterMap is
generated by segmenting the perceived point clouds into different point
clusters and filtering out clusters belonging to dynamic objects. A
location descriptor associated with each cluster is designed for
differentiation. The relocalization in the global map is achieved by
matching cluster descriptors between local and global maps. The solution
does not require high-density point clouds and high-precision
segmentation algorithms. In addition, it prevents the effects of
environmental changes on illumination intensity, object appearance, and
observation direction. A consistent ClusterMap without any scale problem
is built by utilizing a 3D visual-LIDAR simultaneous localization and
mapping solution by fusing LIDAR and visual information. Experiments on
the KITTI dataset and our mobile vehicle illustrates the effectiveness
of the proposed approach.},
  affiliation = {Chen, HY (Corresponding Author), Harbin Inst Technol Shenzhen, Sch Mech Engn \& Automat, Shenzhen 518055, Peoples R China.
Chen, HY (Corresponding Author), Shenzhen Univ Town, Bldg G1011, Shenzhen 518055, Peoples R China.
Pan, Zhichen; Chen, Haoyao; Li, Silin, Harbin Inst Technol Shenzhen, Sch Mech Engn \& Automat, Shenzhen 518055, Peoples R China.
Liu, Yunhui, Chinese Univ Hong Kong, Dept Mech \& Automat Engn, Hong Kong, Peoples R China.
Pan, Zhichen; Chen, Haoyao; Li, Silin, Shenzhen Univ Town, Bldg G1011, Shenzhen 518055, Peoples R China.
Liu, Yunhui, Chinese Univ Hong Kong, Shatin, Room 208,William MW Mong Engn Bldg, Hong Kong, Peoples R China.},
  affiliations = {Harbin Institute of Technology; Chinese University of Hong Kong;
University Town of Shenzhen; Chinese University of Hong Kong},
  article-number = {4252},
  author-email = {zhchpan@163.com
hychen5@hit.edu.cn
lisilin013@163.com
yhliu@mae.cuhk.edu.hk},
  cited-references = {Bogoslavskyi I, 2017, PFG-J PHOTOGRAMM REM, V85, P41, DOI 10.1007/s41064-016-0003-y.
Bosse M, 2013, IEEE INT CONF ROBOT, P2677, DOI 10.1109/ICRA.2013.6630945.
Brenneke C, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P188.
Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754.
Dhall A., 2017, ARXIV170509785.
Dube R., 2016, ARXIV160907720.
Dube R., 2018, P ROB SCI SYST RSS P.
Engel J., 2017, EUR C COMP VIS ECCV, P834.
Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577.
Finman R., 2015, P ICRA WORKSH VIS PL.
Gawel A, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P182, DOI 10.1109/IROS.2016.7759053.
Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297.
Grisetti G, 2007, IEEE T ROBOT, V23, P34, DOI 10.1109/TRO.2006.889486.
Haoyao Chen, 2019, Assembly Automation, V39, P297, DOI 10.1108/AA-04-2018-065.
Hess W, 2016, IEEE INT CONF ROBOT, P1271, DOI 10.1109/ICRA.2016.7487258.
Lenac K, 2017, ROBOT AUTON SYST, V92, P197, DOI 10.1016/j.robot.2017.03.013.
Li H., 2017, ARXIV171105805.
Lin LS, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19153410.
Lynen S, 2013, IEEE INT C INT ROBOT, P3923, DOI 10.1109/IROS.2013.6696917.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671.
Opromolla R, 2016, INT CONF UNMAN AIRCR, P649, DOI 10.1109/ICUAS.2016.7502580.
Pfrunder A, 2017, IEEE INT C INT ROBOT, P2601, DOI 10.1109/IROS.2017.8206083.
Rusu R.B., 2009, IEEE INT C ROB AUT I, P3212, DOI DOI 10.1109/R0B0T.2009.5152473.
Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967.
Scaramuzza D, 2011, IEEE ROBOT AUTOM MAG, V18, P80, DOI 10.1109/MRA.2011.943233.
Schauwecker K, 2014, IEEE INT CONF ROBOT, P6102, DOI 10.1109/ICRA.2014.6907758.
Shehata, 2017, ARXIV PREPRINT ARXIV.
Wang HS, 2017, IEEE T IND ELECTRON, V64, P2893, DOI 10.1109/TIE.2016.2631514.
Wang L, 2017, IFAC PAPERSONLINE, V50, P276, DOI 10.1016/j.ifacol.2017.08.046.
Zhang J, 2018, J FIELD ROBOT, V35, P1242, DOI 10.1002/rob.21809.
Zhang J, 2017, AUTON ROBOT, V41, P401, DOI 10.1007/s10514-016-9548-2.
Zhang J, 2017, AUTON ROBOT, V41, P31, DOI {[}10.1007/s10514-015-9525-1, 10.1109/MWSYM.2015.7167049].
Zhu ZL, 2015, PROCEEDINGS OF THE 31ST INTERNATIONAL CONFERENCE ON COMPUTER ANIMATION AND SOCIAL AGENTS (CASA 2016), P53, DOI 10.1145/3205326.3205357.},
  da = {2022-05-17},
  doc-delivery-number = {JK4OO},
  eissn = {1424-8220},
  funding-acknowledgement = {National Natural Science Foundation of China {[}61673131, U1713206,
U1613218]; Bureau of Industry and Information Technology of Shenzhen
{[}20170505160946600]},
  funding-text = {This work was partially supported by grants from the National Natural
Science Foundation of China (Reference No. 61673131, U1713206 and
U1613218) and the Bureau of Industry and Information Technology of
Shenzhen (Reference No. 20170505160946600).},
  journal-iso = {Sensors},
  keywords = {relocalization; SLAM; Localization; Map Descriptor; LIDAR-based Map
Building; ClusterMap},
  keywords-plus = {SLAM; LOCALIZATION; ODOMETRY; VISION},
  language = {English},
  number-of-cited-references = {34},
  oa = {Green Published, gold},
  orcid-numbers = {Chen, Haoyao/0000-0003-1652-9681},
  research-areas = {Chemistry; Engineering; Instruments \& Instrumentation},
  times-cited = {2},
  type = {Article},
  unique-id = {WOS:000494823200196},
  usage-count-last-180-days = {1},
  usage-count-since-2013 = {6},
  web-of-science-categories = {Chemistry, Analytical; Engineering, Electrical \& Electronic;
Instruments \& Instrumentation},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{WOS:000756469700001,
  author = {Z. Xing and X. Zhu and D. Dong},
  journal = {JOURNAL OF FIELD ROBOTICS},
  title = {DE-SLAM: SLAM for highly dynamic environment},
  doi = {10.1002/rob.22062},
  publisher = {WILEY},
  address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  abstract = {Simultaneous localization and mapping (SLAM) is crucial for autonomous
mobile robots. Most of the current SLAM systems are based on an
assumption: the environment is static. However, the real environment is
full of dynamic elements, such as pedestrians or vehicles, as well as
changes in illumination and appearance over time. In this paper,
DE-SLAM, a visual SLAM system that can deal with short-term and
long-term dynamic elements at the same time is proposed. A novel dynamic
detection and tracking module that utilizes both semantic and metric
information is proposed, and the localization accuracy is highly
improved by eliminating features falling on the dynamic objects. A
unified loop detection, loop check and global optimization module is
used to perform loop closure. Experimental results on datasets and real
environments show that DE-SLAM outperforms other state-of-the-art SLAM
systems in dynamic environments.},
  affiliation = {Zhu, XR (Corresponding Author), Harbin Inst Technol, Sch Mech Engn \& Automat, Shenzhen 518000, Peoples R China.
Xing, Zhiwei; Zhu, Xiaorui; Dong, Dingcheng, Harbin Inst Technol, Sch Mech Engn \& Automat, Shenzhen 518000, Peoples R China.},
  affiliations = {Harbin Institute of Technology},
  author-email = {zhuxiaorui@hotmail.com},
  cited-references = {Arroyo R, 2016, 2016 IEEE 19TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P965, DOI 10.1109/ITSC.2016.7795672.
Bescos B, 2018, IEEE ROBOT AUTOM LET, V3, P4076, DOI 10.1109/LRA.2018.2860039.
Brasch N, 2018, IEEE INT C INT ROBOT, P393, DOI 10.1109/IROS.2018.8593828.
Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033.
Chancan M, 2020, IEEE ROBOT AUTOM LET, V5, P993, DOI 10.1109/LRA.2020.2967324.
Dai WC, 2022, IEEE T PATTERN ANAL, V44, P373, DOI 10.1109/TPAMI.2020.3010942.
Dalal N., 2005, IEEE COMP SOC C COMP.
Falliat D, 2007, IEEE INT CONF ROBOT, P3921.
Gao P, 2020, IEEE INT CONF ROBOT, P1070, DOI 10.1109/ICRA40945.2020.9196906.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Henein M, 2020, IEEE INT CONF ROBOT, P2123.
Hou Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, P2238, DOI 10.1109/ICInfA.2015.7279659.
Klein George, 2007, P1.
Lianos KN, 2018, LECT NOTES COMPUT SC, V11208, P246, DOI 10.1007/978-3-030-01225-0\_15.
Merrill N, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV.
Milford MJ, 2012, IEEE INT CONF ROBOT, P1643, DOI 10.1109/ICRA.2012.6224623.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474.
Stenborg E, 2018, IEEE INT CONF ROBOT, P6484, DOI 10.1109/ICRA.2018.8463150.
Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012.
Tan W, 2013, INT SYM MIX AUGMENT, P209, DOI 10.1109/ISMAR.2013.6671781.
Tzoumas V, 2019, IEEE INT C INT ROBOT, P5383, DOI 10.1109/IROS40897.2019.8968174.
Wang YB, 2014, I C CONT AUTOMAT ROB, P1841, DOI 10.1109/ICARCV.2014.7064596.
Yang H, 2020, IEEE ROBOT AUTOM LET, V5, P1127, DOI 10.1109/LRA.2020.2965893.
Yu C, 2018, IEEE INT C INT ROBOT, P1168, DOI 10.1109/IROS.2018.8593691.
Zhang TW, 2020, IEEE INT CONF ROBOT, P7322.
Zhao C., 2014, P IEEE INT C MULT EX, P1.
Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009.},
  da = {2022-05-17},
  doc-delivery-number = {ZA9IT},
  earlyaccessdate = {FEB 2022},
  eissn = {1556-4967},
  funding-acknowledgement = {National Key R\&D Program of China {[}2018YFB1305500]; Natural Science
Foundation of China {[}U1813219]},
  funding-text = {This study was supported by the National Key R\&D Program of China under
grant 2018YFB1305500 and the Natural Science Foundation of China under
Grant U1813219.},
  issn = {1556-4959},
  journal-iso = {J. Field Robot.},
  keywords = {localization; SLAM},
  language = {English},
  number-of-cited-references = {28},
  research-areas = {Robotics},
  times-cited = {0},
  type = {Article; Early Access},
  unique-id = {WOS:000756469700001},
  usage-count-last-180-days = {58},
  usage-count-since-2013 = {58},
  web-of-science-categories = {Robotics},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}

@article{yang-et-al:2021:12054,
  author = {Z. Yang and Y. Pan and L. Deng and Y. Xie and R. Huan},
  journal = {IET INTELLIGENT TRANSPORT SYSTEMS},
  title = {PLSAV: Parallel loop searching and verifying for loop closure detection},
  volume = {15},
  number = {5},
  pages = {683--698},
  doi = {10.1049/itr2.12054},
  publisher = {WILEY},
  address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
  year = {2021},
  month = {5},
  abstract = {Visual simultaneous localization and mapping (vSLAM), one of the most
important applications in autonomous vehicles and robots to estimate the
position and pose using inexpensive visual sensors, suffers from error
accumulation for long-term navigation without loop closure detection.
Recently, deep neural networks (DNNs) are leveraged to achieve high
accuracy for loop closure detection, however the execution time is much
slower than those employing handcrafted visual features. In this paper,
a parallel loop searching and verifying method for loop closure
detection with both high accuracy and high speed, which combines two
parallel tasks using handcrafted and DNN features, respectively, is
proposed. A fast loop searching is proposed to link the bag-of-words
features and histogram for higher accuracy, and it splits the images
into multiple grids for high parallelism; meanwhile, a DNN feature
extractor is utilized for further verification. A loop state control
method based on a finite state machine to control these tasks is
designed, wherein the loop closure detection is described as a
context-related procedure. The framework is implemented on a real
machine, and the top-2 best accuracy and fastest execution time of
80-543 frames per second (min: 1.84ms, and max: 12.45ms) are achieved on
several public benchmarks compared with some existing algorithms.},
  affiliation = {Pan, Y (Corresponding Author), Zhejiang Univ, 1713 Yuquan Campus,38 Zheda Rd, Hangzhou 310027, Peoples R China.
Yang, Zhe; Pan, Yun, Zhejiang Univ, Coll Informat Sci \& Elect Engn, Hangzhou, Peoples R China.
Deng, Lei; Xie, Yuan, Univ Calif Santa Barbara, Dept Elect \& Comp Engn, Santa Barbara, CA 93106 USA.
Huan, Ruohong, Zhejiang Univ Technol, Coll Comp Sci \& Technol, Hangzhou, Peoples R China.},
  affiliations = {Zhejiang University; University of California System; University of
California Santa Barbara; Zhejiang University of Technology},
  author-email = {panyun@zju.edu.cn},
  cited-references = {Abadi M., 2016, ARXIV 160304467.
Angeli A, 2008, IEEE T ROBOT, V24, P1027, DOI 10.1109/TRO.2008.2004514.
Arroyo R, 2015, IEEE INT CONF ROBOT, P6328, DOI 10.1109/ICRA.2015.7140088.
Bampis L, 2018, CONCURR COMP-PRACT E, V30, DOI 10.1002/cpe.4146.
Bampis L, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4530, DOI 10.1109/IROS.2016.7759667.
Bianco S, 2018, IEEE ACCESS, V6, P64270, DOI 10.1109/ACCESS.2018.2877890.
Blanco JL, 2009, AUTON ROBOT, V27, P327, DOI 10.1007/s10514-009-9138-7.
Cadena C, 2012, IEEE T ROBOT, V28, P871, DOI 10.1109/TRO.2012.2189497.
Ceriani S, 2009, AUTON ROBOT, V27, P353, DOI 10.1007/s10514-009-9156-5.
Chen JB, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P371, DOI 10.1109/ICIVC.2017.7984580.
Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961.
Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2\_54.
Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585.
Galvez-Lopez D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158.
Garcia-Fidalgo Emilio, 2018, IEEE Robotics and Automation Letters, V3, P3051, DOI 10.1109/LRA.2018.2849609.
Garcia-Fidalgo E, 2017, IEEE T ROBOT, V33, P1061, DOI 10.1109/TRO.2017.2704598.
Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074.
Hou Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, P2238, DOI 10.1109/ICInfA.2015.7279659.
Kejriwal N, 2016, ROBOT AUTON SYST, V77, P55, DOI 10.1016/j.robot.2015.12.003.
Khan S, 2015, IEEE INT CONF ROBOT, P5441, DOI 10.1109/ICRA.2015.7139959.
Kim, 2014, ADV INTELLIGENT SYST, P113.
Kim S, 2018, TENCON IEEE REGION, P1269, DOI 10.1109/TENCON.2018.8650164.
Klein George, 2007, P1.
Labbe M, 2013, IEEE T ROBOT, V29, P734, DOI 10.1109/TRO.2013.2242375.
Li LH, 2018, IEEE INT VEH SYM, P965, DOI 10.1109/IVS.2018.8500714.
Li YC, 2017, IEEE ACCESS, V5, P13835, DOI 10.1109/ACCESS.2017.2725387.
Liu K, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P8034.
Milford M., 2014, 2014 IFIP NETW C, P1.
Milford M, 2014, IEEE INT CONF ROBOT, P5571, DOI 10.1109/ICRA.2014.6907678.
Mirowski P, 2013, INT C IND POS IND NA, P1.
Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103.
Mur-Artal R, 2014, IEEE INT CONF ROBOT, P846, DOI 10.1109/ICRA.2014.6906953.
Muresan MP, 2019, INT C INTELL COMP CO, P11, DOI 10.1109/ICCP48234.2019.8959552.
Naseer T, 2018, IEEE T ROBOT, V34, P289, DOI 10.1109/TRO.2017.2788045.
Neubert P, 2016, IEEE ROBOT AUTOM LET, V1, P484, DOI 10.1109/LRA.2016.2517824.
Newman P, 2006, IEEE INT CONF ROBOT, P1180, DOI 10.1109/ROBOT.2006.1641869.
Pang, 2020, ARXIV200606664.
Ponce J., 2006, P IEEE C COMP VIS PA, V2, P2169, DOI DOI 10.1109/CVPR.2006.68.
Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544.
Shahbazi H, 2011, IEEE INT C INT ROBOT, P1228, DOI 10.1109/IROS.2011.6048862.
Shin DW, 2019, J ELECTRON IMAGING, V28, DOI 10.1117/1.JEI.28.1.013014.
Siam Sayem Mohammad, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5702, DOI 10.1109/ICRA.2017.7989671.
Simonyan K., 2015, 3 INT C LEARNING REP.
Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911.
Sunderhauf N, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI.
Vlaminck M, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19010023.
Williams B, 2009, ROBOT AUTON SYST, V57, P1188, DOI 10.1016/j.robot.2009.06.010.
Xu YH, 2020, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR42600.2020.00682.
Yang Z, 2021, IET INTELL TRANSP SY, V15, P683, DOI 10.1049/itr2.12054.
Yang Z, 2019, ELECTRON LETT, V55, P931, DOI 10.1049/el.2019.1148.
Yin JW, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION ENGINEERING (ICRAE), P73, DOI 10.1109/ICRAE.2017.8291356.
Yu C, 2018, IEEE INT C INT ROBOT, P1168, DOI 10.1109/IROS.2018.8593691.
Zhang K, 2017, CHIN AUTOM CONGR, P7916, DOI 10.1109/CAC.2017.8244215.
Zhang XW, 2019, J INTELL ROBOT SYST, V95, P389, DOI 10.1007/s10846-018-0917-2.
Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009.},
  da = {2022-05-17},
  doc-delivery-number = {RH1FM},
  earlyaccessdate = {MAR 2021},
  eissn = {1751-9578},
  funding-acknowledgement = {International Cooperation Research for Doctoral Students in Zhejiang
University; Zhejiang Provincial Natural Science Foundation of China
{[}LY19F020032]; Zhejiang ProvincialKey Research andDevelopment Program
of China {[}2021C03027]},
  funding-text = {International Cooperation Research for Doctoral Students in Zhejiang
University; Zhejiang Provincial Natural Science Foundation of China,
Grant/Award Number: LY19F020032; Zhejiang ProvincialKey Research
andDevelopment Program of China, Grant/Award Number: 2021C03027},
  issn = {1751-956X},
  journal-iso = {IET Intell. Transp. Syst.},
  keywords-plus = {PLACE RECOGNITION; LOCALIZATION; APPEARANCE; BAGS},
  language = {English},
  number-of-cited-references = {55},
  oa = {gold},
  orcid-numbers = {Yang, Zhe/0000-0001-7246-0012},
  research-areas = {Engineering; Transportation},
  times-cited = {7},
  type = {Article},
  unique-id = {WOS:000627604600001},
  usage-count-last-180-days = {4},
  usage-count-since-2013 = {5},
  web-of-science-categories = {Engineering, Electrical \& Electronic; Transportation Science \&
Technology},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
}
