\section{Discussion}
\label{sec:discussion}

The main goal of this review is to synthesize methodologies focused on long-term localization and mapping. Therefore, the discussion first analyzes the techniques proposed in the 142 included works for the five categories of DE1 (see Section~\ref{sec:methodology:data}). Section~\ref{sec:discussion:appearance} discusses methodologies related to dealing with the varying appearance of environments for localization and place recognition. Section~\ref{sec:discussion:dynamics} analyzes works focused on modeling the environment dynamics or identifying dynamic objects within the environment. Section~\ref{sec:discussion:sparsify} focuses on approaches for removing redundant data from the map or identifying novelty data to keep the map size constrained to the environment size. Section~\ref{sec:discussion:multisession} discusses how methods handle multi-session in terms of mapping. Section~\ref{sec:discussion:computational} reviews works related to computation concerns over long-term localization and mapping, in addition to the ones relative to map sparsification discussed in Section~\ref{sec:discussion:sparsify}. However, the discussion should also focus on how the included works evaluated their results in long-term operations. Thus, Section~\ref{sec:discussion:metrics} presents the evaluation metrics used in the experiments, and Section~\ref{sec:discussion:observations} analyzes the experimental data and datasets used for evaluating the proposed methodologies.





\subsection{Appearance variance}
\label{sec:discussion:appearance}

Next, the discussion focuses on included works categorized in DE1 as appearance. The different methodologies found in these works deal with variable lighting changes, perspective or viewpoint variance, moving elements in the scene, different weather conditions, or changes caused by the year's seasons.
In order to improve the discussion, the analysis of the proposed techniques related with appearance invariance is organized into the following topics: experience maps for treating different appearances as multiples experiences, handcrafted features, features extracted using Convolutional Neural Networks (CNN), assessment of feature stability, multi-modal features, leverage of temporal coherence by image sequence matching, and a discussion of the different sensors modalities used in the included works for appearance invariance.



\subsubsection{Experience maps}
\label{sec:discussion:appearance:exp-maps}

One way to deal with the appearance variance of environments is by treating different conditions as multiple experiences.
The biologically inspired RatSLAM~\parencite{ball-et-al:2013:9} introduces the experience map as a semi-metric topological map, where each experience is a view of the environment at a certain position and wheel odometry provides the relative pose for the links. New experiences are created when none of the previous ones saved in the map are sufficiently similar in appearance to the current scene.
\cite{glover-et-al:2010:5509547} combines the mapping of RatSLAM with the place recognition of FAB-MAP~\parencite{discussion:fab-map}. The latter improves the loop closure detection of the original RatSLAM due to FAB-MAP having light invariant characteristics for data association by learning a generative model for the Bag of Words (BoW) model~\parencite{discussion:bow}.
Both RatSLAM and the hybrid RatSLAM+FAB-MAP systems uses visual data to retrieve information from the environment.
Although \cite{martini-et-al:2020:s20216002} uses also experience-based mapping, the main sensor is a radar, where an experience is represented by a point cloud from the sensor and the point descriptors retrieved from it. Radar is known for being less affected by environment changes such as different illumination or weather conditions compared to vision sensors~\parencite{hong-et-al:2022:02783649221080483}.

The concept of adding the environment changes to the map identified by the degradation in localization is also employed by \cite{konolige-bowman:2009:5354121} and \cite{tang-et-al:2019:7}.
The former implements a keyframe SLAM system created from the Visual Odometry (VO) module, where each keyframe represents a view of the environment, while a place recognition module tries to match the current frame to similar views already in the map for loop closure.
The latter applies a similar idea to experience maps based on the 2D manifold assumption for locally smooth navigation. Even though the proposed topological local-metric framework encodes geometric information in the edges, the nodes do not require global pose, i.e., no restriction for global consistency. New nodes are trigered either from localization failure or after a certain length is traveled by the robot. The goal is to restrict the erroneous alignment computed from odometry locally.

Instead of considering an experience as a location or a view of the current scene, \cite{churchill-newman:2013:0278364913499193} defines it as a whole sequence of the saved poses and related features directly obtained from VO. In this case, the topological mapping links experiences not geometrically but instead if two experiences observe the same space. However, the method does not implement a specific place recognition module for loop closure, assuming that the robot will subsequently return to a place that can have successful localization.
\cite{gadd-newman:2016:7759843} builds on the work of \cite{churchill-newman:2013:0278364913499193} for multi-robot systems. This method adds FAB-MAP for place recognition in the existing map maintained by a centralized versioning framework. The selection of the most relevant experiences by the centralized framework for localizing multiple agents in the system assumes that appearance change is only driven by the passage of day time.

Another example of experience maps is Visual Teach \& Repeat systems using spatial-temporal pose graphs, as implemented in \cite{mactavish-et-al:2018:21838} and \cite{zhang-et-al:2018:8460674}.
Similar to \cite{churchill-newman:2013:0278364913499193}, an experience is the output of the VO module defining the appearance of a scene throughout a path. In the teaching phase, the robot is teleoperated by humans creating privileged experiences in the graph.
Autonomous experiences are the ones relative to the repetition phase. These experiences are linked either temporally or spatially if they are sequential in time or related metrically by multi-experience matching, respectively.
Unlike \cite{churchill-newman:2013:0278364913499193}, new experiences have a known metric pose relative to the others in the pose graph.

In general, experience-based navigation methods try to generate new experiences if the environment changes, expecting that at a certain point in time the robot will be able to localize itself relative to previous experiences, not requiring new ones to be added to the map.
However, these approaches are not scalable in the long-term timeframe nor to deal with dynamic elements, even using central servers as in \cite{gadd-newman:2016:7759843} with more computational resources than the robots.
Pruning algorithms would be required to remove redundant or outdated information, as in \cite{konolige-bowman:2009:5354121} or \cite{tang-et-al:2019:7}.
Also, other methods should be employed to deal not only with long-term appearance changes (weather conditions or seasonal changes) but also with dynamic elements in the scene.



\subsubsection{Illumination transformations}
\label{sec:discussion:appearance:illumination}

As a preprocessing step, illumination invariant transformations can be applied to color images for increasing the robustness of visual localization to changing lighting conditions and shadows.
One example is the illumination invariant space that combines the log-responses of the 3 color channels into an one-dimensional space with a weighting parameter conditioned by the peak spectral responses of each channel, usually available in the camera specifications. This one-dimensional space is only dependent on the sensor and elements in the scene, while being independent of the intensities and colors.
Both works of \cite{arroyo-et-al:2018:7} and \cite{yang-et-al:2021:12054} uses this transformation for preprocessing the color images into grayscale ones demonstrating the robustness of the illumination invariant space when lighting changes appear.

An alternative to using predefined illumination invariant transformations is to learn them.
\cite{clement-et-al:2020:2967659} learns a nonlinear transformation mapping function from the RGB color space to grayscale also combining the three-channel log-responses, but relaxing the constraints of the one-dimensional space due to the original weighting parameter used in \cite{arroyo-et-al:2018:7} and \cite{yang-et-al:2021:12054}. Instead of using the same parameters independently of the image content, \cite{clement-et-al:2020:2967659} trains an encoder to predict the optimal transformation weighting parameters of the three-channel log-responses.
The objective function chosen for maximization is based on the number of inlier feature matches from a vision localization pipeline.
The learned nonlinear RGB to grayscale transformation helped achieving a full-day cycle using a single mapping experience and the applying the optimized transformation to the color images.

Even though the Gamma correction does not transform an image to an invariant color space, this transformation can be used to strengthen low-illumination changes. \cite{sun-et-al:2021:9635886} uses the Gamma transform to synthesize low-illumination night-time images from daytime ones. Applying the transformation in the HSV (Hue, Saturation, Value) space, the gamma parameter adjusts the value channel without distorting the colors. Then, the synthesized images are used for training the DarkPoint descriptor proposed by \cite{sun-et-al:2021:9635886} to improve day-to-night matching.



\subsubsection{Handcrafted features}
\label{sec:discussion:appearance:handcrafted}

Many localization and mapping algorithms rely on detection and extraction of features. The designation of handcrafted features refers to properties derived from the sensors data as a two-step process: a keypoint detector to locate the features and their characterization by computing a descriptor capable of distinguishing each feature from the others~\parencite{discussion:handcrafted-features}.
Algorithms for long-term localization and mapping using handcrafted feature should be robust to changing conditions such as illumination, appearance, weather and seasonal changes.


\paragraph{Visual features}

A way to improve long-term feature-based visual localization is to enhance the descriptiveness of visual feature descriptors and their long-term stability.
\cite{kawewong-et-al:2013:826410} defines the Position Invariant Robust Features (PIRF). In a sliding window framework, PIRF tracks the motion of local features such as Scale-Invariant Feature Transform (SIFT) or Speeded Up Robust Features (SURF) selecting the stable ones.
Using an incremental tree-like PIRF (with inverted index as in BoW) dictionary, the method has shown robustness to viewpoint variance and unstable features. Also, PIRF-based localization improved the recall over FAB-MAP in the experiments.

Moreover, Histogram of Oriented Gradients (HOG) features have been used in different works to improve robustness to appearance variance, given that HOG descriptors capture local gradient information robust to seasonal changes \parencite{naseer-et-al:2015:7324181}.
\cite{li-et-al:2015:7139706} computes local HOG descriptors from visually-salient image patch features in an underwater environment. Using a trained Support-Vector Machine (SVM) to classify the matching between corresponding patches, the method achieved approximately 80\% accuracy with dramatic appearance changes.
Although \cite{naseer-et-al:2015:7324181} computes HOG descriptors from each cell of a partitioned image, a global descriptor for the whole image joins all the cell ones. The global descriptor proved to be robust to foliage color changes, occlusions, and seasonal changes.
\cite{vysotska-et-al:2015:7139576} uses the same global HOG descriptor as in \cite{naseer-et-al:2015:7324181}, but applied to image sequence matching requiring a rough global pose estimation for the images (e.g., GPS) for efficient matching.

Local Difference Binary (LDB) features also include gradient comparisons. These features are used in the Able for Binary-appearance Loop-closure Evaluation (ABLE) \parencite{arroyo-et-al:2018:7} approach to achieve higher descriptiveness power for appearance invariance.
ABLE outperformed FAB-MAP in terms of precision-recall evaluation metrics. An advantage of using binary features such as LDB is the possibility of using the Hamming distance to compute descriptor similarity, improving the computational efficiency of this process over cosine similarity or Euclidean distance.

Another work from the included records focused on improving the long-term performance of handcrafted visual features is from \cite{karaoguz-bozma:2016:4}. Their approach uses bubble descriptors for preserving the relative $S^2$ geometry of visual features, being rotationally invariant. The experimental results demonstrated improvements on viewpoint and illumination invariance of bubble features-based localization.

Instead of preserving the long-term appearance-invariance of visual descriptors, \cite{neubert-et-al:2015:005} introduces the SuperPixel-based Appearance Change Prediction (SP-ACP) to predict extreme appearance changes across seasons.
SP-ACP extracts descriptors (combination of color histogram in Lab color space with upright SURF descriptor) from the image superpixels and clusters the descriptors into seasonal-specific vocabularies using hierarchical k-means.
With training images with pixel-accurate alignment between images, the known pixel association creates a translation dictionary between seasons to synthesize a predicted image for cross-season place recognition.
SP-ACP was able to improve cross-season place recognition performance compared to not comparing with the predicted image, although the method has the limitation of requiring pixel-wise alignment in training.

The work of \cite{griffith-pradalier:2017:21664} considers GPS and compass data in addition to visual data. \cite{griffith-pradalier:2017:21664} builds on SIFT Flow to find dense correspondences among images for survey registration in long-term lakeshore monitoring. SIFT Flow combines the precision of point-based feature matching with the robustness of whole-image matching, while the GPS, the feature tracks from a visual SLAM, and the compass measurements bias the image registration. The proposed method was able to match images from different surveys separated by several months with dramatic changes relative to lighting, occlusions, seasonal changes, and even the sun glare.

Even though \cite{cao-et-al:2018:2815956} and \cite{cao-et-al:2021:2962416} require a 2D or a 3D laser for place recognition and not a visual sensor, these methods use 2D image representations of a point cloud to extract visual handcrafted features.
\cite{cao-et-al:2018:2815956} transforms the 3D point clouds of a 3D laser into 2D images using the bearing angle 2D representation (image according to the relative position among adjacent laser points, without projecting the point cloud onto a certain surface). Using a BoW approach with the dictionary learned using ORB features, the query image is matched to the database ones, while performing geometric verification by reprojecting the ORB features into the 3D coordinate frame. One main advantage of using LiDAR in the experiments was its less sensitivity to lighting conditions relative to visual sensors while not being incapacitated in dark environments. The proposed method outperformed M2DP~\parencite{discussion:m2dp} -- global descriptor for point clouds --, given that M2DP could not deal in situations where the point clouds distributions were centralized and similar to each other.
As for \cite{cao-et-al:2021:2962416}, the proposed method accepts also 2D laser data by accumulating a sequence of scans. The 2D representation used differs from \cite{cao-et-al:2018:2815956} by projecting the point cloud into cylindrical coordinates and using the centroid of the point cloud to ensure viewpoint invariance.
Using Gabor filters to detect and describe the contours of the images, \cite{cao-et-al:2021:2962416} generates Binary Robust Independent Elementary Features (BRIEF) descriptors for matching images using a nearest neighbors search. In addition to showing the seasonal appearance variance in laser data (e.g., different foliage in the scene), the proposed methodology outperforms SeqSLAM~\parencite{discussion:seqslam} (sequential place recognition) and PointNetVLAD~\parencite{discussion:pointnetvlad} (CNN-based place recognition for 3D point clouds) on precision-recall.

In terms of visual features from radar data, \cite{hong-et-al:2022:02783649221080483} extracts visual features used for tracking using a blob detector based on a Hessian matrix. These features are extracted from a 2D cartesian image transformed from the polar image representation of radar, while also compensating the distortion from the vehicle's motion.
As for loop closure detection, the peaks in intensity from the polar radar image are evaluated to remove noise of areas without a real object due to speckle noise. Then, the processed polar image is transformed into a point cloud and the M2DP descriptor adapted to 2D point clouds is used to detect loop closure.
The proposed methodology improved the radar odometry tracking, while also outperforming ORB-SLAM2~\parencite{discussion:orb-slam2}.


\paragraph{Environment structure features}

The structure of the environment defined by its geometry is more robust to appearance variance than the appearance itself. Common structure features extracted from sensors data are line and edge features.
\cite{biswas-veloso:2013:0278364913503892} extracts 2D line segments corresponding to the walls from depth and 2D laser sensors. The line segment-based localization had a low failure rate on an over-a-year long-term indoor deployment even in areas with movable objects, due to the long-term stability of the line segment features.
\cite{nuske-et-al:2009:20306} extracts 3D edge features of the scenes using a monocular camera to get the edges of the buildings in the environment, while employing an exposure control to maximize the strength of edges corresponding to the mapped ones. The proposed method was able to successfully track the edges of the buildings along an all-day outdoor experiment.
Instead of using the walls of the buildings, \cite{an-et-al:2016:0} formulates a visual node descriptor based on ceiling salient edge points. Even though the method achieved good results in lighting changing conditions, the method's performance decreases using low and inclined ceilings, due to the image perspective effect that may lead to matching failure in the implemented Iterative Closest Point (ICP) framework.

Furthermore, \cite{meng-et-al:2021:3062647} extracts edge and planar features by evaluating the large and small values of the local surface smoothness over the points of a 3D laser, respectively. ICP estimates the laser odometry while the histogram cross-correlation of the Normal Distribution Transform (NDT) that computes local probability density functions of the surface smoothness identifies the loop closures.
The proposed methods outperformed an ICP-based SLAM approach on Absolute Trajectory Error (ATE) in the experiments.
As for \cite{bosse-zlot:2009:009}, 2D point clouds segmented into connected components are clustered at regions of high curvature to get high curvature keypoints from multiple scans.
The proposed descriptor based on the moment grid improves outdoor place recognition relative to SIFT or Hough transform peaks due to the moment grid descriptor includes higher order of moments relative to other descriptors.

Poles are structures also used for long-term localization.
\cite{schaefer-et-al:2021:103709} retrieves the 2D coordinates of poles registered with a 3D laser. Results demonstrated the ability of reliable long-term localization over more than one year.
In addition to poles, \cite{berrio-et-al:2019:8814289} extracts also corner features from the 3D laser point cloud, being able to localize over a 6 month experiment at different times of the day.

Another possible application of environment structure features found in the included works is in crop fields for agriculture.
\cite{chebrolu-et-al:2018:2849603} formulates an aerial image registration algorithm based on the positions of the crops and the gaps between them remaining the same over time. The method computes a vegetation mask by exploiting the Excess Green Index (ExG) of RGB images. Using the Hough transform to find lines between vegetation, the center of the crops are the peaks on vegetation histograms perpendicular to the rows.
The testing results demonstrated invariance of the registration algorithm to changing conditions caused by weather and crop growth over one month.



\subsubsection{Convolutional Neural Networks (CNN)}
\label{sec:discussion:appearance:cnn}

A more recent direction noted in the included works is the use of CNN.
The evolution of deep learning in computer vision led to researching how CNN could be used for generating feature representations robust to appearance variance, as an alternative to handcrafted features.
CNN-based features are known to offer more discriminate power compared to handcrafted features while being able to be more robust in challenging environments~\parencite{taisho-kanji:2016:7866383}.
The feature representations can be retrieved from the layers of CNN, with earlier ones usually extract low-level features such as edges or corners, while deeper layers extract high-level ones such as semantic structures~\parencite{chen-et-al:2018:2859916}.
In addition to using the CNN feature maps, the included works also used CNN for semantic segmentation to extract semantic information from sensor data and appearance-content disentanglement for generating appearance-invariant descriptors.


\paragraph{CNN feature maps}

One application of CNN features is for image place recognition as a classification task. Instead of comparing pairs or triplets of images, the place recognition is formulated as a classification problem~\parencite{chen-et-al:2018:2859916}.
In \cite{taisho-kanji:2016:7866383}, the layer fc6 (fully connected) of AlexNet extracts 4096-dimensional CNN features from box regions in the query image, then reduced to 128-dimensional features with Principal Component Analysis (PCA). Comparing these features to the ones extracted from the reference images in a cross-domain library (collected in different routes and seasons), \cite{taisho-kanji:2016:7866383} defines the query image as a set of nearest neighbor library features (similar to BoW) and employs the image-to-class distance with the Naive Bayes Nearest Neighbor (NBNN) method. The proposed PCA-NBNN descriptor outperformed BoW and FAB-MAP on a cross-season experiment in precision-recall metrics.
\cite{chen-et-al:2018:2859916} also formulates a classification task for place recognition, using a VGG16 network for generating local features, while adding a convolutional a fully-connected, and a softmax layer to learn the correct label output for classification. The proposed architecture outperformed FABMAP and SeqSLAM on seasonal changing conditions.

Place recognition can also be formulated as a coarse to fine image matching problem. An initial set of reference image candidates is obtained based on nearest neighbor distances of image-wise global descriptors~\parencite{xin-et-al:2017:8310121,camara-et-al:2020:9196967,liu-et-al:2021:9561126}, while local features are used for obtaining a more accurate estimation based on spatial matching~\parencite{xin-et-al:2017:8310121,camara-et-al:2020:9196967} or geometrical verification~\parencite{liu-et-al:2021:9561126}.
\cite{xin-et-al:2017:8310121} extracts both global and local features using a convolutional layer (conv3) of the AlexNet network, where local features are extracted from regions of the image with candidate regions sorted by the objectness score (improves viewpoint invariance).
Instead of using AlexNet, \cite{camara-et-al:2020:9196967} uses layers from VGG16 for feature extraction, specifically, conv5-2 and conv4-2 layers for global and local features, respectively.
As for \cite{liu-et-al:2021:9561126}, the MobileNetV2 network is selected for global feature extraction due to its computational efficiency. However, their work uses grid-based motion statistics with Oriented FAST and Rotated BRIEF (ORB) local features instead of CNN features.

Deep features can be combined with handcrafted features and preprocessing techniques to facilitate learning and further enhance their discriminative properties.
\cite{zhang-et-al:2022:3086822} uses the Key.Net network for keypoint generation, given that combines handcrafted and learned filters to detect keypoints at different scale levels, helping reduce the number of learnable parameters. Combined with HardNet for descriptor extraction, the method outperformed a BoW approach in viewpoint and illumination changing conditions.
\cite{yin-et-al:2020:2905046} proposes a handcrafted rotational invariant feature to be the input of a LocNet network for 3D laser-based place recognition. The proposed handcrafted feature reduced the complexity of the network and improved the efficiency on similarity evaluation.
As for preprocessing techniques to help in training, \cite{sun-et-al:2021:9635886} uses a the Gamma transform and other transformations (translation, scale, in-plane rotation, and symmetric perspective distortion) to generate day-nigh image pairs from daytime ones. These images are used for training the proposed visual descriptor DarkPoint on the keypoints generated by the SuperPoint keypoint detector. DarkPoint achieved approximately 1.7x more inliers during navigation than the original SuperPoint in day-night experiments.

Given that feature maps can extract different types of features depending on the deepness of the respective layers, \cite{zhu-et-al:2018:8500686} extracts features from three different layers (conv3-3, conv4-4, conv5-3) of a VGG16 network and concatenates these to form a global descriptor for an image. A cross-season experiment showed an increasing performance in precision-recall when the single layer gets deeper.
These results are conformal to ones obtained in \cite{yang-et-al:2021:12054}. The conv5-3 achieved higher accuracy than conv4-4 and conv3, indicating that the spatial information increases in deeper layers improving the place recognition. \cite{zhu-et-al:2018:8500686} also showed that fusing the three layers used in their work by concatenating them into a global descriptor improves even further the place recognition performance.
Moreover, \cite{yu-et-al:2019:8961714} chooses DenseNet for feature extraction due to this network reusing feature maps, i.e., connecting all layers with the same map sizes directly with each other. Then, \cite{yu-et-al:2019:8961714} uses the Weighted Vector of Locally Aggregated Descriptor (WVLAD) encoding for obtaining a global descriptor of the image. The proposed descriptor improved precision-recall over other architectures (VGG16, ResNet50) and to a BoW place recognition method.

The included works also focus on LiDAR and radar place recognition with CNN features. However, the raw point cloud data is not directly suitable for the CNN inputs. The most common solution is to project the point clouds onto the surface plane, the so-called Bird's-Eye View (BEV).
\cite{yin-et-al:2018:8593562} encodes directly the BEV of a LiDAR into a low dimensional global feature using a bidirectional Generative Adversarial Network (GAN). Using the extracted features within the SeqSLAM framework, the proposed method improved the precision-recall metrics over the original SeqSLAM in changing conditions.
Similarly, \cite{martini-et-al:2020:s20216002} extracts a global descriptor from the BEV using NetVLAD but with the point cloud from a radar sensor.
\cite{kim-et-al:2019:2897340} formulates the point cloud descriptor Scan Context Image (SCI), also known as ScanContext. The 3D point cloud is converted to a polar representation of BEV named Scan Context (SC) matrix, where each cell of the 2D matrix contains the maximum height of points around a scene. Using the jet colormap to transform the SC into the SCI as a three-channel image suitable for the CNN inputs, \cite{kim-et-al:2019:2897340} uses a LeNet network for feature extraction and place classification. The proposed architecture outperforms PointNetVLAD~\parencite{discussion:pointnetvlad} and the handcrafted point cloud feature M2DP in precision-recall.
Based on SCI~\parencite{kim-et-al:2019:2897340}, \cite{xu-et-al:2021:3060741} proposes the Differentiable Scan Context with Orientation (DiSCO) descriptor. This method distinguishes from SCI by applying the Fast Fourier Transformation (FFT) to convert the polar BEV representation to the frequency domain. Given that frequency spectrum is translation-invariant, DiSCO becomes rotation invariant. The results showed a superior performance to SCI and PointNetVLAD in changing conditions.
Similar to DiSCO~\parencite{xu-et-al:2021:3060741}, \cite{yin-et-al:2021:661199} also uses SCI and FFT for feature extraction of point clouds. The difference is the use of a shared U-Net architecture to extract features of LiDAR and radar data, training simultaneously the radar-to-radar, LiDAR-to-radar, and LiDAR-to-Lidar place recognition tasks. The proposed method had similar or improved performance in these three recognition tasks relative to SCI and DiSCO.
In addition to BEV, \cite{yin-et-al:2021:3061375} also uses the spherical view. Using two separated 2D CNN following the convolutional layers in VGG16 to encode local features, a VLAD layer extracts place features from each view (BEV and spherical). A tightly-coupled fusion network fuses the features of each view. The proposed FusionVLAD descriptor outperformed PointNetVLAD and M2DP on the recall metric in appearance variant conditions.

Lastly, a trend found in the included works to improve the discriminative power of CNN features is the use of triplets~\parencite{martini-et-al:2020:s20216002,liu-et-al:2021:9561126,piasco-et-al:2021:6,sun-et-al:2021:9635886,yin-et-al:2021:3061375,yin-et-al:2021:661199} in training.
A triplet consists in an anchor image, a positive corresponding match, and an unrelated negative example. Triplet loss tries to minimize the matching distance between positive pairs (anchor, positive) and maximize that between negative ones (anchor, negative)~\parencite{sun-et-al:2021:9635886}.
Additionally, \cite{piasco-et-al:2021:6} uses also depth information during training, given that depth maps and their geometric information remain more stable across time than visual ones. A CNN encoder aggregates local features to produce a global descriptor, while a decoder reconstructs the scene geometry from the features obtained by the encoder. Then, triplet loss during training uses the fusion of image and depth map descriptors. In the experiments, the depth map training supervision provided building shapes understanding while improving the performance compared to not using side information.


\paragraph{Semantic segmentation}

Instead of using the feature maps of CNN, the networks can also segment raw data to extract semantic information.
\cite{naseer-et-al:2017:7989305} uses the Fast-Net network for extracting saliency maps for stable structures. These structures considered in training are man-made ones such as buildings or signs that are presumable to be stable in long-term. Then, the salient maps boost the importance of features retrieved from a convolutional layer (conv3) for place recognition. The proposed method improved the precision-recall metrics compared to HOG and place recognition without boosting stable structures on a cross-season experiment.

The included works also use semantic features from pixel-wise labeling of image data.
\cite{qin-et-al:2020:9340939} modifies an U-Net for semantic feature detection specifically trained for parking lots. This network generates pixel-wise segmentation of lanes, parking lines, guide signs, speed bumps, free space, obstacles, and wall, used in both localization and feature mapping. In the experiments, the semantic features were robust to light changes, texture-less-regions, motion blur, and appearance change.
\cite{berrio-et-al:2021:3094485} also segments an image with pixel-wise labels, discriminating 12 classes: pole, building, road, vegetation, undrivable road, pedestrian, rider, sky, fence, vehicle, and unknown. Using the extrinsic parameters of the 3D laser--camera, the pixel-wise semantic information from the labeled images is transferred to the 3D point cloud. Then, pole and corner features are retrieved from the projected point cloud onto the horizontal plane based on the IMU data for localization and mapping. The long-term evaluation of the map corrections showed a decrease over time demonstrating the stability of these features in outdoor environments.
In addition to pixel-wise segmentation, \cite{singh-et-al:2021:9564866} connects the regions of each instance of the semantic classes to characterize them in terms of their centroid in 3D camera coordinates (using also depth information from a stereo camera) and connections to other regions. The proposed global semantic-geometric descriptor defines a location in terms of how the pairs of semantic entities are distributed in the scene. The proposed method obtained higher accuracy when compared to SeqSLAM, FAB-MAP, and a BoW-based place recognition methods in a highly dynamic outdoor experiment.

Similar to \cite{singh-et-al:2021:9564866}, graph embedding of semantic features also tries to integrate the relationships between features for improving the robustness of place recognition.
\cite{han-et-al:2018:2856274} proposes the Holism-And-Landmark Graph Embedding (HALGE) descriptor. In the training phase, an image is represented by its global HOG descriptor and semantic features (static or stable elements such as houses, traffic signs, trees).
A graph relates the training images from different domains and locations, where the nodes are images or the semantic classes, and the edges represent the presence of a semantic class in an image or if two images represent the same location.
Then, HALGE learns a projection matrix of each template database image from the graph to generate an appearance invariant feature from the original global HOG descriptor. The proposed method improved the performance over HOG, SURF, and color and AlexNet-based descriptors in changing conditions.
As for \cite{gao-zhang:2020:9196906}, the proposed method formulated the place recognition task into a graph matching problem. The graph represents each semantic feature (same classes as in \cite{singh-et-al:2021:9564866}) by its central position in the image coordinate frame, while the edges that relate the features represent theirs spatial distance and angular relations, and their appearance similarity (Euclidean distance of local HOG descriptors). Then, a graph optimization optimizes a correspondence matrix between the features in the query to the ones in the template images for obtaining the final matching scores, assuming a long-term worst-case scenario (maximizes the distance and angular similarities of features that have the least similar appearance).
The proposed method outperformed \cite{han-et-al:2018:3} and an HOG-based place recognition method on recall at higher precision in outdoor experiments with seasonal and weather changing conditions.

Semantic information can also be retrieved from other sensors such as LiDAR. \cite{wang-et-al:2021:9739599} uses the RangeNet++ network for inferring semantic labels of 3D point clouds from LiDAR data. Even though the network can label 10 different categories, the method only used the categories representative of pole-like objects (poles, tree trunks). The method achieved a higher localization accuracy than SCI~\parencite{kim-et-al:2019:2897340} in an outdoor experiment with moving elements and dense vegetation.


\paragraph{Appearance-content disentanglement}

A location has different representations due to weather or seasonal changing conditions in the long-term perspective, among other factors. In terms of image data, the information retrieved from these representations could be separated in terms of its contents and appearance. The included works studied this possibility by learning the appearance-content disentanglement for feature representation that assumes the decomposition of the images latent space into apperance and content spaces~\parencite{qin-et-al:2020:103561}.

\cite{oh-eoh:2021:app11198976} adopts a Variational AutoEncoders (VAE) architecture that uses an encoder to generate the appearance and content feature vectors, while a decoder reconstructs the original image from these vectors.
Instead of using a single encoder, \cite{qin-et-al:2020:103561} proposes the Feature Disentanglement Network (FDNet) consisting of independent content and an appearance encoders, a decoder, and also an appearance discriminator to ensure the vectors are unrelated. Even though the content feature vector demonstrated to invariant to seasonal changes, the method significantly reduced its performance on high viewpoint variance, where the content vector changed greatly while the appearance one did not changed at all. This results indicated that viewpoint change is considered to be content in the proposed algorithm.
With a similar architecture to \cite{qin-et-al:2020:103561}, \cite{tang-et-al:2021:17298814211037497} also considers a place domain discriminator to ensure that the content discriminator only contains the place information and not also its appearance, while also using data augmentation in training to increase robustness against viewpoint changes. In the experiments, all images generated from a zero-appearance feature vector looked similar, while their place information remains conserved indicating that the proposed method can disentangle the input image across appearance changes.

Even though \cite{hu-et-al:2022:1003907} does not extract appearance and content independent features from the images, the proposed architecture builds on the same assumption of appearance-content disentanglement that a content representation of a location is shared across multiple domains.
\cite{hu-et-al:2022:1003907} adopts a multi-domain image-to-image architecture that expands the CycleGAN architecture from two to multiple domains, with domain-specific encoder-decoder pairs and discriminators.
For obtaining a shared-latent feature across different domains, the descriptor is learned using the feature consistency loss for domain-invariance.
In the experiments, even though night-time images were not included in the training, the model was able to learn the content space of the places, while also outperforming FAB-MAP.



\subsubsection{Feature stability}

Although long-term handcrafted or CNN-based features intend to remain invariant to changing conditions of the environment, their long-term stability is not guaranteed to be the same for all detected features. In this context, \cite{dymczyk-et-al:2016:66} proposes a CNN architecture based on AlexNet for evaluating the feature stability for long-term visual localization. The network is trained using a set of labeled data pairs (image patch around the feature keypoint, label) or triplets (adds depth information), where the feature label is binary - stable or unstable - computed for training by assessing the number of the feature observations over multiple sessions. In the experiments of over 15 months and changing conditions, the proposed method outperformed random selection of features for localization in terms of f-score, while the addition of depth information improved the method's performance.

Other approaches in the included works define predictor functions for evaluating the feature stability.
\cite{berrio-et-al:2019:8814289} defines the following predictors to evaluate the pole and corner features extracted from a 3D laser: the number of observations, maximum detected and possible spanning angle, maximum length driven while observing the feature, maximum detection area, and concentration ratio. A regression algorithm adjusts the weights of each predictor based on the number of observations across sessions to define the scoring function. A threshold based on the histogram of the feature scores determines which features to include in the long-term map.
Although \cite{berrio-et-al:2021:3094485} also uses the concentration ratio and maximum driven length as predictors, their approach simplifies the selection by including features that have been observed for more than 1m and conserving the ones in sparse density areas to avoid localization failures. \cite{berrio-et-al:2021:3094485} also defines a visibility measure related to the maximum range from where the feature is detected at a particular angle and the respective probability of detection to only the feature metrics when it is a match or if both not detected and not occluded.

Furthermore, \cite{egger-et-al:2018:8593854} and \cite{derner-et-al:2021:103676} propose methodologies for updating the map upon detecting changing conditions of the environment.
\cite{egger-et-al:2018:8593854} defines a minimum time interval between evaluations and the number of reconfirmations before updating the map with new stable and persistent features. The change in the conditions is determined by an overlap measure between the current view and the existing map that measures the relative amount of matched surfels extracted from a 3D laser. The proposed methodology led to a successful deployment of a robot over 18 months in changing conditions.
Even though \cite{derner-et-al:2021:103676} does not add features after creating the visual database used as a map, the method updates the feature weights saved that represent their stability and reliability for localization. After computing the transformation between the current view and the best database match, the descriptors of the latter are compared with their transformed counterparts, i.e., reprojecting the keypoints of the database on the query image using the transformation and re-compute the respective descriptors. The descriptors similarity, a spatial and temporal constraints, and the number of successful matches determine if the environment changed to update the feature weights based on their previous value and on the descriptors similarity. The method outperformed the localization without the weights update.

Instead of assuming observability independence, the observation of the features may be correlated between them.
\cite{nobre-et-al:2018:8461111} models the feature persistence using a Bayesian filter in a time-varying feature-based environmental model. The model considers the correlation between features without assuming no specific-sensor feature descriptor. The approach follows a survivability formulation where each map feature has a latent survival-time (represents the time when the feature ceases to exist) and a persistence variable. The marginal persistence is estimated probabilistically given the detection sequence of all features, following the intuition that if a set of features is co-observed and geometrically close, the likelihood that they belong to the same semantic object is high. The marginal feature persistence weights the data associations. The method was able to maintain track of the localization and updating the map accordingly in a semi-static changing environment.
\cite{luthardt-et-al:2018:8569323} proposes the Long-term Landmarks (LLamas) as persistent features, where the candidate points are the inlier feature tracks from visual odometry (short-term stable points). Considering that the map holds quality and viewpoint information, the correlated quality between neighboring viewpoints is modeled by Markov Random Field. The experiments showed that the identified LLamas over a 2 month experiment consisted on persistent structures in the environment such as curbstone, sign, or a street lamp, discarding varying structures like vegetation, parked carts or shadows.
As for \cite{b√ºrki-et-al:2019:21870}, the proposed appearance equivalence class measure models the probability of observing the feature given the past map sessions. This model expects to observe again the same features together with those already co-observed in the past.
Although the proposed selection measure outperformed the random selection of features in changing environments, the method suffered from the lock-in effect due to abrupt changes in the environment not being reflected in the observation sessions.



\subsubsection{Multi-modal features}

Another type of approach to feature-based localization and mapping is the use of multi-modal features, given that these features can be more discriminative than only considering a single feature space~\parencite{latif-et-al:2017:016}.
\cite{filliat:2007:364080} proposes a two-stage voting scheme for localization integrating 3 different feature spaces: SIFT, local color histograms, and local normalized grey level histograms. First, each feature space votes for the estimated location based on an incremental dictionary, without considering features seen in all known locations. Then, the votes of the different modalities are joined into a score that determines which location is the correct one.
On the contrary, \cite{latif-et-al:2017:016} tested the use of multi-modal features -- GIST and feature maps from a CNN -- by concatenating their descriptors into a single vector. 
In both \cite{filliat:2007:364080} and \cite{latif-et-al:2017:016}, the use of multiple feature spaces improved the localization performance over considering only a single feature space.

The included works also cover a more specific approach to multi-modal features by formulating the place recognition task as a regularized sparse optimization problem. The optimization uses training data for learning the weight of each feature modality when computing the matching score between the query and database images~\parencite{han-et-al:2017:2662061,han-et-al:2018:3,siva-zhang:2018:8461042,siva-et-al:2020:9340992}.
\cite{han-et-al:2017:2662061} formulates the Shared Representative Appearance Learning (SRAL) for fusing multi-modal visual features from 6 different spaces applied on downsampled images as scene descriptors: color histograms, GIST, HOG, Local Binary Patterns (LBP), SURF, and AlexNet (conv3). SRAL outperformed the individual feature spaces and also the concatenation of the 6 spaces into a single descriptor.
\cite{han-et-al:2018:3} proposes the RObust Multimodal Sequence-based loop closure detection (ROMS), that is the adaptation of the regularized optimization to image sequence matching. The modalities considered are LDB \parencite{arroyo-et-al:2018:7}, GIST, Faster R-CNN, and ORB. ROMS outperformed both FAB-MAP and SeqSLAM in appearance changing conditions, while improving the performance over considering a single feature space.
In addition to learn discriminative modalities, \cite{siva-zhang:2018:8461042} formulates the Fusion of Omnidirectional Multisensory Perception (FOMP) that learns the weights representative of discriminative views (omnidirectional vision) and considers both image and depth modalities of features. The feature spaces considered are GIST, HOG, LBP, and AlexNet (conv3). In a cross-season experiment, the depth-related modalities had more importance than the image ones, indicating that the latter are more susceptible to appearance change. Also, FOMP outperformed feature concatenation and only using the front field of view.
As for \cite{siva-et-al:2020:9340992}, the proposed Voxel-Based Representation Learning (VBRL) method identifies representative feature modalities and voxels from 3D point cloud. The feature spaces considered are the HOG in the XY, XZ, and YZ planes, the subvoxel occupancy scene descriptors, and the covariance points containe within each voxel. VBRL outperforms only considering discriminative voxels or features, and also outperformed descriptor concatenation in changing conditions.



\subsubsection{Image sequence matching}

The temporal coherence of a sequence of visual data improves the performance of long-term place recognition in appearance variant conditions due to higher discriminative properties while exploring the temporal sequential relationships of the images~\parencite{ouerghi-et-al:2018:s18040939,nguyen-et-al:2013:004}.
\cite{ouerghi-et-al:2018:s18040939} builds on SeqSLAM~\parencite{discussion:seqslam} by proposing the Sequence Matching Across Route Traversals (SMART) system. The original SeqSLAM defines a location as a sequence of images by searching first for the best sequence match and then performing a local search for place recognition. Given the SeqSLAM's drawback on lack of viewpoint invariance due to global matching, SMART introduces a variable offset in the image match to compare each frame with the database within a range of image offsets, while also fusing the place recognition with visual odometry using an Extended Kalman Filter (EKF). The fusion of topological with local metric localization improved the mean error distance error over visual odometry in changing conditions, while SeqSLAM only provides a location-wise estimation.
\cite{han-et-al:2018:3} compared frame-to-frame matching to the proposed ROMS algorithm that models frame correlation and formulates the image sequence matching problem into a regularized sparse optimization (in addition to learning the features modalities). ROMS improved the place recognition over frame-to-frame matching, while outperforming SeqSLAM and FAB-MAP in changing conditions.

Moreover, \cite{vysotska-et-al:2015:7139576} defines image sequence matching between a query and a database as a data association graph, encoding in the graph the cost proportional to the similarity between two images given by a HOG descriptor~\parencite{naseer-et-al:2015:7324181}. Instead of formulating the sequence matching as a network flow optimization problem, \cite{vysotska-et-al:2015:7139576} estimates the shortest path in the graph. This approach requires a rough global pose estimation for the images (e.g., GPS) to search efficiently through the graph for possible image matches.
\cite{naseer-et-al:2015:7324181} leverages the temporal sequence of images by requiring ordered sequential images in the database. The state transition model of the Bayes filter allows transitions between all places but modeled with different probabilities, while a sequence filtering searches for sequences of local peaks of matching images. The sequential information is accounted by imposing a minimum sequence length and maximum gap in frames between two matches to avoid false-positives.
Both \cite{vysotska-et-al:2015:7139576} and \cite{naseer-et-al:2015:7324181} outperformed SeqSLAM and network flow in the experiments.

Although an image sequence is a set of images, the sequence itself can be described by a descriptor. In both \cite{arroyo-et-al:2018:7} and \cite{zhu-et-al:2018:8500686}, the sequence descriptor is the concatenation of the single images, and the sequence matching is the computation of Hamming distance between the descriptors. \cite{arroyo-et-al:2018:7} uses the LDB binary descriptors for single images, and the experiments showed a lower accuracy for single image matching in long-term compared to the sequence descriptor. Also, the proposed method outperformed FAB-MAP and SeqSLAM in terms of precision-recall metrics.
As for \cite{zhu-et-al:2018:8500686}, the feature maps from VGG16 are normalized into a binary descriptor. The method outperformed FAB-MAP, SeqSLAM, and ABLE~\parencite{arroyo-et-al:2018:7} in a cross-season experiment.

Lastly, \cite{nguyen-et-al:2013:004} proposes an approach to identify topological places based on an image stream. 
The method uses a clustering scheme K-iteration Fast Learning Neural Network (FLANN) to organize the visual input images into scene tokens. These tokes are the input to a Spatio-Temporal Long-Term Memory (LTM) architecture equivalent to an NN-based memory structure, in which the topological locations defined as image sequences are stored in the memory structure (LTM cells). Then, the proposed architecture models the topological structure of an environment by linking the scene clusters into a temporally ordered sequence using a one-shot learning mechanism and only requiring a single representation of the sequence.
A pooling system determines the current topological location of the robot.
The method was able to localize different topological sequences in appearance changing conditions.



\subsubsection{Sensor modalities}

The appearance variance in the environments affects visual sensors as well as ranging-based ones such as 2D/3D lasers or radar. Visual data is affected by the illumination changes of day-night situations, the weather changing conditions, and the changes on visual data caused by the different seasons of the year. Laser-based localization does not suffer from illumination variance. However, the laser is affected by low reflections or occlusions  in unfavorable conditions such as fog, direct light, or moving elements in the scene. As for radar, the sensor is invariant to lighting and weather changes. Still, noisy measurements affect the performance of radar-based localization and mapping in long-term scenarios~\parencite{yin-et-al:2021:661199}.

Consequently, long-term localization and mapping algorithms should also consider fusing different sensor modalities to use the advantages of each one and improve the overall robustness to appearance changes. In addition to the works already discussed previously,\cite{p√©rez-et-al:2015:y}, \cite{coulin-et-al:2022:3136241}, and \cite{nguyen-et-al:2022:3094157} also focus on appearance invariance upon changing environments while using more than one modality.
\cite{p√©rez-et-al:2015:y} introduces an appearance-based particle injection in the Monte Carlo Localization (MCL) framework to account the visual place recognition of FAB-MAP~\cite{discussion:fab-map}. The BoW model of FAB-MAP is created using visual data recorded at different hours and changing conditions. Then, using the BoW model and a 2D occupancy grid as prior, the MCL fuses the odometry (wheel encoders and IMU data), the 2D laser, and the loop closure detection from FAB-MAP. The method did not need any manual recovery even in the case of global localization in a crowded environment with significant lighting changing conditions.
\cite{coulin-et-al:2022:3136241} proposes the use of a magnetic map with a Multi-State Constraint Kalman Filter (MSCKF). The magnetic map is built offline using visual-inertial SLAM in conjunction with global optimization to provide ground-truth positions for the map readings. As for localization, the tightly-coupled visual-inertial MSCKF reuses the magnetic map, while simultaneously estimating the magnetometer bias to avoid calibrating it every session. The experiments compared the proposed method to a visual-inertial SLAM algorithm with a visual map on a run one year after the creation of the map. The proposed method outperformed the other one given that visual data was variant to appearance changes in the environment, while reducing the ATE from 2.4m to 0.033m compared to using vision-only in the MSCKF.
As for \cite{nguyen-et-al:2022:3094157}, the proposed Visual-Inertial-Ranging-Lidar (VIRAL) sensor fusion algorithm includes an IMU, LiDAR, a camera, and Ultra-Wide-Band (UWB) data for localizing an aerial vehicle in indoor environments, with the first three sensor modalities for odometry and UWB for absolute positioning in the world frame. VIRAL formulates cost functions of the sensors evaluated at every time step for inclusion in the optimization. The method improved over ORB-SLAM3~\parencite{discussion:orb-slam3} in the experiments performed with an aerial vehicle in changing lighting conditions.





\subsection{Dynamics modeling}
\label{sec:discussion:dynamics}

This section analyzes included works focused on modeling and identifying dynamic elements in the environment, categorized in DE1 as dynamics.
Even though Section~\ref{sec:discussion:appearance} already discusses appearance changes in the environment that can include moving elements in the scene, this section focuses on how the methods identify these elements and handle them for long-term localization and mapping.
The discussion on dynamics modeling is organized into the following topics: specific map representations used to model or deal with dynamic elements in the scene, identification of dynamic elements matching the current observation to the current map, future prediction of dynamic properties of scene elements, and semantic identification of dynamic objects.



\subsubsection{Map representation}
\label{sec:discussion:dynamics:map}

Inspired by the human memory, \cite{dayoub-et-al:2011:013} and \cite{bacca-et-al:2013:003} adapt the multi-store model of \cite{discussion:atkinson:stm+ltm} for robot mapping. This model divides the memory into three stores: Sensory Memory (SM) to save the perceived information, Short-Term Memory (STM), and Long-Term Memory (LTM).
Three mechanisms move information between memories: selective attention for SM to STM, rehearsal to commit information from STM to LTM or which one is forgotten, and the retrieval mechanism to move unused information from LTM back to STM.
\cite{dayoub-et-al:2011:013} implements two types of state machines for rehearsal and retrieval mechanisms of the STM and LTM. In rehearsal, a STM feature moves closer to LTM or moves back to the initial state (or forgotten if already in that state) when observed consecutively or if not, respectively. Similar for retrieval, where a feature in LTM moves to the initial state or closer to forget if observed in the current view or not.
Consequently, LTM and STM save the most static and dynamic features based on their observability in the current view, respectively.
In a changing environment, the method decreases the localization failure rate compared to a static view.
Instead of using a state machine, \cite{bacca-et-al:2013:003} implements a Feature Stability Histogram (FSH) depending on the feature observability to distinguish between STM and LTM features using k-means clustering. This modification allows that an input feature in SM can bypass STM and become part of LTM depending on the feature strength.
The method was able to filter out pedestrian dynamics from the 2D laser and camera data while also achieving a more accurate representation of the environment compared to a static approach.

Although \cite{biber-duckett:2009:0278364908096286} does not adopt specific memory mechanisms, they implement STM and LTM maps. The method implements a dynamic map as a set of local maps, each maintaining submaps representing different timescales. The timescale parameter of each submap determines probabilistically when to add samples from 2D laser scans.
The dynamics of the environment are represented by using 5 different timescales, where the smaller one ($\sim$3.1s) represents an STM map updated at every instant and the other 4 are LTM submaps ranging from $\sim$0.43session to $\sim$13.5day, updated after each season or daily. Instead of only localizing on the LTM maps as in\cite{dayoub-et-al:2011:013} and \cite{bacca-et-al:2013:003}, \cite{biber-duckett:2009:0278364908096286} selects the best representation of both STM and LTM maps that best explain the sensor data.
In a 5 weeks experiment, the localization with a dynamic map improved while a static representation degraded over time, while the timescales led to static parts as walls emerging in LTM and dynamic elements disappearing from the STM maps.

Similar to \cite{dayoub-et-al:2011:013} and \cite{bacca-et-al:2013:003}, two maps can represent a more stable and a more dynamic representations of the environment.
In \cite{walcott-bryant-et-al:2012:6385561}, an active map represents the most current state of the environment, including parts that did not change from previous passes and objects added to the environment. A dynamic map only saves the points of a 2D laser scan that changed over time.
\cite{wang-et-al:2019:8793499} uses a tracking map with short-term static points and a long-term map only containing long-term static points identified by a semantic segmentation module with ORB-SLAM2~\parencite{discussion:orb-slam2}.
Also, \cite{zhu-et-al:2021:9561584} creates offline a semi-dynamic map and a static one, where the former has semi-dynamic objects (parked cars in a parking lot environment) and the latter has both static and semi-dynamic objects.
The main goal of representing two different dynamics is usually to favor the most stable one in the long-term.
\cite{walcott-bryant-et-al:2012:6385561} and \cite{wang-et-al:2019:8793499} only use static parts in the most current representation of the environment (active and the long-term maps, respectively) for localization.
In the experiments, \cite{walcott-bryant-et-al:2012:6385561} showed that their method was able to identify static parts, although it was affected by false positives and negatives, and by the blur effect in the 2D grid map.
\cite{wang-et-al:2019:8793499} improved the ATE in a dynamic environment over ORB-SLAM2 and DynaSLAM~\parencite{bescos-et-al:2018:2860039}.
As for \cite{zhu-et-al:2021:9561584}, its MCL framework reduces the weight corresponding to observations of moved semi-dynamic objects. The method improved the localization in a parking lot compared to a standard MCL.



\subsubsection{Map matching}

Environment dynamics can be identified by comparing the current observation to the map.
Assuming a prior vector map as a permanent map, \cite{biswas-veloso:2017:005} determines the probability of observed features being long-term ones by the 2D laser scan-to-map matching distance. Short-term features are determined by the scan-to-scan matching distance, while the remaining ones are considered dynamic features and not considered for localization. Compared to MCL with a static map and to \cite{tipaldi-et-al:2013:0278364913502830}, \cite{biswas-veloso:2017:005} had lower localization error in a parking-lot environment. However, the method would not handle semi-static changes.
Instead of using a permanent map, \cite{zhang-et-al:2019:8814347} maintains a Signed Distance Field (SDF) representation based on a prior occupancy map. The method rejects dynamic points identified by range flow and updates the SDF-based map with semi-static changes observed in the scan-to-map difference. Compared to MCL in a semi-static environment, the proposed method had lower pose errors and an improved representation of the environment.
\cite{boniardi-et-al:2019:003} detects semi-static changes leveraging the ICP scan-to-map consistency and a CAD prior of the environment, and updates the map accordingly. The method was capable of maintaining a consistent map when dealing with substantial reconfiguration of the environment.
\cite{du-et-al:2022:3028218} minimizes the Gibbs energy defined on the proposed Long-term Consistent Conditional Random Field (LC-CRF) for detecting dynamic points, considering that these points have often a large reprojection error in frame-to-map matching and points tend to have the same dynamic properties as the neighbor ones. In a dynamic scene, LC-CRF achieved lower ATE than ORB-SLAM.

Furthermore, \cite{pan-et-al:2019:s19194252} and \cite{ding-et-al:2020:2942760} leverage clustering properties of the observations evaluating the observations count.
\cite{pan-et-al:2019:s19194252} segments the points of a LiDAR point cloud into different clusters assuming that dynamic points do not appear frequently in the same place. The map only considers clusters that appear in same location more than 10 times.
As for \cite{ding-et-al:2020:2942760}, the method build on the assumption that dynamic and static parts of the environment have a clustering property relative to its neighbors (similar to \cite{du-et-al:2022:3028218}). The number of observations in different sessions combined with its consistency relative to its neighbors determine if a map point is static throughout the sessions.
Both representations of the environment in \cite{pan-et-al:2019:s19194252} and \cite{ding-et-al:2020:2942760} were stable to structural changes in the environment.

The concept of ray tracing is also used by the included works to handle dynamic changes.
\cite{l√°zaro-et-al:2018:8594310} uses ray tracing to exploit the free space information. When comparing two 2D point clouds from a viewpoint, the ray tracing evaluation identifies new objects added to the scene (observed point closer to viewpoint than the old one) and outdated information (observed point further way), allowing the identification of dynamic changes and having an up-to-date representation for localization.
Given that ray tracing in 3D is expensive in terms of memory and requires dense map representations, \cite{pomerleau-et-al:2014:6907397} uses directly the sparse point cloud from a 3D laser. The map points are associated with each single reading in small conical apertures in spherical coordinates, updating the observed points closer than the mapped ones and the further ones are left untouched. The approximation of ray tracing results are used to update the probability of points in the map to be dynamic, based on a Bayesian approach. The probability of being dynamic can be used in ICP to not trust dynamic points and indeed, in the experiments, \cite{pomerleau-et-al:2014:6907397} had a more precise and cleaner map of the environment than using a standard ICP matching.
Instead of weighting the map points, \cite{an-et-al:2016:0} proposes the Dynamic Edge Link (DEL) to model the dynamics in the edges of a pose graph instead on the data itself. The observation of moving obstacles between two poses change the weight of the respective edge, decrease gradually the weight until not detecting the obstacle. Integrating DEL in an exploration scheme, nodes with a edge weight average lower than a certain threshold, meaning frequent moving obstacles or changed structure near that node, are not considered for exploration due to the robot may be unable to move to that position.

Although the standard NDT representation does not model free space, \cite{einhorn-gross:2013:6698849}, \cite{saarinen-et-al:2013:0278364913499415}, and \cite{einhorn-gross:2015:008} use NDT with occupancy maps to model explicitly the free space and adopt exponential weighted moving average and covariance for new measurements having an higher influence than old ones.
\cite{einhorn-gross:2015:008} proposes a generic 2D/3D mapping using NDT and occupancy maps. The hit cells considering the current observation are updated incrementally with exponential weighting. The other cells along the sensor beam potentially empty are updated using the standard update rule of occupancy maps based on the log-odds of the occupancy value and on the inverse range sensor model.
Instead of using the standard occupancy map update, the sensor model in \cite{saarinen-et-al:2013:0278364913499415} depends on the inconsistency between observation and map. Also, the occupancy value describes the confidence of the NDT based on past observations.
As for \cite{einhorn-gross:2015:008}, the method defines two probabilities for the occupancy map: occupancy and statically occupied, where the first is updated based on the sensor model (2D/3D generic beam sensor), and the second one is adapted slowly to high probability for static objects in the environment. The statically occupancy probability follows the proposed ad-hoc model that is parameterized to control how fast the static occupancy probabilities are adapted, depending also on the occupancy probability itself.
\cite{einhorn-gross:2013:6698849} and \cite{einhorn-gross:2015:008} were able to handle semi-static and dynamic changes having a consistent and up-to-date representation of the environment, while \cite{saarinen-et-al:2013:0278364913499415} favored long-term static structures in dynamic environments.



\subsubsection{Prediction modeling}

In the included works, Markov processes are used to predict the dynamics of the environment.
\cite{tipaldi-et-al:2013:0278364913502830} uses a dynamic occupancy grid and exploits the stationary distribution and the state holding time associated with Hidden Markov Models (HMM) on a 2D grid. The method uses past observations for each run to learn the state transition probabilities iteratively to estimate the HMM parameters. Then, the localization can infer how often is expected to see a dynamic object in the environment and for how long. Comparing the proposed HMM-based localization to MCL using a standard grid, the former had a lower localization failure rate than MCL, capable of dealing with high dynamics (moving cars) and lower ones (parked cars).
\cite{rapp-et-al:2015:77} implements a semi-Markov process extended by a Levy process to model a time dependency on the state holding time of Markov processes, also predicting as \cite{tipaldi-et-al:2013:0278364913502830} the expected retention time for each cell being in a specific state. In the experiments, the proposed model integrated in MCL improved the classic MCL in a dynamic environment.

The environment dynamics can have periodic patterns associated with them.
Assuming periodic changing patterns, \cite{krajn√≠k-et-al:2017:2665664} proposes the FreMEn (Frequency Map Enhancement) to model the probability of occupancy or feature visibility in a grid as a combination of harmonic functions related to periodic processes. FreMEn uses spectral analysis (Fourier transform) to compute the harmonic functions and predict future state with a given confidence. In a changing environment, FreMEn outperformed a static map and experience maps~\parencite{churchill-newman:2013:0278364913499193} in terms of localization error by selecting the most likely visible features at each location for localization.
\cite{santos-et-al:2016:2516594} adopts the FreMEn within an exploration scheme, where the planner predicts which areas are more likely to change at a certain time and generate the subsequent locations to explore. The experimental results showed that considering the environment dynamics increases the amount of information gathered compared to static models.
Unlike FreMEn, \cite{wang-et-al:2020:9468884} models both aperiodic and periodic changes by an Auto-regressive Moving Average Model (ARMA). This model describes time series as stationary stochastic processes in terms of polynomials. While FreMEn is able to update recursively its model online, ARMA only is updated once a day based on past observations. However, the model achieved a higher prediction accuracy than FreMEn and a lower localization failure rate than both FreMEn and \cite{tipaldi-et-al:2013:0278364913502830}.

Instead of modeling the dynamics in the map, \cite{thomas-et-al:2021:9561701} uses a KPConv network to predict online dynamic motion labels of points with single 3D laser scans as input. The method is a self-supervised learning approach with two main modules: PointMap and PointRay. PointMap is an ICP-based SLAM algorithm to provide a point cloud map for the annotation process. PointRay uses a similar approach to \cite{pomerleau-et-al:2014:6907397} to approximate ray tracing using spherical coordinates for obtaining the training annotation of dynamic labels: permanent (static points over all sessions), ground (to avoid ray tracing ground samples), and long-term (still objects in single sessions but relocated between sessions) and short-term (dynamic objects) movables, with the localization not considering the latter two. In the simulation experiments, PointMap with the proposed prediction module led to lower localization pose errors than an MCL algorithm.



\subsubsection{Dynamic objects detection}

In terms of detecting dynamic objects, \cite{yue-et-al:2020:9197072} proposes a collaborative dynamic mapping for detecting humans using visual and thermal images and a 3D LiDAR. The YOLOv3 algorithm extracts the bounding boxes from the images relative to humans. The 3D point cloud projection onto the images allows the creation of a static point cloud for localization and mapping of each robot by filtering out the points corresponding to humans. In the experiments, the dynamic objects removal generated a more accurate relative transformation of the collaborative maps compared to not removing those objects.
\cite{zhu-et-al:2021:9561584} also uses YOLOv3 to extract bounding boxes of dynamic classes (parked cars) from a RGB image, and the projection of LiDAR points allows the creation of the static and semi-dynamic maps required for localization. Even though Zhu does not discard the dynamic objects, the localization module reduce the importance of weight of the corresponding observations.
Instead of using visual data to detect object classes, \cite{sun-et-al:2018:2856268} adapts the PointNet for object recognition (pedestrian, cyclist, car, or background) to classify the scan points of a LiDAR. The proposed Recurrent-OctoMap maintains the occupancy and semantic information, whereas the latter specifies the cell semantic state and the probability of the prediction. The transition between states is learned by a Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN). In a long-term experiment, the method was able to improve its 3D semantic map compared to a standard Bayes update.

Moreover, pixel-wise semantic segmentation is another way to identify dynamic objects.
Additionally to the proposed semantic-descriptor in \cite{singh-et-al:2021:9564866}, the method sets lower weights to features detected on sky and dynamic classes (person, car, etc.) from the semantic segmentation of EdgeNet.
Instead of identifying object classes, \cite{song-et-al:2019:8967749} proposes the MD-Net CNN to segment a grayscale image into unstable, static, and moving pixel points, only using static points for localization. The localization error was reduced compared to not estimated the pixel dynamic attribute.
\cite{ganti-waslander:2019:00024} proposes the Semantically Informed Visual Odometry (SIVO) to improve the performance of ORB-SLAM2 by using the Bayesian neural network SegNet for segmentation and computation of the network uncertainty. SegNet is trained to distinguish different object classes for identifying dynamic objects (sky, car, truck/bus, person/rider, motorcycle/bicycle, and void) from static ones (road, traffic sign, building, wall/fence, pole, vegetation, sidewalk, traffic light, and terrain). Only static keypoints that reduce the most of the state's uncertainty (considering the network uncertainty) are considered as input for ORB-SLAM2.
SIVO was able to remove uninformative and dynamic keypoints from the current frame. However, the immediate rejection of potential dynamic objects without verifying if they are moving reduced the localization performance of the module in certain scenarios.

Dynamic objects identification can be improved by verifying geometric constraints.
\cite{bescos-et-al:2018:2860039} proposes DynaSLAM as a front end for ORB-SLAM2 to segment potential dynamic classes using a Mask R-CNN. The semantic labeling is improved using a multi-view geometry verification. DynaSLAM outperformed ORB-SLAM2 in highly dynamic scenarios while having similar accuracy in static ones. However, its performance reduced in slower dynamics.
Similar to DynaSLAM, \cite{wang-et-al:2019:8793499} implements a front end for ORB-SLAM2 to identify movable objects with a ResNet-based network for segmentation. The segmentation of the previous frame and a geometric verification based on the reprojection error improves the labeling of dynamic objects. The method improved the ATE over DynaSLAM and ORB-SLAM2 in a scenario with movable objects.
Instead of using semantic segmentation, the Semantic and Geometric Contraints Visual SLAM (SGC-VSLAM)~\parencite{yang-et-al:2020:s20082432} uses YOLOv3 to extract bounding boxes of dynamic objects for also improving ORB-SLAM2. A constraint based on epipolar geometry improves the labeling. SGC-VSLAM decrease the RMSE of the ATE by 96\% compared to ORB-SLAM2 in highly dynamic environments. However, similar to DynaSLAM, its performance decreased in lower dynamics.
Finally, \cite{xing-et-al:2022:22062} proposes the DE-SLAM to deal with Short-Term Dynamics (STD) and Long-Term Dynamics (LTD) at the same time. A MobileNetv2 identifies bounding boxes of movable objects (cars, persons, etc.) classified as STD. A motion check of STD elements recognizes all moving objects in the current keyframe. As for LTD, DE-SLAM uses HOG features extracted from ORB keypoints to improve its invariance to illumination changes. In the experiments, DE-SLAM improved the localization over ORB-SLAM2 in a changing environment.
All of these methods using geometric constraints to improve the identification of dynamic objects only use static features for localization and mapping.





\subsection{Map sparsification}
\label{sec:discussion:sparsify}

The next subject in this discussion analyzes included works categorized as sparsify in DE1. The methodologies proposed in those works manage the map size of the environment representation perceived by the mapping agent, where the size should be dependent on the operation area and not on the trajectory length of the robots.
Hence, the discussion on map sparsification is organized into the following topics: sparsification of graph SLAM to remove redundant nodes or outdated environment observations, management of the keyframe graph and its features relative to the keyframe formulation of the SLAM problem, and generic sparsification methods proposed in the included works for feature maps.



\subsubsection{Pose graph SLAM}

In graph-based SLAM, the constant update of the map leads to the ever-growing problem of the pose graph, where most basic approaches grow with the length of the trajectory or operation time. However, this growth should be bounded only by the size of the mapped environment. Information-theoretic methods focus on removing redundant nodes based on the concept of mutual information for limiting graph growth~\parencite{kretzschmar-stachniss:2012:0278364912455072}. Outdated nodes should also be removed to limit the graph size and update the current map representation upon changes in the environment. Additionally, the spatial distribution, time recency, and fusion of information onto existing nodes can also enable the reduction of nodes over time.


\paragraph{Mutual information}

The concept of mutual information from information theory can be used to determine which nodes to remove from the pose graph.
\cite{kretzschmar-et-al:2010:2} and \cite{kretzschmar-stachniss:2012:0278364912455072} estimate the expected information gain of a node based on its entropy contributing to the robot's pose belief in the current pose's neighborhood. The nodes with the lowest information gain are removed until the value is greater than a threshold. \cite{kretzschmar-stachniss:2012:0278364912455072} also sets a limit on the total number of nodes of the graph.
However, these two methods differ on how to marginalize the edges.
\cite{kretzschmar-et-al:2010:2} removes all $N$ edges of a removed node and adds $N-1$ edges between the removed one and a neighbor node. The latter is selected as the node that minimizes the edges length of the affected nodes by the removal. Effectively, the method only decreases by 1 the total number of edges per node removal.
As for \cite{kretzschmar-stachniss:2012:0278364912455072}, this method summarizes the information of the original edges into the nodes that are kept using an approximate marginalization that preserves sparsity. This approximation is based on using Chow-Liu trees to approximate a local probability distribution of the graph minimizing the relative entropy, or also known as the Kullback-Leibler Divergence (KLD), to reduce the number of edges locally.
In the experiments, both \cite{kretzschmar-et-al:2010:2} and \cite{kretzschmar-stachniss:2012:0278364912455072} stabilize the number of nodes and edges over time, limiting the computational requirements of online execution, while full marginalization (densely connected graph after removal) leads to an increasing number of edges.

The works focused on edge marginalization assume the prior selection of the node for removal.
\cite{carlevaris-bianco-et-al:2014:2347571} proposes the Generic Linear Constraints (GLC) to produce a set of constraints over the subset of nodes affected by the node removal. These constraints can produce either the full marginalization (dense GLC) or a sparse approximation using a Chow-Liu tree (sparse GLC). The repeated application of a sparse GLC node removal only had a low difference in both mean pose error and KLD compared to the full graph.
\cite{ozog-et-al:2016:21582} applies the same graph marginalization as \cite{carlevaris-bianco-et-al:2014:2347571} on a pose graph map obtained with an underwater vehicle. The system also had similar KLD compared to full graph.
\cite{huang-et-al:2013:6698835} formulates an $l_1$-regularized optimization problem to minimize the KLD of the approximation estimating Generic Linear Constraints (GLC) from the discarded measurements. The proposed method did not impact the localization error while also reducing by 77\% the non-zero elements of the information matrix, improving the sparsity of the graph.
As for \cite{mazuran-et-al:2016:0278364915581629}, this work proposes the Nonlinear Factor Recovery (NFR) edge marginalization, recovering the set of nonlinear factors that best represent the marginal distribution of the subset of nodes affected in the removal in terms of KLD, while considering both global and local linearization points. \cite{mazuran-et-al:2016:0278364915581629} demonstrated that NFR is equivalent to GLC when using only relative measurements. In the experiments, NFR tended to achieve similar or improved performance relative to GLC between the sparsified and full graph, having similar results or improved KLD depending on good or poor linearization points, respectively.

Other methods in the included works doe not require edge marginalization.
\cite{maddern-et-al:2012:6224622} does not impose global geometric corrections on loop closure to ensure similar odometric sequences on different passages in the same locations, only requiring the update on existent odometric edges. The method sets a maximum limit on the number of nodes eliminating the ones with lowest relative information content computed by the negative log of odometric and appearance-based matching likelihoods. The algorithm stabilized at a constant execution time and memory occupation due to the limit of nodes.
\cite{ila-et-al:2017:0278364917691110} proposes an incremental solution to decide whether a node should be added or not. Only nodes part of informative links or establishing informative links are added to the graph, avoiding to add unnecessary edges and thus not requiring edge marginalization. Although the method was able to slow the growth rate of nodes and edges in the experiments, the method was not able to bound it.
\cite{egger-et-al:2018:8593854} filters all poses with an overlap with their closest neighbors of the respective submaps higher than a threshold. The removal also updates the scores of affected neighbors relative to the number of observations required for evaluating the stability of the 3D LiDAR surfel features. Considering an overlap threshold of 0.6, the resulting map in the experiments was 9.4MB compared to the 5GB of the initial point cloud map.


\paragraph{Outdated information}

Instead of selecting nodes based on mutual information, the removal of outdated nodes based on the current information can limit the size of the pose graph.
\cite{walcott-bryant-et-al:2012:6385561} removes inactive nodes that do not represent the current state of the environment, considering the creation of nodes for each run to be able to create the active and dynamic maps required for dealing with dynamic environments. The removed points labeled in the dynamic map (points no longer present in the active map) over time allow the identification of inactive nodes. In the experiments, the method was only able to remove approximately 50\% of the nodes and edges compared to the full graph.
\cite{tang-et-al:2019:7} filters submaps in the proposed manifold navigation considering the number of successful localization in each location versus the attempted ones to indicate the outdated submaps. Even though the number of nodes stabilizes over time, the stabilization only happens on the third day, possibly due to the proposed manifold navigation treating new conditions of the environment as new nodes.
\cite{boniardi-et-al:2019:003} also evaluates the current localization of the robot to select nodes for removal. The method prunes outdated nodes when the pose's belief drops below a tolerance level, possibly related to changes in the environment. Additionally, upon loop detection, the method builds a local map from the subset of nodes candidate for loop closure. This local map allows for discarding candidates that are not topologically consistent with the local environment surrounding the robot using ray tracing, avoiding the addition of unnecessary edges. The method was able to limit the graph size in the same environment over multiple runs, being dependent on the size of the operational area and not on the operation time.


\paragraph{Spatial density and time recency}

The spatial distribution of the nodes is another approach to evaluate the graph sparsity.
\cite{johannsson-et-al:2013:6630556} proposes an incremental approach for managing the addition of new nodes, similar to \cite{ila-et-al:2017:0278364917691110}. However, the method only adds a new node if there is no existing node in the current position, instead of being based on mutual information. If a loop is detected, though the first one would not generate any new node due to the spatial constraint, in the case of a second loop, the method compounds the chain of constraints between the first and second loops to add a new constraint.
In contrast to \cite{johannsson-et-al:2013:6630556}, \cite{zeng-si:2021:6} adds nodes upon revisiting locations for optimizing the pose graph. Then, the method identifies redundant nodes clustering loop closure edges to identify similar trajectories.
Both works showed reduced growth compared to the full graph in the number of nodes in the experiments, even though the growth did seem to stabilize.

In addition to the spatial density, the time recency of the node can be another factor for selecting nodes for removal.
\cite{kurz-et-al:2021:9636530} tries to keep the spatial density of nodes below a certain threshold across the entire map. The proposed scale-invariant density measure determines to remove the nodes with the highest densities, marginalizing their edges with an approach similar to \cite{kretzschmar-et-al:2010:2}, until the density is lower than a threshold. The removal process keeps the most recent nodes from being removed, even though the density measure is computed considering all nodes. The method reduced the growth in the number of nodes compared to the full graph, having stabilized over time.
Similarly, \cite{ali-et-al:2021:3100882} also favors older nodes for removal, moving older nodes after a certain traveled distance from the online graph to an offline database to not lose the information. Also, upon loop detection, the older submap is substituted with a new one. Compared to \cite{discussion:orb-slam2}, the proposed method had lower computational requirements in terms of CPU and memory usage.


\paragraph{Information fusion}

The information fusion between nodes and/or with the current observation also allows the reduction of the graph growth.
Both \cite{einhorn-gross:2013:6698849} and \cite{einhorn-gross:2015:008} fuse NDT map fragments that cover a similar region of the environment, only fusing nodes whose relative pose is known with low uncertainty. The marginalization of the affected edges is performed using the same approach as \cite{kretzschmar-et-al:2010:2}. In the experiments, the number of vertices in both works did not increase significantly on revisiting locations.
Assuming that a loop closure occurs due to spatial closeness having overlap between the affected nodes, Lazaro merges loop closure-related nodes (including intra- and inter-sessions) following a similar methodology to the ray tracing one used for detecting dynamic changes in the environment. The oldest point cloud is used as a reference for refinement with the newest one based on their timestamps. Edge marginalization is based on condensed measurements where the remaining node is connected to the neighbors using a star-like topology. The method achieved a 50\% node reduction in an experiment while retaining the localization accuracy.
\cite{karaoguz-bozma:2020:2} uses a Topological Spatial Cognition (TSC) model to organize the visual place memory as a collection of appearances and respective descriptors for each robot, with a hierarchical organization to cluster places with a similar appearance. Based on the similarity of the descriptors, \cite{karaoguz-bozma:2020:2} merges place memories of TSC models on a multi-robot system, incorporating all places known by other robots but not known to itself. The merge is performed based on the nature of overlap of the descriptor hyperspheres in appearance space. Although the method was able to merge the TSC models between two robots leading to 18 final locations, the merged locations are more than the 15 predicted ones due to limited field of view and appearance changes.



\subsubsection{Keyframe SLAM}

A specific formulation of the pose graph is the keyframe SLAM, where the keyframes are selected from the frames usually captured by a camera, the 3D map points are triangulated considering features extracted from the camera images, and the edges determine the keyframes that observe the map points. Odometry and shared observations of map points induce additional edges between keyframes~\parencite{schmuck-chli:2019:00071}.
The keyframe SLAM also suffers the same ever-growing graph problem as the standard pose graph formulation. Thus, long-term localization and mapping methods using keyframe SLAM should employ policies to manage the growth of the number of keyframes and map points, or restricting the selection of the frames from the views captures from a sensor to ensure the graph sparsity.


\paragraph{Keyframe graph management}

Clustering techniques are employed in the included works to identify keyframes for removal.
\cite{konolige-bowman:2009:5354121} proposes the Least-Recently Used (LRU) algorithm to limit the keyframes in a local neighborhood, while preserving their diversity and removing preferably older unmatched views. LRU clusters keyframes  based on its feature matching closeness, assuming that keyframes capturing similar environment appearances will be in the same cluster. If the number of local keyframes exceeds a threshold, older ones are removed from each cluster. Then, having clusters only with a single view, LRU removes the oldest exemplars. LRU reduced significantly the number of views and edges relative to having no management rule for keyframes while preserving the mean number of clusters in local neighborhoods, i.e., preserving the diversity.
Instead of evaluating single keyframes, \cite{bouaziz-et-al:2022:4} exploits the similarity between traversals in the keyframe map that represent runs possibly in different environment conditions. Hierarchical clustering on a proposed similarity matrix between traversals identifies which one to remove when their number exceeds a predefined threshold, trying to maintain the map diversity as possible as in \cite{konolige-bowman:2009:5354121}. The limitation on the number of traversals bound the computational requirements of the method.

Moreover, \cite{gadd-newman:2016:7759843} implements a merge process in its centralized versioning framework to measure the relevance of discovered segments by every single agent using stereo matching from visual odometry. The merging strategy with multiple agents built the map in an experiment in 3.6h, while a single agent would require 12.4h, while the size of the merged database was smaller than a single agent due to the redundancy check.
Similar to methods for sparsifying the standard pose graph, \cite{ding-et-al:2019:8968550} uses the KLD measure to determine which keyframe to remove based on their contribution. When adding a new one, the method updates the KLD of each keyframe that has common features to the new frame. The keyframes with KLD lower than a threshold are removed using GLC for edge marginalization if needed. The proposed sparsification approach reduced the map size for transmitting it between an external agent and the robot.

Although the keyframe management removes map points indirectly, e.g., when the points are not well constrained with less than 2 observations~\parencite{schmuck-chli:2019:00071}, management techniques on the keyframe graph can also remove points directly from the feature map.
LLamaSLAM~\parencite{luthardt-et-al:2018:8569323} considers only high-quality LLama points for adding to the map, while ensuring their spatial coverage in a 2D grid selecting only the best 10 points within each cell. The keyframe selection is based on the overall quality threshold of the observed LLama points in the frame.
Furthermore, \cite{mur-artal-et-al:2015:2463671} implements in ORB-SLAM both keyframe and feature addition and removal rules. ORB-SLAM only adds a keyframe if the current view tracks at least 50 points in the sliding window and less than 90\% of the points of the current reference keyframe, while discards all keyframes whose 90\% points are seen in at least more than 3 other keyframes. As for map points, the points are only retained if the tracking finds them in more than 25\% of the frames which are predicted to be observed in and must be observed from at least 3 keyframes.
\cite{zhang-et-al:2018:1729881418780178} implements the same map management method as ORB-SLAM in a multi-robot system to reduce redundant data, where each robot executes independently monocular SLAM and communications their map with other robots.
Instead of using thresholds, \cite{schmuck-chli:2019:00071} modifies ORB-SLAM with a redundancy score to classify the map points. The method defines a maximum score of 1 for features seen in more than 5 keyframes (also removing these points from the map) and a score of 0 for the minimum of 2 observations, while scoring the keyframes by the normalized sum of their features' scores. Considering a maximum limit of keyframes, the algorithm was able to compress up to 50\% relative to no management while also outperforming the original ORB-SLAM in RMSE.

The selection process of features for removal in the keyframe graph can use more than a single scoring function.
\cite{dymczyk-et-al:2015:7139575} scores the map points on the number of observations (considering a lower bound for removal, while restraining the deletion in rarely visited areas and not well-constrained points), the descriptors variance (rejecting the ones with high variances), and the number of observations between different sessions. The method also sets a minimum number of keypoints to retain a keyframe and a total limit on the number of map points. These policies led to an approximately constant number of keyframes and map points in posterior sessions in the same environment.


\paragraph{Representative keyframes}

Another approach to reduce the growth of keyframe graph found in the included works is to restrict the selection of the keyframes from the sensor live data.
\cite{pirker-et-al:2011:6048253} only adds keyframes to the graph if at least 55\% of the image area is covered by keypoints used for tracking, in contrast with the standard rule of adding frames accordingly to a certain motion relative to the previous keyframe. Also, the method removes points based on their visibility accordingly to the Histogram of Oriented Cameras (HOC) descriptor that represents a map point. The method was able to keep the map size proportional to explored space stabilizing the number of features.

Topic-probabilistic models used in both \cite{paul-newman:2013:0278364913509859} and \cite{murphy-sibley:2014:6907022} also try to identify the most representative keyframes from camera images.
\cite{paul-newman:2013:0278364913509859} applies Latent Dirichlet Allocation (LDA) as a probabilistic topic model to identify perplexing observations and for retrieving images similar in thematic content, providing a compact representation of the sampling set to improve the timing efficiency of FAB-MAP. Topic models provide a low-dimensional representation of BoW, capturing their thematic content via word-occurrences. Although the method reduced the number of generated keyframes, Paul did not implemented a deletion or forgetting rule for the map frames.
As for \cite{murphy-sibley:2014:6907022}, their approach implements probabilistic Latent Semantic Indexing (pLSI) as the topic modeling engine in an incremental manner to expand the vocabulary and perform topic updating online. The topic clustering using DBSCAN identifies temporally smoothed unique places. In the experiments, the proposed method retained up to 1.06\% of the image stream while having similar precision and recall metrics relative to considering all frames, respectively.



\subsubsection{Features management}

In addition to feature management policies applied in keyframe SLAM and discussed previously, the included works also propose techniques to sparsify feature maps for improving the mapping scalability in the long-term time frame.
\cite{hochdorfer-schlegel:2009} and \cite{hochdorfer-et-al:2009:5339626} focus on ensuring the spatial distribution of the features in the environment, removing the ones that cover nearly the same region.
The first clusters SURF-based features based on the $l_1$ distance of their 2D position in the map using k-means with the number of clusters as 25\% of known features in the map. Considering feature information content as dependent on the covariance matrix, the proposed approach selects the cluster with the maximum difference in information content and removes the feature with the lowest localization benefit while limiting the number of map features to 50. This limit constrains the computation requirements of the method. The second work uses DBSCAN instead of k-means because the former is a density-based algorithm while the latter is a partitioning one. Even though both \cite{hochdorfer-schlegel:2009} and \cite{hochdorfer-et-al:2009:5339626} limit the number of map features and improve their spatial coverage, the DBSCAN-based method led to a better spatial distribution of features compared to k-means.

The evaluation on matching observations with the features in the map versus the number of attempts could indicate unstable or dynamic features that could be removed from the map and reducing its size.
\cite{davison-murray:2002:1017615} deletes features after a predetermined number of matching fails when they should be visible and sets a maximum of 2 features in the map, the minimum for the robot to localize itself. Even though the limit was to improve the computational efficiency with the resources available at the time, the map should have consider more features to improve localization robustness.
Similar to \cite{davison-murray:2002:1017615}, the STM/LTM memory scheme implemented by \cite{dayoub-et-al:2011:013} and \cite{bacca-et-al:2013:003} imposes a consecutive observation of the features for retaining them in the map. \cite{bacca-et-al:2013:003} showed in its experimental results that removing useless and old features avoided the ever-increasing number of features, leading to an approximately constant map size over different runs.

Furthermore, the use of multiple predictors for evaluating feature stability on changing conditions also helps the sparsification of the feature map.
The removal of features with low scores on the predictor proposed by \cite{berrio-et-al:2019:8814289} had similar localization covariance in the experiments while removing up to 70\% of the least valuable features in the map.
\cite{berrio-et-al:2021:3094485} also evaluates the concentration ratio and maximum driven length predictors used in their work. Features in high concentration areas and low visibility in terms of the maximum driven length while observing them are discarded from the map. The removal method contributed to the map size being approximately constant in later runs throughout a 24 weeks experiment.
Similar to \cite{berrio-et-al:2019:8814289}, \cite{dymczyk-et-al:2016:7759673} formulates a regression for optimizing the weights given to the predictors and combining all predictors into a single score. The method considers as predictors the number of frames the feature is re-observed, traveled distance while observing the landmark and the one between the two most distant keyframes while tracking, maximum angle between observation rays, the mean reprojection error, a gravity constant to favor anchored objects presumable more useful for localization, the vertical coordinate, and the descriptor appearance classification. The results showed a 80\% reduction on data transfers with similar localization recall when selecting a subset of the map features compared to retaining all features.
Also, \cite{m√ºhlfellner-et-al:2016:21595} creates a Summary Map from a map gathered over multiple traversals in the environment by selecting a limited number of features, first, based on the observations in distinct traversals, and then, on the total number of observations. The authors compared the Summary Map with 1200 features to only selecting features seen during the most recent traversals, features seen in two or more traversals, and to the works of \cite{konolige-bowman:2009:5354121} and \cite{dayoub-et-al:2011:013}. The proposed selection method, \cite{konolige-bowman:2009:5354121}, and retaining features seen in two or more traversals achieved the higher localization accuracy, while the Summary Map had higher accuracy that the other methods at the same map size.

In terms of managing a BoW dictionary, \cite{tsintotas-et-al:2021:103782} presents an incremental Bow model to remove multiple codewords of repetitive patterns representing the same environmental elements at different time instants. A spatial check identifies the redundant words upon loop closure, where the words are ignored if not associated with the chosen loop image and are merged with the ones in the database accordingly to the median of the descriptors. The incremental approach reduced the model size compared to other BoW-based approaches while also improving the timing efficiency due to having fewer visual words for searching for loop closures.
Instead of managing the addition and removal of words from the BoW model, \cite{opdenbosch-et-al:2018:00114} proposes a culling map point algorithm by minimizing the points coding cost to keep map points that exhibit good compression properties and favor the ones with many visually similar observations when assigning the features descriptors to its closest visual word from a pre-trained BoW model. The method allows the definition of an apriori size of the desired BoW model to constrain the computation requirements. The integration of the proposed method in ORB-SLAM2 reduced by 3 times the map size (3MB to 1MB) having a similar localization success rate, while also reducing the number of points substantially (17426 to 2370). 

Lastly, the works of \cite{schaefer-et-al:2021:103709} and \cite{wang-et-al:2021:9739599} that retrieve pole features from 3D LiDAR data for appearance invariance in changing conditions also employ feature management policies to avoid redundant points in the maps.
\cite{schaefer-et-al:2021:103709} merges ambiguous poles by projecting them onto the ground plane and evaluate their overlap. The merge process computes a weighted average over their center coordinates and widths over the mean pole score, determined by averaging over the scores of all voxels that touch the pole.
\cite{wang-et-al:2021:9739599} segments the point clouds into clusters based on the semantic labels obtained with the RangeNet++ network. For each cluster, the label is voted by the statistical number of the point labels in the cluster. Considering the clusters of the global map versus the ones found in the current laser scan, each cluster of the current scan is searched by the closest neighbor and is only added into the map if the cluster is not found in the global map.





\subsection{Multi-session}
\label{sec:discussion:multisession}

\subsection{Computational}
\label{sec:discussion:computational}

taisho-kanji:2016:7866383
Principal Component Analysis (PCA) can be used for dimensionality reduction (4096 to 128-dimensional features), defining the proposed descriptor PCA-NBNN.

xin-et-al:2017:8310121
Studies the possibility of using a random selection technique for feature dimension reduction, given that requires no need for further training nor significant loss in efficiency and effectiveness compared to Local Sensitive Hushing (LSH) and Principal Component Analysis (PCA).
Cosine distance for evaluate query image compared to database (global descriptor).

yu-et-al:2019:8961714
The 4 max-pooling by channel proposed to reduce the descriptors dimensions with minimal accuracy reduction. 1024-dimension divided into 256 groups and maximum of ech group used as final descriptor. Compared to PCA, 4 max-pooling by channel has less computational complexity but similar performance.

camara-et-al:2020:9196967
PCA to reduce each vector of the 16 cubes to 100 dimensions.

piasco-et-al:2021:6
Reduce dimension of the descriptors by applying PCA and whitening.

yang-et-al:2021:12054
select max-pooling layer instead of convulutional one

naseer-et-al:2017:7989305
Uses Sparse Random Projections for embedding high-dimensional feature vectors into lower dimensions. The precision-recall metrics is very similar to the full dimensional descriptor.

\subsection{Long-term experimental data}
\label{sec:discussion:experiments}

\input{tables/datasets}

\subsection{Evaluation metrics}
\label{sec:discussion:metrics}

\subsection{Final observations}
\label{sec:discussion:observations}

